[
  {
    "objectID": "Continuous-Random-Variables.html",
    "href": "Continuous-Random-Variables.html",
    "title": "11  Continuous Random Variables",
    "section": "",
    "text": "11.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "Continuous-Random-Variables.html#objectives",
    "href": "Continuous-Random-Variables.html#objectives",
    "title": "11  Continuous Random Variables",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables, and construct examples to demonstrate their proper use in context.\nFor a given continuous random variable, derive and interpret the probability density function (pdf) and apply this function to calculate the probabilities of various events.\nCalculate and interpret the moments, such as the expected value/mean and variance, of a continuous random variable.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "Continuous-Random-Variables.html#continuous-random-variables",
    "href": "Continuous-Random-Variables.html#continuous-random-variables",
    "title": "11  Continuous Random Variables",
    "section": "11.2 Continuous random variables",
    "text": "11.2 Continuous random variables\nIn the last chapter, we introduced random variables, and explored examples and behavior of discrete random variables. In this chapter, we will shift our focus to continuous random variables, their properties, their distribution functions, and how they differ from discrete random variables.\nRecall that a continuous random variable has a domain that is a continuous interval (or possibly a group of intervals). For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. While our measurement will necessitate “discretizing” height to some degree, technically, height is a continuous random variable since a person could measure 67.3 inches or 67.4 inches or anything in between.\n\n11.2.1 Continuous distribution functions\nSo how do we describe the randomness of continuous random variables? In the case of discrete random variables, the probability mass function (pmf) and the cumulative distribution function (cdf) are used to describe randomness. However, recall that the pmf is a function that returns the probability that the random variable takes the inputted value. Due to the nature of continuous random variables, the probability that a continuous random variable takes on any one individual value is technically 0. Thus, a pmf cannot apply to a continuous random variable.\nRather, we describe the randomness of continuous random variables with the probability density function (pdf) and the cumulative distribution function (cdf). Note that the cdf has the same interpretation and application as in the discrete case.\n\n\n11.2.2 Probability density function\nLet \\(X\\) be a continuous random variable. The probability density function (pdf) of \\(X\\), given by \\(f_X(x)\\) is a function that describes the behavior of \\(X\\). It is important to note that in the continuous case, \\(f_X(x)\\neq \\mbox{P}(X=x)\\), because the probability of \\(X\\) taking any one individual value is 0.\nThe pdf is a function. The input of a pdf is any real number. The output is known as the density. The pdf has three main properties:\n\n\\(f_X(x)\\geq 0\\)\n\\(\\int_{S_X} f_X(x)\\mbox{d}x = 1\\)\n\\(\\mbox{P}(X\\in A)=\\int_{x\\in A} f_X(x)\\mbox{d}x\\), or another way to write this \\(\\mbox{P}(a \\leq X \\leq b)=\\int_{a}^{b} f_X(x)\\mbox{d}x\\)\n\nProperties 2) and 3) imply that the area underneath a pdf represents probability. Property 1) says the pdf is a non-negative function, it cannot have negative values.\n\n\n11.2.3 Cumulative distribution function\nThe cumulative distribution function (cdf) of a continuous random variable has the same interpretation as it does for a discrete random variable. It is a function. The input of a cdf is any real number, and the output is the probability that the random variable takes a value less than or equal to the inputted value. It is denoted as \\(F\\) and is given by: \\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\int_{-\\infty}^x f_x(t) \\mbox{d}t\n\\]\n\nExample:\nLet \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\). Verify that \\(f\\) is a valid pdf. Find the cdf of \\(X\\). Also, find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\), \\(\\mbox{P}(X&gt;0.5)\\), and \\(\\mbox{P}(0.1\\leq X &lt; 0.75)\\). Finally, find the median of \\(X\\).\n\nTo verify that \\(f\\) is a valid pdf, we simply note that \\(f_X(x) \\geq 0\\) on the range \\(0 \\leq x \\leq 1\\). Also, we note that \\(\\int_0^1 2x \\mbox{d}x = x^2\\bigg|_0^1 = 1\\). That is, the area under the pdf curve is one. We are essentially taking a sum over the sample space of \\(X\\), but \\(X\\) is a continuous random variable so we must integration to find the area under the curve.\nUsing R, we find\n\nintegrate(function(x)2*x, 0, 1)\n\n1 with absolute error &lt; 1.1e-14\n\n\nWe can also use the mosaicCalc package to find the anti-derivative, but we leave that method to the interested learner. If the package is not installed, you can use the Packages tab in RStudio or type install.packages(\"mosaicCalc\") at the command prompt. Then, load the library.\nGraphically, the pdf is displayed in Figure 11.1:\n\n\n\n\n\n\n\n\nFigure 11.1: pdf of \\(X\\)\n\n\n\n\n\nProbabilities are found by integrating the pdf. The interested learner can also derive and use the cdf, but we continue to de-emphasize the cdf in this book.\nWe first want to find \\(P(X &lt; 0.5)\\). See Figure 11.2 for an illustration of the area under the curve, the probability of interest.\n\n\n\n\n\n\n\n\nFigure 11.2: Probability represented by shaded area\n\n\n\n\n\nOne way to find the probability is to integrate \\(f_X(x)\\) from 0 to 0.5.\n\\[\\mbox{P}(X &lt; 0.5)=\\mbox{P}(X\\leq 0.5) = \\int_{0}^{0.5}2x = x^2\\big|_{0}^{0.5} = 0.5^2 = 0.25\\]\nRather than performing this integral by hand, we can also use the integrate() function in R.\n\nintegrate(function(x) 2*x, lower = 0, upper = 0.5)\n\n0.25 with absolute error &lt; 2.8e-15\n\n\nNext, let’s find \\(P(X &gt; 0.5)\\). See Figure 11.3 for an illustration of the region of interest. We can find this value using some algebra skills or using integrate().\n\n\n\n\n\n\n\n\nFigure 11.3: Probability represented by shaded area\n\n\n\n\n\nWe can find the value mathematically: \\(\\mbox{P}(X &gt; 0.5) = 1-\\mbox{P}(X\\leq 0.5)=1-0.25 = 0.75\\)\nOr by using R:\n\nintegrate(function(x) 2*x, lower = 0.5, upper = 1)\n\n0.75 with absolute error &lt; 8.3e-15\n\n\nFinally, let’s find \\(\\mbox{P}(0.1\\leq X &lt; 0.75)\\). See Figure 11.4 for an illustration of this region under the curve.\n\n\n\n\n\n\n\n\nFigure 11.4: Probability represented by shaded area\n\n\n\n\n\nAgain, we can find the value mathematically: \\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\int_{0.1}^{0.75}2x dx = 0.75^2 - 0.1^2 = 0.5525\\)\nOr by using R:\n\nintegrate(function(x) 2*x, lower = 0.1, upper = 0.75)\n\n0.5525 with absolute error &lt; 6.1e-15\n\n\n\n\n\n\n\n\nFollow the signs\n\n\n\nNotice for a continuous random variable, we are loose with the use of the = sign. This is because for a continuous random variable \\(\\mbox{P}(X=x)=0\\). Do not get sloppy when working with discrete random variables.\n\n\nThe median of \\(X\\) is the value \\(x\\) such that \\(\\mbox{P}(X\\leq x)=0.5\\). One way to find this is to set the cdf equal to the quantile of interest, 0.5.1 But we don’t need the cdf if we perform a simulation and then calculate the median; we’ll do this next.\n\n\n11.2.4 Simulation\nLike with discrete random variables, we can get an estimate of the pdf via simulation. However, simulating continuous random variables take a little more work.\nLet’s start by generating random samples from \\(X\\). We can do this using a method called rejection sampling. Rejection sampling involves generating candidate points from a proposal distribution (usually a uniform distribution, with values between 0 and 1) and accepting them with a certain probability that ensures the desired distribution. To determine the probability of rejection, we essentially check whether a second random number \\(u\\) is less than or equal to a specific function value. For example, if we want values where \\(y = 2x\\), we’ll check whether \\(u\\leq 2x\\).\nOur candidate points will be generated from a uniform distribution, with values between 0 and 1. Then, we will generate a second set of values from a uniform distribution, with values between 0 and the maximum value of the pdf (in this case, 2). To understand why we do things in this way, imagine a rectangular box whose width is the possible \\(x\\) values \\((0\\leq x\\leq 1)\\) and whose height is the possible values of the pdf \\(f_X(x)\\) (ranging from zero to two). We then sample from uniform distributions over those spans. For the \\(x\\) value, we calculate the height of the pdf and compare with the sampled \\(y\\) value to determine acceptance or rejection.\nAn alternative method for this simulation uses the inverse transform method, which requires deriving the cdf.2 Because we have de-emphasized the cdf in this book, we’ll focus on the rejection sampling method.\n\nset.seed(809)\n\nnum_sim &lt;- 10000\n\n# select a candidate value between 0 and 1 for x\n# then, select another value from 0 to the max of the pdf\n#   This is our acceptance value\n# generate more points than needed (10000*2) to ensure \n#   enough accepted samples\ncandidates &lt;- runif(num_sim*2)\nu &lt;- runif(num_sim*2, max = 2)\n\n# create a tibble for candidates and acceptance probabilities\ndf &lt;- tibble(candidate = candidates, u = u)\n\n# filter out accepted samples based on the acceptance criterion\naccepted_samples &lt;- df %&gt;% \n  filter(u &lt;= 2*candidate) %&gt;% \n  slice_head(n = num_sim) %&gt;%   # ensure exactly n samples \n  pull(candidate)         # use only the accepted candidate values\n\n# Create a tibble for the accepted samples\naccepted &lt;- tibble(X = accepted_samples)\n\nNow, we can plot the estimated pdf.\n\naccepted %&gt;% \n  gf_density(~X) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\nNow we can calculate the probabilities of interest.\n\naccepted %&gt;% \n  summarize(Prob1 = count(X &lt; 0.5) / 10000, \n            Prob2 = count(X &gt; 0.5) / 10000, \n            Prob3 = count(X &gt;= 0.1 & X &lt; 0.75) / 10000)\n\n# A tibble: 1 × 3\n  Prob1 Prob2 Prob3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.249 0.745 0.542\n\n\nThe first probability is \\(P(X &lt; 0.5)\\), the second is \\(P(X &gt; 0.5)\\), and the third is \\(P(0.1\\leq X &lt; 0.75)\\). All three simulated probabilities are very close to those found mathematically or with integration in R previously.\nThe median of \\(X\\) is easily found by calculating the median of our simulated values.\n\nmedian(~X, data = accepted)\n\n[1] 0.7102987\n\n\nThis is also very close to the value of 0.707 that can be found mathematically.3",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "Continuous-Random-Variables.html#moments",
    "href": "Continuous-Random-Variables.html#moments",
    "title": "11  Continuous Random Variables",
    "section": "11.3 Moments",
    "text": "11.3 Moments\nAs with discrete random variables, moments can be calculated to summarize characteristics such as center and spread. In the discrete case, expectation is found by multiplying each possible value by its associated probability and summing across the sample space of \\(X\\) (\\(\\mbox{E}(X)=\\sum_x x\\cdot f_X(x)\\)). In the continuous case, the sample space of \\(X\\) consists of an infinite set of values. From your calculus days, recall that the sum across an infinite domain is represented by an integral.\nLet \\(g(X)\\) be any function of \\(X\\). The expectation of \\(g(X)\\) is found by: \\[\n\\mbox{E}(g(X)) = \\int_{S_X} g(x)f_X(x)\\mbox{d}x\n\\]\n\n11.3.1 Mean and variance\nLet \\(X\\) be a continuous random variable. The mean of \\(X\\), or \\(\\mu_X\\), is simply \\(\\mbox{E}(X)\\). Thus, \\[\n\\mbox{E}(X)=\\int_{S_X}x\\cdot f_X(x)\\mbox{d}x\n\\]\nAs in the discrete case, the variance of \\(X\\) is the expected squared difference from the mean, or \\(\\mbox{E}[(X-\\mu_X)^2]\\). Thus, \\[\n\\sigma^2_X = \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]= \\int_{S_X} (x-\\mu_X)^2\\cdot f_X(x) \\mbox{d}x\n\\]\nRecall from the last chapter that \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\). Thus, \\[\n\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_{S_X} x^2\\cdot f_X(x)\\mbox{d}x - \\mu_X^2\n\\]\n\nExample:\nConsider the random variable \\(X\\) from above. Find the mean and variance of \\(X\\). \\[\n\\mu_X= \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mbox{d}x = \\frac{2x^3}{3}\\bigg|_0^1 = \\frac{2}{3}- 0 = 0.667\n\\]\n\nSide note: Because the mean of \\(X\\) is smaller than the median of \\(X\\), we say that \\(X\\) is skewed to the left, or negatively skewed.\nUsing R, we can use integrate():\n\nintegrate(function(x)x*2*x, 0, 1)\n\n0.6666667 with absolute error &lt; 7.4e-15\n\n\nUsing our simulation.\n\nmean(~X, data = accepted)\n\n[1] 0.6678136\n\n\n\\[\n\\sigma^2_X = \\mbox{Var}(X)= \\mbox{E}(X^2)-\\mbox{E}(X)^2\n\\] \\[= \\int_0^1 x^2\\cdot 2x\\mbox{d}x - \\left(\\frac{2}{3}\\right)^2 = \\frac{2x^4}{4}\\bigg|_0^1-\\frac{4}{9}=\\frac{1}{2}-\\frac{4}{9}=\\frac{1}{18}=0.056\n\\]\n\nintegrate(function(x)x^2*2*x,0,1)$value-(2/3)^2\n\n[1] 0.05555556\n\n\nor\n\nvar(~X, data = accepted)*9999/10000\n\n[1] 0.05645031\n\n\nAnd finally, the standard deviation of \\(X\\) is \\(\\sigma_X = \\sqrt{\\sigma^2_X}=\\sqrt{1/18}=0.236\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "Continuous-Random-Variables.html#footnotes",
    "href": "Continuous-Random-Variables.html#footnotes",
    "title": "11  Continuous Random Variables",
    "section": "",
    "text": "We simply solve \\(x^2=0.5\\) for \\(x\\). Thus, the median of \\(X\\) is \\(\\sqrt{0.5}=0.707\\).\nWe can also find this in R using the uniroot() function.↩︎\nAs in the case of the discrete random variable, we can simulate a continuous random variable if we have an inverse for the cdf. The range of the cdf is \\([0,1]\\), so we generate a random number in this interval and then apply the inverse cdf to obtain a random variable. In a similar manner, for a continuous random variable, we use the following pseudo code:\n\nGenerate a random number in the interval \\([0,1]\\), \\(U\\).\nFind the random variable \\(X\\) from \\(F_{X}^{-1}(U)\\).\n\n↩︎\nThe median of \\(X\\) is the value \\(x\\) such that \\(P(X\\leq x) = 0.5\\).\n\\[\n\\int_0^t 2xdx = 0.5\n\\]\n\\[\nx^2 \\big|_0^t = 0.5\n\\]\nSo, we simply solve \\(t^2 = 0.5\\) for \\(t\\) and find the median of is \\(\\sqrt{0.5} = 0.707\\).↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "00-Objectives.html",
    "href": "00-Objectives.html",
    "title": "Objectives",
    "section": "",
    "text": "Descriptive Statistical Modeling",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#descriptive-statistical-modeling",
    "href": "00-Objectives.html#descriptive-statistical-modeling",
    "title": "Objectives",
    "section": "",
    "text": "1 - Data Case Study\n\nUse R for basic analysis and visualization.\nCompile a PDF report using knitr.\n\n\n\n2 - Data Basics\n\nDifferentiate between various statistical terminologies such as case, observational unit, variables, data frame, tidy data, numerical variable, discrete numeric, continuous numeric, categorical variable, levels, scatterplot, associated variables, and independent, and construct examples to demonstrate their proper use in context.\nWithin a given dataset, evaluate different types of variables and justify their classifications (e.g. categorical, discrete numerical, continuous numerical).\nGiven a study description, develop an appropriate research question and justify the organization of data as tidy.\nCreate and interpret scatterplots using R to analyze the relationship between two numerical variables by evaluating the strength and direction of the association.\n\n\n\n3 - Overview of Data Collection Principles\n\nDifferentiate between various statistical terminologies such as population, sample, anecdotal evidence, bias, simple random sample, systematic sample, representative sample, non-response bias, convenience sample, explanatory variable, response variable, observational study, cohort, experiment, randomized experiment, and placebo, and construct examples to demonstrate their proper use in context.\nEvaluate descriptions of research project to identify the population of interest, assess the generalizability of the study, determine the explanatory and response variables, classify the study as observational or experimental, and determine the type of sample used.\nDesign and justify sampling procedures for various research contexts, comparing the strengths and weaknesses of different sampling methods (simple random, systematic, convenience, etc.) and proposing improvements to minimize bias and enhance representativeness.\n\n\n\n4 - Studies\n\nDifferentiate between various statistical terminologies such as observational study, confounding variable, prospective study, retrospective study, simple random sampling, stratified sampling, strata, cluster sampling, multistage sampling, experiment, randomized experiment, control, replicate, blocking, treatment group, control group, blinded study, placebo, placebo effect, and double-blind, and construct examples to demonstrate their proper use in context.\nEvaluate study descriptions using appropriate terminology, and analyze the study design for potential biases or confounding variables.\nGiven a scenario, identify and assess flaws in reasoning, and propose robust study and sampling methodologies to address these flaws.\n\n\n\n5 - Numerical Data\n\nDifferentiate between various statistical terminologies such as scatterplot, mean, distribution, point estimate, weighted mean, histogram, data density, right skewed, left skewed, symmetric, mode, unimodal, bimodal, multimodal, variance, standard deviation, density plot, box plot, median, interquartile range, first quartile, third quartile, whiskers, outlier, robust estimate, and transformation, and construct examples to demonstrate their proper use in context.\nUsing R, generate and interpret summary statistics for numerical variables.\nCreate and evaluate graphical summaries of numerical variables using R, choosing the most appropriate types of plots for different data characteristics and research questions.\nSynthesize numerical and graphical summaries to provide interpretations and explanations of a data set.\n\n\n\n6 - Categorical Data\n\nDifferentiate between various statistical terminologies such as factor, contingency table, marginal counts, joint counts, frequency table, relative frequency table, bar plot, conditioning, segmented bar plot, mosaic plot, pie chart, side-by-side box plot, and density plot, and construct examples to demonstrate their proper use in context.\nUsing R, generate and interpret tables for categorical variables.\nUsing R, generate and interpret summary statistics for numerical variables by groups.\nCreate and evaluate graphical summaries of both categorical and numerical variables using R, selecting the most appropriate visualization techniques for different types of data and research questions.\nSynthesize numerical and graphical summaries to provide interpretations and explanations of a data set.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#probability-modeling",
    "href": "00-Objectives.html#probability-modeling",
    "title": "Objectives",
    "section": "Probability Modeling",
    "text": "Probability Modeling\n\n7 - Probability Case Study\n\nUse R to simulate a probabilistic model.\nGain an introduction to probabilistic thinking through computational, mathematical, and data science approaches.\n\n\n\n8 - Probability Rules\n\nDifferentiate between various statistical terminologies such as sample space, outcome, event, subset, intersection, union, complement, probability, mutually exclusive, exhaustive, independent, multiplication rule, permutation, and combination, and construct examples to demonstrate their proper use in context.\nApply basic probability properties and counting rules to calculate the probabilities of events in different scenarios. Interpret the calculated probabilities in context.\nExplain and illustrate the basic axioms of probability.\nUse R to perform calculations and simulations for determining the probabilities of events.\n\n\n\n9 - Conditional Probability\n\nDefine and differentiate between conditional probability and joint probability, and provide real-world examples to illustrate these concepts and their differences.\nCalculate conditional probabilities from given data or scenarios using their formal definition, and interpret these probabilities in the context of practical examples.\nUsing conditional probability, determine whether two events are independent and justify your conclusion with appropriate calculations and reasoning.\nApply Bayes’ Rule to solve problems both mathematically and through simulation usinng R.\n\n\n\n10 - Random Variables\n\nDifferentiate between various statistical terminologies such as random variable, discrete random variable, continuous random variable, sample space/support, probability mass function, cumulative distribution function, moment, expectation, mean, and variance, and construct examples to demonstrate their proper use in context.\nFor a given discrete random variable, derive and interpret the probability mass function (pmf) and apply this function to calculate the probabilities of various events.\nSimulate random variables for a discrete distribution using R.\nCalculate and interpret the moments, such as expected value/mean and variance, of a discrete random variable.\nCalculate and interpret the expected value/mean and variance of a linear transformation of a random variable.\n\n\n\n11 - Continuous Random Variables\n\nDifferentiate between various statistical terminologies such as probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables, and construct examples to demonstrate their proper use in context.\nFor a given continuous random variable, derive and interpret the probability density function (pdf) and apply this function to calculate the probabilities of various events.\nCalculate and interpret the moments, such as the expected value/mean and variance, of a continuous random variable.\n\n\n\n12 - Named Discrete Distributions\n\nDifferentiate between common discrete distributions (Uniform, Binomial, Poisson) by identifying their parameters, assumptions, and moments. Evaluate scenarios to determine the most appropriate distribution to model various types of data.\nApply R to calculate probabilities and quantiles, and simulate random variables for common discrete distributions.\n\n\n\n13 - Named Continuous Distributions\n\nDifferentiate between common continuous distributions (Uniform, Exponential, Normal) by identifying their parameters, assumptions, and moments. Evaluate scenarios to determine the most appropriate distribution to model various types of data.\nApply R to calculate probabilities and quantiles, and simulate random variables for common continuous distributions.\nState and apply the empirical rule (68-95-99.7 rule).\nExplain the relationship between the Poisson process and the Poisson and Exponential distributions, and describe how these distributions model different aspects of the same process.\nApply the memory-less property in context of the Exponential distribution and use it to simplify probability calculations.\n\n\n\n14 - Multivariate Distributions\n\nDifferentiate between joint probability mass/density functions (pmf/pdf), marginal pmfs/pdfs, and conditional pmfs/pdfs, and provide real-world examples to illustrate these concepts and their differences.\nFor a given joint pmf/pdf, derive the marginal and conditional pmfs/pdfs through summation or integration or using R.\nApply joint, marginal, and conditional pmfs/pdfs to calculate probabilities of events involving multiple random variables.\n\n\n\n15 - Multivariate Expectation\n\nGiven a joint pmf/pdf, calculate and interpret the expected values/means and variances of random variables and functions of random variables.\nDifferentiate between covariance and correlation, and given a joint pmf/pdf, calculate and interpret the covariance and correlation between two random variables.\nGiven a joint pmf/pdf, determine whether random variables are independent of one another and justify your conclusion with appropriate calculations and reasoning.\nCalculate and interpret conditional expectations for given joint pmfs/pdfs.\n\n\n\n16 - Transformations\n\nDetermine the distribution of a transformed discrete random variable using appropriate methods, and use it to calculate probabilities.\nDetermine the distribution of a transformed continuous random variable using appropriate methods, and use it to calculate probabilities.\nDetermine the distribution of a transformation of multivariate random variables using simulation, and use it to calculate probabilities.\n\n\n\n17 - Estimation Methods\n\nApply the method of moments to estimate parameters or sets of parameters from given data.\nDerive the likelihood function for a given random sample from a distribution.\nDerive a maximum likelihood estimate of a parameter or set of parameters.\nCalculate and interpret the bias of an estimator by analyzing its expected value relative to the true parameter.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#inferential-statistical-modeling",
    "href": "00-Objectives.html#inferential-statistical-modeling",
    "title": "Objectives",
    "section": "Inferential Statistical Modeling",
    "text": "Inferential Statistical Modeling\n\n18 - Inferential Thinking Case Study\n\nUsing bootstrap methods, obtain and interpret a confidence interval for an unknown parameter, based on a random sample.\nConduct a hypothesis test using a randomization test, to include all 4 steps.\n\n\n\n19 - Sampling Distributions\n\nDifferentiate between various statistical terminologies such as point estimate, parameter, sampling error, bias, sampling distribution, and standard error, and construct examples to demonstrate their proper use in context.\nConstruct a sampling distribution for various statistics, including the sample mean, using R.\nUsing a sampling distribution, make decisions about the population. In other words, understand the effect of sampling variation on our estimates.\n\n\n\n20 - Bootstrap\n\nDifferentiate between various statistical terminologies such as sampling distribution, bootstrapping, bootstrap distribution, resample, sampling with replacement, and standard error, and construct examples to demonstrate their proper use in context.\nApply the bootstrap technique to estimate the standard error of a sample statistic.\nUtilize bootstrap methods to construct and interpret confidence intervals for unknown parameters from random samples.\nAnalyze and discuss the advantages, disadvantages, and underlying assumptions of bootstrapping for constructing confidence intervals.\n\n\n\n21 - Hypothesis Testing with Simulation\n\nDifferentiate between various statistical terminologies such as null hypothesis, alternative hypothesis, test statistic, p-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type I error, type II error, false positive, false negative, null distribution, and sampling distribution, and construct examples to demonstrate their proper use in context.\nApply and evaluate all four steps of a hypothesis test using randomization methods: formulating hypotheses, calculating a test statistic, determining the p-value through randomization, and making a decision based on the test outcome.\nAnalyze and discuss the concepts of decision errors (type I and type II errors), the differences between one-sided and two-sided tests, and the impact of choosing a significance level. Evaluate how these factors influence the conclusions and reliability of hypothesis tests and their practical implications in statistical decision-making.\nAnalyze how confidence intervals and hypothesis testing complement each other in making statistical inferences.\n\n\n\n22 - Hypothesis Testing with Known Distributions\n\nDifferentiate between various statistical terminologies such as permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, p-value, and power, and construct examples to demonstrate their proper use in context.\nApply and evaluate all four steps of a hypothesis test using probability models: formulating hypotheses, calculating a test statistic, determining the p-value through randomization, and making a decision based on the test outcome.\n\n\n\n23 - Hypothesis Testing with the Central Limit Theorem\n\nExplain the central limit theorem and when it can be used for inference.\nApply the CLT to conduct hypothesis tests using R and interpret the results with an understanding of the CLT’s role in justifying normal approximations.\nAnalyze the relationship between the \\(t\\)-distribution and normal distribution, explain usage contexts, and evaluate how changes in parameters impact their shape and location using visualizations.\n\n\n\n24 - Confidence Intervals\n\nApply asymptotic methods based on the normal distribution to construct and interpret confidence intervals for unknown parameters.\nAnalyze the relationships between confidence intervals, confidence level, and sample size.\nAnalyze how confidence intervals and hypothesis testing complement each other in making statistical inferences.\n\n\n\n25 - Additional Hypothesis Tests\n\nConduct and interpret a goodness of fit test using both Pearson’s chi-squared and randomization to evaluate the independence between two categorical variables. Evaluate the assumptions for Pearson’s chi-square test.\nAnalyze the relationship between the chi-squared and normal distributions, explain usage contexts, and evaluate the effects of changing degrees of freedom on the chi-squared distribution using visualizations.\nConduct and interpret a hypothesis test for equality of two means and equality of two variances using both permutation and the CLT. Evaluate the assumptions for two-sample t-tests.\n\n\n\n26 - Analysis of Variance\n\nConduct and interpret a hypothesis test for equality of two or more means using both permutation and the \\(F\\) distribution. Evaluate the assumptions of ANOVA.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#predictive-statistical-modeling",
    "href": "00-Objectives.html#predictive-statistical-modeling",
    "title": "Objectives",
    "section": "Predictive Statistical Modeling",
    "text": "Predictive Statistical Modeling\n\n27 - Linear Regression Case Study\n\nUsing R, generate a linear regression model and use it to produce a prediction model.\nUsing plots, check the assumptions of a linear regression model.\n\n\n\n28 - Linear Regression Basics\n\nDifferentiate between various statistical terminologies such as response, predictor, linear regression, simple linear regression, coefficients, residual, and extrapolation, and construct examples to demonstrate their proper use in context.\nEstimate the parameters of a simple linear regression model using a given sample of data.\nInterpret the coefficients of a simple linear regression model.\nCreate and evaluate scatterplots with regression lines.\nIdentify and assess the assumptions underlying linear regression models.\n\n\n\n29 - Linear Regression Inference\n\nApply statistical inference methods for \\(\\beta_0\\) and \\(\\beta_1\\), and evaluate the implications for the predictor-response relationship.\nWrite the estimated simple linear regression model and calculate and interpret the predicted response for a given value of the predictor.\nConstruct and interpret confidence and prediction intervals for the response variable.\n\n\n\n30 - Linear Regression Diagnostics\n\nCalculate and interpret the R-squared and F-statistic for a linear regression model. Evaluate these metrics to assess the model’s goodness-of-fit and overall significance.\nUse R to evaluate the assumptions underlying a linear regression model.\nIdentify, analyze, and explain the impact of outliers and leverage points in a linear regression model.\n\n\n\n31 - Simulated-Based Linear Regression\n\nApply the bootstrap to generate and interpret confidence intervals and estimates of standard error for parameter estimates in a linear regression model.\nApply the bootstrap to generate and interpret confidence intervals for predicted values from a linear regression model.\nGenerate bootstrap samples using two methods: sampling rows of the data and sampling residuals. Justify why you might prefer one method over the other.\nGenerate and interpret regression coefficients in a linear regression model with categorical explanatory variables.\n\n\n\n32 - Multiple Linear Regression\n\nGenerate and interpret the coefficients of a multiple linear regression model. Asses the assumptions underlying multiple linear regression models.\nWrite the estimates multiple linear regression model and calculate and interpret the predicted response for given values of the predictors.\nGenerate and interpret confidence intervals for parameter estimates in a multiple linear regression model.\nGenerate and interpret confidence and prediction intervals for predicted values in a multiple linear regression model.\nExplain the concepts of adjusted R-squared and multicollinearity in the context of multiple linear regression.\nDevelop and interpret multiple linear regression models that include higher-order terms, such as polynomial terms or interaction effects.\n\n\n\n33 - Logistic Regression\n\nApply logistic regression using R to analyze binary outcome data. Interpret the regression output, and perform model selection.\nWrite the estimated logistic regression, and calculate and interpret the predicted outputs for given values of the predictors.\nCalculate and interpret confidence intervals for parameter estimates and predicted probabilities in a logistic regression model.\nGenerate and analyze a confusion matrix to evaluate the performance of a logistic regression model.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "Data-Case-Study.html",
    "href": "Data-Case-Study.html",
    "title": "1  Data Case Study",
    "section": "",
    "text": "1.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Case-Study.html#objectives",
    "href": "Data-Case-Study.html#objectives",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Use R for basic analysis and visualization.\nCompile a PDF report using knitr.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "href": "Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "title": "1  Data Case Study",
    "section": "1.2 Introduction to descriptive statistical modeling",
    "text": "1.2 Introduction to descriptive statistical modeling\nIn this first block of material, we will focus on data types, collection methods, summaries, and visualizations. We also intend to introduce computing via the R package. Programming in R requires some focus early in this book and we will supplement with some online courses. There is relatively little mathematics in this first block.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Case-Study.html#the-data-analytic-process",
    "href": "Data-Case-Study.html#the-data-analytic-process",
    "title": "1  Data Case Study",
    "section": "1.3 The data analytic process",
    "text": "1.3 The data analytic process\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. It is helpful to put statistics in the context of a general process of investigation:\n\nIdentify a question or problem.\nCollect relevant data on the topic.\nExplore and understand the data.\nAnalyze the data.\nForm a conclusion.\nMake decisions based on the conclusion.\n\nThis is typical of an explanatory process because it starts with a research question and proceeds. However, sometimes an analysis is exploratory in nature. There is data but not necessarily a research question. The purpose of the analysis is to find interesting features in the data and sometimes generate hypotheses. In this book, we focus on the explanatory aspects of analysis.\nStatistics as a subject focuses on making stages 2-5 objective, rigorous, and efficient. That is, statistics has three primary components:\n\nHow best can we collect data?\n\nHow should it be analyzed?\n\nAnd what can we infer from the analysis?\n\nThe topics scientists investigate are as diverse as the questions they ask. However, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference. This chapter provides a glimpse into these and other themes we will encounter throughout the rest of the book.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Case-Study.html#case-study",
    "href": "Data-Case-Study.html#case-study",
    "title": "1  Data Case Study",
    "section": "1.4 Case study",
    "text": "1.4 Case study\nIn this chapter, we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke. 1 2 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer:\n\n1.4.1 Research question\n\nDoes the use of stents reduce the risk of stroke?\n\n\n\n1.4.2 Collect the relevant data\nThe researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups:\nTreatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification.\nControl group. Patients in the control group received the same medical management as the treatment group but did not receive stents.\nResearchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group.\nThis is an experiment and not an observational study. We will learn more about these ideas in this block.\nResearchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment.\n\n\n1.4.3 Import data\nWe begin our first use of R.\nIf you need to install a package, most likely it will be on CRAN, the Comprehensive R Archive Network. Before a package can be used, it must be installed on the computer (once per computer or account) and loaded into a session (once per R session). When you exit R, the package stays installed on the computer but will not be reloaded when R is started again.\nIn summary, R has packages that can be downloaded and installed from online repositories such as CRAN. When you install a package, which only needs to be done once per computer or account, in R all it is doing is placing the source code in a library folder designated during the installation of R. Packages are typically collections of functions and variables that are specific to a certain task or subject matter.\nFor example, to install the mosaic package, enter:\ninstall.packages(\"mosaic\") # fetch package from CRAN\nIn RStudio, there is a Packages tab that makes it easy to add and maintain packages.\nTo use a package in a session, we must load it. This makes it available to the current session only. When you start R again, you will have to load packages again. The command library() with the package name supplied as the argument is all that is needed. For this session, we will load tidyverse and mosaic. Note: the box below is executing the R commands, this is known as reproducible research since you can see the code and then you can run or modify as you need.\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nNext read in the data into the working environment.\n\n# This code reads the `stent_study.csv` file into the `stent_study` object.\nstent_study &lt;- read_csv(\"data/stent_study.csv\")\n\nNote on commenting code: It is good practice to comment code. Here are some of the best practices for commenting computer code:\nComments should explain why code is written the way it is, rather than explaining what the code does. This means that you should explain the intent of the code, not just the steps that it takes to achieve that intent.\nComments should be brief and to the point. There is no need to write long, rambling comments. Just write enough to explain what the code is doing and why.\nComments should be clear and concise. Use plain language that is easy to understand. Avoid jargon and technical terms that the reader may not be familiar with.\nComments should be consistent with the style of the code. If the code is written in a formal style, then the comments should also be formal. If the code is written in a more informal style, then the comments should be informal.\nComments should be up-to-date. If you make changes to the code, then you should also update the comments to reflect those changes.\nIn additional, consider the following practices in writing your code:\nUsing a consistent comment style. This will make it easier for other people to read and understand your code.\nUsing meaningful names for variables and functions. This will help to reduce the need for comments.\nUse indentation and whitespace to make your code easier to read. This will also help to reduce the need for comments.\nDocument your code. This means writing a separate document that explains the purpose of the code, how to use it, and any known limitations.\nBy following these best practices, you can write code that is easy to understand and maintain. This will make your code more reusable and will help to prevent errors.\nNow back to our code. Let’s break this code down. We are reading from a .csv file and assigning the results into an object called stent_study. The assignment arrow &lt;- means we assign what is on the right to what is on the left. The R function we use in this case is read_csv(). When using R functions, you should ask yourself:\n\nWhat do I want R to do?\nWhat information must I provide for R to do this?\n\nWe want R to read in a .csv file. We can get help on this function by typing ?read_csv or help(read_csv) at the prompt. The only required input to read_csv() is the file location. We have our data stored in a folder called “data” under the working directory. We can determine the working directory by typing getwd() at the prompt.\n\ngetwd()\n\nSimilarly, if we wish to change the working directory, we can do so by using the setwd() function:\n\nsetwd('C:/Users/Brianna.Hitt/Documents/ProbStat/Another Folder')\n\nIn R if you use the view(), you will see the data in what looks like a standard spreadsheet.\n\nView(stent_study)\n\n\n\n1.4.4 Explore data\nBefore we attempt to answer the research question, let’s look at the data. We want R to print out the first 10 rows of the data. The appropriate function is head() and it needs the data object. By default, R will output the first 6 rows. By using the n = argument, we can specify how many rows we want to view.\n\nhead(stent_study, n = 10)\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nWe also want to “inspect” the data. The function is inspect() and R needs the data object stent_study.\n\ninspect(stent_study)\n\n\ncategorical variables:  \n        name     class levels   n missing\n1      group character      2 451       0\n2  outcome30 character      2 451       0\n3 outcome365 character      2 451       0\n                                   distribution\n1 control (50.3%), trmt (49.7%)                \n2 no_event (89.8%), stroke (10.2%)             \n3 no_event (83.8%), stroke (16.2%)             \n\n\nTo keep things simple, we will only look at the outcome30 variable in this case study. We will summarize the data in a table. Later in the book, we will learn to do this using the tidy package; for now we use the mosaic package. This package makes use of the modeling formula that you will use extensively later in this book. The modeling formula is also used in Math 378.\nWe want to summarize the data by making a table. From mosaic, we use the tally() function. Before using this function, we have to understand the basic formula notation that mosaic uses. The basic format is:\ngoal(y ~ x, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x as “y tilde x” and interpret it in the equivalent forms: “y broken down by x”; “y modeled by x”; “y explained by x”; “y depends on x”; or “y accounted for by x.” For graphics, it’s reasonable to read the formula as “y vs. x”, which is exactly the convention used for coordinate axes.\nFor this exercise, we want to apply tally() to the variables group and outcome30. In this case it does not matter which we call y and x; however, it is more natural to think of outcome30 as a dependent variable.\n\ntally(outcome30 ~ group, data = stent_study, margins = TRUE)\n\n          group\noutcome30  control trmt\n  no_event     214  191\n  stroke        13   33\n  Total        227  224\n\n\nThe margins option totals the columns.\nOf the 224 patients in the treatment group, 33 had a stroke by the end of the first month. Using these two numbers, we can use R to compute the proportion of patients in the treatment group who had a stroke by the end of their first month.\n\n33 / (33 + 191)\n\n[1] 0.1473214\n\n\n\nExercise:\nWhat proportion of the control group had a stroke in the first 30 days of the study? And why is this proportion different from the proportion reported by inspect()?\n\nLet’s have R calculate proportions for us. Use ? or help() to look at the help menu for tally(). Note that one of the option arguments of the tally() function is format =. Setting this equal to proportion will output the proportions instead of the counts.\n\ntally(outcome30 ~ group, data = stent_study, format = 'proportion', margins = TRUE)\n\n          group\noutcome30     control       trmt\n  no_event 0.94273128 0.85267857\n  stroke   0.05726872 0.14732143\n  Total    1.00000000 1.00000000\n\n\nWe can compute summary statistics from the table. A summary statistic is a single number summarizing a large amount of data.3 For instance, the primary results of the study after 1 month could be described by two summary statistics: the proportion of people who had a stroke in the treatment group and the proportion of people who had a stroke in the control group.\n\nProportion who had a stroke in the treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\)\nProportion who had a stroke in the control group: \\(13/227 = 0.06 = 6\\%\\)\n\n\n\n1.4.5 Visualize the data\nIt is often important to visualize the data. The table is a type of visualization, but in this section we will introduce a graphical method called bar charts.\nWe will use the ggformula package to visualize the data. It is a wrapper to the ggplot2 package which is becoming the industry standard for generating professional graphics. However, the interface for ggplot2 can be difficult to learn and we will ease into it by using ggformula, which makes use of the formula notation introduced above. The ggformula package was loaded when we loaded mosaic.4\nTo generate a basic graphic, we need to ask ourselves what information we are trying to see, what particular type of graph is best, what corresponding R function to use, and what information that R function needs in order to build a plot. For categorical data, we want a bar chart and the R function gf_bar() needs the data object and the variable(s) of interest.\nHere is our first attempt. In Figure 1.1, we leave the y portion of our formula blank. Doing this implies that we simply want to view the number/count of outcome30 by type. We will see the two levels of outcome30 on the x-axis and counts on the y-axis.\n(ref:ggfbold) Using ggformula to create a bar chart.\n\ngf_bar(~outcome30, data = stent_study)\n\n\n\n\n\n\n\nFigure 1.1: Using ggformula to create a bar chart.\n\n\n\n\n\n\nExercise:\nExplain Figure 1.1.\n\nThis plot graphically shows us the total number of “stroke” and the total number of “no_event”. However, this is not what we want. We want to compare the 30-day outcomes for both treatment groups. So, we need to break the data into different groups based on treatment type. In the formula notation, we now update it to the form:\ngoal(y ~ x|z, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x|z as “y tilde x by z” and interpret it in the equivalent forms: “y modeled by x for each z”; “y explained by x within each z”; or “y accounted for by x within z.” For graphics, it’s reasonable to read the formula as “y vs. x for each z”. Figure Figure 1.2 shows the results.\n\ngf_bar(~outcome30|group, data = stent_study) \n\n\n\n\n\n\n\nFigure 1.2: Bar charts conditioned on the group variable.\n\n\n\n\n\n\n1.4.5.1 More advanced graphics\nAs a prelude for things to come, the above graphic needs work. The labels don’t help and there is no title. We could add color. Does it make more sense to use proportions? Here is the code and results for a better graph, see Figure Figure 1.3. Don’t worry if this seems a bit advanced, but feel free to examine each new component of this code.\n\n# This code creates a graph showing the impact of stents on stroke.\n# The `gf_props()` function creates a bar graph showing the number of events\n# for each experimental group. The `fill` argument specifies the fill color\n# for each group. The `position = 'fill'` argument specifies that the bars\n# should be filled to the top.\n\n# The `gf_labs()` function adds the title, subtitle, x-axis label, and y-axis\n# label to the graph.\n\n# The `gf_theme()` function applies a black-and-white theme to the graph.\n\nstent_study %&gt;%\ngf_props(~group, fill = ~outcome30, position = 'fill') %&gt;%\n  gf_labs(title = \"Impact of Stents of Stroke\",\n          subtitle = 'Experiment with 451 Patients',\n          x = \"Experimental Group\",\n          y = \"Number of Events\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 1.3: Better graph.\n\n\n\n\n\nNotice that we used the pipe operator, %&gt;%. This operator allows us to string functions together in a manner that makes it easier to read the code. In the above code, we are sending the data object stent_study into the function gf_props() to use as data, so we don’t need the data = argument. In math, this is a composition of functions. Instead of f(g(x)) we could use a pipe f(g(x)) = g(x) %&gt;% f().\n\n\n\n1.4.6 Conclusion\nThese two summary statistics (the proportions of people who had a stroke) are useful in looking for differences in the groups, and we are in for a surprise: an additional 9% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a real difference due to the treatment?\nThis second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 9% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance?\nThis is a preview of step 4, analyze the data, and step 5, form a conclusion, of the analysis cycle. While we haven’t yet covered statistical tools to fully address these steps, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients.\nBe careful: Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Case-Study.html#footnotes",
    "href": "Data-Case-Study.html#footnotes",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003.↩︎\nNY Times article reporting on the study: http://www.nytimes.com/2011/09/08/health/research/08stent.html↩︎\nFormally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others.↩︎\nhttps://cran.r-project.org/web/packages/ggformula/vignettes/ggformula-blog.html↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "Data-Basics.html",
    "href": "Data-Basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "Data-Basics.html#objectives",
    "href": "Data-Basics.html#objectives",
    "title": "2  Data Basics",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as case, observational unit, variables, data frame, tidy data, numerical variable, discrete numeric variable, continuous numeric variable, categorical variable, levels, scatterplot, associated variables, and independent, and construct examples to demonstrate their proper use in context.\nWithin a given dataset, evaluate different types of variables and justify their classifications (e.g. categorical, discrete numerical, continuous numerical).\nGiven a study description, develop an appropriate research question and justify the organization of data as tidy.\nCreate and interpret scatterplots using R to analyze the relationship between two numerical variables by evaluating the strength and direction of the association.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "Data-Basics.html#data-basics",
    "href": "Data-Basics.html#data-basics",
    "title": "2  Data Basics",
    "section": "2.2 Data basics",
    "text": "2.2 Data basics\nEffective presentation and description of data is a first step in most analyses. This chapter introduces one structure for organizing data, as well as some terminology that will be used throughout this book.\n\n2.2.1 Observations, variables, and data matrices\nFor reference we will be using a data set concerning 50 emails received in 2012. These observations will be referred to as the email50 data set, and they are a random sample from a larger data set. This data is in the openintro package so let’s install and then load this package.\n\ninstall.packages(\"openintro\")\nlibrary(openintro)\n\nTable 2.1 shows 4 rows of the email50 data set and we have elected to only list 5 variables for ease of observation.\nEach row in the table represents a single email or case. A case is also sometimes called a unit of observation or an observational unit. The columns represent variables, which represent characteristics for each of the cases (emails). For example, the first row represents email 1, which is not spam, contains 21,705 characters, 551 line breaks, is written in HTML format, and contains only small numbers.\n\n\n\n\nTable 2.1: First 5 rows of email data frame\n\n\n\n\n\n\nspam\nnum_char\nline_breaks\nformat\nnumber\n\n\n\n\n0\n21.705\n551\n1\nsmall\n\n\n0\n7.011\n183\n1\nbig\n\n\n1\n0.631\n28\n0\nnone\n\n\n0\n15.829\n242\n1\nsmall\n\n\n\n\n\n\n\n\nLet’s look at the first 10 rows of data from email50 using R. Remember to ask the two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want the first 10 rows so we use head() and R needs the data object and the number of rows. The data object is called email50 and is accessible once the openintro package is loaded.\n\nhead(email50, n = 10)\n\n# A tibble: 10 × 21\n   spam  to_multiple from     cc sent_email time                image attach\n   &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 0     0           1         0 1          2012-01-04 13:19:16     0      0\n 2 0     0           1         0 0          2012-02-16 20:10:06     0      0\n 3 1     0           1         4 0          2012-01-04 15:36:23     0      2\n 4 0     0           1         0 0          2012-01-04 17:49:52     0      0\n 5 0     0           1         0 0          2012-01-27 09:34:45     0      0\n 6 0     0           1         0 0          2012-01-17 17:31:57     0      0\n 7 0     0           1         0 0          2012-03-18 04:18:55     0      0\n 8 0     0           1         0 1          2012-03-31 13:58:56     0      0\n 9 0     0           1         1 1          2012-01-11 01:57:54     0      0\n10 0     0           1         0 0          2012-01-07 19:29:16     0      0\n# ℹ 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;,\n#   password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;,\n#   re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;,\n#   number &lt;fct&gt;\n\n\nIn practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and the units of measurement. Descriptions of all variables in the email50 data set are given in its documentation which can be accessed in R by using the ? command:\n?email50\n(Note that not all data sets will have associated documentation; the authors of openintro package included this documentation with the email50 data set contained in the package.)\nThe data in email50 represent a data matrix, or in R terminology a data frame or tibble 1, which is a common way to organize data. Each row of a data matrix corresponds to a unique case, and each column corresponds to a variable. This is called tidy data.2 The data frame for the stroke study introduced in the previous chapter had patients as the cases and there were three variables recorded for each patient. If we are thinking of patients as the unit of observation, then this data is tidy.\n\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nIf we think of an outcome as a unit of observation, then it is not tidy since the two outcome columns are variable values (month or year). The tidy data for this case would be:\n\n\n# A tibble: 10 × 4\n   patient_id group   time  result  \n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   \n 1          1 control month no_event\n 2          1 control year  no_event\n 3          2 trmt    month no_event\n 4          2 trmt    year  no_event\n 5          3 control month no_event\n 6          3 control year  no_event\n 7          4 trmt    month no_event\n 8          4 trmt    year  no_event\n 9          5 trmt    month no_event\n10          5 trmt    year  no_event\n\n\nThere are three interrelated rules which make a data set tidy:\n\nEach variable must have its own column.\n\nEach observation must have its own row.\n\nEach value must have its own cell.\n\nWhy ensure that your data is tidy? There are two main advantages:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. This will be more clear as we progress in our studies. Since most built-in R functions work with vectors of values, it makes transforming tidy data feel particularly natural.\n\nData frames are a convenient way to record and store data. If another individual or case is added to the data set, an additional row can be easily added. Similarly, another column can be added for a new variable.\n\nExercise:\nWe consider a publicly available data set that summarizes information about the 3,142 counties in the United States, and we create a data set called county_subset data set. This data set will include information about each county: its name, the state where it resides, its population in 2000 and 2010, per capita federal spending, poverty rate, and four additional characteristics. We create this data object in the code following this description. The parent data set is part of the usdata library and is called county_complete. The variables are summarized in the help menu built into the usdata package3. How might these data be organized in a data matrix? 4\n\nUsing R we will create our data object. First we load the library usdata.\n\nlibrary(usdata)\n\nWe only want a subset of the columns and we will use the select verb in dplyr to select and rename columns. We also create a new variable which is federal spending per capita using the mutate function.\n\ncounty_subset &lt;- county_complete %&gt;% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %&gt;%\n  mutate(fed_spend = fed_spend / pop2010)\n\nUsing R, we will display seven rows of the county_subset data frame.\n\nhead(county_subset, n = 7)\n\n            name   state pop2000 pop2010 fed_spend poverty homeownership\n1 Autauga County Alabama   43671   54571  6.068095    10.6          77.5\n2 Baldwin County Alabama  140415  182265  6.139862    12.2          76.7\n3 Barbour County Alabama   29038   27457  8.752158    25.0          68.0\n4    Bibb County Alabama   20826   22915  7.122016    12.6          82.9\n5  Blount County Alabama   51024   57322  5.130910    13.4          82.0\n6 Bullock County Alabama   11714   10914  9.973062    25.3          76.9\n7  Butler County Alabama   21399   20947  9.311835    25.0          69.0\n  multi_unit income med_income\n1        7.2  24568      53255\n2       22.6  26469      50147\n3       11.1  15875      33219\n4        6.6  19918      41770\n5        3.7  21070      45549\n6        9.9  20289      31602\n7       13.7  16916      30659\n\n\n\n\n2.2.2 Types of variables\nExamine the fed_spend, pop2010, and state variables in the county data set. Each of these variables is inherently different from the others, yet many of them share certain characteristics.\nFirst consider fed_spend. It is said to be a numerical variable (sometimes called a quantitative variable) since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical; even though area codes are made up of numerical digits, their average, sum, and difference have no clear meaning.\nThe pop2010 variable is also numerical; it is sensible to add, subtract, or take averages with those values, although it seems to be a little different than fed_spend. This variable of the population count can only be a whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). For this reason, the population variable is said to be discrete since it can only take specific numerical values. On the other hand, the federal spending variable is said to be continuous because it can take on any value in some interval. Now technically, there are no truly continuous numerical variables since all measurements are finite up to some level of accuracy or measurement precision (e.g., we typically measure federal spending in dollars and cents). However, in this book, we will treat both types of numerical variables the same, that is as continuous variables for statistical modeling. The only place this will be different in this book is in probability models, which we will see in the probability modeling block.\nThe variable state can take up to 51 values, after accounting for Washington, DC, and are summarized as: Alabama, Alaska, …, and Wyoming. Because the responses themselves are categories, state is a categorical variable (sometimes also called a qualitative variable), and the possible values are called the variable’s levels.\n\n\n\n\n\n\n\n\nFigure 2.1: Taxonomy of Variables.\n\n\n\n\n\nFinally, consider a hypothetical variable on education, which describes the highest level of education completed and takes on one of the values noHS, HS, College or Graduate_school. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable. A categorical variable with levels that do not have a natural ordering is called a nominal variable. To simplify analyses, any ordinal variables in this book will be treated as nominal categorical variables. In R, categorical variables can be treated in different ways; one of the key differences is that we can leave them as character values (character strings, or text) or as factors. A factor is essentially a categorical variable with defined levels. When R handles factors, it is only concerned about the levels of the factors. We will learn more about this as we progress.\nFigure 2.1 captures this classification of variables we have described.\n\nExercise:\nData were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical.5\n\n\nExercise:\nConsider the variables group and outcome30 from the stent study in the case study chapter. Are these numerical or categorical variables? 6\n\n\n\n2.2.3 Relationships between variables\nMany analyses are motivated by a researcher looking for a relationship between two or more variables. This is the heart of statistical modeling. A social scientist may like to answer some of the following questions:\n\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\n\nIf homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average?\n\nThese are what statisticians refer to as research questions, specific and measurable questions that guide the data collection and analysis process. To answer these questions, data must be collected, such as the county_complete data set. Examining summary statistics could provide insights for each of the two questions about counties. Graphs can be used to visually summarize data and are useful for answering such questions as well.\nScatterplots are one type of graph used to study the relationship between two numerical variables. Figure 2.2 compares the variables fed_spend and poverty. Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 1088 in the county_subset data set: Owsley County, Kentucky, which had a poverty rate of 41.5% and federal spending of $21.50 per capita. The dense cloud in the scatterplot suggests a relationship between the two variables: counties with a high poverty rate also tend to have slightly more federal spending. We might brainstorm as to why this relationship exists and investigate each idea to determine which is the most reasonable explanation.\n\n\n\n\n\n\n\n\nFigure 2.2: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted.\n\n\n\n\n\n\nExercise:\nExamine the variables in the email50 data set. Create two research questions about the relationships between these variables that are of interest to you.7\n\nThe fed_spend and poverty variables are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa.\n\nExample:\nThe relationship between the homeownership rate and the percent of units in multi-unit structures (e.g. apartments, condos) is visualized using a scatterplot in Figure 2.3. Are these variables associated?\n\nIt appears that the larger the fraction of units in multi-unit structures, the lower the homeownership rate. Since there is some relationship between the variables, they are associated.\n\n\n\n\n\n\n\n\nFigure 2.3: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties.\n\n\n\n\n\nBecause there is a downward trend in Figure 2.3 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. A positive association (upward trend) is shown in the relationship between the poverty and fed_spend variables represented in Figure 2.2, where counties with higher poverty rates tend to receive more federal spending per capita.\nIf two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two.\n\nA pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.\n\n\n\n2.2.4 Creating a scatterplot\nIn this section, we will create a simple scatterplot and then ask you to create one on your own. First, we will recreate the scatterplot seen in Figure 2.2. This figure uses the county_subset data set.\nHere are two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want R to create a scatterplot and to do this it needs, at a minimum, the data object, what we want on the \\(x\\)-axis, and what we want on the \\(y\\)-axis. More information on ggformula can be found here.\n\ncounty_subset %&gt;%\n  gf_point(fed_spend ~ poverty)\n\n\n\n\n\n\n\nFigure 2.4: Scatterplot with ggformula.\n\n\n\n\n\nFigure 2.4 is bad. There are poor axis labels, no title, dense clustering of points, and the \\(y\\)-axis is being driven by a couple of extreme points. We will need to clear this up. Again, try to read the code and use help() or ? to determine the purpose of each command in Figure 2.5.\n\ncounty_subset %&gt;%\n  filter(fed_spend &lt; 32) %&gt;%\n  gf_point(fed_spend ~ poverty,\n           xlab = \"Poverty Rate (Percent)\", \n           ylab = \"Federal Spending Per Capita\",\n           title = \"A scatterplot showing fed_spend against poverty\", \n           cex = 1, alpha = 0.2) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 2.5: Better example of a scatterplot.\n\n\n\n\n\n\nExercise:\nCreate the scatterplot in Figure 2.3.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "Data-Basics.html#footnotes",
    "href": "Data-Basics.html#footnotes",
    "title": "2  Data Basics",
    "section": "",
    "text": "A tibble is a data frame with attributes for such things as better display and printing.↩︎\nTidy data is data in which each row corresponds to a unique case and each column represents a single variable. For more information on tidy data, see the Simply Statistics blog and the R for Data Science book by Hadley Wickham and Garrett Grolemund.↩︎\nThese data were collected from the US Census website.↩︎\nEach county may be viewed as a case, and there are ten pieces of information recorded for each case. A table with 3,142 rows and 10 columns could hold these data, where each row represents a county and each column represents a particular piece of information.↩︎\nThe number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical.↩︎\nThere are only two possible values for each variable, and in both cases they describe categories. Thus, each is a categorical variable.↩︎\nTwo sample questions: (1) Intuition suggests that if there are many line breaks in an email then there would also tend to be many characters: does this hold true? (2) Is there a connection between whether an email format is plain text (versus HTML) and whether it is a spam message?↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "Overview-of-Data-Collection-Principles.html",
    "href": "Overview-of-Data-Collection-Principles.html",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "3.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "Overview-of-Data-Collection-Principles.html#objectives",
    "href": "Overview-of-Data-Collection-Principles.html#objectives",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as population, sample, anecdotal evidence, bias, simple random sample, systematic sample, representative sample, non-response bias, convenience sample, explanatory variable, response variable, observational study, cohort, experiment, randomized experiment, and placebo, and construct examples to demonstrate their proper use in context.\nEvaluate descriptions of research project to identify the population of interest, assess the generalizability of the study, determine the explanatory and response variables, classify the study as observational or experimental, and determine the type of sample used.\nDesign and justify sampling procedures for various research contexts, comparing the strengths and weaknesses of different sampling methods (simple random, systematic, convenience, etc.) and proposing improvements to minimize bias and enhance representativeness.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "href": "Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "title": "3  Overview of Data Collection Principles",
    "section": "3.2 Overview of data collection principles",
    "text": "3.2 Overview of data collection principles\nThe first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals.\n\n3.2.1 Populations and samples\nConsider the following three research questions:\n\nWhat is the average mercury content in swordfish in the Atlantic Ocean?\n\nOver the last 5 years, what is the average time to complete a degree for Duke undergraduate students?\n\nDoes a new drug reduce the number of deaths in patients with severe heart disease?\n\nEach research question refers to a target population, the entire collection of individuals about which we want information. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question.\n\nExercise:\nFor the second and third questions above, identify the target population and what represents an individual case.1\n\n\n\n3.2.2 Anecdotal evidence\nConsider the following possible responses to the three research questions:\n\nA man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high.\nI met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges.\nMy friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work.\n\nEach conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence.\n\n\n\n\n\nIn February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, It’s one storm, in one region, of one country.\n\n\n\n\n\nAnecdotal evidence: Be careful of data collected haphazardly. Such evidence may be true and verifiable, but it may only represent extraordinary cases.\n\nAnecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population.\n\n\n3.2.3 Sampling from a population\nWe might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. This is illustrated in Figure 3.1 .\n\n\n\n\n\n\n\n\nFigure 3.1: In this graphic, five graduates are randomly selected from the population to be included in the sample.\n\n\n\n\n\nWhy pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario.\n\nExample:\nSuppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates? 2\n\n\n\n\n\n\n\n\n\nFigure 3.2: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often.\n\n\n\n\n\nIf someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces sampling bias (see Figure 3.2), where some individuals in the population are more likely to be sampled than others. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample.\nSometimes a simple random sample is difficult to implement and an alternative method is helpful. One such substitute is a systematic sample, where one case is sampled after letting a fixed number of others, say 10 other cases, pass by. Since this approach uses a mechanism that is not easily subject to personal biases, it often yields a reasonably representative sample. This book will focus on simple random samples since the use of systematic samples is uncommon and requires additional considerations of the context.\nThe act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative3 of the entire population, the survey might suffer from non-response bias4.\n\n\n\n\n\n\n\n\nFigure 3.3: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem.\n\n\n\n\n\nAnother common pitfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample, see Figure 3.3. For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents.\n\nExercise:\nWe can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?5\n\n\n\n3.2.4 Explanatory and response variables\nConsider the following question for the county data set:\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\nIf we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.6 If there are many variables, it may be possible to consider a number of them as explanatory variables.\n\nExplanatory and response variables\nTo identify the explanatory variable in a pair of variables, identify which of the two variables is suspected as explaining or causing changes in the other. In data sets with more than two variables, it is possible to have multiple explanatory variables. The response variable is the outcome or result of interest.\n\n\nCaution: Association does not imply causation. Labeling variables as explanatory and response does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other. We also use this language to help in our use of R and the formula notation.\n\nIn some cases, there is no explanatory or response variable. Consider the following question:\nIf homeownership in a particular county is lower than the national average, will the percent of multi-unit structures in that county likely be above or below the national average?\nIt is difficult to decide which of these variables should be considered the explanatory and response variable; i.e. the direction is ambiguous, so no explanatory or response labels are suggested here.\n\n\n3.2.5 Introducing observational studies and experiments\nThere are two primary types of data collection: observational studies and experiments.\nResearchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort7 of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but by themselves, they cannot show a causal connection.\nWhen researchers want to investigate the possibility of a causal connection, they conduct an experiment, a study in which the explanatory variables are assigned rather than observed. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a treatment group, and we are comparing at least two treatments, the experiment is called a randomized comparative experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. The case study at the beginning of the book is another example of an experiment, though that study did not employ a placebo. Math 359 is a course on the design and analysis of experimental data, DOE, at USAFA. In the Air Force, these types of experiments are an important part of test and evaluation. Many Air Force analysts are expert practitioners of DOE. In this book though, we will minimize our discussion of DOE.\n\nAssociation \\(\\neq\\) Causation\nAgain, association does not imply causation. In a data analysis, association does not imply causation, and causation can only be inferred from a randomized experiment. Although, a hot field is the analysis of causal relationships in observational data. This is important because consider cigarette smoking, how do we know it causes lung cancer? We only have observational data and clearly cannot do an experiment. We think analysts will be charged in the near future with using causal reasoning on observational data.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "Overview-of-Data-Collection-Principles.html#footnotes",
    "href": "Overview-of-Data-Collection-Principles.html#footnotes",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "2) Notice that the second question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. 3) A person with severe heart disease represents a case. The population includes all people with severe heart disease.↩︎\nPerhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern.↩︎\nA representative sample accurately reflects the characteristics of the population.↩︎\nNon-response bias is bias that can be introduced when subjects elect not to participate in a study. Often, the individuals that do participate are systematically different from the individuals who do not.↩︎\nAnswers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind.↩︎\nSometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so be careful and consider the context when using or reading these words.↩︎\nA cohort is a group of individuals who are similar in some way.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "Studies.html",
    "href": "Studies.html",
    "title": "4  Studies",
    "section": "",
    "text": "4.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "Studies.html#objectives",
    "href": "Studies.html#objectives",
    "title": "4  Studies",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as observational study, confounding variable, prospective study, retrospective study, simple random sampling, stratified sampling, strata, cluster sampling, multistage sampling, experiment, randomized experiment, control, replicate, blocking, treatment group, control group, blinded study, placebo, placebo effect, and double-blind, and construct examples to demonstrate their proper use in context.\nEvaluate study descriptions using appropriate terminology, and analyze the study design for potential biases or confounding variables.\nGiven a scenario, identify and assess flaws in reasoning, and propose robust study and sampling methodologies to address these flaws.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "Studies.html#observational-studies-sampling-strategies-and-experiments",
    "href": "Studies.html#observational-studies-sampling-strategies-and-experiments",
    "title": "4  Studies",
    "section": "4.2 Observational studies, sampling strategies, and experiments",
    "text": "4.2 Observational studies, sampling strategies, and experiments\n\n4.2.1 Observational studies\nGenerally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers.\nMaking causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations.\n\nExercise:\nSuppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?1\n\nSome previous research2 tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation.\n\n\n\n\n\n\n\n\nFigure 4.1: Sun exposure is a confounding variable because it is related to both response and explanatory variables.\n\n\n\n\n\nSun exposure is what is called a confounding variable,3 which is a variable that is correlated with both the explanatory and response variables, see Figure 4.1. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured.\nLet’s look at an example of confounding visually. Using the SAT data from the mosaic package let’s look at expenditure per pupil versus SAT scores. Figure 4.2 is a plot of the data.\n\nExercise:\nWhat conclusion do you reach from the plot in Figure 4.2?4\n\n\n\n\n\n\n\n\n\nFigure 4.2: Average SAT score versus expenditure per pupil; reminder: each observation represents an individual state.\n\n\n\n\n\nThe implication that spending less might give better results is not justified. Expenditures are confounded with the proportion of students who take the exam, and scores are higher in states where fewer students take the exam.\nIt is interesting to look at the original plot if we place the states into two groups depending on whether more or fewer than 40% of students take the SAT. Figure 4.3 is a plot of the data broken down into the 2 groups.\n\n\n\n\n\n\n\n\nFigure 4.3: Average SAT score versus expenditure per pupil; broken down by level of participation.\n\n\n\n\n\nOnce we account for the fraction of students taking the SAT, the relationship between expenditures and SAT scores changes.\nIn the same way, the county data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions.\n\nExercise:\nFigure 4.4 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in the Figure 4.4.5\n\n\n\n\n\n\n\n\n\nFigure 4.4: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties.\n\n\n\n\n\nObservational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses Health Study, started in 1976 and expanded in 1989.6 This prospective study recruits registered nurses and then collects data from them using questionnaires.\nRetrospective studies collect data after events have taken place; e.g. researchers may review past events in medical records. Some data sets, such as county, may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retail sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population).\n\n\n4.2.2 Three sampling methods\nAlmost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. Figure 4.5, Figure 4.6, and Figure 4.7 provide a graphical representation of these techniques.\n\n\n\n\n\n\n\n\nFigure 4.5: Examples of simple random sampling. In this figure, simple random sampling was used to randomly select the 18 cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: In this figure, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: In this figure, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected.\n\n\n\n\n\nSimple random sampling is probably the most intuitive form of random sampling, in which each individual in the population has an equal chance of being chosen. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that season’s 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included or not.\nStratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (we’re looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players.\nStratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling.\n\nExample:\nWhy would it be good for cases within each stratum to be very similar?7\n\nIn cluster sampling, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this book can be extended to handle such data.\n\nExample:\nSuppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed?8\n\nAnother technique called multistage sampling is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling.\n\n\n4.2.3 Experiments\nStudies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables.\n\n4.2.3.1 Principles of experimental design\nRandomized experiments are generally built on four principles.\n\nControlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill.\nRandomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study.\nReplication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. You replicate to the level of variability you want to estimate. For example, in flight test, we can run the same flight conditions again to get a replicate; however, if the same plane and pilot are being used, the replicate is not getting the pilot-to-pilot or the plane-to-plane variability.\nBlocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block, or group, to the treatments. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 4.8. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients.\n\n\n\n\n\n\n\n\n\nFigure 4.8: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories.\n\n\n\n\n\nIt is important to incorporate the first three experimental design principles into any study, and this chapter describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this chapter may be extended to analyze data collected using blocking. Math 359 is an entire course at USAFA devoted to the design and analysis of experiments.\n\n\n4.2.3.2 Reducing bias in human experiments\nRandomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.9 In particular, researchers wanted to know if the drug reduced deaths in patients.\nThese researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers10 were randomly placed into two study groups. One group, the treatment group, received the experimental treatment of interest (the new drug to treat heart attack patients). The other group, called the control group, did not receive any drug treatment. The comparison between the treatment and control groups allows researchers to determine whether the treatment really has an effect.\nPut yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify.\nResearchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect.\nThe patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.11\n\nExercise:\nLook back to the stent study in the first chapter where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?12",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "Studies.html#footnotes",
    "href": "Studies.html#footnotes",
    "title": "4  Studies",
    "section": "",
    "text": "No. See the paragraph following the exercise for an explanation.↩︎\nhttp://www.sciencedirect.com/science/article/pii/S0140673698121682\nhttp://archderm.ama-assn.org/cgi/content/abstract/122/5/537\nStudy with a similar scenario to that described here:\nhttp://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full↩︎\nAlso called a lurking variable, confounding factor, or a confounder.↩︎\nIt appears that average SAT score declines as expenditures per student increases.↩︎\nAnswers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents.↩︎\nhttp://www.channing.harvard.edu/nhs/↩︎\nWe might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population.↩︎\nA simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information.↩︎\nAnturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.↩︎\nHuman subjects are often called patients, volunteers, or study participants.↩︎\nThere are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the study’s patients and do not tell the blinded health care professionals who is receiving which treatment.↩︎\nThe researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "Numerical-Data.html",
    "href": "Numerical-Data.html",
    "title": "5  Numerical Data",
    "section": "",
    "text": "5.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "Numerical-Data.html#objectives",
    "href": "Numerical-Data.html#objectives",
    "title": "5  Numerical Data",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as scatterplot, mean, distribution, point estimate, weighted mean, histogram, data density, right skewed, left skewed, symmetric, mode, unimodal, bimodal, multimodal, density plot, variance, standard deviation, box plot, median, interquartile range, first quartile, third quartile, whiskers, outlier, robust estimate, and transformation, and construct examples to demonstrate their proper use in context.\nUsing R, generate and interpret summary statistics for numerical variables.\nCreate and evaluate graphical summaries of numerical variables using R, choosing the most appropriate types of plots for different data characteristics and research questions.\nSynthesize numerical and graphical summaries to provide interpretations and explanations of a data set.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "Numerical-Data.html#numerical-data",
    "href": "Numerical-Data.html#numerical-data",
    "title": "5  Numerical Data",
    "section": "5.2 Numerical Data",
    "text": "5.2 Numerical Data\nThis chapter introduces techniques for exploring and summarizing numerical variables. The email50 and mlb data sets from the openintro package and a subset of county_complete from the usdata package provide rich opportunities for examples. Recall that outcomes of numerical variables are numbers on which it is reasonable to perform basic arithmetic operations. For example, the pop2010 variable, which represents the population of counties in 2010, is numerical since we can sensibly discuss the difference or ratio of the populations in two counties. On the other hand, area codes and zip codes are not numerical.\n\n5.2.1 Scatterplots for paired data\nA scatterplot provides a case-by-case view of data for two numerical variables. In Figure 5.1, we again present a scatterplot used to examine how federal spending and poverty are related in the county data set.\n\n\n\n\n\n\n\n\nFigure 5.1: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted.\n\n\n\n\n\nAnother scatterplot is shown in Figure 5.2, comparing the number of line_breaks and number of characters, num_char, in emails for the email50 data set. In any scatterplot, each point represents a single case. Since there are 50 cases in email50, there are 50 points in Figure 5.2.\n\n\n\n\n\n\n\n\nFigure 5.2: A scatterplot of line_breaks versus num_char for the email50 data.\n\n\n\n\n\nTo put the number of characters in perspective, this paragraph in the text has 357 characters. Looking at Figure 5.2, it seems that some emails are incredibly long! Upon further investigation, we would actually find that most of the long emails use the HTML format, which means most of the characters in those emails are used to format the email rather than provide text.\n\nExercise:\nWhat do scatterplots reveal about the data, and how might they be useful?1\n\n\nExample:\nConsider a new data set of 54 cars with two variables: vehicle price and weight.2 A scatterplot of vehicle price versus weight is shown in Figure 5.3. What can be said about the relationship between these variables?\n\n\n\n\n\n\n\n\n\nFigure 5.3: A scatterplot of price versus weight for 54 cars.\n\n\n\n\n\nThe relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we’ve seen which show relationships that are very linear.\n\nExercise:\nDescribe two variables that would have a horseshoe-shaped association in a scatterplot.3\n\n\n\n5.2.2 The mean\nThe mean, sometimes called the average, is a common way to measure the center of a distribution4 of data. To find the mean number of characters in the 50 emails, we add up all the character counts and divide by the number of emails. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.\n\\[\\bar{x} = \\frac{21.7 + 7.0 + \\cdots + 15.8}{50} = 11.6\\]\nThe sample mean is often labeled \\(\\bar{x}\\). There is a bar over the letter, and the letter \\(x\\) is being used as a generic placeholder for the variable of interest, num_char.\n\nMean\nThe sample mean of a numerical variable is the sum of all of the observations divided by the number of observations, Equation 1.\n\n\\[\\begin{equation}\n  \\bar{x} = \\frac{x_1+x_2+\\cdots+x_n}{n}\n  \\tag{1}\n\\end{equation}\\]\nwhere \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\n\nExercise:\nExamine the two equations above. What does \\(x_1\\) correspond to? And \\(x_2\\)? Can you infer a general meaning to what \\(x_i\\) might represent?5\n\n\nExercise:\nWhat was \\(n\\) in this sample of emails?6\n\nThe email50 data set is a sample from a larger population of emails that were received in January and March. We could compute a mean for this population in the same way as the sample mean. However, there is a difference in notation: the population mean has a special label: \\(\\mu\\). The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g. \\(\\mu_x\\).\n\nExample: The average number of characters across all emails can be estimated using the sample data. Based on the sample of 50 emails, what would be a reasonable estimate of \\(\\mu_x\\), the mean number of characters in all emails in the email data set? (Recall that email50 is a sample from email.)\n\nThe sample mean, 11.6, may provide a reasonable estimate of \\(\\mu_x\\). While this number will not be perfect, it provides a point estimate, a single plausible value, of the population mean. Later in the text, we will develop tools to characterize the accuracy of point estimates, and we will find that point estimates based on larger samples tend to be more accurate than those based on smaller samples.\n\nExample:\nWe might like to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes from the 3,143 counties in the county data set. What would be a better approach?\n\nThe county data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the county data, we would find that the per capita income for the US is $27,348.43. Had we computed the simple mean of per capita income across counties, the result would have been just $22,504.70!\nThis previous example used what is called a weighted mean7, which will be a key topic in the probability section. As a look ahead, the probability mass function gives the population proportions of each county’s mean value, and thus, to find the population mean \\(\\mu\\), we will use a weighted mean.\n\n\n5.2.3 Histograms and shape\nRather than showing the exact value of each observation for a single variable, think of the value as belonging to a bin. For example, in the email50 data set, we create a table of counts for the number of cases with character counts between 0 and 5,000, then the number of cases between 5,000 and 10,000, and so on. Observations that fall on the boundary of a bin (e.g. 5,000) are allocated to the lower bin. This tabulation is shown below.\n\n\n\n  (0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] \n     19      12       6       2       3       5       0       0       2       0 \n(50,55] (55,60] (60,65] \n      0       0       1 \n\n\nThese binned counts are plotted as bars in Figure 5.4 in what is called a histogram8.\n\n\n\n\n\n\n\n\nFigure 5.4: A histogram of num_char. This distribution is very strongly skewed to the right.\n\n\n\n\n\nHistograms provide a view of the data density. Higher bars represent where the data are relatively more dense. For instance, there are many more emails between 0 and 10,000 characters than emails between 10,000 and 20,000 characters in the data set. The bars make it easy to see how the density of the data changes relative to the number of characters.\nHistograms are especially convenient for describing the shape of the data distribution. Figure 5.4 shows that most emails have a relatively small number of characters, while fewer emails have a very large number of characters. When data trail off to the right in this way and have a longer right tail, the shape is said to be right skewed.9\nData sets with the reverse characteristic – a long, thin tail to the left – are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric.\n\nLong tails to identify skew\nWhen data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed.\n\n\n5.2.3.1 Making our own histogram\nLet’s take some time to make a simple histogram. We will use the ggformula package, which is a wrapper for the ggplot2 package.\nHere are two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want R to make a histogram. In ggformula, the plots have the form gf_plottype so we will use the gf_histogram(). To find options and more information about the function, type:\n?gf_histogram\nTo start, we just have to give the formulas and data to R.\n\ngf_histogram(~num_char, data = email50, color = \"black\", fill = \"cyan\")\n\n\n\n\n\n\n\n\n\nExercise:\nLook at the help menu for gf_histogram and change the x-axis label, change the bin width to 5, and have the left bin start at 0.\n\nHere is the code for the exercise:\nemail50 %&gt;%\n   gf_histogram(~num_char, binwidth = 5,boundary = 0,\n   xlab = \"The Number of Characters (in thousands)\", \n   color = \"black\", fill = \"cyan\") %&gt;%\n   gf_theme(theme_classic())\nIn addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution.10 There is only one prominent peak in the histogram of num_char.\nFigure 5.5 shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since the separation between the two peaks is relatively small, and it only differs from its neighboring bins by a few observations.\n\n\n\n\n\n\n\n\nFigure 5.5: Histograms that demonstrate unimodal, bimodal, and multimodal data.\n\n\n\n\n\n\nExercise:\nHeight measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you anticipate in this height data set?11\n\n\nLooking for modes\nLooking for modes isn’t about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in these notes. The important part of this examination is to better understand your data and how it might be structured.\n\n\n\n\n5.2.4 Density plots\nAnother useful plotting method uses density plots to visualize numerical data. A histogram bins data but is highly dependent on the number and boundary of the bins. A density plot also estimates the distribution of a numerical variable but does this by estimating the density of data points in a small window around each data point. The overall curve is the sum of this small density estimate. A density plot can be thought of as a smooth version of the histogram. Several options go into a density estimate, such as the width of the window and type of smoothing function. These ideas are beyond the scope here and we will just use the default options. Figure 5.6 shows the same distribution of the number of characters but in a smoother way than a histogram.\n\n\n\n\n\n\n\n\nFigure 5.6: A histogram of num_char. This distribution is very strongly skewed to the right.\n\n\n\n\n\n\nExercise:\nCompare and contrast the histogram in Figure 5.4 and the density plot in Figure 5.6. What can you see in the histogram you can’t see in the density plot?\n\n\n\n5.2.5 Variance and standard deviation\nThe mean is used to describe the center of a data set, but the variability in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though the formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to conceptually understand; it roughly describes how far away the typical observation is from the mean. Equation 2 is the equation for sample variance. We will demonstrate it with data so that the notation is easier to understand.\n\\[\\begin{align}\ns_{}^2 &= \\sum_{i = 1}^{n} \\frac{(x_i - \\bar{x})^2}{n - 1} \\\\\n    &= \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}\n  \\tag{2}\n\\end{align}\\]\nwhere \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nWe call the distance of an observation from its mean the deviation. Below are the deviations for the \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), and \\(50^{th}\\) observations of the num_char variable. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.\n\\[\n\\begin{aligned}\nx_1^{}-\\bar{x} &= 21.7 - 11.6 = 10.1 \\hspace{5mm}\\text{ } \\\\\nx_2^{}-\\bar{x} &= 7.0 - 11.6 = -4.6 \\\\\nx_3^{}-\\bar{x} &= 0.6 - 11.6 = -11.0 \\\\\n            &\\ \\vdots \\\\\nx_{50}^{}-\\bar{x} &= 15.8 - 11.6 = 4.2\n\\end{aligned}\n\\]\nIf we square these deviations and then take an average, the result is equal to the sample variance, denoted by \\(s_{}^2\\):\n\\[\n\\begin{aligned}\ns_{}^2 &= \\frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \\cdots + 4.2_{}^2}{50-1} \\\\\n    &= \\frac{102.01 + 21.16 + 121.00 + \\cdots + 17.64}{49} \\\\\n    &= 172.44\n\\end{aligned}\n\\]\nWe divide by \\(n - 1\\), rather than dividing by \\(n\\), when computing the variance; you need not worry about this mathematical nuance yet. Notice that squaring the deviations does two things. First, it makes large values much larger, seen by comparing \\(10.1^2\\), \\((-4.6)^2\\), \\((-11.0)^2\\), and \\(4.2^2\\). Second, it gets rid of any negative signs.\nThe sample standard deviation, \\(s\\), is the square root of the variance:\n\\[s = \\sqrt{172.44} = 13.13\\]\nThe sample standard deviation of the number of characters in an email is 13.13 thousand. A subscript of \\(_x\\) may be added to the variance and standard deviation, i.e. \\(s_x^2\\) and \\(s_x^{}\\), as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1^{}\\), \\(x_2^{}\\), …, \\(x_n^{}\\). The \\(_{x}\\) subscript is usually omitted when it is clear which data the variance or standard deviation is referencing.\n\nVariance and standard deviation\nThe variance is roughly the average squared distance from the mean. The standard deviation is the square root of the variance and describes how close the data are to the mean.\n\nFormulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.12 However, like the mean, the population values have special symbols: \\(\\sigma_{}^2\\) for the variance and \\(\\sigma\\) for the standard deviation. The symbol \\(\\sigma\\) is the Greek letter sigma.\n\nTip: standard deviation describes variability\nFocus on the conceptual meaning of the standard deviation as a descriptor of variability rather than the formulas. Usually 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, as we have seen, these percentages are not strict rules.\n\n\n\n\n\n\n\n\n\nFigure 5.7: The first of three very different population distributions with the same mean, 0, and standard deviation, 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: The second plot with mean 0 and standard deviation 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: The final plot with mean 0 and standard deviation 1.\n\n\n\n\n\n\nExercise:\nEarlier, the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using the three figures, Figures 5.7, 5.8, 5.9 as examples, explain why such a description is important.13\n\n\nExample:\nDescribe the distribution of the num_char variable using the histogram in Figure 5.4. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context: the number of characters in emails. Also note any especially unusual cases/observations.14\n\nIn practice, the variance and standard deviation are sometimes used as a means to an end, where the end is being able to accurately estimate the uncertainty associated with a sample statistic. For example, later in the book we will use the variance and standard deviation to assess how close the sample mean is to the population mean.\n\n\n5.2.6 Box plots, quartiles, and the median\nA box plot summarizes a data set using five statistics, while also plotting unusual observations. Figure 5.10 provides an annotated vertical dot plot alongside a box plot of the num_char variable from the email50 data set.\n\n\n\n\n\n\n\n\nFigure 5.10: A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6,890), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively.\n\n\n\n\n\nThe first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. Figure 5.10 shows 50% of the data falling below the median (red dashes) and the other 50% falling above the median (blue open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the \\(50^{th}\\) percentile: \\((\\text{6,768} + \\text{7,012}) / 2 = \\text{6,890}\\). When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed).\n\nMedian: the number in the middle\nIf the data are ordered from smallest to largest, the median is the observation in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average.\n\nThe second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The total length of the box, shown vertically in Figure 5.10, is called the interquartile range (IQR, for short). It, like the standard deviation, is a measure of variability in the data. The more variable the data, the larger the standard deviation and IQR. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e. 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile), and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively.\n\nInterquartile range (IQR)\nThe IQR is the length of the box in a box plot. It is computed as \\[ IQR = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) are the \\(25^{th}\\) and \\(75^{th}\\) percentiles, respectively.\n\n\nExercise:\nWhat percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?15\n\nExtending out from the box, the whiskers attempt to capture the data outside of the box, however, their reach is never allowed to be more than \\(1.5\\times IQR\\).16 They capture everything within this reach. In Figure 5.10, the upper whisker does not extend to the last three points, which are beyond \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whisker’s limit is not shown in the figure because the plot does not extend down to \\(Q_1 - 1.5\\times IQR\\). In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data.\nAny observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points – instead of just extending the whiskers to the minimum and maximum observed values – is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called outliers. In this case, it would be reasonable to classify the emails with character counts of 41,623, 42,793, and 64,401 as outliers since they are numerically distant from most of the data.\n\nOutliers are extreme\nAn outlier is an observation that is extreme, relative to the rest of the data.\n\n\nWhy it is important to look for outliers\nExamination of data for possible outliers serves many useful purposes, including:\n1. Identifying strong skew in the distribution.\n2. Identifying data collection or entry errors. For instance, we re-examined the email purported to have 64,401 characters to ensure this value was accurate.\n3. Providing insight into interesting properties of the data.\n\n\nExercise:\nThe observation with value 64,401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?17\n\n\nExercise:\nUsing Figure 5.10, estimate the following values for num_char in the email50 data set:\n(a) \\(Q_1\\),\n(b) \\(Q_3\\), and\n(c) IQR.18\n\nOf course, R can calculate these summary statistics for us. First, we will do these calculations individually and then in one function call. Remember to ask yourself what you want R to do and what it needs to do this.\n\nmean(~num_char, data = email50)\n\n[1] 11.59822\n\nsd(~num_char, data = email50)\n\n[1] 13.12526\n\nquantile(~num_char, data = email50)\n\n      0%      25%      50%      75%     100% \n 0.05700  2.53550  6.88950 15.41075 64.40100 \n\niqr(~num_char, data = email50)\n\n[1] 12.87525\n\n\n\nfavstats(~num_char, data = email50)\n\n   min     Q1 median       Q3    max     mean       sd  n missing\n 0.057 2.5355 6.8895 15.41075 64.401 11.59822 13.12526 50       0\n\n\n\n\n5.2.7 Robust statistics\nHow are the sample statistics of the num_char data set affected by the observation with value 64,401? What would we see if this email wasn’t present in the data set? What would happen to these summary statistics if the observation at 64,401 had been even larger, say 150,000? These scenarios are plotted alongside the original data in Figure 5.11, and sample statistics are computed in R.\nFirst, we create a new data frame containing the three scenarios: 1) the original data, 2) the data with the extreme observation dropped, and 3) the data with the extreme observation increased.\n\n# code to create the `robust` data frame\np1 &lt;- email50$num_char\np2 &lt;- p1[-which.max(p1)]\np3 &lt;- p1\np3[which.max(p1)] &lt;- 150\n\nrobust &lt;- data.frame(value = c(p1, p2, p3),\n                     group = c(rep(\"Original\", 50),\n                             rep(\"Dropped\", 49), rep(\"Increased\", 50)))\nhead(robust)\n\n   value    group\n1 21.705 Original\n2  7.011 Original\n3  0.631 Original\n4  2.454 Original\n5 41.623 Original\n6  0.057 Original\n\n\nNow, we create a side-by-side boxplots for each scenario.\n\ngf_boxplot(value ~ group, data = robust, xlab = \"Data Group\",\n           ylab = \"Number of Characters (in thousands)\") %&gt;%\n   gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 5.11: Box plots of the original character count data and two modified data sets, one where the outlier at 64,401 is dropped and one where its value is increased.\n\n\n\n\n\nWe can also use favstats() to calculate summary statistics of value by group, using the robust data frame created above.\n\nfavstats(value ~ group, data = robust)\n\n      group   min     Q1 median       Q3     max     mean       sd  n missing\n1   Dropped 0.057 2.4540 6.7680 14.15600  42.793 10.52061 10.79768 49       0\n2 Increased 0.057 2.5355 6.8895 15.41075 150.000 13.31020 22.43436 50       0\n3  Original 0.057 2.5355 6.8895 15.41075  64.401 11.59822 13.12526 50       0\n\n\nNotice by using the formula notation, we were able to calculate the summary statistics within each group.\n\nExercise:\n(a) Which is affected more by extreme observations, the mean or median? The data summary may be helpful.19\n(b) Which is affected more by extreme observations, the standard deviation or IQR?20\n\nThe median and IQR are called robust statistics because extreme observations have little effect on their values. The mean and standard deviation are affected much more by changes in extreme observations.\n\nExample:\nThe median and IQR do not change much under the three scenarios above. Why might this be the case?21\n\n\nExercise:\nThe distribution of vehicle prices tends to be right skewed, with a few luxury and sports cars lingering out into the right tail. If you were searching for a new car and cared about price, should you be more interested in the mean or median price of vehicles sold, assuming you are in the market for a regular car?22\n\n\n\n5.2.8 Transforming data\nWhen data are very strongly skewed, we sometimes transform them so they are easier to model. Consider the histogram of Major League Baseball players’ salaries from 2010, which is shown in Figure 5.12.\n\n\n\n\n\n\n\n\nFigure 5.12: Histogram of MLB player salaries for 2010, in millions of dollars.\n\n\n\n\n\n\nExample:\nThe histogram of MLB player salaries is somewhat useful because we can see that the data are extremely skewed and centered (as gauged by the median) at about $1 million. What about this plot is not useful?23\n\nThere are some standard transformations that are often applied when much of the data cluster near zero (relative to the larger values in the data set) and all observations are positive. A transformation is a rescaling of the data using a function. For instance, a plot of the natural logarithm24 of player salaries results in a new histogram in Figure 5.13. Transformed data are sometimes easier to work with when applying statistical models because the transformed data are much less skewed and outliers are usually less extreme.\n\n\n\n\n\n\n\n\nFigure 5.13: Histogram of the log-transformed MLB player salaries for 2010.\n\n\n\n\n\nTransformations can also be applied to one or both variables in a scatterplot. A scatterplot of the original line_breaks and num_char variables is shown in Figure 5.2 above. We can see a positive association between the variables and that many observations are clustered near zero. Later in this text, we might want to use a straight line to model the data. However, we’ll find that the data in their current state cannot be modeled very well. Figure 5.14 shows a scatterplot where both line_breaks and num_char have been transformed using a natural log (log base \\(e\\)) transformation. While there is a positive association in each plot, the transformed data show a steadier trend, which is easier to model than the original (un-transformed) data.\n\n\n\n\n\n\n\n\nFigure 5.14: A scatterplot of line_breaks versus num_char for the email50 data, where both variables have been log-transformed.\n\n\n\n\n\nTransformations other than the logarithm can be useful, too. For instance, the square root (\\(\\sqrt{\\text{original observation}}\\)) and inverse \\(\\left(\\frac{1}{\\text{original observation}}\\right)\\) are used commonly by statisticians. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "Numerical-Data.html#footnotes",
    "href": "Numerical-Data.html#footnotes",
    "title": "5  Numerical Data",
    "section": "",
    "text": "Answers may vary. Scatterplots are helpful in quickly spotting associations between variables, whether those associations represent simple or more complex relationships.↩︎\nSubset of data from http://www.amstat.org/publications/jse/v1n1/datasets.lock.html↩︎\nConsider the case where your vertical axis represents something “good” and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description since water becomes toxic when consumed in excessive quantities.↩︎\nThe distribution of a variable is essentially the collection of all values of the variable in the data set. It tells us what values the variable takes on and how often. In the email50 data set, we used a dotplot to view the distribution of num_char.↩︎\n\\(x_1\\) corresponds to the number of characters in the first email in the sample (21.7, in thousands), \\(x_2\\) to the number of characters in the second email (7.0, in thousands), and \\(x_i\\) corresponds to the number of characters in the \\(i^{th}\\) email in the data set.↩︎\nThe sample size, \\(n = 50\\).↩︎\nA weighted mean is an average in which some observations contribute more “weight” than others. In the county data set, we “weighted” the income for each county by dividing income by the county population.↩︎\nA histogram displays the distribution of a quantitative variable. It shows binned counts, the number of observations in a bin, or range of values.↩︎\nOther ways to describe data that are skewed to the right: skewed to the right, skewed to the high end, or skewed to the positive end.↩︎\nAnother definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have no observations with the same value in a data set, which makes this other definition useless for many real data sets.↩︎\nThere might be two height groups visible in the data set: one for the students and one for the adults. That is, the data are probably bimodal. But it could be multimodal because within each group we may be able to see a difference in males and females.↩︎\nThe only difference is that the population variance has a division by \\(n\\) instead of \\(n - 1\\).↩︎\nStarting with Figure @ref(fig:hist53-fig), the three figures show three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution.↩︎\nThe distribution of email character counts is unimodal and very strongly skewed to the high end (right skewed). Many of the counts fall near the mean at 11,600, and most fall within one standard deviation (13,130) of the mean. There is one exceptionally long email with about 65,000 characters.↩︎\nSince \\(Q_1\\) and \\(Q_3\\) capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between \\(Q_1\\) and the median, and another 25% fall between the median and \\(Q_3\\).↩︎\nWhile the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.↩︎\nThat occasionally there may be very long emails.↩︎\nThese visual estimates will vary a little from one person to the next: \\(Q_1\\) ~ 3,000, \\(Q_3\\) ~ 15,000, IQR = \\(Q_3 - Q_1\\) ~ 12,000. (The true values: $Q_1 = $ 2,536, $Q_3 = $ 15,411, IQR = 12,875.)↩︎\nThe mean is affected more.↩︎\nThe standard deviation is affected more.↩︎\nThe median and IQR are only sensitive to numbers near \\(Q_1\\), the median, and \\(Q_3\\). Since values in these regions are relatively stable – there aren’t large jumps between observations – the median and IQR estimates are also quite stable.↩︎\nBuyers of a regular car should be more concerned about the median price. High-end car sales can drastically inflate the mean price while the median will be more robust to the influence of those sales.↩︎\nMost of the data are collected into one bin in the histogram and the data are so strongly skewed that many details in the data are obscured.↩︎\nStatisticians often write the natural logarithm as \\(\\log\\). You might be more familiar with it being written as \\(\\ln\\).↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "Categorical-Data.html",
    "href": "Categorical-Data.html",
    "title": "6  Categorical Data",
    "section": "",
    "text": "6.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "Categorical-Data.html#objectives",
    "href": "Categorical-Data.html#objectives",
    "title": "6  Categorical Data",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as factor, contingency table, marginal counts, joint counts, frequency table, relative frequency table, bar plot, conditioning, segmented bar plot, mosaic plot, pie chart, side-by-side box plot, and density plot, and construct examples to demonstrate their proper use in context.\nUsing R, generate and interpret tables for categorical variables.\nUsing R, generate and interpret summary statistics for numerical variables by groups.\nCreate and evaluate graphical summaries of both categorical and numerical variables using R, selecting the most appropriate visualization techniques for different types of data and research questions.\nSynthesize numerical and graphical summaries to provide interpretations and explanations of a data set.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "Categorical-Data.html#categorical-data",
    "href": "Categorical-Data.html#categorical-data",
    "title": "6  Categorical Data",
    "section": "6.2 Categorical data",
    "text": "6.2 Categorical data\nLike numerical data, categorical data can also be organized and analyzed. This section introduces tables and other basic tools for use with categorical data. Remember at the beginning of this block of material, our case study had categorical data so we have already seen some of the ideas in this chapter.\nThe email50 data set represents a sample from a larger email data set called email. This larger data set contains information on 3,921 emails. In this section, we will use the email data set to examine whether the presence of numbers, small or large, in an email provides any useful information in classifying email as spam or not spam.\n\n6.2.1 Contingency tables and bar plots\nIn the email data set, we have two variables, spam and number, that we want to summarize. Let’s use inspect() to get information and insight about the two variables. We can also type ?email or help(email) to learn more about the data. First, load the openintro library.\n\nlibrary(openintro)\n\n\nemail %&gt;%\n  select(spam, number) %&gt;%\n  inspect()\n\n\ncategorical variables:  \n    name  class levels    n missing\n1 number factor      3 3921       0\n                                   distribution\n1 small (72.1%), none (14%) ...                \n\nquantitative variables:  \n  name   class min Q1 median Q3 max       mean        sd    n missing\n1 spam numeric   0  0      0  0   1 0.09359857 0.2913066 3921       0\n\n\nNotice the use of the pipe operator and how it adds to the ease of reading the code. The select() function allows us to narrow down the columns/variables to the two of interest. Then inspect() gives us information about those variables. We read from top line; we start with the data set email, input it into select() and select variables from it, and then use inspect() to summarize the variables.\nAs indicated above, number is a categorical variable (a factor) that describes whether an email contains no numbers, only small numbers (values under 1 million), or at least one big number (a value of 1 million or more). The variable spam is a numeric variable, where 1 indicates the email is spam and 0 indicates the email is not spam. To treat spam as categorical, we will want to change it to a factor, but first we will build a table that summarizes data for the two variables (Table 6.1). This table is called a contingency table1. Each value in the table represents the number of times a particular combination of variable outcomes occurred.\n\n\n\n\nTable 6.1: A contingency table for the `email` data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpam\n\n\nNumber\n\n\n\n\n\nnone\nsmall\nbig\nTotal\n\n\n\n\n0\n400\n2659\n495\n3554\n\n\n1\n149\n168\n50\n367\n\n\nTotal\n549\n2827\n545\n3921\n\n\n\n\n\n\n\n\n\n\nBelow is the R code to generate the contingency table.\n\ntally(~spam + number, data = email, margins = TRUE)\n\n       number\nspam    none small  big Total\n  0      400  2659  495  3554\n  1      149   168   50   367\n  Total  549  2827  545  3921\n\n\nThe value 149 corresponds to the number of emails in the data set that are spam and had no numbers listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g. \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. The row and column totals are known as marginal2 counts (hence, margins = TRUE) and the values in the table are known as joint3 counts.\nLet’s turn spam into a factor and update the email data object. We will use mutate() to do this.\n\nemail &lt;- email %&gt;%\n  mutate(spam = factor(email$spam, levels = c(1, 0), \n                       labels = c(\"spam\", \"not spam\")))\n\nNow, let’s check the data again.\n\nemail %&gt;%\n  select(spam, number) %&gt;%\n  inspect()\n\n\ncategorical variables:  \n    name  class levels    n missing\n1   spam factor      2 3921       0\n2 number factor      3 3921       0\n                                   distribution\n1 not spam (90.6%), spam (9.4%)                \n2 small (72.1%), none (14%) ...                \n\n\nLet’s generate the contingency table again.\n\ntally(~spam + number, data = email, margins = TRUE)\n\n          number\nspam       none small  big Total\n  spam      149   168   50   367\n  not spam  400  2659  495  3554\n  Total     549  2827  545  3921\n\n\nA table for a single variable is called a frequency table. The table below is a frequency table for the number variable.\n\ntally(~number, data = email)\n\nnumber\n none small   big \n  549  2827   545 \n\n\nIf we replaced the counts with percentages or proportions, the table would be called a relative frequency table.\n\ntally(~number, data = email, format = 'proportion')\n\nnumber\n     none     small       big \n0.1400153 0.7209895 0.1389952 \n\n\n\nround(tally(~number, data = email, format = 'percent'), 2)\n\nnumber\n none small   big \n 14.0  72.1  13.9 \n\n\nA bar plot is a common way to display a single categorical variable. Figure 6.1 shows a bar plot for the number variable.\n\nemail %&gt;%\n  gf_bar(~number) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 6.1: Bar chart of the number variable.\n\n\n\n\n\nNext, the counts are converted into proportions (e.g., \\(549 / 3921 = 0.140\\) for none) in Figure 6.2.\n\nemail %&gt;%\n  gf_props(~number) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Proportion\")\n\n\n\n\n\n\n\nFigure 6.2: Bar chart of the number variable as a proportion.\n\n\n\n\n\nAgain, let’s clean up the plot into a style that we could use in a report.\n\nemail %&gt;%\n  gf_props(~number, \n           title = \"The proportions of emails with a number in it\",\n           subtitle = \"From 2012\", xlab = \"Type of number in the email\",\n           ylab = \"Proportion of emails\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\n\n\n\n6.2.2 Column proportions\nThe table below shows the column proportions. The column proportions are computed as the counts divided by their column totals. The value 149 at the intersection of spam and none is replaced by \\(149 / 549 = 0.271\\), i.e., 149 divided by its column total, 549. So what does 0.271 represent? It corresponds to the proportion of emails in the sample with no numbers that are spam. That is, the proportion of emails that are spam, out of all the emails with no numbers. We are conditioning, restricting, on emails with no number. This rate of spam is much higher than emails with only small numbers (5.9%) or big numbers (9.2%). Because these spam rates vary between the three levels of number (none, small, big), this provides evidence that the spam and number variables are associated.\n\ntally(spam ~ number, data = email, margins = TRUE, format = 'proportion')\n\n          number\nspam             none      small        big\n  spam     0.27140255 0.05942695 0.09174312\n  not spam 0.72859745 0.94057305 0.90825688\n  Total    1.00000000 1.00000000 1.00000000\n\n\nThe tally() function will always condition on the variable on the right-hand side of the tilde, ~, when calculating proportions. Thus, tally() only generates column or overall proportions. It cannot generate row proportions. The more general table() function of R will allow either column or row proportions.\n\nExercise:\nCreate a table of column proportions where the variable spam is the column variable.\n\n\ntally(number ~ spam, data = email, margins = TRUE, format = 'proportion')\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\n\nExercise:\nIn the table you just created, what does 0.748 represent?4\n\n\nExercise: Create a table of proportions, where spam is the column variable and the values shown represent the proportion of the entire sample in each category.\n\n\ntally(~ number + spam, data = email, margins = TRUE, format = \"proportion\")\n\n       spam\nnumber        spam   not spam      Total\n  none  0.03800051 0.10201479 0.14001530\n  small 0.04284621 0.67814333 0.72098954\n  big   0.01275185 0.12624331 0.13899515\n  Total 0.09359857 0.90640143 1.00000000\n\n\n\nExample:\nData scientists use statistics to filter spam from incoming email messages. By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. One of those characteristics is whether the email contains no numbers, small numbers, or big numbers. Another characteristic is whether or not an email has any HTML content (given by the format variable). A contingency table for the spam and format variables is needed.\n1. Make format into a categorical factor variable. The levels should be “text” and “HTML”.5\n2. Create a contingency table from the email data set with format in the columns and spam in the rows.\n\n\nemail &lt;- email %&gt;% \n  mutate(format = factor(email$format, levels = c(1, 0), \n                         labels = c(\"HTML\", \"text\")))\n\nIn deciding which variable to use as a column, the data scientist would be interested in how the proportion of spam changes within each email format. This corresponds to column proportions based on format: the proportion of spam in plain text emails and the proportion of spam in HTML emails.\n\ntally(spam ~ format, data = email, margins = TRUE, format = \"proportion\")\n\n          format\nspam             HTML       text\n  spam     0.05796038 0.17489540\n  not spam 0.94203962 0.82510460\n  Total    1.00000000 1.00000000\n\n\nIn generating the column proportions, we can see that a higher fraction of plain text emails are spam (\\(209 / 1195 = 17.5\\%\\)) compared to HTML emails (\\(158 / 2726 = 5.8\\%\\)). This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. Yet, when we carefully combine this information with many other characteristics, such as number and other variables, we stand a reasonable chance of being able to classify an email as spam or not spam.\nIn constructing a table, we need to think about which variable we want in the column and which in the row. The formula notation in some ways makes us think about the response and predictor variables, with the response variable (left-hand side) displayed in the rows and the predictor variable (right-hand side) displayed in the columns. However, in some cases, it is not clear which variable should be in the column and row and the analyst must decide what is being communicated with the table. Before settling on one form for a table, it is important to consider the audience and the message they are to receive from the table.\n\nExercise:\nCreate two tables with number and spam: one where number is in the columns, and one where spam is in the columns. Which table would be more useful to someone hoping to identify spam emails based on the type of numbers in the email?6\n\n\ntally(spam ~ number, data = email, format = 'proportion', margin = TRUE)\n\n          number\nspam             none      small        big\n  spam     0.27140255 0.05942695 0.09174312\n  not spam 0.72859745 0.94057305 0.90825688\n  Total    1.00000000 1.00000000 1.00000000\n\n\n\ntally(number ~ spam, data = email, format = 'proportion', margin = TRUE)\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\n\n\n6.2.3 Segmented bar and mosaic plots\nContingency tables using column proportions are especially useful for examining how two categorical variables are related. Segmented bar and mosaic plots provide a way to visualize the information in these tables.\nA segmented bar plot is a graphical display of contingency table information. For example, a segmented bar plot representing the table with number in the columns is shown in Figure 6.3, where we have first created a bar plot using the number variable and then separated each group by the levels of spam using the fill argument.\n\nemail %&gt;%\n  gf_bar(~number, fill = ~spam) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 6.3: Segmented bar plot for numbers found in emails, where the counts have been further broken down by spam.\n\n\n\n\n\nThe column proportions of the table have been translated into a standardized segmented bar plot in Figure 6.4, which is a helpful visualization of the fraction of spam emails within each level of number.\n\nemail %&gt;%\n  gf_props(~number, fill = ~spam, position = 'fill') %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Proportion\")\n\n\n\n\n\n\n\nFigure 6.4: Standardized version of Figure 6.3.\n\n\n\n\n\n\nExample:\nExamine both of the segmented bar plots. Which is more useful?7\n\nSince the proportion of spam changes across the groups in Figure 6.4, we can conclude the variables are dependent, which is something we were also able to discern using table proportions. Because both the none and big groups have relatively few observations compared to the small group, the association is more difficult to see in Figure 6.3.\nIn other cases, a segmented bar plot that is not standardized will be more useful in communicating important information. Before settling on a particular segmented bar plot, create standardized and non-standardized forms and decide which is more effective at communicating features of the data.\nA mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. It seems strange, but mosaic plots are not part of the mosaic package. We must load another set of packages called vcd and vcdExtra. Mosaic plots help to visualize the pattern of associations among variables in two-way and larger tables. Mosaic plots are controversial because they rely on the perception of area; human vision is not good at distinguishing areas.\nWe introduce mosaic plots as another way to visualize contingency tables. Figure 6.5 shows a one-variable mosaic plot for the number variable. Each row represents a level of number, and the row heights correspond to the proportion of emails of each number type. For instance, there are fewer emails with no numbers than emails with only small numbers, so the none outcome row is shorter in height. In general, mosaic plots use box areas to represent the number of observations. Since there is only one variable, the widths are all constant. Thus area is simply related to row height making this visual easy to read.\n\nlibrary(vcd)\n\n\nmosaic(~number, data = email)\n\n\n\n\n\n\n\nFigure 6.5: Mosaic plot where emails are grouped by the number variable.\n\n\n\n\n\nThis one-variable mosaic plot can be further divided into pieces as in Figure 6.6 using the spam variable. The first variable in the formula is used to determine row height. That is, each row is split proportionally according to the fraction of emails in each number category. These heights are similar to those in Figure 6.5. Next, each row is split horizontally according to the proportion of emails that were spam in that number group. For example, the second row, representing emails with only small numbers, was divided into emails that were spam (left) and not spam (right). The area of the rectangles represents the overall proportions in the table, where each cell count is divided by the total count. First, we will generate the table and then represent it as a mosaic plot.\n\ntally(~number + spam, data = email, format = 'proportion')\n\n       spam\nnumber        spam   not spam\n  none  0.03800051 0.10201479\n  small 0.04284621 0.67814333\n  big   0.01275185 0.12624331\n\n\n\nmosaic(~number + spam, data = email)\n\n\n\n\n\n\n\nFigure 6.6: Mosaic plot with number as the first (row) variable.\n\n\n\n\n\nThese plots are hard to use in a visual comparison of area. For example, is the area for small number spam emails different from none number spam emails? The rectangles have different shapes but from the table we can tell the areas are very similar.\nAn important use of the mosaic plot is to determine if an association between variables may be present. The bottom row of the first column represents spam emails that had big numbers, and the bottom row of the second column represents regular emails that had big numbers. We can again use this plot to see that the spam and number variables are associated since some rows are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized version of the segmented bar plot.\nIn a similar way, a mosaic plot representing column proportions where spam is in the column could be constructed.\n\nmosaic(~spam + number, data = email)\n\n\n\n\n\n\n\nFigure 6.7: Mosaic plot with spam as the first (row) variable.\n\n\n\n\n\nTo completely understand the mosaic plot as shown in Figure 6.7, let’s first find the proportions of spam.\n\ntally(~spam, data = email, format = \"proportion\")\n\nspam\n      spam   not spam \n0.09359857 0.90640143 \n\n\nSo, the row heights will be split 90-10. Next, let’s find the proportions of number within each value of spam. In the spam row, none will be 41%, small will be 46%, and big will be 13%. In the not spam row, none will be 11%, small will be 75%, and big will be 14%.\n\ntally(number ~ spam, data = email, margins = TRUE, format = \"proportion\")\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\nHowever, because it is more insightful for this application to consider the fraction of spam in each category of the number variable, we prefer Figure 6.6.\n\n\n6.2.4 The only pie chart you will see in this book, hopefully\nWhile pie charts are well known, they are typically not as useful as other charts in a data analysis. A pie chart is shown in Figure 6.8. It is generally more difficult to compare group sizes in a pie chart than in a bar plot, especially when categories have nearly identical counts or proportions. Just as human vision is bad at distinguishing areas, human vision is also bad at distinguishing angles. In the case of the none and big categories, the difference is so slight you may be unable to distinguish any difference in group sizes.\n\npie(table(email$number), col = COL[c(3, 1, 2)], radius = 0.75)\n\n\n\n\n\n\n\nFigure 6.8: A pie chart for number in the email data set.\n\n\n\n\n\nPie charts are popular in the Air Force due to the ease of generating them in Excel and PowerPoint. However, the values for each slice are often printed on top of the chart making the chart irrelevant. We recommend a minimal use of pie charts in your work.\n\n\n6.2.5 Comparing numerical data across groups\nSome of the more interesting investigations can be done by examining numerical data across groups. This is the case where one variable is categorical and the other is numerical. The methods required here aren’t really new. All that is required is to make a numerical plot for each group. Here, two convenient methods are introduced: side-by-side box plots and density plots.\nWe will again take a look at the subset of the county_complete data set. Let’s compare the median household income for counties that gained population from 2000 to 2010 versus counties that had no gain. While we might like to make a causal connection here, remember that these are observational data, so such an interpretation would be unjustified.\nThis section will give us a chance to perform some data wrangling. We will be using the tidyverse verbs in the process. Data wrangling is an important part of analysis work and typically makes up a significant portion of the analysis work.\nHere is the code to generate the data we need.\n\nlibrary(usdata)\n\n\ncounty_tidy &lt;- county_complete %&gt;% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %&gt;%\n  mutate(fed_spend = fed_spend / pop2010)\n\nFirst, as a reminder, let’s look at the data.\nWhat do we want R to do?\nWe want to select the variables pop2000, pop2010, and med_income.\nWhat does R need in order to do this?\nIt needs the data object, and the desired variable names.\nWe will use the select() and inspect() functions.\n\ncounty_tidy %&gt;%\n  select(pop2000, pop2010, med_income) %&gt;%\n  inspect()\n\n\nquantitative variables:  \n        name   class   min       Q1 median    Q3     max     mean        sd\n1    pop2000 numeric    67 11223.50  24621 61775 9519338 89649.99 292547.67\n2    pop2010 numeric    82 11114.50  25872 66780 9818605 98262.04 312946.70\n3 med_income numeric 19351 36956.25  42450 49144  115574 44274.12  11547.49\n     n missing\n1 3139       3\n2 3142       0\n3 3142       0\n\n\nNotice that three counties are missing population values for the year 2000, reported as NA. Let’s remove them and find which counties increased in population by creating a new variable.\n\ncc_reduced &lt;- county_tidy %&gt;%\n  drop_na(pop2000) %&gt;%\n  select(pop2000, pop2010, med_income) %&gt;%\n  mutate(pop_gain = sign(pop2010-pop2000))\n\n\ntally(~pop_gain, data = cc_reduced)\n\npop_gain\n  -1    0    1 \n1097    1 2041 \n\n\nThere were 2,041 counties where the population increased from 2000 to 2010, and there were 1,098 counties with no gain. Only 1 county had a net of zero, and 1,0987 had a loss. Let’s just look at the counties with a gain or loss in a side-by-side boxplot. Again, we will use filter() to select the two groups and then make the variable pop_gain into a categorical variable. It’s time for more data wrangling.\n\ncc_reduced &lt;- cc_reduced %&gt;%\n  filter(pop_gain != 0) %&gt;%\n  mutate(pop_gain = factor(pop_gain, levels = c(-1, 1), \n                           labels = c(\"Loss\", \"Gain\")))\n\n\ninspect(cc_reduced)\n\n\ncategorical variables:  \n      name  class levels    n missing\n1 pop_gain factor      2 3138       0\n                                   distribution\n1 Gain (65%), Loss (35%)                       \n\nquantitative variables:  \n        name   class   min       Q1  median      Q3     max     mean        sd\n1    pop2000 numeric    67 11217.25 24608.0 61783.5 9519338 89669.37 292592.28\n2    pop2010 numeric    82 11127.00 25872.0 66972.0 9818605 98359.23 313133.28\n3 med_income numeric 19351 36950.00 42443.5 49120.0  115574 44253.24  11528.95\n     n missing\n1 3138       0\n2 3138       0\n3 3138       0\n\n\nThe side-by-side box plot is a traditional tool for comparing across groups. An example is shown in Figure 6.9 where there are two box plots, one for each group, drawn on the same scale.\n\ncc_reduced %&gt;%\n  gf_boxplot(med_income ~ pop_gain,\n             subtitle = \"The income data were collected between 2006 and 2010.\",\n             xlab = \"Population change from 2000 to 2010\",\n             ylab = \"Median Household Income\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 6.9: Side-by-side box plot for median household income, where the counties are split by whether there was a population gain or loss from 2000 to 2010.\n\n\n\n\n\nFigure 6.10 is a plot of the two density curves as another way of comparing median income by whether there was a population gain or loss.\n\ncc_reduced %&gt;%\n  gf_dens(~med_income, color = ~pop_gain, lwd = 1) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Median household income\", y = \"Density\", col = \"Population \\nChange\")\n\n\n\n\n\n\n\nFigure 6.10: Density plots of median household income for counties with population gain versus population loss.\n\n\n\n\n\n\nExercise:\nUse the box plots and density plots to compare the incomes for counties across the two groups. What do you notice about the approximate center of each group? What do you notice about the variability between groups? Is the shape relatively consistent between groups? How many prominent modes are there for each group?8\n\n\nExercise:\nWhat components of Figures 6.9, 6.10 do you find most useful?9",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "Categorical-Data.html#footnotes",
    "href": "Categorical-Data.html#footnotes",
    "title": "6  Categorical Data",
    "section": "",
    "text": "A contingency table is a two-way table that shows the distribution of one variable in rows and a second variable in columns.↩︎\nMarginal counts are counts based on only one of the variables in a contingency table. For example, there are 367 spam emails in the table.↩︎\nJoint counts are counts based on both variables in a contingency table. For example, there are 149 emails that are spam and contain no numbers.↩︎\nThis is the proportion of not spam emails that had a small number in it.↩︎\nFrom the help menu on the data, HTML is coded as a 1.↩︎\nThe table with number in the columns will probably be most useful. This table makes it easier to see that emails with small numbers are spam about 5.9% of the time (relatively rare). In contrast, we see that about 27.1% of emails with no numbers are spam, and 9.2% of emails with big numbers are spam.↩︎\nFigure 6.3 contains more information, but Figure 6.4 presents the information more clearly. This second plot makes it clear that emails with no number have a relatively high rate of spam email – about 27%! On the other hand, less than 10% of emails with small or big numbers are spam.↩︎\nAnswers may vary a little. The counties with population gains tend to have higher income (median of about $45,000) versus counties without a gain (median of about $40,000). The variability is also slightly larger for the population gain group. This is evident in the IQR, which is about 50% bigger in the gain group. Both distributions show slight to moderate right skew and are unimodal. There is a secondary small bump at about $60,000 for the no gain group, visible in the density plot, that seems out of place. (Looking into the data set, we would find that 8 of these 15 counties are in Alaska and Texas.) The box plots indicate there are many observations far above the median in each group, though we should anticipate that many observations will fall beyond the whiskers when using such a large data set.↩︎\nThe side-by-side box plots are especially useful for comparing centers and spreads, while the density plots are more useful for seeing distribution shape, skew, and groups of anomalies.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html",
    "href": "Probability-Case-Study.html",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html#objectives",
    "href": "Probability-Case-Study.html#objectives",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "Use R to simulate a probabilistic model.\nGain an introduction to probabilistic thinking through computational, mathematical, and data science approaches.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html#introduction-to-probability-models",
    "href": "Probability-Case-Study.html#introduction-to-probability-models",
    "title": "7  Probability Case Study",
    "section": "7.2 Introduction to probability models",
    "text": "7.2 Introduction to probability models\nIn this second block of material, we will focus on probability models. We will provide both a mathematical approach and a computational approach. We will focus on the latter and leave much of the mathematical details to the interested learner. In some cases we can use both methods on a problem and in others only the computational approach is feasible. The mathematical approach to probability modeling allows us insight into the problem and the ability to understand the process. The computational approach has a much greater ability to generalize but can be time intensive to run and often requires the writing of custom functions.\nThis case study is extensive and may seem overwhelming, but do not worry. We will discuss these ideas again in the many chapters we have coming up this block.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html#probability-models",
    "href": "Probability-Case-Study.html#probability-models",
    "title": "7  Probability Case Study",
    "section": "7.3 Probability models",
    "text": "7.3 Probability models\nProbability models are an important tool for data analysts. They are used to explain variation in outcomes that cannot be explained by other variables. We will use these ideas in the Statistical Modeling Block to help us make decisions about our statistical models.\nOften probability models are used to answer a question of the form “What is the chance that …..?” This means that we typically have an experiment or trial where multiple outcomes are possible and we only have an idea of the frequency of those outcomes. We use this frequency as a measure of the probability of a particular outcome.\nFor this block we will focus just on probability models. To apply a probability model we will need to\n\nDescribe the experiment and its possible outcomes.\nDetermine probability values for the outcomes, which may include parameters that determine the probabilities.\nUnderstand the assumptions behind the model.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html#case-study",
    "href": "Probability-Case-Study.html#case-study",
    "title": "7  Probability Case Study",
    "section": "7.4 Case study",
    "text": "7.4 Case study\nThere is a famous example of a probability question that we will examine in this case study. The question we want to answer is “In a room of \\(n\\) people, what is the chance at least two people have the same birthday?”\n\nExercise:\nThe typical classroom at USAFA has 18 students in it. What do you think is the chance that at least two students have the same birthday?1\n\n\n7.4.1 Break down the question\nThe first action we should take is to understand what is being asked.\n\nWhat is the experiment or trial?\nWhat does it mean to have the same birthday?\nHow should we handle leap years?\nShould we consider the frequency of births? Are some days less likely than others?\n\n\nExercise:\nDiscuss these questions and others that you think are relevant.2\n\nThe best first step is to develop a simple model. Often these are the only ones that will have a mathematical solution. For our problem this means we answer the above questions.\n\nWe have a room of 18 people and we look at their birthdays. We either have two or more birthdays matching or not; thus there are two outcomes.\nAt least two people having the same birthday means we could have multiple matches on the same day or we could have several different days where multiple people have matching birthdays.\nWe don’t care about the year, only the day and month. Thus, two people born on May 16th are a match.\nWe will ignore leap years, for now.\nWe will assume that a person has equal probability of being born on any of the 365 days of the year.\n\n\n\n7.4.2 The computational approach (simulation)\nNow that we have an idea about the structure of the problem, we next need to think about how we would simulate a single classroom. We have 18 students in the classroom and they all could have any of the 365 days of the year as a birthday. What we need to do is sample birthdays for each of the 18 students. But how do we code the days of the year?\nAn easy solution is to just label the days from 1 to 365. The function seq() does this for us.\n\ndays &lt;- seq(1,365)\n\nNext we need to pick one of the days using the sample() function. Note that we set a seed to make our results reproducible. This is not required, but is strongly encouraged.\n\nset.seed(2022)\nsample(days,1)\n\n[1] 228\n\n\nThe first student in the classroom was born on the 228th day of the year, which corresponds to the 16th of August.\nSince R works on vectors of information, we don’t have to write a loop to select 18 days. We have the sample() function do it for us.\nRemember to ask yourself:\n\nWhat do we want R to do?\n\nWe want R to sample 18 birthdays from the numbers 1 to 365. We want to sample with replacement because we’re allowing students to have the same birthday.\n\nWhat does R need to do that?\n\nA quick look at the documentation for the sample() function (using ?sample or help(sample)) tells us we need a vector of data, size specifying the number of items to choose from the vector, and a logical value for replace, specifying whether to sample with replacement or not.\n\nset.seed(2022)\nclass &lt;- sample(days, size = 18, replace = TRUE)\nclass\n\n [1] 228 206 311 331 196 262 191 206 123 233 270 248   7 349 112   1 307 288\n\n\nNotice in our sample we have at least one match, although it is difficult to look at this list and see the match. Let’s sort them to make it easier for us to see.\n\nsort(class)\n\n [1]   1   7 112 123 191 196 206 206 228 233 248 262 270 288 307 311 331 349\n\n\nThere are two birthdays on day 206, corresponding to the 25th of July.\nThe next step is to find a way in R for the code to detect that there is a match.\n\nExercise:\nWhat idea(s) do you have to determine if a match exists?\n\nWe could sort the data and look at differences in sequential values and then check if the set of differences contains a zero. This seems to be computationally expensive. Instead we will use the function unique() which gives a vector of unique values in an object. The function length() gives the number of elements in the vector.\n\nlength(unique(class))\n\n[1] 17\n\n\nSince we only have 17 unique values in a vector of size 18, we have a match. Now let’s put this all together to generate another classroom of size 18.\n\nlength(unique(sample(days, size = 18, replace = TRUE)))\n\n[1] 16\n\n\nThe next problem that needs to be solved is how to repeat the classrooms and keep track of those that have a match. There are several functions we could use to include replicate(), but we will use do() from the mosaic package because it returns a data frame so we can use tidyverse verbs to wrangle the data.\nThe do() function allows us to repeat an operation many times. The following template can be used,\ndo(n) * {stuff to do}              # pseudo-code\nwhere {stuff to do} is typically a single R command, but may be something more complicated.\nLet’s try it out. First, we’ll load the libraries.\n\nlibrary(mosaic)\nlibrary(tidyverse)\n\n\ndo(5)*length(unique(sample(days,size=18,replace = TRUE)))\n\n  length\n1     18\n2     17\n3     17\n4     17\n5     18\n\n\nLet’s repeat this process for a larger number of simulated classrooms. Remember, you should be asking yourself two questions:\n\nWhat do I want R to do?\nWhat does R need to do this?\n\n\n(do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;%  # simulate 1000 classrooms\n  mutate(match = if_else(length == 18, 0, 1)) %&gt;%                       # check for matches, 1 if length &lt; 18\n  summarise(prob = mean(match))                                         # probability is the mean of 0/1s\n\n   prob\n1 0.365\n\n\nThis is within two decimal places of the mathematical solution we will develop shortly.\nHow many classrooms do we need to simulate to get an accurate estimate of the probability of a match? That is a statistical modeling question and it depends on how much variability we can accept. We will discuss these ideas later in the book. For now, we can run the code multiple times and see how the estimate varies. When computational power is cheap, we can increase the number of simulations.\n\n(do(10000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;% \n  mutate(match = if_else(length == 18, 0, 1)) %&gt;%                        \n  summarise(prob = mean(match))                                        \n\n    prob\n1 0.3402\n\n\n\n\n7.4.3 Plotting\nThe method we have used to create the data allows us to summarize the number of unique birthdays using a table or bar chart. Let’s do that now. Note that since the first argument in tally() is not data, the pipe operator will not work without some extra effort. We must tell R that the data is the previous argument in the pipeline and thus use the symbol . to denote this.\n\n(do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;%\n  tally(~length, data = .)\n\nlength\n 15  16  17  18 \n  5  46 269 680 \n\n\nFigure 7.1 is a plot of the number of unique birthdays (out of 18) in our sample.\n\n(do(1000)*length(unique(sample(days, size = 18, replace = TRUE)))) %&gt;%\n  gf_bar(~length) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Number of unique birthdays\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 7.1: Bar chart of the number of unique birthdays in the sample.\n\n\n\n\n\n\nExercise:\nWhat does it mean if the length of unique birthdays is 16, in terms of matches?3\n\n\n\n7.4.4 The mathematical approach\nTo solve this problem mathematically, we will work through the logic one step at a time. One of the key ideas that we will see many times is the idea of the multiplication rule. This idea is the foundation for permutations and combinations, which are counting methods frequently used in probability calculations.\nThe first step that we take is to understand the idea of two or more people having the same birthday. With 18 people, there are a great deal of possibilities for two or more people to have the same birthday. We could have exactly two people with the same birthday. We could have 18 people with the same birthday. We could have three people with the same birthday and another two people with the same birthday but different from the other three. Accounting for all these possibilities is too large a counting process. Instead, we will take the approach of finding the probability of no one having a matching birthday. Then the probability of at least two people having a matching birthday is one minus the probability that no one has a matching birthday. This is known as a complementary probability. A simpler example is to think about rolling a single die. The probability of rolling a six is equivalent to one minus the probability of not rolling a six (rolling any number other than six).\nWe first need to think about all the different ways we could get 18 birthdays. This is going to be our denominator in the probability calculation. The first person could have 365 different days for their birthday. The second person could also have 365 different birthdays. The same is true for all 18 people. This is an example of the multiplication rule. For 18 people, there are \\(365^{18}\\) possible sets of birthdays.4 Again, this will be our denominator in calculating the probability.\nBecause we’re using the complement, the numerator is the number of ways that 18 people can have birthdays with no matches.\n\nExercise:\nWhat is the number of ways for 18 people to have no birthday matches?\n\nThe first person can have a birthday on any day of the year, so there are 365 possibilities. Because we don’t want a match, the second person only has 364 possibilities for a birthday. The third person can’t match either of the first two, so there are only 363 possibilities for that birthday. Thus, there are \\(365 \\times 364 \\times 363 ... \\times 349 \\times 348\\) ways for 18 people to have no birthday matches.\nThis looks like a truncated factorial. Remember a factorial, written as \\(n!\\) with an explanation point, is the product of successive positive integers. For example, \\(3!\\) is \\(3 \\times 2 \\times 1\\) or 6. In our birthday example, we could write the multiplication for the numerator as \\[365\\times 364\\times ...\\times 348 = \\frac{365!}{(365-18)!}\\] As we will learn, the multiplication rule for the numerator is known as a permutation.5\nWe are ready to put it all together. For 18 people, the probability of two or more people with the same birthday is one minus the probability that no one has the same birthday, which is\n\\[1 - \\frac{\\frac{365!}{(365-18)!}}{365^{18}}\\] or\n\\[1 - \\frac{\\frac{365!}{347!}}{365^{18}}\\]\nIn R, there is a factorial() function but factorials get large fast and will overflow the memory. Try factorial(365) in R to see what happens.\n\nfactorial(365)\n\n[1] Inf\n\n\nIt is returning infinity because the number is too large for the buffer. As is often the case when using a computational method, we must be clever about our approach. Instead of using factorials, we can make use of R and its ability to work on vectors. If we provide R with a vector of values, the prod() function will calculate the product of all the elements.\n\n365*364\n\n[1] 132860\n\n\n\nprod(365:364)\n\n[1] 132860\n\n\nNow, we calculate the probability of at least two people in a room of 18 having the same birthday.\n\n1- prod(365:348) / (365^18)\n\n[1] 0.3469114\n\n\nThis is close to the probability we found with simulation.\n\n\n7.4.5 General solution\nWe now have the mathematics to understand the problem. We can easily generalize this to any number of people. To do this, we have to write a function in R. As with everything in R, we save a function as an object. The general format for creating a function is\n\nmy_function &lt;- function(parameters){\n  code for function\n}\n\nFor this problem we will call the function birthday_prob(). The only parameter we need is the number of people in the room, n. Let’s write this function.\n\nbirthday_prob &lt;- function(n=20){\n  1 - prod(365:(365 - (n - 1))) / (365^n)\n}\n\nNotice we assigned the function to the name birthday_prob, we told R to expect one argument to the function, which we are calling n, and then we provide R with the code to find the probability. We set a default value for n in case one is not provided to prevent an error when the function is run. We will learn more about writing functions throughout this book and in the follow-on USAFA course, Math 378: Applied Statistical Modeling.\nLet’s test the code with a known answer.\n\nbirthday_prob(18)\n\n[1] 0.3469114\n\n\nNow we can determine the probability for any size room. You may have heard that it only takes about 23 people in a room to have a 50% probability of at least two people matching birthdays.\n\nbirthday_prob(23)\n\n[1] 0.5072972\n\n\nLet’s create a plot of the probability versus the number of people in the room. To do this, we need to apply the function to a vector of values. The function sapply() will work or we can also use Vectorize() to alter our existing function. We choose the latter option.\nFirst notice what happens if we input a vector into our function.\n\nbirthday_prob(1:20)\n\nWarning in 365:(365 - (n - 1)): numerical expression has 20 elements: only the\nfirst used\n\n\n [1] 0.0000000 0.9972603 0.9999925 1.0000000 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n\n\nIt only uses the first value. There are several ways to solve this problem. We can use the map() function in the purrr package. This idea of mapping a function to a vector is important in data science. It is used in scenarios where there is a lot of data. In this case, the idea of map-reduce is used to make the analysis amenable to parallel computing.\n\nmap_dbl(1:20, birthday_prob)\n\n [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n[13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n[19] 0.379118526 0.411438384\n\n\nWe could also just vectorize the function.\n\nbirthday_prob &lt;- Vectorize(birthday_prob)\n\nNow notice what happens.\n\nbirthday_prob(1:20)\n\n [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n[13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n[19] 0.379118526 0.411438384\n\n\nWe now have what we want, so let’s create our line plot, Figure 7.2.\n\ngf_line(birthday_prob(1:100) ~ seq(1, 100),\n        xlab = \"Number of People\",\n        ylab = \"Probability of Match\",\n        title = \"Probability of at least two people with matching birthdays by room size\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.2: The probability of at least 2 people having matching birthdays.\n\n\n\n\n\nIs this what you expected the curve to look like? We, the authors, did not expect this. It has a sigmoidal shape with a large increase in the middle range and flattening in the tails.\n\n\n7.4.6 Data science approach\nThe final approach we will take is one based on data, a data science approach. The mosaicData package includes a data set called Births that contains the number of births in the US from 1969 to 1988. This data will allow us to estimate the number of births on any day of the year. This allows us to eliminate the reliance on the assumption that each day is equally likely. Let’s first inspect() the data object.\n\ninspect(Births)\n\n\ncategorical variables:  \n  name   class levels    n missing\n1 wday ordered      7 7305       0\n                                   distribution\n1 Wed (14.3%), Thu (14.3%), Fri (14.3%) ...    \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 date  Date 1969-01-01 1988-12-31   1 days   1 days 7305       0\n\nquantitative variables:  \n          name   class  min   Q1 median    Q3   max        mean          sd\n1       births integer 6675 8792   9622 10510 12851 9648.940178 1127.315229\n2         year integer 1969 1974   1979  1984  1988 1978.501027    5.766735\n3        month integer    1    4      7    10    12    6.522930    3.448939\n4  day_of_year integer    1   93    184   275   366  183.753593  105.621885\n5 day_of_month integer    1    8     16    23    31   15.729637    8.800694\n6  day_of_week integer    1    2      4     6     7    4.000274    1.999795\n     n missing\n1 7305       0\n2 7305       0\n3 7305       0\n4 7305       0\n5 7305       0\n6 7305       0\n\n\nNotice there are leap years present in this data. It could be argued that we could randomly pick one year and use it. Let’s see what happens if we just used 1969, a non-leap year. Figure 7.3 is a scatter plot of the number of births in 1969 for each day of the year.\n\nBirths %&gt;%\n  filter(year == 1969) %&gt;%\n  gf_point(births ~ day_of_year) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Day of the Year\", y = \"Number of Births\")\n\n\n\n\n\n\n\nFigure 7.3: The number of births for each day of the year in 1969.\n\n\n\n\n\n\nExercise:\nWhat patterns do you see in Figure 7.3? What might explain them?6\n\nThere are definitely bands appearing in the data which could be the day of the week; there are fewer birthdays on the weekend. There is also seasonality with more birthdays in the summer and fall. There is also probably an impact from holidays.\nQuickly, let’s look at the impact of day of the week by using color for day of the week. Figure 7.4 makes it clear that the weekends have fewer births as compared to the work week.\n\nBirths %&gt;%\n  filter(year == 1969) %&gt;%\n  gf_point(births ~ day_of_year, color = ~factor(day_of_week)) %&gt;%\n  gf_labs(x = \"Day of the Year\", col = \"Day of Week\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.4: The number of births for each day of the year in 1969 broken down by day of the week.\n\n\n\n\n\nBy only using one year, this data might give poor results because holidays will fall on certain days of the week and the weekends will also be impacted. Note that we also still have the problem of leap years.\n\nBirths %&gt;%\n  group_by(year) %&gt;%\n  summarise(n = n())\n\n# A tibble: 20 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1969   365\n 2  1970   365\n 3  1971   365\n 4  1972   366\n 5  1973   365\n 6  1974   365\n 7  1975   365\n 8  1976   366\n 9  1977   365\n10  1978   365\n11  1979   365\n12  1980   366\n13  1981   365\n14  1982   365\n15  1983   365\n16  1984   366\n17  1985   365\n18  1986   365\n19  1987   365\n20  1988   366\n\n\nThe years 1972, 1976, 1980, 1984, and 1988 are all leap years. At this point, to make the analysis easier, we will drop those years.\n\nBirths %&gt;%\n  filter(!(year %in% c(1972, 1976, 1980, 1984, 1988))) %&gt;%\n  group_by(year) %&gt;%\n  summarise(n = n())\n\n# A tibble: 15 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1969   365\n 2  1970   365\n 3  1971   365\n 4  1973   365\n 5  1974   365\n 6  1975   365\n 7  1977   365\n 8  1978   365\n 9  1979   365\n10  1981   365\n11  1982   365\n12  1983   365\n13  1985   365\n14  1986   365\n15  1987   365\n\n\nNotice that we used the %in% operator inside the filter() function. This is a logical argument checking whether year is one of the specified values. The ! at the front negates this, in a sense requiring year to not be one of those values.\nWe are almost ready to simulate. We need to get the count of births on each day of the year for the non-leap years.\n\nbirth_data &lt;- Births %&gt;%\n  filter(!(year %in% c(1972, 1976, 1980, 1984, 1988))) %&gt;%\n  group_by(day_of_year) %&gt;%\n  summarise(n = sum(births)) \n\n\nhead(birth_data)\n\n# A tibble: 6 × 2\n  day_of_year      n\n        &lt;int&gt;  &lt;int&gt;\n1           1 120635\n2           2 129042\n3           3 135901\n4           4 136298\n5           5 137319\n6           6 140044\n\n\nLet’s look at a plot of the number of births versus day of the year for all the non-leap years in Figure 7.5.\n\nbirth_data %&gt;%\n  gf_point(n ~ day_of_year,\n          xlab = \"Day of the year\",\n          ylab = \"Number of births\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.5: Number of births by day of the year for all years.\n\n\n\n\n\nThis curve has the seasonal cycling we would expect. The smaller scale cycling is unexpected. Maybe because we are dropping the leap years, we are getting some days appearing in our time interval more frequently on weekends. We leave it to you to investigate this phenomenon.\nWe use these counts as weights in a sampling process. Days with more births will have a higher probability of being selected. Days such as Christmas and Christmas Eve have a lower probability of being selected. Let’s save the weights in an object to use in the sample() function.\n\nbirth_data_weights &lt;- birth_data %&gt;%\n  select(n) %&gt;%\n  pull()\n\nThe pull() function pulls the vectors of values out of the data frame format into a vector format which the sample() function needs.\nNow let’s simulate the problem. We will use our code from before, but add probability weights in the prob argument. The probability of a match should change slightly, but not much because most of the days have about the same probability or number of occurrences.\n\nset.seed(20)\n(do(1000)*length(unique(sample(days, size = 18, replace = TRUE, prob = birth_data_weights)))) %&gt;%\n  mutate(match = if_else(length == 18, 0, 1)) %&gt;%\n  summarise(prob = mean(match))\n\n   prob\n1 0.352\n\n\nIt would not be possible to solve this problem of varying frequency of birthdays using mathematics, at least as far as we know.\nThis is fascinating stuff! Let’s get to learning more about probability models in the following chapters.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Case-Study.html#footnotes",
    "href": "Probability-Case-Study.html#footnotes",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "The answer is around 34.7%. How close were you? Did you think it was lower or higher?↩︎\nAnother question may be “What does it mean at least two people have matching birthdays?”↩︎\nIt is possible that 3 people all have the same birthday or two sets of 2 people have the same birthday but different from the other pair.↩︎\n\\(365^{18} = 1.322\\times 10^{46}\\)↩︎\nThis could be more generally written as \\(\\frac{365!}{(365 - n)!}\\) for a group of \\(n\\) people.↩︎\nThis could be due to doctors tending not to schedule inductions or C-sections on weekends. Fun fact: while more than 30% of all births were C-sections in 2023, only around 5% of births were C-sections in 1969.↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html",
    "href": "Probability-Rules.html",
    "title": "8  Probability Rules",
    "section": "",
    "text": "8.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#objectives",
    "href": "Probability-Rules.html#objectives",
    "title": "8  Probability Rules",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as sample space, outcome, event, subset, intersection, union, complement, probability, mutually exclusive, exhaustive, independent, multiplication rule, permutation, and combination, and construct examples to demonstrate their proper use in context.\nApply basic probability properties and counting rules to calculate the probabilities of events in different scenarios. Interpret the calculated probabilities in context.\nExplain and illustrate the basic axioms of probability.\nUse R to perform calculations and simulations for determining the probabilities of events.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#probability-vs-statistics",
    "href": "Probability-Rules.html#probability-vs-statistics",
    "title": "8  Probability Rules",
    "section": "8.2 Probability vs Statistics",
    "text": "8.2 Probability vs Statistics\nRemember this book is divided into four general blocks: data collection/summary, probability models, inference and statistical modeling/prediction. This second block on probability models is the study of stochastic (random) processes and their properties. Specifically, we will explore random experiments. As the name suggests, a random experiment is an experiment whose outcome is not predictable with exact certainty. In the statistical models we develop in the last two blocks of this book, we will use other variables to explain the variance of the outcome of interest. Any remaining variance is modeled with probability models.\nEven though an outcome is determined by chance, this does not mean that we know nothing about the random experiment. Our favorite simple example is that of a coin flip. If we flip a coin, the possible outcomes are heads and tails. We don’t know for sure what outcome will occur, but we still know something about the experiment. If we assume the coin is fair, we know that each outcome is equally likely. Also, we know that if we flip the coin 100 times (independently), we are likely to see around 50 heads, and very unlikely to see 10 heads or fewer.\nIt is important to distinguish probability from inference and modeling. In probability, we consider a known random experiment, including knowing the parameters, and answer questions about what we expect to see from this random experiment. In statistics (inference and modeling), we consider data (the results of a mysterious random experiment) and infer about the underlying process. For example, suppose we have a coin and we are unsure whether this coin is fair or unfair (i.e., the parameter is unknown). We might flip it 20 times and observe it land on heads 14 times. Inferential statistics will help us answer questions about the underlying process (e.g., could this coin be unfair?).\n\n\n\n\n\nA graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know about the underlying process. In statistics, we don’t know the underlying process, and must infer based on representative samples.\n\n\n\n\nThis block (10 chapters or so) is devoted to the study of random experiments. First, we will explore simple experiments, counting rule problems, and conditional probability. Next, we will introduce the concept of a random variable and the properties of random variables. Following this, we will cover common distributions of discrete and continuous random variables. We will end the block on multivariate probability (joint distributions and covariance).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#basic-probability-terms",
    "href": "Probability-Rules.html#basic-probability-terms",
    "title": "8  Probability Rules",
    "section": "8.3 Basic probability terms",
    "text": "8.3 Basic probability terms\nWe will start our work with some definitions and examples.\n\n8.3.1 Sample space\nSuppose we have a random experiment. The sample space of this experiment, \\(S\\), is the set of all possible results of that experiment. For example, in the case of a coin flip, we could write \\(S=\\{H,T\\}\\) because the sample space has two outcomes, heads and tails. Each element of the sample space is considered an outcome. An event is a set of outcomes, a subset of the sample space.\n\nExample:\nLet R flip a coin for us and record the number of heads and tails. We will have R flip the coin twice. What is the sample space, what is an example of an outcome, and what is an example of an event?\n\nWe will load the mosaic package as it has a function rflip() that will simulate flipping a coin.\n\nlibrary(mosaic)\n\n\nset.seed(18)\nrflip(2)\n\n\nFlipping 2 coins [ Prob(Heads) = 0.5 ] ...\n\nH H\n\nNumber of Heads: 2 [Proportion Heads: 1]\n\n\nThe sample space is \\(S=\\{HH, TH, HT, TT\\}\\), an example of an outcome is \\(HH\\) which we see in the output from R, and an example of an event is two tails, \\({TT}\\). Another example of an event is “at least one heads”, \\(\\{HH,TH, HT\\}\\). Also, notice that \\(TH\\) is different from \\(HT\\) as an outcome; this is because those are different outcomes from flipping a coin twice.\n\nExample of Event:\nSuppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]\nEach vehicle represents a possible outcome of the experiment.\n\n\n\n8.3.2 Union and intersection\nSuppose we have two events, \\(A\\) and \\(B\\).\n\n\\(A\\) is considered a subset of \\(B\\) if all of the outcomes of \\(A\\) are also contained in \\(B\\). This is denoted as \\(A \\subset B\\).\nThe intersection of \\(A\\) and \\(B\\) is all of the outcomes contained in both \\(A\\) and \\(B\\). This is denoted as \\(A \\cap B\\).\nThe union of \\(A\\) and \\(B\\) is all of the outcomes contained in either \\(A\\) or \\(B\\), or both. This is denoted as \\(A \\cup B\\).\nThe complement of \\(A\\) is all of the outcomes not contained in \\(A\\). This is denoted as \\(A^C\\) or \\(A'\\).\n\nNote: Here we are treating events as sets and the above definitions are basic set operations.\nIt is sometimes helpful when reading probability notation to think of Union as an or and Intersection as an and.\n\nExample:\nConsider our rental car example above. Let \\(A\\) be the event that a blue vehicle is selected, let \\(B\\) be the event that a black vehicle is selected, and let \\(C\\) be the event that an SUV is selected.\n\nFirst, let’s list all of the outcomes of each event. \\(A = \\{\\mbox{blue sedan},\\mbox{blue SUV}\\}\\), \\(B=\\{\\mbox{black SUV}\\}\\), and \\(C= \\{\\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\).\nBecause all outcomes in \\(B\\) are contained in \\(C\\), we know that \\(B\\) is a subset of \\(C\\). This can be written as \\(B\\subset C\\). Also, because \\(A\\) and \\(B\\) have no outcomes in common, \\(A \\cap B = \\emptyset\\). Note that \\(\\emptyset = \\{ \\}\\) is the empty set and contains no elements.1 Further, \\(A \\cup C = \\{\\mbox{blue sedan}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\). The complement of \\(C\\) is \\(C'=\\{\\text{red sedan, red truck, grey truck}\\}\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#probability",
    "href": "Probability-Rules.html#probability",
    "title": "8  Probability Rules",
    "section": "8.4 Probability",
    "text": "8.4 Probability\nProbability is a number assigned to an event or outcome that describes how likely it is to occur. A probability model assigns a probability to each element of the sample space. What makes a probability model is not just the values assigned to each element but the idea that this model contains all the information about the outcomes and there are no other explanatory variables involved.\nA probability model can be thought of as a function that maps outcomes, or events, to a real number in the interval \\([0,1]\\).\n\n8.4.1 Probability axioms\nThere are some basic axioms of probability you should know, although this list is not complete. Let \\(S\\) be the sample space of a random experiment and let \\(A\\) be an event where \\(A\\subset S\\).\n\n\\(\\mbox{P}(A) \\geq 0\\).\n\\(\\mbox{P}(S) = 1\\).\n\nThese two axioms essentially say that probability must be positive, and the probability of all outcomes must sum to one.\n\n\n8.4.2 Probability properties\nLet \\(A\\) and \\(B\\) be events in a random experiment. Most of the probabilities below can be proven fairly easily.\n\n\\(\\mbox{P}(\\emptyset)=0\\).\n\\(\\mbox{P}(A')=1-\\mbox{P}(A)\\). We used this in the case study.\nIf \\(A\\subset B\\), then \\(\\mbox{P}(A)\\leq \\mbox{P}(B)\\).\n\\(\\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)\\). This property can be generalized to more than two events. The intersection is subtracted because outcomes in both events \\(A\\) and \\(B\\) get counted twice in the first sum.\nLaw of Total Probability: Let \\(B_1, B_2,...,B_n\\) be mutually exclusive, this means disjoint or no outcomes in common, and exhaustive, this means the union of all the events labeled with a \\(B\\) is the sample space. Then\n\n\\[\n\\mbox{P}(A)=\\mbox{P}(A\\cap B_1)+\\mbox{P}(A\\cap B_2)+...+\\mbox{P}(A\\cap B_n)\n\\]\nA specific application of this law appears in Bayes’ Rule (more to follow in Chapter 9). It says that \\(\\mbox{P}(A)=\\mbox{P}(A \\cap B)+\\mbox{P}(A \\cap B')\\). Essentially, it points out that \\(A\\) can be partitioned into two parts: 1) everything in \\(A\\) and \\(B\\) and 2) everything in \\(A\\) and not in \\(B\\).\n\nExample:\nConsider rolling a six sided die. Let event \\(A\\) be that a number less than five is showing on the die. Let event \\(B\\) be that the number is even. Then,\n\\[\\mbox{P}(A)=\\mbox{P}(A \\cap B) + \\mbox{P}(A \\cap B')\\]\n\\[\n\\mbox{P}(&lt; 5)=\\mbox{P}(&lt;5 \\cap \\text{Even})+\\mbox{P}(&lt;5 \\cap \\text{Odd})\n\\]\n\n\nDeMorgan’s Laws: \\[\n\\mbox{P}((A \\cup B)')=\\mbox{P}(A' \\cap B')\n\\] \\[\n\\mbox{P}((A \\cap B)')=\\mbox{P}(A' \\cup B')\n\\]\n\n\nExercise: \nLet \\(A\\), \\(B\\), and \\(C\\) be events such that \\(P(A)=0.5\\), \\(P(B)=0.3\\), and \\(P(C)=0.4\\). Also, we know that \\(P(A\\cap B)=0.2\\), \\(P(B\\cap C)=0.12\\), \\(P(A\\cap C)=0.1\\), and \\(P(A\\cap B\\cap C)=0.05\\). Find the following:\n\n\\(P(A\\cup B)\\)\n\\(P(A\\cup B\\cup C)\\)\n\\(P(B'\\cap C')\\)\n\\(P(A\\cup (B\\cap C))\\)\n\\(P((A\\cup B\\cup C)\\cap (A\\cap B\\cap C)')\\)\n\n\nIt can be a good idea to draw a picture to work through exercises like these. Figure 8.1 is an illustration of the above probability problem. Letters A, B, and C denote the corresponding events, represented by the entire circle in which they reside. The letter S represents the entire sample space. The numbers in the diagram represent the probability of each exclusive event. For example, the value 0.25 represents the probability of only event A (and no other events). Can you figure out where the other probability values come from?2\n\n\n\n\n\n\nFigure 8.1: Probability illustration.\n\n\n\n\n\n8.4.3 Equally likely scenarios\nIn some random experiments, outcomes can be defined such that each individual outcome is equally likely. In this case, probability becomes a counting problem. Let \\(A\\) be an event in an experiment where each outcome is equally likely. \\[\n\\mbox{P}(A)=\\frac{\\mbox{Number of outcomes in A}}{\\mbox{Number of outcomes in S}}\n\\]\n\nExample:\nSuppose a family has three children, with each child being either male (M) or female (F). Assume that the likelihood of males and females are equal and independent. This is the idea that the probability of the sex of the second child does not change based on the sex of the first child. The sample space can be written as: \\[\nS=\\{\\mbox{MMM},\\mbox{MMF},\\mbox{MFM},\\mbox{FMM},\\mbox{MFF},\\mbox{FMF},\\mbox{FFM},\\mbox{FFF}\\}\n\\] What is the probability that the family has exactly 2 female children?\n\nThis only happens in three ways: MFF, FMF, and FFM. Thus, the probability of exactly 2 females is 3/8 or 0.375.\n\n\n8.4.4 Simulation with R (Equally likely scenarios)\nThe previous example above is an example of an “Equally Likely” scenario, where the sample space of a random experiment contains a list of outcomes that are equally likely. In these cases, we can sometimes use R to list out the possible outcomes and count them to determine probability. We can also use R to simulate the scenario.\n\nExample:\nUse R to simulate the family of three children where each child has the same probability of being male or female.\n\nInstead of writing our own function, we can use rflip() in the mosaic package. We will let \\(H\\) stand for female.\nFirst simulate one family.\n\nset.seed(73)\nrflip(3)\n\n\nFlipping 3 coins [ Prob(Heads) = 0.5 ] ...\n\nT T H\n\nNumber of Heads: 1 [Proportion Heads: 0.333333333333333]\n\n\nIn this case, we got 1 female. Next, we will use the do() function to repeat this simulation.\n\nresults &lt;- do(10000)*rflip(3)\nhead(results)\n\n  n heads tails      prop\n1 3     1     2 0.3333333\n2 3     3     0 1.0000000\n3 3     3     0 1.0000000\n4 3     3     0 1.0000000\n5 3     1     2 0.3333333\n6 3     1     2 0.3333333\n\n\nNext, we can visualize the distribution of the number of females, heads, in Figure Figure 8.2.\n\nresults %&gt;%\n  gf_bar(~heads) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Number of females\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 8.2: Number of females in a family of size 3.\n\n\n\n\n\nFinally, we can estimate the probability of exactly 2 females. We need the tidyverse library.\n\nlibrary(tidyverse)\n\n\nresults %&gt;%\n  filter(heads == 2) %&gt;%\n  summarize(prob = n()/10000)\n\n    prob\n1 0.3782\n\n\nOr we can use slightly different code.\n\nresults %&gt;%\n  count(heads) %&gt;%\n  mutate(prop = n/sum(n))\n\n  heads    n   prop\n1     0 1241 0.1241\n2     1 3786 0.3786\n3     2 3782 0.3782\n4     3 1191 0.1191\n\n\nThis is not a bad estimate of the exact probability, 0.375.\nLet’s now use an example of cards to simulate some probabilities and learn more about counting. The file Cards.csv contains the data for cards from a standard 52-card deck. There are four suits and two colors: spades and clubs are black, hearts and diamonds are red. Within each suit, there are 13 card values or ranks. These include ace, king, queen, jack, and numbers ranging from ten down to two. Let’s read in the data and summarize.\n\nCards &lt;- read_csv(\"data/Cards.csv\")\ninspect(Cards)\n\n\ncategorical variables:  \n  name     class levels  n missing\n1 rank character     13 52       0\n2 suit character      4 52       0\n                                   distribution\n1 10 (7.7%), 2 (7.7%), 3 (7.7%) ...            \n2 Club (25%), Diamond (25%) ...                \n\nquantitative variables:  \n   name   class        min         Q1     median         Q3        max\n1 probs numeric 0.01923077 0.01923077 0.01923077 0.01923077 0.01923077\n        mean sd  n missing\n1 0.01923077  0 52       0\n\n\n\nhead(Cards)\n\n# A tibble: 6 × 3\n  rank  suit   probs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 2     Club  0.0192\n2 3     Club  0.0192\n3 4     Club  0.0192\n4 5     Club  0.0192\n5 6     Club  0.0192\n6 7     Club  0.0192\n\n\nWe can see 4 suits, and 13 ranks, the value on the face of the card.\n\nExample:\nSuppose we draw one card out of a standard deck. Let \\(A\\) be the event that we draw a Club. Let \\(B\\) be the event that we draw a 10 or a face card (Jack, Queen, King or Ace). We can use R to define these events and find probabilities.\n\nLet’s find all the Clubs.\n\nCards %&gt;%\n  filter(suit == \"Club\") %&gt;%\n  select(rank, suit)\n\n# A tibble: 13 × 2\n   rank  suit \n   &lt;chr&gt; &lt;chr&gt;\n 1 2     Club \n 2 3     Club \n 3 4     Club \n 4 5     Club \n 5 6     Club \n 6 7     Club \n 7 8     Club \n 8 9     Club \n 9 10    Club \n10 J     Club \n11 Q     Club \n12 K     Club \n13 A     Club \n\n\nSo just by counting, we find the probability of drawing a Club is \\(\\frac{13}{52}\\) or 0.25. We can also do this with simulation. This may be overkill for this example, but it gets the idea of simulation across.\nRemember, ask yourself what we want R to do and what R needs to do this. Below we sample one card from “the deck” (i.e., the data set) 10,000 times.\n\nset.seed(573)\nresults &lt;- do(10000)*sample(Cards, 1)\nhead(results)\n\n# A tibble: 6 × 6\n  rank  suit     probs orig.id  .row .index\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 2     Diamond 0.0192 14          1      1\n2 7     Spade   0.0192 45          1      2\n3 6     Spade   0.0192 44          1      3\n4 3     Heart   0.0192 28          1      4\n5 5     Diamond 0.0192 17          1      5\n6 10    Spade   0.0192 48          1      6\n\n\n\nresults %&gt;%\n  filter(suit == \"Club\") %&gt;%\n  summarize(prob = n()/10000)\n\n# A tibble: 1 × 1\n   prob\n  &lt;dbl&gt;\n1 0.240\n\n\n\nresults %&gt;%\n  count(suit) %&gt;%\n  mutate(prob = n/sum(n))\n\n# A tibble: 4 × 3\n  suit        n  prob\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 Club     2405 0.240\n2 Diamond  2594 0.259\n3 Heart    2529 0.253\n4 Spade    2472 0.247\n\n\nIn 10,000 samples, each suit occurs around 25% of the time, just as we’d expect.\nNow let’s count the number of outcomes in \\(B\\).\n\nCards %&gt;%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\")) %&gt;%\n  select(rank,suit)\n\n# A tibble: 20 × 2\n   rank  suit   \n   &lt;chr&gt; &lt;chr&gt;  \n 1 10    Club   \n 2 J     Club   \n 3 Q     Club   \n 4 K     Club   \n 5 A     Club   \n 6 10    Diamond\n 7 J     Diamond\n 8 Q     Diamond\n 9 K     Diamond\n10 A     Diamond\n11 10    Heart  \n12 J     Heart  \n13 Q     Heart  \n14 K     Heart  \n15 A     Heart  \n16 10    Spade  \n17 J     Spade  \n18 Q     Spade  \n19 K     Spade  \n20 A     Spade  \n\n\nSo just by counting, we find the probability of drawing a 10 or greater is \\(\\frac{20}{52}\\) or 0.3846154.\n\nExercise:\nUse simulation to estimate the probability of a 10 or higher (10, jack, queen, king).\n\nWe can simply examine the previously generated 10,000 samples to answer this question via simulation.\n\nresults %&gt;%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\")) %&gt;%\n  summarize(prob = n()/10000)\n\n# A tibble: 1 × 1\n   prob\n  &lt;dbl&gt;\n1 0.382\n\n\nThis is close to the value of 0.385 we found with just counting the possibilities.\nNotice that this code is not robust to change in the number of simulations. If we use a different number of simulations, then we have to adjust the denominator in the summarize() function. We can do this by using mutate() instead of filter().\n\nresults %&gt;%\n  mutate(face = rank %in% c(10, \"J\", \"Q\", \"K\", \"A\"))%&gt;%\n  summarize(prob = mean(face))\n\n# A tibble: 1 × 1\n   prob\n  &lt;dbl&gt;\n1 0.382\n\n\nNotice in the mutate() function we are creating a new logical variable called face. This variable takes on the values of TRUE and FALSE. In the next line, we use a summarize() command with the function mean(). In R, a function that requires numeric input takes a logical variable and converts TRUE into 1 and FALSE into 0. Thus, the mean() will find the proportion of TRUE values and that is why we report it as a probability.\nNext, let’s find a card that is 10 or greater and a club.\n\nCards %&gt;%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\"), suit == \"Club\") %&gt;%\n  select(rank, suit)\n\n# A tibble: 5 × 2\n  rank  suit \n  &lt;chr&gt; &lt;chr&gt;\n1 10    Club \n2 J     Club \n3 Q     Club \n4 K     Club \n5 A     Club \n\n\nBy counting, we find the probability of drawing a 10 or greater club is \\(\\frac{5}{52}\\) or 0.0961538.\n\nExercise:\nSimulate drawing one card and estimate the probability of a club that is 10 or greater.\n\nWe can again utilize the previously generated 10,000 samples.\n\nresults %&gt;%\n  mutate(face = (rank %in% c(10, \"J\", \"Q\", \"K\", \"A\")) & (suit == \"Club\"))%&gt;%\n  summarize(prob = mean(face))\n\n# A tibble: 1 × 1\n    prob\n   &lt;dbl&gt;\n1 0.0903\n\n\nAgain, our simulation results are very close to the exact value found by counting.\n\n\n8.4.5 Note\nWe have been using R to count the number of outcomes in an event. This helped us to determine probabilities, but we limited the problems to simple ones. In our cards example, it would be more interesting for us to explore more complex events such as drawing five cards from a standard 52-card deck. Each draw of five cards is equally likely, so in order to find the probability of a flush (five cards of the same suit), we could simply list all the possible flushes and compare that to the entire sample space. Because of the large number of possible outcomes, this becomes difficult. Thus, we need to explore counting rules in more detail to help us solve more complex problems. In this course, we will limit our discussion to three basic cases. You should know that there are entire courses on discrete math and counting rules, so we will still be limited in our methods and the type of problems we can solve in this course.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#counting-rules",
    "href": "Probability-Rules.html#counting-rules",
    "title": "8  Probability Rules",
    "section": "8.5 Counting rules",
    "text": "8.5 Counting rules\nThere are three types of counting problems we will consider. In each case, we are utilizing (and possibly augmenting) the multiplication rule. All that changes is whether an element is allowed to be reused (replacement), and whether the order of selection matters. This latter question is difficult. Each case will be demonstrated with an example.\n\n8.5.1 Rule 1: Order matters, sample with replacement\nThe multiplication rule is at the center of each of the three methods In this first case, we are using the idea that order matters and items can be reused. This is the multiplication rule without any modifications. Let’s use an example to help our understanding.\n\nExample:\nA license plate consists of three numeric digits (0-9) followed by three single letters (A-Z). How many possible license plates exist?\n\nWe can divide this problem into two sections. In the numeric section, we are selecting 3 objects from 10, with replacement. This means that a value can be used more than once. Clearly, order matters because a license plate starting with “432” is distinct from a license plate starting with “234”. There are \\(10^3 = 1000\\) ways to select the first three digits; 10 for the first, 10 for the second, and 10 for the third.\n\nQuestion: Why do we multiply and not add these probabilities?3\n\nIn the alphabet section, we are selecting 3 objects from 26, where again order matters. Thus, there are \\(26^3=17576\\) ways to select the last three letters of the plate. Combined, there are \\(10^3 \\times 26^3 = 17576000\\) ways to select license plates. Visually, \\[\n\\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} = 17,576,000\n\\]\nNext, we are going to use this new counting method to find a probability.\n\nExercise:\nWhat is the probability a license plate starts with the number “8” or “0” and ends with the letter “B”?\n\nIn order to find this probability, we simply need to determine the number of ways to select a license plate starting with “8” or “0” and ending with the letter “B”. We can visually represent this event as: \\[\n\\underbrace{\\underline{\\quad 2 \\quad }}_\\text{8 or 0} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 1 \\quad }}_\\text{B} = 135,200\n\\]\nDividing this number by the total number of possible license plates yields the probability of this event.\n\ndenom &lt;- 10*10*10*26*26*26\nnum &lt;- 2*10*10*26*26*1\nnum / denom\n\n[1] 0.007692308\n\n\nThe probability of obtaining a license plate starting with “8” or “0” and ending with “B” is 0.0077. Simulating this would be difficult because we would need special functions to check the first number and last letter. This gets into text mining, an important subject in data science, but unfortunately we don’t have much time in this book for the topic.\n\n\n8.5.2 Rule 2 (Permutation): Order Matters, Sampling Without Replacement\nConsider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment depends on the order of the outcomes so order matters. The number of ways to select \\(k\\) objects is given by \\(n(n-1)(n-2)...(n-k+1)\\). This is known as a permutation and is sometimes written as \\[\n{}_nP_{k} = \\frac{n!}{(n-k)!}\n\\]\nRecall that \\(n!\\) is read as “\\(n\\) factorial” and represents the number of ways to arrange \\(n\\) objects.\n\nExample:\nTwenty-five friends participate in a Halloween costume party. Three prizes are given during the party: most creative costume, scariest costume, and funniest costume. No one can win more than one prize. How many possible ways can the prizes by distributed?\n\nThere are \\(k=3\\) prizes to be assigned to \\(n=25\\) people. Once someone is selected for a prize, they are removed from the pool of eligible prize winners. In other words, we are sampling without replacement. Also, order matters because there are specific labels on the awards. For example, if Tom, Mike, and Jane win most creative, scariest and funniest costume, respectively, we have a different outcome than if Mike won creative, Jane won scariest and Tom won funniest costume. Thus, the number of ways the prizes can be distributed is given by \\({}_{25}P_3 = \\frac{25!}{22!} = 13,800\\). A way to visualize this expression is shown as: \\[\n\\underbrace{\\underline{\\quad 25 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{funniest} = 13,800\n\\]\nIt is sometimes difficult to determine whether order matters in a problem, but in this example the named prizes were a hint that order matters. This is usually the case when there is some type of label to be attributed to individuals.\nLet’s use the idea of a permutation to calculate a probability.\n\nExercise:\nAssume that all 25 participants are equally likely to win any one of the three prizes. What is the probability that Tom doesn’t win any of them?\n\nJust like in the previous probability calculation, we simply need to count the number of ways Tom doesn’t win any prize. In other words, we need to count the number of ways that prizes are distributed without Tom. So, remove Tom from the group of 25 eligible participants. The number of ways Tom doesn’t get a prize is \\({}_{24}P_3 = \\frac{24!}{21!}=12,144\\). Again visually: \\[\n\\underbrace{\\underline{\\quad 24 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 22 \\quad }}_\\text{funniest} = 12,144\n\\]\nThe probability Tom doesn’t get a prize is simply the second number divided by the first:\n\ndenom &lt;- factorial(25) / factorial(25 - 3)\n# Or, denom &lt;- 25*24*23\nnum &lt;- 24*23*22\nnum / denom\n\n[1] 0.88\n\n\n\n\n8.5.3 Rule 3 (Combination): Order Does Not Matter, Sampling Without Replacement\nConsider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment does not depend on the order of the outcomes (i.e., order does not matter). The number of ways to select \\(k\\) objects is given by \\(\\frac{n!} {(n-k)!k!}\\). This is known as a combination and is written as: \\[\n\\binom{n}{k} = \\frac{n!}{(n-k)!k!}\n\\]\nThis is read as “\\(n\\) choose \\(k\\)”. Take a moment to compare combinations to permutations, discussed in Rule 2. The difference between these two rules is that in a combination, order no longer matters. A combination is equivalent to a permutation divided by \\(k!\\), the number of ways to arrange the \\(k\\) objects selected.\n\nExample:\nSuppose we draw five cards out of a standard 52-card deck (no jokers). How many possible five card hands are there?\n\nIn this example, order does not matter. I don’t care if I receive 3 jacks then 2 queens or 2 queens then 3 jacks. Either way, it’s the same collection of five cards in my hand. Also, we are drawing without replacement. Once a card is selected, it cannot be selected again. Thus, the number of ways to select five cards is given by: \\[\n\\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960\n\\]\n\nExample:\nWhen drawing 5 cards, what is the probability of drawing a “flush” (five cards of the same suit)?\n\nLet’s determine how many ways to draw a flush. Recall there are four suits (clubs, hearts, diamonds and spades) and each suit has 13 ranks or values. We would like to pick five of those 13 cards and 0 of the remaining 39. Let’s consider just one of those suits (clubs): \\[\n\\mbox{P}(\\mbox{5 clubs})=\\frac{\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]\nThe second part of the numerator (\\(\\binom{39}{0}\\)) isn’t necessary, since it simply represents the number of ways to select 0 objects from a group (1 way), but it helps clearly lay out the events. This brings up the point of what \\(0!\\) equals. By definition it is 1. This allows us to use \\(0!\\) in our work.\nNow, we expand this to all four suits by multiplying by 4, or \\(\\binom{4}{1}\\) since we are selecting 1 suit out of the 4: \\[\n\\mbox{P}(\\mbox{flush})=\\frac{\\binom{4}{1}\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]\n\nnum&lt;-4*choose(13,5)*1\ndenom&lt;-choose(52,5)\nnum/denom\n\n[1] 0.001980792\n\n\nThere is a probability of 0.0020 of drawing a flush in a draw of five cards from a standard 52-card deck.\n\nExercise:\nWhen drawing five cards, what is the probability of drawing a “full house” (three cards of the one rank and the two of another)?\n\nThis problem uses several ideas from this chapter. We need to pick the rank of the three of a kind. Then pick 3 cards from the 4 possible. Next we pick the rank of the pair from the remaining 12 ranks. Finally pick 2 cards of that rank from the 4 possible.\n\\[\n\\mbox{P}(\\mbox{full house})=\\frac{\\binom{13}{1}\\binom{4}{3}\\binom{12}{1}\\binom{4}{2}}{\\binom{52}{5}}\n\\]\n\nnum&lt;-choose(13,1)*choose(4,3)*choose(12,1)*choose(4,2)\ndenom&lt;-choose(52,5)\nnum/denom\n\n[1] 0.001440576\n\n\n\nQuestion:\nWhy can’t we use \\(\\binom{13}{2}\\) instead of \\(\\binom{13}{1}\\binom{12}{1}\\)?4\n\nWe have just determined that a full house has a lower probability of occurring than a flush. This is why in gambling, a flush is valued less than a full house.\n\n\n8.5.4 Summary of counting rules\nIt can be difficult to remember which counting rule to use in each situation. Remember to consider whether we are sampling with or without replacement, and whether order matters. Figure 8.3 summarizes the three counting rules we discussed in this chapter.\n\n\n\n\n\n\nFigure 8.3: Summary of three counting rules",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Probability-Rules.html#footnotes",
    "href": "Probability-Rules.html#footnotes",
    "title": "8  Probability Rules",
    "section": "",
    "text": "We call events \\(A\\) and \\(B\\) mutually exclusive. We’ll define this term a little later in the chapter.↩︎\nThe easiest way to create this illustration is to start from the middle intersection piece. \\(P(A\\cap B\\cap C)=0.05\\), represented by the 0.05 at the center of the diagram. \\(P(A\\cap B)=0.2\\), which leaves 0.15 for the rest of the \\(A\\cap B\\) piece of the diagram. Because \\(P(A\\cap C)=0.1\\), the remaining piece of \\(A\\cap C\\) not yet accounted for is 0.05. Then the remaining piece of \\(A\\) is 0.25. Thus, \\(P(A)\\) can be found by summing all the pieces in the \\(A\\) circle, which adds up to 0.5. Make sure you can track how the rest of the diagram is appropriately labeled.↩︎\nMultiplication is repeated addition, so in a sense we are adding. For this problem, every value for the first number has 10 possibilities for the second number, and for every value of the second number there are 10 possibilities for the third number. This is an “and” problem and requires multiplication.↩︎\nBecause this implies the order selection of the ranks does not matter. In other words, this assumes that, for example, three Kings and two fours is the same full house as 3 fours and 2 Kings. This is not true so we break the rank selection about essentially making it a permutation.↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "Conditional-Probability.html",
    "href": "Conditional-Probability.html",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "9.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "Conditional-Probability.html#objectives",
    "href": "Conditional-Probability.html#objectives",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "Define and differentiate between conditional probability and joint probability, and provide real-world examples to illustrate these concepts and their differences.\nCalculate conditional probabilities from given data or scenarios using their formal definition, and interpret these probabilities in the context of practical examples.\nUsing conditional probability, determine whether two events are independent and justify your conclusion with appropriate calculations and reasoning.\nApply Bayes’ Rule to solve problems both mathematically and through simulation usinng R.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "Conditional-Probability.html#conditional-probability",
    "href": "Conditional-Probability.html#conditional-probability",
    "title": "9  Conditional Probability",
    "section": "9.2 Conditional Probability",
    "text": "9.2 Conditional Probability\nSo far, we’ve covered the basic axioms of probability, the properties of events (set theory) and counting rules. Another important concept, perhaps one of the most important, is conditional probability. Often, we know a certain event or sequence of events has occurred and we are interested in the probability of another event.\n\nExample:\nSuppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]\nWhat is the probability that a blue vehicle is selected, given a sedan was selected?\n\nSince we know that a sedan was selected, our sample space has been reduced to just “red sedan” and “blue sedan”. The probability of selecting a blue vehicle out of this sample space is simply 1/2.\nIn set notation, let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. We are looking for \\(\\mbox{P}(A \\mbox{ given } B)\\), which is also written as \\(\\mbox{P}(A|B)\\). By definition, \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)}\n\\]\nIt is important to distinguish between the event \\(A|B\\) and \\(A \\cap B\\). This is a common misunderstanding about probability. \\(A \\cap B\\) is the event that an outcome was selected at random from the total sample space, and that outcome was contained in both \\(A\\) and \\(B\\). On the other hand, \\(A|B\\) assumes the \\(B\\) has occurred, and an outcome was drawn from the remaining sample space, and that outcome was contained in \\(A\\).\nAnother common misunderstanding involves the direction of conditional probability. Specifically, \\(A|B\\) is NOT the same event as \\(B|A\\). For example, consider a medical test for a disease. The probability that someone tests positive given they had the disease is different than the probability that someone has the disease given they tested positive. We will explore this example further in our Bayes’ Rule section.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "Conditional-Probability.html#independence",
    "href": "Conditional-Probability.html#independence",
    "title": "9  Conditional Probability",
    "section": "9.3 Independence",
    "text": "9.3 Independence\nTwo events, \\(A\\) and \\(B\\), are said to be independent if the probability of one occurring does not change the probability of the other occurring. We looked at this idea in the last chapter, but now we have another way of thinking about it using conditional probabilities. For example, let’s say the probability that a randomly selected student has seen the latest superhero movie is 0.55. What if we randomly select a student and we see that he/she is wearing a black backpack? Does that probability change? Likely not, since movie attendance is probably not related to choice of backpack color. These two events are independent.\nMathematically, \\(A\\) and \\(B\\) are considered independent if and only if \\[\n\\mbox{P}(A|B)=\\mbox{P}(A)\n\\]\nResult: \\(A\\) and \\(B\\) are independent if and only if \\[\n\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\n\\]\nThis follows from the definition of conditional probability and from above: \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A\\cap B)}{\\mbox{P}(B)}=\\mbox{P}(A)\n\\]\nThus, \\(\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\\).\n\nExample:\nConsider the rental car example from before. Recall events \\(A\\) and \\(B\\): let \\(A\\) be the event that a blue vehicle is selected and let \\(B\\) be the event that a sedan is selected. Are \\(A\\) and \\(B\\) independent?\n\nNo, events \\(A\\) and \\(B\\) are not independent. First, recall that \\(\\mbox{P}(A|B)=0.5\\). The probability of selecting a blue vehicle (\\(\\mbox{P}(A)\\)) is \\(2/7\\) (the number of blue vehicles in our sample space divided by 7, the total number vehicles in \\(S\\)). This value is different from 0.5; thus, \\(A\\) and \\(B\\) are not independent.\nWe could also use the result above to determine whether \\(A\\) and \\(B\\) are independent. Note that \\(\\mbox{P}(A)= 2/7\\). Also, we know that \\(\\mbox{P}(B)=2/7\\). So, \\(\\mbox{P}(A)\\mbox{P}(B)=4/49\\). But, \\(\\mbox{P}(A\\cap B) = 1/7\\), since there is just one blue sedan in the sample space. \\(4/49\\) is not equal to \\(1/7\\); thus, \\(A\\) and \\(B\\) are not independent.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "Conditional-Probability.html#bayes-rule",
    "href": "Conditional-Probability.html#bayes-rule",
    "title": "9  Conditional Probability",
    "section": "9.4 Bayes’ Rule",
    "text": "9.4 Bayes’ Rule\nAs mentioned in the introduction to this section, \\(\\mbox{P}(A|B)\\) is not the same quantity as \\(\\mbox{P}(B|A)\\). However, if we are given information about \\(A|B\\) and \\(B\\), we can use Bayes’ Rule to find \\(\\mbox{P}(B|A)\\). Let \\(B_1, B_2, ..., B_n\\) be mutually exclusive and exhaustive events and let \\(\\mbox{P}(A)&gt;0\\). Then, \\[\n\\mbox{P}(B_k|A)=\\frac{\\mbox{P}(A|B_k)\\mbox{P}(B_k)}{\\sum_{i=1}^n \\mbox{P}(A|B_i)\\mbox{P}(B_i)}\n\\]\nLet’s use an example to dig into where this comes from.\n\nExample:\nSuppose a doctor has developed a blood test for a certain rare disease (only one out of every 10,000 people have this disease). After careful and extensive evaluation of this blood test, the doctor determined the test’s sensitivity and specificity.\n\nSensitivity is the probability of detecting the disease for those who actually have it. We can also write this as the probability of someone testing positive, given they actually have the disease. Note that this is a conditional probability.\nSpecificity is the probability of correctly identifying “no disease” for those who do not have it. Again, this is a conditional probability.\nSee Figure 9.1 for a visual representation of these terms and others related to what is termed a confusion matrix.\n\n\n\n\n\n\n\n\nFigure 9.1: A table of true results and test results for a hypothetical disease. The terminology is included in the table. These ideas are important when evaluating machine learning classification models.\n\n\n\n\n\nIn fact, this test had a sensitivity of 100% and a specificity of 99.9%. Now suppose a patient walks in, the doctor administers the blood test, and it returns positive. What is the probability the patient actually has the disease?\nThis is a classic example of how probability could be misunderstood. Upon reading this question, you might guess that the answer to our question is quite high. After all, this is a nearly perfect test. After exploring the problem more in depth, we find a different result.\n\n9.4.1 Approach using whole numbers\nWithout going directly to the formulaic expression above, let’s consider a collection of 100,000 randomly selected people. What do we know?\n\nBased on the prevalence of this disease (one out of every 10,000 people have this disease), we know that 10 of them should have the disease.\nThis test is perfectly sensitive. Thus, of the 10 people that have the disease, all of them test positive.\nThis test has a specificity of 99.9%. Of the 99,990 that don’t have the disease, \\(0.999*99990\\approx 99890\\) will test negative. The remaining 100 will test positive.\n\nThus, of our 100,000 randomly selected people, 110 will test positive. Of these 110, only 10 actually have the disease. Thus, the probability that someone has the disease given they’ve tested positive is actually around \\(10/110 = 0.0909\\).\n\n\n9.4.2 Simulation\nTo do the simulation, we can think of it as flipping a coin. First let’s assume we are pulling 1,000,000 people from the population. The probability that any one person has the disease is 0.0001. We will use rflip() to get 1,000,000 people and designate them as “no disease” or “disease”.\n\nset.seed(43)\nresults &lt;- rflip(1000000, 0.0001, summarize = TRUE)\nresults\n\n      n heads  tails  prob\n1 1e+06   100 999900 1e-04\n\n\nIn this case, 100 people had the disease (heads). Now, let’s find the positive test results. Of the 100 with the disease, all will test positive. Of those without disease, there is a 0.001 probability of testing positive.\n\nrflip(as.numeric(results['tails']), prob = 0.001, summarize = TRUE)\n\n       n heads  tails  prob\n1 999900   959 998941 0.001\n\n\nNow, 959 of those without the disease tested positive. Thus, the probability of having the disease given a positive test result is approximately:\n\n100 / (100 + 959)   # number with disease divided by total number who tested positive\n\n[1] 0.09442871\n\n\n\n\n9.4.3 Mathematical approach\nNow let’s put this in context of Bayes’ Rule as stated above. First, let’s define some events. Let \\(D\\) be the event that someone has the disease. Thus, \\(D'\\) would be the event that someone does not have the disease. Similarly, let \\(T\\) be the event that someone has tested positive. What do we already know? \\[ \\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D')=0.9999  \\] \\[ \\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T'|D)=0 \\] \\[ \\mbox{P}(T'|D')=0.999 \\hspace{1cm} \\mbox{P}(T|D') = 0.001 \\]\nWe are looking for \\(\\mbox{P}(D|T)\\), the probability that someone has the disease, given he/she has tested positive. By the definition of conditional probability, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)} \\]\nThe numerator can be rewritten, again utilizing the definition of conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\).\nThe denominator can be rewritten using the Law of Total Probability (discussed in Section 8.4.2) and then the definition of conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D') = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')\\). So, putting it all together, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')} \\]\nNow we have stated our problem in the context of quantities we know: \\[ \\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909 \\]\nNote that in the original statement of Bayes’ Rule, we considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). In this example, we only have two: \\(D\\) and \\(D'\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "Discrete-Random-Variables.html",
    "href": "Discrete-Random-Variables.html",
    "title": "10  Discrete Random Variables",
    "section": "",
    "text": "10.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "Discrete-Random-Variables.html#objectives",
    "href": "Discrete-Random-Variables.html#objectives",
    "title": "10  Discrete Random Variables",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as random variable, discrete random variable, continuous random variable, sample space/support, probability mass function, cumulative distribution function, moment, expectation, mean, and variance, and construct examples to demonstrate their proper use in context.\nFor a given discrete random variable, derive and interpret the probability mass function (pmf) and apply this function to calculate the probabilities of various events.\nSimulate random variables for a discrete distribution using R.\nCalculate and interpret the moments, such as expected value/mean and variance, of a discrete random variable.\nCalculate and interpret the expected value/mean and variance of a linear transformation of a random variable.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "Discrete-Random-Variables.html#random-variables",
    "href": "Discrete-Random-Variables.html#random-variables",
    "title": "10  Discrete Random Variables",
    "section": "10.2 Random variables",
    "text": "10.2 Random variables\nWe have already discussed random experiments. We have also discussed \\(S\\), the sample space for an experiment. A random variable essentially maps the events in the sample space to the real number line. For a formal definition: A random variable \\(X\\) is a function \\(X: S\\rightarrow \\mathbb{R}\\) that assigns exactly one number to each outcome in an experiment.\n\nExample:\nSuppose you flip a coin three times. The sample space, \\(S\\), of this experiment is \\[\nS=\\{\\mbox{HHH}, \\mbox{HHT}, \\mbox{HTH}, \\mbox{HTT}, \\mbox{THH}, \\mbox{THT}, \\mbox{TTH}, \\mbox{TTT}\\}\n\\]\nLet the random variable \\(X\\) be the number of heads in three coin flips.\n\nWhenever introduced to a new random variable, we should take a moment to think about what possible values \\(X\\) can take. When flipping a coin three times, we can get zero, one, two, or three heads. The random variable \\(X\\) assigns each outcome in our experiment to one of these values. We can visualize the possible outcomes and possible values of \\(X\\): \\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]\nThe sample space of \\(X\\) is the list of numerical values that \\(X\\) can take: \\[\nS_X=\\{0,1,2,3\\}\n\\]\nBecause the sample space of \\(X\\) is a countable list of numbers, we consider \\(X\\) to be a discrete random variable (more on that later).\n\nQuestion:\nWhat are some other examples of random variables for this coin flip experiment? What are some counterexamples?1\n\n\n10.2.1 How does this help?\nSticking with our example, we can now frame a problem of interest in the context of our random variable \\(X\\). For example, suppose we wanted to know the probability of at least two heads. Without our random variable, we have to write this as: \\[\n\\mbox{P}(\\mbox{at least two heads})= \\mbox{P}(\\{\\mbox{HHH},\\mbox{HHT},\\mbox{HTH},\\mbox{THH}\\})\n\\]\nIn the context of our random variable, this simply becomes \\(\\mbox{P}(X\\geq 2)\\). It may not seem important in a case like this, but imagine if we were flipping a coin 50 times and wanted to know the probability of obtaining at least 30 heads. It would not be feasible to write out all possible ways to obtain at least 30 heads. It is much easier to write \\(\\mbox{P}(X\\geq 30)\\) and then explore the distribution of \\(X\\).\nEssentially, a random variable often helps us reduce a complex random experiment to a simple variable that is easy to characterize.\n\n\n10.2.2 Discrete versus continuous random variables\nA discrete random variable has a sample space that consists of a countable set of values. \\(X\\) in our example above is a discrete random variable. Note that “countable” does not necessarily mean “finite”. For example, a random variable with a Poisson distribution (a topic for a later chapter) has a sample space of \\(\\{0,1,2,...\\}\\). This sample space is unbounded, but it is considered countably infinite, and thus the random variable would be considered discrete.\nA continuous random variable has a sample space that is a continuous interval. For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. \\(Y\\) is a continuous random variable because a person could measure 68.1 inches, 68.2 inches, or perhaps any value in between. Note that when we measure height, our precision is limited by our measuring device, so we are technically “discretizing” height. However, even in these cases, we typically consider height to be a continuous random variable.\nA mixed random variable is exactly what it sounds like. It has a sample space that is both discrete and continuous. How could such a thing occur? Consider an experiment where a person rolls a standard six-sided die. If it lands on anything other than one, the result of the die roll is recorded. If it lands on one, the person spins a wheel, and the angle in degrees of the resulting spin, divided by 360, is recorded. If our random variable \\(Z\\) is the number that is recorded in this experiment, the sample space of \\(Z\\) is \\([0,1] \\cup \\{2,3,4,5,6\\}\\). We will not be spending much time on mixed random variables in this course. However, they do occur in practice. Consider the job of analyzing bomb error data. If the bomb hits within a certain radius, the error is 0. Otherwise, the error is measured in a radial direction. This is a mixed random variable.\n\n\n10.2.3 Discrete distribution functions\nNow that we have defined a random variable, we need a way to describe its behavior. We will use probabilities for this purpose.\nDistribution functions describe the behavior of random variables. We can use these functions to determine the probability that a random variable takes a value or range of values. For discrete random variables, there are two distribution functions of interest: the probability mass function (pmf) and the cumulative distribution function (cdf).\n\n\n10.2.4 Probability mass function\nLet \\(X\\) be a discrete random variable. The probability mass function (pmf) of \\(X\\), given by \\(f_X(x)\\), is a function that assigns probability to each possible outcome of \\(X\\). \\[\nf_X(x)=\\mbox{P}(X=x)\n\\]\nNote that the pmf is a function. Functions have input and output. The input of a pmf is any real number. The output of a pmf is the probability that the random variable takes the inputted value. The pmf must follow the axioms of probability described in the Chapter 8. Primarily,\n\nFor all \\(x \\in \\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\). That is, for all values of \\(x\\), the outcome of \\(f_X(x)\\) is between 0 and 1.\n\\(\\sum_x f_X(x) = 1\\), where the \\(x\\) in the index of the sum simply denotes that we are summing across the entire sample space of \\(X\\). In words, all the probabilities must sum to one.\n\n\nExample:\nRecall our coin flip example again. We flip a coin three times. Let \\(X\\) be the number of heads. We know that \\(X\\) can only take values 0, 1, 2 or 3. But at what probability does it take these three values? We previously listed out the possible outcomes of the experiment and denoted the value of \\(X\\) corresponding to each outcome. \\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]\n\nEach of these eight outcomes is equally likely (each with a probability of \\(\\frac{1}{8}\\)). Thus, building the pmf of \\(X\\) becomes a matter of counting the number of outcomes associated with each possible value of \\(X\\): \\[\nf_X(x)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{1}{8}, & x=0 \\\\\n\\frac{3}{8}, & x=1 \\\\\n\\frac{3}{8}, & x=2 \\\\\n\\frac{1}{8}, & x=3 \\\\\n0, & \\mbox{otherwise} \\end{array} \\right .\n\\]\nNote that this function specifies the probability that \\(X\\) takes any of the four values in the sample space (0, 1, 2, and 3). It also specifies that the probability \\(X\\) takes any other value is 0.\nGraphically, the pmf is not terribly interesting. The pmf is 0 at all values of \\(X\\) except for 0, 1, 2 and 3, Figure 10.1 .\n\n\n\n\n\n\n\n\nFigure 10.1: Probability Mass Function of \\(X\\) from Coin Flip Example\n\n\n\n\n\n\nExample:\nWe can use a pmf to answer questions about an experiment. For example, consider the same coin flip context. What is the probability that we flip at least one head? We can write this in the context of \\(X\\): \\[\n\\mbox{P}(\\mbox{at least one head})=\\mbox{P}(X\\geq 1)=\\mbox{P}(X=1)+\\mbox{P}(X=2)+\\mbox{P}(X=3)\n\\] \\[=\\frac{3}{8} + \\frac{3}{8}+\\frac{1}{8}=\\frac{7}{8}\n\\]\nAlternatively, we can recognize that \\(\\mbox{P}(X\\geq 1)=1-\\mbox{P}(X=0)=1-\\frac{1}{8}=\\frac{7}{8}\\).\n\n\n\n10.2.5 Cumulative distribution function\nLet \\(X\\) be a discrete random variable. The cumulative distribution function (cdf) of \\(X\\), given by \\(F_X(x)\\), is a function2 that assigns to each value of \\(X\\) the probability that \\(X\\) takes that value or lower: \\[\nF_X(x)=\\mbox{P}(X\\leq x)\n\\]\nIf we know the pmf, we can obtain the cdf: \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\sum_{y\\leq x} f_X(y) \\]\nThe main idea behind the cdf is a summation of probabilities of interest. In this course, we de-emphasize the derivation of and symbolic notation for the cdf. We instead focus on using the idea of a cdf to calculate probabilities.\n\nExample:\nFind the following probabilities for the coin flip example: \\(P(X\\leq 2)\\), \\(P(X &lt; 1)\\), and \\(P(1\\leq X &lt; 3)\\).\nThe easiest way to find these probabilities is by summing the probabilities of interest.\n\\[\nP(X\\leq 2) = P(X=0) + P(X=1) + P(X=2) = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}\n\\]\n\\[\nP(X &lt; 1) = P(X = 0) = \\frac{1}{8}\n\\]\n\\[\nP(1\\leq X &lt; 3) = P(X=1) + P(X=2) = \\frac{3}{8} + \\frac{3}{8} = \\frac{6}{8}\\quad\\text{or}\\quad \\frac{3}{4}\n\\]\nWe could also find these probabilities using the symbolic form of the cdf,3 but the method shown above is much more intuitive. Instead of worrying about the signs (e.g., \\(\\leq\\) versus \\(&lt;\\)) in the probability statements and what that means for utilizing the cdf, we can simply use the pmf and sum the appropriate probabilities.\n\n\n\n10.2.6 Simulating random variables\nWe can get an estimate of the distribution of a random variable via simulation.\n\nExample:\nSimulate a random variable for the number of heads in flipping a coin three times.\n\nWe can use the rflip() function to simulate coin tosses.4 Remember to ask yourself, what do we want R to do and what does R need to do that? We can use ? or help() to access the documentation for the rflip() function. We need to provide n, the number of coins to toss, and prob, the probability of heads on each toss. There are other optional arguments we can provide as well if desired.\nLet’s do one simulation, flipping a coin three times.\n\nset.seed(1)\nrflip(n = 3, prob = 0.5)\n\n\nFlipping 3 coins [ Prob(Heads) = 0.5 ] ...\n\nT T H\n\nNumber of Heads: 1 [Proportion Heads: 0.333333333333333]\n\n\nNow we need to do this many times. We can use the do() function to help us repeat the process of flipping three coins.\n\nset.seed(1)\nresults &lt;- do(10000)*rflip(n = 3, prob = 0.5)\nhead(results)\n\n  n heads tails      prop\n1 3     2     1 0.6666667\n2 3     3     0 1.0000000\n3 3     0     3 0.0000000\n4 3     2     1 0.6666667\n5 3     2     1 0.6666667\n6 3     2     1 0.6666667\n\n\nEach row of our results represents one of the 10,000 simulations, containing the number of flips, the number of heads, the number of tails, and the probability of heads. We can now plot the estimated distribution of \\(X\\), the number of heads in flipping a coin three times, using the heads column from our results. We use gf_props() to display the proportion (out of 10,000 simulations) of each outcome (0, 1, 2, or 3 heads).\n\n\n\n\n\n\n\n\nFigure 10.2: The estimated probability mass function of X from the coin flip example\n\n\n\n\n\nWe can also calculate probabilities like \\(P(\\text{at least one head})\\) via simulation by simply counting up how many times, out of 10,000 simulations, we flipped at least one head.\n\nresults %&gt;% \n  summarize(Prob = count(heads &gt;= 1) / 10000)\n\n    Prob\n1 0.8716\n\n\nWe see the estimated probability is very close to the exact value of \\(\\frac{7}{8} = 0.875\\) found earlier.\nAn alternative method for this simulation uses the inverse transform method, which requires deriving the cdf and utilizing the inverse transform method.5 Because we have de-emphasized the cdf in this book, we’ll focus on simulating the pmf directly whenever possible.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "Discrete-Random-Variables.html#moments",
    "href": "Discrete-Random-Variables.html#moments",
    "title": "10  Discrete Random Variables",
    "section": "10.3 Moments",
    "text": "10.3 Moments\nDistribution functions are excellent characterizations of random variables. The pmf and cdf will tell you exactly how often the random variable takes particular values. However, distribution functions are often a lot of information. Sometimes, we may want to describe a random variable \\(X\\) with a single value or small set of values. For example, we may want to know some measure of center of \\(X\\). We also may want to know a measure of spread of \\(X\\). Moments are values that summarize random variables with single numbers. Since we are dealing with the population, these moments are population values and not summary statistics as we used in the first block of material.\n\n10.3.1 Expectation\nAt this point, we should define the term expectation. Let \\(g(X)\\) be some function of a discrete random variable \\(X\\). The expected value of \\(g(X)\\) is given by: \\[\n\\mbox{E}(g(X))=\\sum_x g(x) \\cdot f_X(x)\n\\]\nNote that \\(g(X)\\) can be almost any function of \\(X\\), such as \\(X - 3\\) or \\(X^2\\).\n\n\n10.3.2 Mean\nThe most common moments used to describe random variables are mean and variance. The mean (often referred to as the expected value of \\(X\\)), is simply the average value of a random variable. It is denoted as \\(\\mu_X\\) or \\(\\mbox{E}(X)\\). In the discrete case, the mean is found by: \\[\n\\mu_X=\\mbox{E}(X)=\\sum_x x \\cdot f_X(x)\n\\]\nWe are using \\(\\mu\\) because it is a population parameter. The mean is also known as the first moment of \\(X\\) around the origin. It is a weighted sum with the weight being the probability of each outcome.\n\nExample:\nFind the expected value (or mean) of \\(X\\): the number of heads in three flips of a fair coin. \\[\n\\mbox{E}(X)=\\sum_x x\\cdot f_X(x) = 0*\\frac{1}{8} + 1*\\frac{3}{8} + 2*\\frac{3}{8} + 3*\\frac{1}{8}=1.5\n\\]\n\nFrom our simulation before, we can find the mean as an estimate of the expected value. This is really a statistic since we simulated data from the population and thus, will have variance from sample to sample.\n\nmean(~heads, data = results)\n\n[1] 1.4878\n\n\nOur estimate is pretty close to the exact value found mathematically.\n\n\n10.3.3 Variance\nVariance is a measure of the spread of a random variable. The variance of \\(X\\) is denoted as \\(\\sigma^2_X\\) or \\(\\mbox{Var}(X)\\). It is equivalent to the average squared deviation from the mean: \\[\n\\sigma^2_X=\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\n\\]\nIn the discrete case, this can be evaluated by: \\[\n\\mbox{E}[(X-\\mu_X)^2]=\\sum_x (x-\\mu_X)^2f_X(x)\n\\]\nVariance is known as the second moment of \\(X\\) around the mean. It can also be shown6 that\n\\[\n\\mbox{Var}(X) = E(X^2) - \\big[E(X)\\big]^2\n\\]\nThe square root of \\(\\mbox{Var}(X)\\) is denoted as \\(\\sigma_X\\), the standard deviation of \\(X\\). The standard deviation is often reported because it is measured in the same units as \\(X\\), while the variance is measured in squared units and is thus harder to interpret.\n\nExample:\nFind the variance of \\(X\\): the number of heads in three flips of a fair coin.\n\n\\[\n\\mbox{Var}(X)=\\sum_x (x-\\mu_X)^2 \\cdot f_X(x)\n\\]\n\\[\n= (0-1.5)^2 \\times \\frac{1}{8} + (1-1.5)^2 \\times \\frac{3}{8}+(2-1.5)^2 \\times \\frac{3}{8} + (3-1.5)^2\\times \\frac{1}{8}\n\\] In R this is:\n\n(0-1.5)^2*1/8 + (1-1.5)^2*3/8 + (2-1.5)^2*3/8 + (3-1.5)^2*1/8\n\n[1] 0.75\n\n\nThe variance of \\(X\\) is 0.75.\nWe can find the variance of the simulation too, but the var() function in R calculates the sample variance. To find the population variance, we need to multiply by \\(\\frac{n-1}{n}\\).\n\nvar(~heads, data = results)*(10000 - 1) / 10000\n\n[1] 0.7504512\n\n\nAgain, our simulated results are very similar to the exact value found mathematically.\n\n\n10.3.4 Mean and variance of Linear Transformations\nLemma: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Then: \\[\n\\mbox{E}(aX+b)=a\\mbox{E}(X)+b\n\\] and \\[\n\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\n\\]\nThe proof of this lemma is left to the reader.\n\nExample:\nConsider our coin flip example with \\(X\\) as the number of heads in three coin flips. Find \\(E(2X + 3)\\) and \\(\\mbox{Var}(2X + 3)\\).\n\nUsing the lemma above, we find\n\\[\nE(2X + 3) = 2E(X) + 3 = 2\\times(1.5) + 3 = 6\n\\]\n\\[\n\\mbox{Var}(2X + 3) = 2^2\\mbox{Var}(X) = 4\\times (0.75) = 3\n\\]\nWe can also use our simulation:\n\nresults %&gt;% \n  mutate(gx = 2*heads + 3) %&gt;% \n  summarize(mean = mean(gx), var = var(gx)*(10000 - 1)/10000)\n\n    mean      var\n1 5.9756 3.001805",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "Discrete-Random-Variables.html#footnotes",
    "href": "Discrete-Random-Variables.html#footnotes",
    "title": "10  Discrete Random Variables",
    "section": "",
    "text": "Remember, a random variable must assign each outcome in the experiment to exactly one number. Other examples of random variables when flipping a coin three times are 1) the number of tails in three coin flips, 2) the number of times the result changes (from heads to tails, or from tails to heads), 3) the position of the first head (i.e., does the first head occur on the first, second, or third coin flip?), or 4) the longest run of consecutive heads. Can you think of other examples?\nSome counterexamples (or non-examples) of a random variable for this experiment are 1) whether the coin flip results in heads or tails, and 2) the position of each head in the three flips. Because the first example does not map the possible outcomes to a single number, this is not a random variable. The second is also not a random variable because it maps the possible outcomes to multiple numbers (e.g., if a head occurs on the first and third flip ,the result would be 1 and 3).↩︎\nAgain, note that the cdf is a function with an input and output. The input of a cdf is any real number. The output of a cdf is the probability that the random variable takes the inputted value or less.\nLike the pmf, the value of the cdf must be between 0 and 1. Also, since the pmf is always non-negative, the cdf must be non-decreasing.↩︎\nObtain and plot the cdf of \\(X\\) from the previous example. \\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, & x &lt;0 \\\\\n\\frac{1}{8}, & 0\\leq x &lt; 1 \\\\\n\\frac{4}{8}, & 1\\leq x &lt; 2 \\\\\n\\frac{7}{8}, & 2\\leq x &lt; 3 \\\\\n1, & x\\geq 3 \\end{array}\\right .\n\\]\nVisually, the cdf of a discrete random variable has a stair-step appearance. In this example, the cdf takes a value 0 up until \\(X=0\\), at which point the cdf increases to 1/8. It stays at this value until \\(X=1\\), and so on. At and beyond \\(X=3\\), the cdf is equal to 1.\n\n\n\nCumulative distribution function of X from the coin flip example.\n\n\n↩︎\nWe could also simulate flipping a coin three times by sampling from a vector such as c(\"H\", \"T\"). We would sample with replacement (since we can get heads or tails for each coin flip) three times, and repeat this process many times.↩︎\nSince the range of the cdf is in the interval \\([0,1]\\) we will generate a random number in that same interval and then use the inverse function to find the value of the random variable. The pseudo code is:\n\nGenerate a random number, \\(U\\).\nFind the index \\(k\\) such that \\(\\sum_{j=1}^{k-1}f_X(x_{j}) \\leq U &lt; \\sum_{j=1}^{k}f_X(x_{j})\\) or \\(F_x(k-1) \\leq U &lt; F_{x}(k)\\).\n\nIn R, we generate the pmf and cdf as vectors. We then generate a random number between 0 and 1. Next, we use the random number to find the value of the random variable as described above. We do this 10,000 (or some other large number) times and plot the estimated pmf.↩︎\nShow that \\(\\mbox{Var}(X)\\) is equal to \\(E(X^2) - [E(X)]^2\\).\n$$ (X) = E= E= E(X^2) - E(2*{X}X) + E(*^2)$$$$\nThe quantity \\(\\mu_{X}\\) is a constant with respect to \\(X\\), so\n\\[\n\\mbox{Var}(X) = E(X^2) - 2\\mu_{X}E(X) + \\mu_{X}^2 = E(X^2) - 2\\mu_{X}^2 + \\mu_{X}^2 = E(X^2) - \\mu_{X}^2\n\\]\nAnd we recognize that \\(\\mu_{X}^2\\) is just \\([E(X)]^2\\).↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "Named-Discrete-Distributions.html",
    "href": "Named-Discrete-Distributions.html",
    "title": "12  Named Discrete Distributions",
    "section": "",
    "text": "12.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "Named-Discrete-Distributions.html#objectives",
    "href": "Named-Discrete-Distributions.html#objectives",
    "title": "12  Named Discrete Distributions",
    "section": "",
    "text": "Differentiate between common discrete distributions (Uniform, Binomial, and Poisson) by identifying their parameters, assumptions, and moments. Evaluate scenarios to determine the most appropriate distribution to model various types of data.\nApply R to calculate probabilities and quantiles, and simulate random variables for common discrete distributions.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "Named-Discrete-Distributions.html#named-distributions",
    "href": "Named-Discrete-Distributions.html#named-distributions",
    "title": "12  Named Discrete Distributions",
    "section": "12.2 Named distributions",
    "text": "12.2 Named distributions\nIn the previous two chapters, we introduced the concept of random variables, distribution functions, and expectations. In some cases, the nature of an experiment may yield random variables with common distributions. In these cases, we can rely on easy-to-use distribution functions and built-in R functions in order to calculate probabilities and quantiles.\n\n12.2.1 Discrete uniform distribution\nThe first distribution we will discuss is the discrete uniform distribution. It is not a very commonly used distribution, especially compared to its continuous counterpart. A discrete random variable has the discrete uniform distribution if probability is evenly allocated to each value in the sample space. A variable with this distribution has parameters \\(a\\) and \\(b\\) representing the minimum and maximum of the sample space, respectively. (By default, that sample space is assumed to consist of integers only, but that is by no means always the case.)\n\nExample:\nRolling a fair six-sided die is an example of the discrete uniform. Each side of the die has an equal probability.\n\nLet \\(X\\) be a discrete random variable with the uniform distribution. If the sample space is consecutive integers, this distribution is denoted as \\(X\\sim\\textsf{DUnif}(a,b)\\). The pmf of \\(X\\) is given by: \\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{b-a+1}, & x \\in \\{a, a+1,...,b\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nThe expected value of \\(X\\) is found by: \\[\n\\mbox{E}(X)=\\sum_{x=a}^b x\\cdot\\frac{1}{b-a+1}= \\frac{1}{b-a+1} \\cdot \\sum_{x=a}^b x=\\frac{1}{b-a+1}\\cdot\\frac{b-a+1}{2}\\cdot (a+b) = \\frac{a+b}{2},\n\\]\nwhere the sum of consecutive integers is a common result from discrete math. Research on your own for more information.\nThe variance of \\(X\\) is found by: (derivation not included) \\[\n\\mbox{Var}(X)=\\mbox{E}[(X-\\mbox{E}(X))^2]=\\frac{(b-a+1)^2-1}{12}\n\\]\nLet’s summarize the discrete uniform distribution for the die:\nLet \\(X\\) be the result of a single roll of a fair six-sided die. We will report the distribution of \\(X\\), the pmf, \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\).\nThe sample space of \\(X\\) is \\(S_X=\\{1,2,3,4,5,6\\}\\). Since each of those outcomes is equally likely, \\(X\\) follows the discrete uniform distribution with \\(a=1\\) and \\(b=6\\). Thus, \\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6}, & x \\in \\{1,2,3,4,5,6\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nFinally, \\(\\mbox{E}(X)=\\frac{1+6}{2}=3.5\\). Also, \\(\\mbox{Var}(X)=\\frac{(6-1+1)^2-1}{12}=\\frac{35}{12}=2.917\\).\n\n\n12.2.2 Simulating\nTo simulate the discrete uniform, we use sample().\n\nExample:\nTo simulate rolling a die 4 times, we use sample().\n\n\nset.seed(61)\nsample(1:6, size = 4, replace = TRUE)\n\n[1] 4 2 2 1\n\n\nLet’s roll it 10,000 times and find the distribution and its moments.\n\nresults &lt;- do(10000)*sample(1:6, size = 1, replace = TRUE)\n\nThe pmf can be found using tally(). We see that each outcome has approximately the same probability, near the true value of \\(1/6\\).\n\ntally(~sample, data = results, format = \"percent\")\n\nsample\n    1     2     3     4     5     6 \n16.40 16.46 16.83 17.15 16.92 16.24 \n\n\nWe can find the moments by taking the mean and variance of the 10,000 simulations.\n\nmean(~sample, data = results)\n\n[1] 3.5045\n\n\n\nvar(~sample, data = results)*(10000 - 1) / 10000\n\n[1] 2.87598\n\n\nNotice we multiply by \\(\\frac{(10000-1)}{10000}\\) when finding the variance. This is because the function var() is calculating a sample variance using \\(n-1\\) in the denominator but we need the population variance.\n\n\n12.2.3 Binomial distribution\nThe binomial distribution is extremely common, and appears in many situations. In fact, we have already discussed several examples where the binomial distribution is heavily involved.\nConsider an experiment involving repeated independent trials of a binary process (two outcomes), where in each trial, there is a constant probability of “success” (one of the outcomes, which is arbitrary). If the random variable \\(X\\) represents the number of successes out of \\(n\\) independent trials, then \\(X\\) is said to follow the binomial distribution with parameters \\(n\\) (the number of trials) and \\(p\\) (the probability of a success in each trial).\nThe pmf of \\(X\\) is given by: \\[\nf_X(x)=\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}\n\\]\nfor \\(x \\in \\{0,1,...,n\\}\\) and 0 otherwise.\nLet’s take a moment to dissect this pmf. We are looking for the probability of obtaining \\(x\\) successes out of \\(n\\) trials. The \\(p^x\\) represents the probability of \\(x\\) successes, using the multiplication rule because of the independence assumption. The term \\((1-p)^{n-x}\\) represents the probability of the remainder of the trials as failures. Finally, the \\(n\\choose x\\) term represents the number of ways to obtain \\(x\\) successes out of \\(n\\) trials. For example, there are three ways to obtain 1 success out of 3 trials (one success followed by two failures; one failure, one success and then one failure; or two failures followed by a success).\nThe expected value of a binomially distributed random variable is given by \\(\\mbox{E}(X)=np\\) and the variance is given by \\(\\mbox{Var}(X)=np(1-p)\\).\n\nExample:\nLet \\(X\\) be the number of heads out of 20 independent flips of a fair coin. Note that this is a binomial because the trials are independent and the probability of success, in this case a heads, is constant, and there are two outcomes. Find the distribution, mean and variance of \\(X\\). Find \\(\\mbox{P}(X=8)\\). Find \\(\\mbox{P}(X\\leq 8)\\).\n\n\\(X\\) has the binomial distribution with \\(n=20\\) and \\(p=0.5\\). The pmf is given by: \\[\nf_X(x)=\\mbox{P}(X=x)={20 \\choose x}0.5^x (1-0.5)^{20-x}\n\\]\nAlso, \\(\\mbox{E}(X)=20*0.5=10\\) and \\(\\mbox{Var}(X)=20*0.5*0.5=5\\).\nTo find \\(\\mbox{P}(X=8)\\), we can simply use the pmf: \\[\n\\mbox{P}(X=8)=f_X(8)={20\\choose 8}0.5^8 (1-0.5)^{12}\n\\]\n\nchoose(20, 8)*0.5^8*(1 - 0.5)^12\n\n[1] 0.1201344\n\n\nTo find \\(\\mbox{P}(X\\leq 8)\\), we would need to find the cumulative probability: \\[\n\\mbox{P}(X\\leq 8)=\\sum_{x=0}^8 {20\\choose 8}0.5^x (1-0.5)^{20-x}\n\\]\n\nx &lt;- 0:8\nsum(choose(20, x)*0.5^x*(1 - 0.5)^(20 - x))\n\n[1] 0.2517223\n\n\n\n\n12.2.4 Software Functions\nOne of the advantages of using named distributions is that most software packages have built-in functions that compute probabilities and quantiles for common named distributions. Over the course of this chapter, you will notice that each named distribution is treated similarly in R. There are four main functions tied to each distribution. For the binomial distribution, these are dbinom(), pbinom(), qbinom(), and rbinom().\ndbinom(): This function is equivalent to the probability mass function for discrete random variables. We use this to find \\(\\mbox{P}(X=x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes three inputs: x (the value of the random variable), size (the number of trials, \\(n\\)), and prob (the probability of success, \\(p\\)). So, \\[\n\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}=\\textsf{dbinom(x,n,p)}\n\\]\npbinom(): This function is equivalent to the cumulative distribution function. We use this to find \\(\\mbox{P}(X\\leq x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes the same inputs as dbinom() but returns the cumulative probability: \\[\n\\mbox{P}(X\\leq x)=\\sum_{k=0}^x{n\\choose{k}}p^k(1-p)^{n-k}=\\textsf{pbinom(x,n,p)}\n\\]\nqbinom(): This is the inverse of the cumulative distribution function and will return a percentile. This function has three inputs: p (a probability), size and prob. It returns the smallest value \\(x\\) such that \\(\\mbox{P}(X\\leq x) \\geq p\\).\nrbinom(): This function is used to randomly generate values from the binomial distribution. It takes three inputs: n (the number of values to generate), size and prob. It returns a vector containing the randomly generated values.\nTo learn more about these functions, type ? followed by the function name in the console or use the help() function.\n\nExercise:\nUse the built-in functions for the binomial distribution to plot the pmf of \\(X\\) from the previous coin flip example. Also, use the built-in functions to compute the probabilities from the example.\n\nFigure 12.1 displays the pmf of a binomial distribution, where \\(X\\) is the number of heads out of 20 independent flips of a fair coin \\((p=0.5)\\). The gf_dist() function helps us plot distributions by specifying the distribution name (or an abbreviation of it) and its parameters.\n\ngf_dist(\"binom\", size = 20, prob = 0.5) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"X\", y = \"P(X = x)\")\n\n\n\n\n\n\n\nFigure 12.1: The pmf of a binomial random variable\n\n\n\n\n\nWe use dbinom() to calculate probabilities of exact values such as \\(P(X = 8)\\). We use pbinom() to calculate cumulative probabilities such as \\(P(X\\leq 8)\\). We can also find \\(P(X\\leq 8)\\) by using dbinom() to calculate \\(P(X=0), P(X=1), ..., P(X=8)\\) and then sum them up.\n\n###P(X = 8)\ndbinom(8, 20, 0.5)\n\n[1] 0.1201344\n\n###P(X &lt;= 8)\npbinom(8, 20, 0.5)\n\n[1] 0.2517223\n\n## or \nsum(dbinom(0:8, 20, 0.5))\n\n[1] 0.2517223\n\n\n\n\n12.2.5 Poisson distribution\nThe Poisson distribution is very common when considering count or arrival data. Consider a random process where events occur according to some rate over time (think arrivals to a retail register). Often, these events are modeled with the Poisson process. The Poisson process assumes a consistent rate of arrival and a memoryless arrival process (the time until the next arrival is independent of time since the last arrival). If we assume a particular process is a Poisson process, then there are two random variables that take common named distributions. The number of arrivals in a specified amount of time follows the Poisson distribution. Also, the amount of time until the next arrival follows the exponential distribution. We will defer discussion of the exponential distribution until the next chapter. What is random in the Poisson is the number of occurrences while the interval is fixed. That is why it is a discrete distribution. The parameter \\(\\lambda\\) is the average number of occurrences in the specific interval. Note that the interval must be the same as is specified in the random variable.\nLet \\(X\\) be the number of arrivals in a length of time, \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals in length of time \\(T\\). Then \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\): \\[\nX\\sim \\textsf{Poisson}(\\lambda)\n\\]\nThe pmf of \\(X\\) is given by: \\[\nf_X(x)=\\mbox{P}(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}, \\hspace{0.5cm} x=0,1,2,...\n\\]\nOne unique feature of the Poisson distribution is that \\(\\mbox{E}(X)=\\mbox{Var}(X)=\\lambda\\).\n\nExample:\nSuppose fleet vehicles arrive to a maintenance garage at an average rate of 0.4 per day. Let’s assume that these vehicles arrive according to a Poisson process. Let \\(X\\) be the number of vehicles that arrive to the garage in a week (7 days). Notice that the time interval has changed! What is the random variable \\(X\\)? What is the distribution (with parameter) of \\(X\\). What are \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\)? Find \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), and \\(\\mbox{P}(2 \\leq X \\leq 8)\\). Also, find the median of \\(X\\), and the 95th percentile of \\(X\\) (the value of \\(x\\) such that \\(\\mbox{P}(X\\leq x)\\geq 0.95\\)). Further, plot the pmf of \\(X\\).\n\nSince vehicles arrive according to a Poisson process, the probability question leads us to define the random variable \\(X\\) as the number of vehicles that arrive in a week.\nWe know that \\(X\\sim \\textsf{Poisson}(\\lambda=0.4*7=2.8)\\). Thus, \\(\\mbox{E}(X)=\\mbox{Var}(X)=2.8\\).\nThe parameter is the average number of vehicles that arrive in a week. Now, we can write the pmf of \\(X\\) and calculate probabilities of interest:\n\\[\n\\mbox{P}(X=0)=\\frac{2.8^0 e^{-2.8}}{0!}=e^{-2.8}=0.061\n\\]\nAlternatively, we can use the built-in R functions for the Poisson distribution.\nNow, let’s calculate \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), and \\(\\mbox{P}(2 \\leq X \\leq 8)\\).\n\n##P(X = 0)\ndpois(0, lambda = 2.8)\n\n[1] 0.06081006\n\n##P(X &lt;= 6)\nppois(6, lambda = 2.8)\n\n[1] 0.9755894\n\n## or\nsum(dpois(0:6, 2.8))\n\n[1] 0.9755894\n\n##P(X &gt;= 2) = 1 - P(X &lt; 2) = 1 - P(X &lt;= 1)\n1 - ppois(1, 2.8)\n\n[1] 0.7689218\n\n## or\nsum(dpois(2:1000, 2.8))\n\n[1] 0.7689218\n\n\nNote that when considering \\(\\mbox{P}(X\\geq 2)\\), we recognize that this is equivalent to \\(1-\\mbox{P}(X\\leq 1)\\). We can use ppois() to find this probability or sum the probabilities for \\(X\\geq 2\\).\nWhen considering \\(\\mbox{P}(2\\leq X \\leq 8)\\), we need to make sure we formulate this correctly. Below are two possible methods:\n\n##P(2 &lt;= X &lt;= 8) = P(X &lt;= 8) - P(X &lt;= 1)\nppois(8, lambda = 2.8) - ppois(1, lambda = 2.8)\n\n[1] 0.766489\n\n## or\nsum(dpois(2:8, lambda = 2.8))\n\n[1] 0.766489\n\n\nThe median is \\(x\\) such that \\(P(X\\leq x)=0.5\\). To find the median and the 95th percentiles, we use qpois:\n\nqpois(0.5, lambda = 2.8)\n\n[1] 3\n\nqpois(0.95, lambda = 2.8)\n\n[1] 6\n\n\nFigure 12.2 is a plot of the pmf of a Poisson random variable.\n\ngf_dist(\"pois\", lambda = 2.8) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"X\", y = \"P(X = x)\")\n\n\n\n\n\n\n\nFigure 12.2: The pmf of a Poisson random variable.\n\n\n\n\n\nWe can also simulated Poisson random variables from this distribution using the rpois() function. Suppose we want to simulate 5 random values from this Poisson distribution with a rate of \\(\\lambda = 2.8\\).\n\nset.seed(1)\nrpois(5, lambda = 2.8)\n\n[1] 2 2 3 5 1\n\n\nRemember, \\(X\\) is the number of vehicles that arrive to the maintenance garage in a week. These simulated values represent a week where two vehicles arrived, then another week where two vehicles arrived, followed by a week with three vehicles, then five, and then one vehicle.\n\n\n12.2.6 Other named discrete distributions\nThere are several other named discrete distributions such as:\n\nBernoulli - utilized for single trial experiments with binary outcomes\nGeometric - models the number of trials until the first success (often used with reliability testing)\nNegative binomial - number of trials until a specified number of successes (models counts that occur in nature; used in epidemiology)\nHypergeometric - used for finding the number of successes in draws without replacement (e.g., number of red chips from a bag of red and blue chips)\nMultinomial - similar to the binomial distribution, but allows for more than two outcomes (often used with Likert scale data or in polling)\n\nWe leave the details of additional discrete distributions to interested learners.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "Named-Continuous-Distributions.html",
    "href": "Named-Continuous-Distributions.html",
    "title": "13  Named Continuous Distributions",
    "section": "",
    "text": "13.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "Named-Continuous-Distributions.html#objectives",
    "href": "Named-Continuous-Distributions.html#objectives",
    "title": "13  Named Continuous Distributions",
    "section": "",
    "text": "Differentiate between common continuous distributions (Uniform, Exponential, Normal) by identifying their parameters, assumptions, and moments. Evaluate scenarios to determine the most appropriate distribution to model various types of data.\nApply R to calculate probabilities and quantiles, and simulate random variables for common continuous distributions.\nState and apply the empirical rule (68-95-99.7 rule).\nExplain the relationship between the Poisson process and the Poisson and Exponential distributions, and describe how these distributions model different aspects of the same process.\nApply the memory-less property in context of the Exponential distribution and use it to simplify probability calculations.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "Named-Continuous-Distributions.html#continuous-distributions",
    "href": "Named-Continuous-Distributions.html#continuous-distributions",
    "title": "13  Named Continuous Distributions",
    "section": "13.2 Continuous distributions",
    "text": "13.2 Continuous distributions\nIn this chapter we will explore continuous distributions. This means we work with probability density functions and use them to find probabilities. Thus we must integrate, either numerically, graphically, or mathematically. The cumulative distribution function will also play an important role in this chapter.\nThere are many more distributions than the ones in this chapter but these are the most common and will set you up to learn and use any others in the future.\n\n13.2.1 Uniform distribution\nThe first continuous distribution we will discuss is the uniform distribution. By default, when we refer to the uniform distribution, we are referring to the continuous version. When referring to the discrete version, we use the full term “discrete uniform distribution.”\nA continuous random variable has the uniform distribution if probability density is constant, uniform. The parameters of this distribution are \\(a\\) and \\(b\\), representing the minimum and maximum of the sample space. This distribution is commonly denoted as \\(U(a,b)\\).\nLet \\(X\\) be a continuous random variable with the uniform distribution. This is denoted as \\(X\\sim \\textsf{Unif}(a,b)\\). The pdf of \\(X\\) is given by: \\[\nf_X(x)=\\left\\{\\begin{array}{ll} \\frac{1}{b-a}, & a\\leq x \\leq b \\\\ 0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nThe mean of \\(X\\) is \\(\\mbox{E}(X)=\\frac{a+b}{2}\\) and the variance is \\(\\mbox{Var}(X)=\\frac{(b-a)^2}{12}\\). The derivation of the mean is left to the exercises.\nThe most common uniform distribution is \\(U(0,1)\\) which we have already used several times in this book. Again, notice in Figure 13.1 that the plot of the pdf is a constant or uniform value.\n\n\n\n\n\n\n\n\nFigure 13.1: The pdf of Uniform random variable.\n\n\n\n\n\nTo check that it is a proper pdf, all values must be non-negative and the total probability must be 1. In R the function for probability density will start with the letter d, followed by some short descriptor for the distribution (e.g., “unif” or “norm”). For the uniform distribution, we use dunif(). We integrate the pdf from 0 to 1 and find that the area under the curve, the total probability, is one.\n\nintegrate(function(x) dunif(x), lower = 0, upper = 1)\n\n1 with absolute error &lt; 1.1e-14\n\n\n\n\n13.2.2 Exponential distribution\nRecall from the chapter on named discrete distributions, we discussed the Poisson process. If arrivals follow a Poisson process, we know that the number of arrivals in a specified amount of time follows a Poisson distribution, and the time until the next arrival follows the exponential distribution. In the Poisson distribution, the number of arrivals is random and the interval is fixed. In the exponential distribution we change this, the interval is random and the arrivals are fixed at 1. This is a subtle point but worth the time to make sure you understand.\nLet \\(X\\) be the number of arrivals in a time interval \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals per unit time interval. From the previous chapter, we know that \\(X\\sim \\textsf{Poisson}(\\lambda T)\\). Now let \\(Y\\) be the time until the next arrival. Then \\(Y\\) follows the exponential distribution with parameter \\(\\lambda\\) which has units of inverse base time:\n\\[\nY \\sim \\textsf{Expon}(\\lambda)\n\\]\nNote on \\(\\lambda\\): One point of confusion involving the parameters of the Poisson and exponential distributions. The parameter of the Poisson distribution (usually denoted as \\(\\lambda\\)) represents the average number of arrivals in whatever amount of time specified by the random variable. In the case of the exponential distribution, the parameter (also denoted as \\(\\lambda\\)) represents the average number of arrivals per unit time. For example, suppose arrivals follow a Poisson process with an average of 10 arrivals per day. \\(X\\), the number of arrivals in 5 days, follows a Poisson distribution with parameter \\(\\lambda=50\\), since that is the average number of arrivals in the amount of time specified by \\(X\\). Meanwhile, \\(Y\\), the time in days until the next arrival, follows an exponential distribution with parameter \\(\\lambda=10\\) (the average number of arrivals per day).\nThe pdf of \\(Y\\) is given by:\n\\[\nf_Y(y)=\\lambda e^{-\\lambda y}, \\hspace{0.3cm} y&gt;0\n\\]\nThe mean and variance of \\(Y\\) are: \\(\\mbox{E}(Y)=\\frac{1}{\\lambda}\\) and \\(\\mbox{Var}(Y)=\\frac{1}{\\lambda^2}\\). You should be able to derive these results but they require integration by parts and can be lengthy algebraic exercises.\n\nExample:\nSuppose at a local retail store, customers arrive to a checkout counter according to a Poisson process with an average of one arrival every three minutes. Let \\(Y\\) be the time (in minutes) until the next customer arrives to the counter. What is the distribution (and parameter) of \\(Y\\)? What are \\(\\mbox{E}(Y)\\) and \\(\\mbox{Var}(Y)\\)? Find \\(\\mbox{P}(Y&gt;5)\\), \\(\\mbox{P}(Y\\leq 3)\\), and \\(\\mbox{P}(1 \\leq Y &lt; 5)\\)? Also, find the median and 95th percentile of \\(Y\\). Finally, plot the pdf of \\(Y\\).\n\nSince one arrival shows up every three minutes, the average number of arrivals per unit time is 1/3 arrival per minute. Thus, \\(Y\\sim \\textsf{Expon}(\\lambda=1/3)\\). This means that \\(\\mbox{E}(Y)=3\\) and \\(\\mbox{Var}(Y)=9\\).\nTo find \\(\\mbox{P}(Y&gt;5)\\), we could integrate the pdf of \\(Y\\):\n\\[\n\\mbox{P}(Y&gt;5)=\\int_5^\\infty \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y = \\lim_{a \\to +\\infty}\\int_5^a \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y =\n\\]\n\\[\n\\lim_{a \\to +\\infty} -e^{-\\frac{1}{3}y}\\bigg|_5^a=\\lim_{a \\to +\\infty} -e^{-\\frac{a}{3}}-(-e^{-\\frac{5}{3}})= 0 + 0.189 = 0.189\n\\]\nAlternatively, we could use R. We first use the pexp() function, which finds \\(P(Y\\leq y)\\). Since we want \\(P(Y &gt; 5)\\), we take the complement by subtracting from one.\n\n##Prob(Y &gt; 5) = 1 - Prob(Y &lt;= 5)\n1 - pexp(5, rate = 1/3)\n\n[1] 0.1888756\n\n\nOr using integrate(), we integrate the pdf from 5 to infinity because, for the exponential distribution, \\(y &gt; 0\\).\n\nintegrate(function(x) 1/3*exp(-1/3*x), 5, Inf)\n\n0.1888756 with absolute error &lt; 8.5e-05\n\n\nFor the remaining probabilities, we will use the functions in R:\n\n##Prob(Y &lt;= 3)\npexp(3, rate = 1/3)\n\n[1] 0.6321206\n\n##Prob(1 &lt;= Y &lt; 5)\npexp(5, rate = 1/3)-pexp(1, rate = 1/3)\n\n[1] 0.5276557\n\n\nThe median is \\(y\\) such that \\(\\mbox{P}(Y\\leq y)=0.5\\). We can find this by solving the following for \\(y\\): \\[\n\\int_0^y \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y = 0.5\n\\]\nAlternatively, we can use qexp in R:\n\n## median\nqexp(0.5, rate = 1/3)\n\n[1] 2.079442\n\n## 95th percentile\nqexp(0.95, rate = 1/3)\n\n[1] 8.987197\n\n\n\n\n\n\n\n\n\n\nFigure 13.2: The pdf of exponential random variable \\(Y\\)\n\n\n\n\n\nBoth from Figure 13.2 and the mean and median, we know that the exponential distribution is skewed to the right.\n\n13.2.2.1 Memory-less property\nThe Poisson process is known for its memory-less property. Essentially, this means that the time until the next arrival is independent of the time since last arrival. Thus, the probability of an arrival within the next 5 minutes is the same regardless of whether an arrival just occurred or an arrival has not occurred for a long time.\nTo show this let’s consider random variable \\(Y\\) ( time until the next arrival in minutes) where \\(Y\\sim\\textsf{Expon}(\\lambda)\\). We will show that, given it has been at least \\(t\\) minutes since the last arrival, the probability we wait at least \\(y\\) additional minutes is equal to the marginal probability that we wait \\(y\\) additional minutes.\nFirst, note that the cdf of \\(Y\\), \\(F_Y(y)=\\mbox{P}(Y\\leq y)=1-e^{-\\lambda y}\\), you should be able to derive this. So, \\[\n\\mbox{P}(Y\\geq y+t|Y\\geq t) = \\frac{\\mbox{P}(Y\\geq y+t \\cap Y\\geq t)}{\\mbox{P}(Y\\geq t)}=\\frac{\\mbox{P}(Y\\geq y +t)}{\\mbox{P}(Y\\geq t)} = \\frac{1-(1-e^{-(y+t)\\lambda})}{1-(1-e^{-t\\lambda})}\n\\] \\[\n=\\frac{e^{-\\lambda y }e^{-\\lambda t}}{e^{-\\lambda t }}=e^{-\\lambda y} = 1-(1-e^{-\\lambda y})=\\mbox{P}(Y\\geq y).\n\\blacksquare\n\\]\nLet’s simulate values for a Poisson. The Poisson is often used in modeling customer service situations such as service at Chipotle. However, some people have the mistaken idea that arrivals will be equally spaced. In fact, arrivals will come in clusters and bunches. Maybe this is the root of the common expression, “Bad news comes in threes”?\n\n\n\n\n\n\n\n\nFigure 13.3: Simulations of Poisson random variable.\n\n\n\n\n\nIn Figure 13.3, the number of events in a box is \\(X\\sim \\textsf{Poisson}(\\lambda = 5)\\). As you can see, some boxes have more than 5 and some less because 5 is the average number of arrivals. Also note that the spacing is not equal. The 8 different runs are just repeated simulations of the same process. We can see spacing and clusters in each run.\n\n\n\n13.2.3 Normal distribution\nThe normal distribution (also referred to as Gaussian) is a common distribution found in natural processes. You have likely seen a bell curve in various contexts. The bell curve is often indicative of an underlying normal distribution. There are two parameters of the normal distribution: \\(\\mu\\) (the mean of \\(X\\)) and \\(\\sigma\\) (the standard deviation of \\(X\\)).\nSuppose a random variable \\(X\\) has a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The pdf of \\(X\\) is given by:\n\\[\nf_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\hspace{0.3cm} -\\infty &lt; x &lt;\\infty\n\\]\nSome plots of normal distributions for different parameters are plotted in Figure 13.4.\n\n\n\n\n\n\n\n\nFigure 13.4: The pdf of Normal for various values of mu and sigma\n\n\n\n\n\n\n13.2.3.1 Standard normal\nWhen random variable \\(X\\) is normally distributed with \\(\\mu=0\\) and \\(\\sigma=1\\), \\(X\\) is said to follow the standard normal distribution. Sometimes, the standard normal pdf is denoted by \\(\\phi(x)\\).\nNote that any normally distributed random variable can be transformed to have the standard normal distribution. Let \\(X \\sim \\textsf{Norm}(\\mu,\\sigma)\\). Then, \\[\nZ=\\frac{X-\\mu}{\\sigma} \\sim \\textsf{Norm}(0,1)\n\\]\nPartially, one can show this is true by noting that the mean of \\(Z\\) is 0 and the variance (and standard deviation) of \\(Z\\) is 1: \\[\n\\mbox{E}(Z)=\\mbox{E}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\left(\\mbox{E}(X)-\\mu\\right)=\\frac{1}\\sigma(\\mu-\\mu)=0\n\\] \\[\n\\mbox{Var}(Z)=\\mbox{Var}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma^2}\\left(\\mbox{Var}(X)-0\\right)=\\frac{1}{\\sigma^2} \\sigma^2=1\n\\]\nNote that this does not prove that \\(Z\\) follows the standard normal distribution; we have merely shown that \\(Z\\) has a mean of 0 and a variance of 1. We will discuss transformation of random variables in a later chapter.\n\nExample:\nLet \\(X \\sim \\textsf{Norm}(\\mu=200,\\sigma=15)\\). Compute \\(\\mbox{P}(X\\leq 160)\\), \\(\\mbox{P}(180\\leq X &lt; 230)\\), and \\(\\mbox{P}(X&gt;\\mu+\\sigma)\\). Find the median and 95th percentile of \\(X\\).\n\nTo find probabilities and quantiles, integration will be difficult, so it’s best to use the built-in R functions:\n\n## Prob(X &lt;= 160)\npnorm(160, mean = 200, sd = 15)\n\n[1] 0.003830381\n\n##Prob(180 &lt;= X &lt; 230)\npnorm(230, mean = 200, sd = 15) - pnorm(180, mean = 200, sd = 15)\n\n[1] 0.8860386\n\n##Prob(X &gt; mu + sig)\n1 - pnorm(215, mean = 200, sd = 15)\n\n[1] 0.1586553\n\n## median\nqnorm(0.5, mean = 200, sd = 15)\n\n[1] 200\n\n## 95th percentile\nqnorm(0.95, mean = 200, sd = 15)\n\n[1] 224.6728\n\n\nWe can also use rnorm() to simulate observations from the normal distribution. Let’s simulate 100 values from the normal distribution above. We’ll view only the first few simulated values.\n\nset.seed(1)\nnorm_vals &lt;- rnorm(100, mean = 200, sd = 15)\nhead(norm_vals)\n\n[1] 190.6032 202.7546 187.4656 223.9292 204.9426 187.6930\n\n\n\n\n13.2.3.2 The Empirical Rule (68-95-99.7 Rule)\nThe normal distribution is bell-shaped and symmetrical around the mean, meaning that the left and right sides of the distribution are mirror images. The empirical rule, also known as the 68-95-99.7 rule, describes in a little more detail how data is distributed in a normal distribution.\n\n\n\n\n\n\n\n\nFigure 13.5: An illustration of the empirical rule (68-95-99.7 rule).\n\n\n\n\n\nThe 68-95-99.7 rule states the following about the normal distribution:\n\nApproximately 68% of the data falls within one standard deviation of the mean.\nApproximately 95% of the data falls within two standard deviations of the mean.\nApproximately 99.7% of the data falls within three standard deviations of the mean.\n\n\nExample: Suppose the heights of female cadets are normally distributed with a mean \\(\\mu\\) of 65 inches and a standard deviation \\(\\sigma\\) of 3 inches.\n\nWhat percent of female cadet heights fall between 62 and 68 inches?\nBetween what heights do 95% of female cadets fall?\nWhat percent of female cadet heights fall between 62 and 74 inches?\nWhat percent of female cadets are taller than 71 inches?\n\n\nThe 68-95-99.7 rule tells us that approximately 68% of female cadet heights will fall between 62 and 68 inches (within one standard deviation of the mean), 95% will fall between 59 and 71 inches (within two standard deviations of the mean), and 99.7% will fall between 56 and 74 inches (within three standard deviations of the mean).\nThis information gives us the answers to our first two questions. But how do we figure out what percent of female cadets fall between 62 and 74 inches? It’s always a good idea to draw a picture.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that the normal distribution is symmetric, meaning the left and right sides (with zero at the middle) are mirror images. So, approximately 50% of the female cadet heights will be greater than the mean of 65 inches. Figure 13.6 shows the area of interest in our example. Approximately half of 99.7%, 49.85%, of female cadet heights are between 65 and 74 inches. Because 68% of female cadet heights fall between 62 and 68 inches, 34% fall between 62 and 65 inches. Thus, a total of 83.85% of female cadet heights fall between 62 and 74 inches.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor our last question, Figure 13.7 shows the area of interest. We know from the empirical rule that 95% of female cadet heights fall between 59 and 71 inches. This means that 5% of cadet heights fall outside that range. Because the normal distribution is symmetric, 2.5% falls between 59 inches and 2.5% falls above 71 inches.\nWhile the 68-95-99.7 rule is a powerful tool for normal distributions, its applicability is limited to such distributions. Understanding the shape and properties of the data distribution is crucial before applying this rule or making inferences based on it.\n\n\n\n13.2.4 Other named continuous distributions\nThere are several other named continuous distributions such as:\n\nGamma - models waiting times for multiple events\nWeibull - utilized for failure analysis in fields like reliability engineering\nBeta - used for modeling proportions and probabilities\nLog-normal - logarithm normally distributed; models quantities like biological measurements, income, and stock prices\n\nWe will learn about some other named continuous distributions with specific types of hypothesis tests, but in general, we leave the details of additional continuous distributions to interested learners.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Distributions.html",
    "href": "Multivariate-Distributions.html",
    "title": "14  Multivariate Distributions",
    "section": "",
    "text": "14.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Distributions.html#objectives",
    "href": "Multivariate-Distributions.html#objectives",
    "title": "14  Multivariate Distributions",
    "section": "",
    "text": "Differentiate between joint probability mass/density functions (pmf/pdf), marginal pmfs/pdfs, and conditional pmfs/pdfs, and provide real-world examples to illustrate these concepts and their differences.\nFor a given joint pmf/pdf, derive the marginal and conditional pmfs/pdfs through summation or integration or using R.\nApply joint, marginal, and conditional pmfs/pdfs to calculate probabilities of events involving multiple random variables.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Distributions.html#multivariate-distributions",
    "href": "Multivariate-Distributions.html#multivariate-distributions",
    "title": "14  Multivariate Distributions",
    "section": "14.2 Multivariate distributions",
    "text": "14.2 Multivariate distributions\nMultivariate situations are more common in practice. We are often dealing with more than one variable. We have seen univariate situations in the previous block of material and will now see multivariate distributions throughout the remainder of the book.\nThe basic idea is that we want to determine the relationship between two or more variables to include variable(s) conditional on variables.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Distributions.html#joint-probability",
    "href": "Multivariate-Distributions.html#joint-probability",
    "title": "14  Multivariate Distributions",
    "section": "14.3 Joint probability",
    "text": "14.3 Joint probability\nThus far, we have only considered situations involving one random variable. In some cases, we might be concerned with the behavior of multiple random variables simultaneously. This chapter and the next are dedicated to jointly distributed random variables.\n\n14.3.1 Discrete random variables\nIn the discrete case, joint probability is described by the joint probability mass function. In the bivariate case, suppose \\(X\\) and \\(Y\\) are discrete random variables. The joint pmf is given by \\(f_{X,Y}(x,y)\\) and represents \\(\\mbox{P}(X=x,Y=y) = \\mbox{P}(X=x \\cap Y=y)\\). Note: it is common in statistical and probability models to use a comma to represent and, in fact the select() function in tidyverse does this.\nThe same rules of probability apply to the joint pmf. Each value of \\(f\\) must be between 0 and 1, and the total probability must sum to 1: \\[\n\\sum_{x\\in S_X}\\sum_{y \\in S_Y} f_{X,Y}(x,y) = 1\n\\] This notation means that if we sum the joint probabilities over all values of the random variables \\(X\\) and \\(Y\\) we will get 1.\nIf given a joint pmf, one can obtain the marginal pmf of individual variables. The marginal pmf is simply the mass function of an individual random variable, summing over the possible values of all the other variables. In the bivariate case, the marginal pmf of \\(X\\), \\(f_X(x)\\) is found by: \\[\nf_X(x)=\\sum_{y \\in S_Y}f_{X,Y}(x,y)\n\\]\nNotice that in the above summation, we summed over only the \\(y\\) values.\nSimilarly, \\[\nf_Y(y)=\\sum_{x \\in S_X}f_{X,Y}(x,y)\n\\]\nThe marginal pmf must be distinguished from the conditional pmf. The conditional pmf describes a discrete random variable given other random variables have taken particular values. In the bivariate case, the conditional pmf of \\(X\\), given \\(Y=y\\), is denoted as \\(f_{X|Y=y}(x)\\) and is found by: \\[\nf_{X|Y=y}(x)=\\mbox{P}(X=x|Y=y)=\\frac{\\mbox{P}(X=x,Y=y)}{\\mbox{P}(Y=y)}=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\n\nExample:\nLet \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below.\n\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\ & \\hline 0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &2 & 0.18 & 0.20 & 0.12  \n\\\\ & 4 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\n\nFind the marginal pmfs of \\(X\\) and \\(Y\\).\nFind \\(f_{X|Y=0}(x)\\) and \\(f_{Y|X=2}(y)\\).\n\nThe marginal pmfs can be found by summing across the other variable. So, to find \\(f_X(x)\\), we simply sum across the rows:\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=2 \\\\\n0.07+0.05+0.09, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=2 \\\\\n0.21, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nSimilarly, \\(f_Y(y)\\) can be found by summing down the columns of the joint pmf: \\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nTo find the conditional pmf of \\(X\\) given \\(Y=0\\), it helps to recognize that once we know that \\(Y=0\\), the overall sample space has changed. Now the only outcomes we consider are in the first column (corresponding to \\(Y=0\\)):\n\n\n\n\n\n\n\n\n\nWe are looking for the distribution of \\(X\\) within the circled area. So, we need to find the proportion of probability assigned to each outcome of \\(X\\). Mathematically: \\[\nf_{X|Y=0}(x)=\\mbox{P}(X=x|Y=0)=\\frac{\\mbox{P}(X=x,Y=0)}{\\mbox{P}(Y=0)}=\\frac{f_{X,Y}(x,0)}{f_Y(0)}\n\\]\nAbove, we found the marginal pmf of \\(Y\\). We know that \\(f_Y(0)=0.35\\). So, \\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{X|Y=0}(x)=\\left\\{\\begin{array}{ll} \\frac{0.10}{0.35}, & x=0 \\\\\n\\frac{0.18}{0.35}, & x=2 \\\\\n\\frac{0.07}{0.35}, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.286, & x=0 \\\\\n0.514, & x=2 \\\\\n0.200, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nNote that the probabilities in this pmf sum to 1. It is always wise to confirm this to ensure we did not make a simple computational error along the way.\nSimilarly, we can find \\(f_{Y|X=2}(y)\\). First we recognize that \\(f_X(2)=0.5\\). \\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{Y|X=2}(x)=\\left\\{\\begin{array}{ll} \\frac{0.18}{0.50}, & y=0 \\\\\n\\frac{0.20}{0.50}, & y=1 \\\\\n\\frac{0.12}{0.50}, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.36, & y=0 \\\\\n0.40, & y=1 \\\\\n0.24, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nTogether, these pmfs can be used to find relevant probabilities. For example, see the homework exercises.\n\n\n14.3.2 Continuous random variables\nMany of the ideas above for discrete random variables also apply to the case of multiple continuous random variables. Suppose \\(X\\) and \\(Y\\) are continuous random variables. Their joint probability is described by the joint probability density function. As in the discrete case, the joint pdf is represented by \\(f_{X,Y}(x,y)\\). Recall that while pmfs return probabilities, pdfs return densities, which are not equivalent to probabilities. In order to obtain probability from a pdf, one has to integrate the pdf across the applicable subset of the domain.\nThe rules of a joint pdf are analogous to the univariate case. For all \\(x\\) and \\(y\\), \\(f_{X,Y}(x,y)\\geq 0\\) and the probability must sum to one: \\[\n\\int_{S_X}\\int_{S_Y}f_{X,Y}(x,y)\\mbox{d}y \\mbox{d}x = 1\n\\]\nThe marginal pdf is the density function of an individual random variable, integrating out all others. In the bivariate case, the marginal pdf of \\(X\\), \\(f_X(x)\\), is found by summing, integrating, across the other variable: \\[\nf_X(x)=\\int_{S_Y}f_{X,Y}(x,y)\\mbox{d}y\n\\]\nSimilarly, \\[\nf_Y(y)=\\int_{S_X}f_{X,Y}(x,y)\\mbox{d}x\n\\]\nThe conditional pdf of \\(X\\), given \\(Y=y\\) is denoted as \\(f_{X|Y=y}(x)\\) and is found in the same way as in the discrete case: \\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\nSimilarly, \\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\]\nNote that we are working with the pdf and not probabilities in this case. That is because we can’t determine the probability at a point for a continuous random variable. Thus we work with conditional pdfs to find probabilities for conditional statements.\n\nExample:\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf:\n\\[\nf_{X,Y}(x,y)=xy\n\\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\).\n\n\nVerify \\(f\\) is a valid joint pdf.\n\nWe need to ensure the total volume under the pdf is 1. Note that the double integral with constant limits of integration is just like doing single integrals. We just treat the other variable as a constant. In this book we will not work with limits of integration that have variables in them, this is the material of Calc III.\nFor our simple case of constant limits of integration, the order of integration does not matter. We will arbitrarily integrate \\(x\\) first, treating \\(y\\) as a constant. Then integrate with respect to \\(y\\).\n\\[\n\\int_0^1 \\int_0^2 xy \\mbox{d}x \\mbox{d}y = \\int_0^1 \\frac{x^2y}{2}\\bigg|_0^2 \\mbox{d}y = \\int_0^1 2y\\mbox{d}y = y^2\\bigg|_0^1 = 1\n\\]\nUsing R to do this requires a new package cubature. You can install it from RStudio package tab or the command line using install.packages(\"cubature\"). Then we can use it as follows:\n\n library(cubature) # load the package \"cubature\"\n\n\nf &lt;- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 2))\n\n$integral\n[1] 1\n\n$error\n[1] 0\n\n$functionEvaluations\n[1] 17\n\n$returnCode\n[1] 0\n\n\nNotice the function adaptIntegrate returned four objects. You can read the help menu to learn more about them but we are only interested in the result contained in the object integral.\n\nFind \\(\\mbox{P}(X &gt; 1, Y \\leq 0.5)\\). \\[\n\\mbox{P}(X&gt;1,Y\\leq 0.5)=\\int_0^{0.5}\\int_1^2 xy \\mbox{d}x \\mbox{d}y = \\int_0^{0.5} \\frac{x^2 y}{2}\\bigg|_1^2 \\mbox{d}y = \\int_0^{0.5}2y - \\frac{y}{2}\\mbox{d}y\n\\] \\[\n= \\frac{3y^2}{4}\\bigg|_0^{0.5}=0.1875\n\\]\n\n\nf &lt;- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(1, 0), upperLimit = c(2, 1/2))\n\n$integral\n[1] 0.1875\n\n$error\n[1] 2.775558e-17\n\n$functionEvaluations\n[1] 17\n\n$returnCode\n[1] 0\n\n\n\nFind the marginal pdfs of \\(X\\) and \\(Y\\). \\[\nf_X(x)=\\int_0^1 xy \\mbox{d}y = \\frac{xy^2}{2}\\bigg|_0^1=\\frac{x}{2}\n\\]\n\nwhere \\(0 \\leq x \\leq 2\\).\n\\[\nf_Y(y)=\\int_0^2 xy \\mbox{d}x = \\frac{x^2y}{2}\\bigg|_0^2= 2y\n\\]\nwhere \\(0 \\leq y \\leq 1\\).\n\nFind the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{xy}{2y}=\\frac{x}{2}\n\\]\n\nwhere \\(0 \\leq x \\leq 2\\).\nSimilarly, \\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}=\\frac{xy}{\\frac{x}{2}}=2y\\]\nwhere \\(0 \\leq y \\leq 1\\).\n\nQuestion:\nNotice that \\(f_X(x) = f_{X|Y=y}(x)\\) and \\(f_Y(y) = f_{Y|X=x}(y)\\) in this example. This is not common and is important. What does this imply about \\(X\\) and \\(Y\\)?1",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Distributions.html#footnotes",
    "href": "Multivariate-Distributions.html#footnotes",
    "title": "14  Multivariate Distributions",
    "section": "",
    "text": "Since the conditional density function is always equal to the marginal, it means that \\(X\\) and \\(Y\\) are independent of one another. Also, if the conditioning variable does not appear in the conditional density function and the domain of the joint density is rectangular (i.e., the bounds of the two variables are constants), the random variables are independent.↩︎",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html",
    "href": "Multivariate-Expectation.html",
    "title": "15  Multivariate Expectation",
    "section": "",
    "text": "15.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#objectives",
    "href": "Multivariate-Expectation.html#objectives",
    "title": "15  Multivariate Expectation",
    "section": "",
    "text": "Given a joint pmf/pdf, calculate and interpret the expected values/means and variances of random variables and functions of random variables.\nDifferentiate between covariance and correlation, and given a joint pmf/pdf, calculate and interpret the covariance and correlation between two random variables.\nGiven a joint pmf/pdf, determine whether random variables are independent of one another and justify your conclusion with appropriate calculations and reasoning.\nCalculate and interpret conditional expectations for given joint pmfs/pdfs.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#expectation---moments",
    "href": "Multivariate-Expectation.html#expectation---moments",
    "title": "15  Multivariate Expectation",
    "section": "15.2 Expectation - moments",
    "text": "15.2 Expectation - moments\nComputing expected values of random variables in the joint context is similar to the univariate case. Let \\(X\\) and \\(Y\\) be discrete random variables with joint pmf \\(f_{X,Y}(x,y)\\). Let \\(g(X,Y)\\) be some function of \\(X\\) and \\(Y\\). Then: \\[\n\\mbox{E}[g(X,Y)]=\\sum_x\\sum_y g(x,y)f_{X,Y}(x,y)\n\\]\n(Note that \\(\\sum\\limits_{x}\\) is shorthand for the sum across all possible values of \\(x\\).)\nIn the case of continuous random variables with a joint pdf \\(f_{X,Y}(x,y)\\), expectation becomes: \\[\n\\mbox{E}[g(X,Y)]=\\int_x\\int_y g(x,y)f_{X,Y}(x,y)\\mbox{d} y \\mbox{d} x\n\\]\n\n15.2.1 Expectation of discrete random variables\nGiven a joint pmf, one can find the mean of \\(X\\) by using the joint function or by finding the marginal pmf first and then using that to find \\(\\mbox{E}(X)\\). In the end, both ways are the same. For the discrete case: \\[\n\\mbox{E}(X)=\\sum_x\\sum_y xf_{X,Y}(x,y) = \\sum_x x \\sum_y f_{X,Y}(x,y)\n\\]\nThe \\(x\\) can be moved outside the inner sum since the inner sum is with respect to variable \\(y\\) and \\(x\\) is a constant with respect to \\(y\\). Note that the inner sum is the marginal pmf of \\(X\\). So, \\[\n\\mbox{E}(X)=\\sum_x x \\sum_y f_{X,Y}(x,y)=\\sum_x x f_X(x)\n\\]\n\n\nExample:\nLet \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below.\n\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\nFind \\(\\mbox{E}(X)\\)\nFirst we will use the joint pmf directly, then we find the marginal pmf of \\(X\\) and use that as we would in a univariate case.\n\\[\n\\mbox{E}(X)=\\sum_{x=0}^2 \\sum_{y=0}^2 x f_{X,Y}(x,y)\n\\] \\[\n=0*0.10+0*0.08+0*0.11+1*0.18+...+2*0.09 = 0.92\n\\]\nThe marginal pmf of \\(X\\) is \\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=1 \\\\\n0.07+0.05+0.09, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nSo, \\(\\mbox{E}(X)=0*0.29+1*0.5+2*0.21=0.92\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#exercises-to-apply-what-we-learned",
    "href": "Multivariate-Expectation.html#exercises-to-apply-what-we-learned",
    "title": "15  Multivariate Expectation",
    "section": "15.3 Exercises to apply what we learned",
    "text": "15.3 Exercises to apply what we learned\n\nExercise: Let \\(X\\) and \\(Y\\) be defined above. Find \\(\\mbox{E}(Y)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right)\\).\n\n\n15.3.1 E(Y)\nAs with \\(\\mbox{E}(X)\\), \\(\\mbox{E}(Y)\\) can be found in two ways. We will use the marginal pmf of \\(Y\\), which we will not derive:\n\\[\n\\mbox{E}(Y)=\\sum_{y=0}^2 y \\cdot f_Y(y)=0*0.35+1*0.33+2*0.32 = 0.97\n\\]\n\n\n15.3.2 E(X+Y)\nTo find \\(\\mbox{E}(X+Y)\\) we will use the joint pmf. In the discrete case, it helps to first identify all of the possible values of \\(X+Y\\) and then figure out what probabilities are associated with each value. This problem is really a transformation problem where we are finding the distribution of \\(X+Y\\). In this example, \\(X+Y\\) can take on values 0, 1, 2, 3, and 4. The value 0 only happens when \\(X=Y=0\\) and the probability of this outcome is 0.10. The value 1 occurs when \\(X=0\\) and \\(Y=1\\) or when \\(X=1\\) and \\(Y=0\\). This occurs with probability 0.08 + 0.18. We continue in this manner: \\[\n\\mbox{E}(X+Y)=\\sum_{x=0}^2\\sum_{y=0}^2 (x+y)f_{X,Y}(x,y)\n\\] \\[\n= 0*0.1+1*(0.18+0.08)+2*(0.11+0.07+0.20)+3*(0.12+0.05)+4*0.09\n\\] \\[\n= 1.89\n\\]\nNote that \\(\\mbox{E}(X+Y)=\\mbox{E}(X)+\\mbox{E}(Y)\\). (The proof of this is left to the reader.)\n\n\n15.3.3 E(XY)\n\\[\n\\mbox{E}(XY)=\\sum_{x=0}^2\\sum_{y=0}^2 xyf_{X,Y}(x,y)\n\\] \\[\n= 0*(0.1+0.08+0.11+0.18+0.07)+1*0.20 +2*(0.12+0.05)+4*0.09\n\\] \\[\n= 0.9\n\\]\nNote that \\(\\mbox{E}(XY)\\) is not necessarily equal to \\(\\mbox{E}(X)\\mbox{E}(Y)\\).\n\n\n15.3.4 E(1/2X+Y+1)\n\\[\n\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right) = \\sum_{x=0}^2\\sum_{y=0}^2 \\frac{1}{2x+y+1}f_{X,Y}(x,y)\n\\] \\[\n= 1*0.1+\\frac{1}{2}*0.08+\\frac{1}{3}*(0.11+0.18)+\\frac{1}{4}*0.20 + \\] \\[\n\\frac{1}{5}*(0.12+0.07)+\\frac{1}{6}*0.05+\\frac{1}{7}*0.09\n\\] \\[\n= 0.3125\n\\]\n\n\n15.3.5 Expectation of continuous random variables\nLet’s consider an example with continuous random variables where summation is replaced with integration:\n\nExample:\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[\nf_{X,Y}(x,y)=xy\n\\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#exercises-to-apply-what-we-learned-1",
    "href": "Multivariate-Expectation.html#exercises-to-apply-what-we-learned-1",
    "title": "15  Multivariate Expectation",
    "section": "15.4 Exercises to apply what we learned",
    "text": "15.4 Exercises to apply what we learned\n\nExercise: Find \\(\\mbox{E}(X)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{Var}(XY)\\).\n\n\n15.4.1 E(X)\nWe found the marginal pdf of \\(X\\) in a previous chapter, so we should use that now: \\[\n\\mbox{E}(X)=\\int_0^2 x\\frac{x}{2}\\mbox{d} x = \\frac{x^3}{6}\\bigg|_0^2= \\frac{4}{3}\n\\] Or using R\n\nfractions(integrate(function(x){x^2/2},0,2)$value)\n\n[1] 4/3\n\n\n\n\n15.4.2 E(X+Y)\nTo find \\(\\mbox{E}(X+Y)\\), we could use the joint pdf directly, or use the marginal pdf of \\(Y\\) to find \\(\\mbox{E}(Y)\\) and then add the result to \\(\\mbox{E}(X)\\). The reason this is valid is because when we integrate \\(x\\) with the joint pdf, integrating with respect to \\(y\\) first, we can treat \\(x\\) as a constant and bring it out side the integral. Then we are integrating the joint pdf with respect \\(y\\) which results in the marginal pdf of \\(X\\).\nWe’ll use the joint pdf: \\[\n\\mbox{E}(X+Y)=\\int_0^2\\int_0^1 (x+y)xy\\mbox{d} y \\mbox{d} x\n\\] \\[\n=\\int_0^2\\int_0^1 (x^2y+xy^2)\\mbox{d} y \\mbox{d} x = \\int_0^2 \\frac{x^2y^2}{2}+\\frac{xy^3}{3} \\bigg|_{y=0}^{y=1}\\mbox{d} x\n\\]\n\\[\n= \\int_0^2 \\frac{x^2}{2}+\\frac{x}{3} \\mbox{d} x= \\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^2=\\frac{8}{6}+\\frac{4}{6}=2\n\\]\nOr using R:\n\nadaptIntegrate(function(x){(x[1]+x[2])*x[1]*x[2]},\n    lowerLimit = c(0,0),\n    upperLimit = c(1,2))$integral\n\n[1] 2\n\n\nIf we wanted to use simulation to find this expectation, we could simulate variables from the marginal of \\(X\\) and \\(Y\\) and then add them together to create a new variable.\nThe cdf for \\(X\\) is \\(\\frac{x^2}{4}\\) so we simulate a random variable from \\(X\\) by sampling from a random uniform and then taking the inverse of the cdf. For \\(Y\\) the cdf is \\(y^2\\) and do a similar simulation.\n\nset.seed(1820)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=x+y) %&gt;%\n  summarize(Ex=mean(x),Ey=mean(y),Explusy = mean(z))\n\n        Ex        Ey  Explusy\n1 1.338196 0.6695514 2.007748\n\n\nWe can see that \\(E(X + Y) = E(X) + E(Y)\\).\n\n\n15.4.3 E(XY)\nNext, we have\n\\[\n\\mbox{E}(XY)=\\int_0^2\\int_0^1 xy*xy\\mbox{d} y \\mbox{d} x = \\int_0^2 \\frac{x^2y^3}{3}\\bigg|_0^1 \\mbox{d} x = \\int_0^2 \\frac{x^2}{3}\\mbox{d} x\n\\] \\[\n=\\frac{x^3}{9}\\bigg|_0^2 = \\frac{8}{9}\n\\]\nUsing R:\n\nfractions(adaptIntegrate(function(x){(x[1]*x[2])*x[1]*x[2]},\n      lowerLimit = c(0,0),\n      upperLimit = c(1,2))$integral)\n\n[1] 8/9\n\n\nOr by simulating, we have:\n\nset.seed(191)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=x*y) %&gt;%\n  summarize(Ex=mean(x),Ey=mean(y),Extimesy = mean(z))\n\n       Ex        Ey  Extimesy\n1 1.33096 0.6640436 0.8837552\n\n\n\n\n15.4.4 V(XY)\nRecall that the variance of a random variable is the expected value of the squared difference from its mean. So, \\[\n\\mbox{Var}(XY)=\\mbox{E}\\left[\\left(XY-\\mbox{E}(XY)\\right)^2\\right]=\\mbox{E}\\left[\\left(XY-\\frac{8}{9}\\right)^2\\right]\n\\] \\[\n=\\int_0^2\\int_0^1 \\left(xy-\\frac{8}{9}\\right)^2 xy\\mbox{d} y \\mbox{d} x =\\int_0^2\\int_0^1 \\left(x^2y^2-\\frac{16xy}{9}+\\frac{64}{81}\\right)xy\\mbox{d} y \\mbox{d} x\n\\]\nYuck!! But we will continue because we are determined to integrate after so much Calculus in our core curriculum.\n\\[\n=\\int_0^2\\int_0^1 \\left(x^3y^3-\\frac{16x^2y^2}{9}+\\frac{64xy}{81}\\right)\\mbox{d} y \\mbox{d} x\n\\]\n\\[\n=\\int_0^2 \\frac{x^3y^4}{4}-\\frac{16x^2y^3}{27}+\\frac{32xy^2}{81}\\bigg|_0^1 \\mbox{d} x\n\\]\n\\[\n= \\int_0^2 \\frac{x^3}{4}-\\frac{16x^2}{27}+\\frac{32x}{81}\\mbox{d} x\n\\]\n\\[\n= \\frac{x^4}{16}-\\frac{16x^3}{81}+\\frac{16x^2}{81}\\bigg|_0^2\n\\] \\[\n=\\frac{16}{16}-\\frac{128}{81}+\\frac{64}{81}=\\frac{17}{81}\n\\]\nHere’s a better way using R:\n\nfractions(adaptIntegrate(function(x){(x[1]*x[2]-8/9)^2*x[1]*x[2]},\n    lowerLimit = c(0,0),\n    upperLimit = c(1,2))$integral)\n\n[1] 17/81\n\n\nNext we will estimate the variance using a simulation:\n\nset.seed(816)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=(x*y-8/9)^2) %&gt;%\n  summarize(Var = mean(z))\n\n        Var\n1 0.2098769\n\n\nThat was much easier. Notice that we are really just estimating these expectations with the simulations. The mathematical answers are the true population values while our simulations are sample estimates. In a few chapters we will discuss estimators in more detail.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#covariancecorrelation",
    "href": "Multivariate-Expectation.html#covariancecorrelation",
    "title": "15  Multivariate Expectation",
    "section": "15.5 Covariance/Correlation",
    "text": "15.5 Covariance/Correlation\nWe have discussed expected values of random variables and functions of random variables in a joint context. It would be helpful to have some kind of consistent measure to describe how two random variables are related to one another. Covariance and correlation do just that. It is important to understand that these are measures of a linear relationship between variables.\nConsider two random variables \\(X\\) and \\(Y\\). (We could certainly consider more than two, but for demonstration, let’s consider only two for now). The covariance between \\(X\\) and \\(Y\\) is denoted as \\(\\mbox{Cov}(X,Y)\\) and is found by: \\[\n\\mbox{Cov}(X,Y)=\\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right]\n\\]\nWe can simplify this expression to make it a little more usable: \\[\n\\begin{align}\n\\mbox{Cov}(X,Y) & = \\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right] \\\\\n& = \\mbox{E}\\left[XY - Y\\mbox{E}(X) - X\\mbox{E}(Y) + \\mbox{E}(X)\\mbox{E}(Y)\\right] \\\\\n& = \\mbox{E}(XY) - \\mbox{E}(Y\\mbox{E}(X)) - \\mbox{E}(X\\mbox{E}(Y)) + \\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)-\\mbox{E}(X)\\mbox{E}(Y)+\\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\end{align}\n\\]\nThus, \\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\]\nThis expression is a little easier to use, since it’s typically straightforward to find each of these quantities.\nIt is important to note that while variance is a positive quantity, covariance can be positive or negative. A positive covariance implies that as the value of one variable increases, the other tends to increase. This is a statement about a linear relationship. Likewise, a negative covariance implies that as the value of one variable increases, the other tends to decrease.\n\nExample:\nAn example of positive covariance is human height and weight. As height increase, weight tends to increase. An example of negative covariance is gas mileage and car weight. As car weight increases, gas mileage decreases.\n\nRemember that if \\(a\\) and \\(b\\) are constants, \\(\\mbox{E}(aX+b) =a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\). Similarly, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are all constants, \\[\n\\mbox{Cov}(aX+b,cY+d)=ac\\mbox{Cov}(X,Y)\n\\]\nOne disadvantage of covariance is its dependence on the scales of the random variables involved. This makes it difficult to compare covariances of multiple sets of variables. Correlation avoids this problem. Correlation is a scaled version of covariance. It is denoted by \\(\\rho\\) and found by: \\[\n\\rho = \\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]\nWhile covariance could take on any real number, correlation is bounded by -1 and 1. Two random variables with a correlation of 1 are said to be perfectly positively correlated, while a correlation of -1 implies perfect negative correlation. Two random variables with a correlation (and thus covariance) of 0 are said to be uncorrelated, that is they do not have a linear relationship but could have a non-linear relationship. This last point is important; random variables with no relationship will have a 0 covariance. However, a 0 covariance only implies that the random variables do not have a linear relationship.\nLet’s look at some plots, Figures Figure 15.1, Figure 15.2, Figure 15.3, and Figure 15.4 of different correlations. Remember that the correlation we are calculating in this section is for the population, while the plots are showing sample points from a population.\n\n\n\n\n\n\n\n\nFigure 15.1: Correlation of 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: Correlation of 0.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.3: Correlation of 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Correlation of 0\n\n\n\n\n\n\n\n15.5.1 Variance of sums\nSuppose \\(X\\) and \\(Y\\) are two random variables. Then, \\[\n\\mbox{Var}(X+Y)=\\mbox{E}\\left[(X+Y-\\mbox{E}(X+Y))^2\\right]=\\mbox{E}[(X+Y)^2]-\\left[\\mbox{E}(X+Y)\\right]^2\n\\]\nIn the last step, we are using the alternative expression for variance (\\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\)). Evaluating: \\[\n\\mbox{Var}(X+Y)=\\mbox{E}(X^2)+\\mbox{E}(Y^2)+2\\mbox{E}(XY)-\\mbox{E}(X)^2-\\mbox{E}(Y)^2-2\\mbox{E}(X)\\mbox{E}(Y)\n\\]\nRegrouping the terms: \\[\n=\\mbox{E}(X^2)-\\mbox{E}(X)^2+\\mbox{E}(Y^2)-\\mbox{E}(Y)^2+2\\left(\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\\right)\n\\]\n\\[\n=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)\n\\]\n\nExample:\nLet \\(X\\) and \\(Y\\) be defined as above. Find \\(\\mbox{Cov}(X,Y)\\), \\(\\rho\\), and \\(\\mbox{Var}(X+Y)\\).\n\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0.9-0.92*0.97=0.0076\n\\]\n\\[\n\\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]\nQuickly, \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2= 1.34-0.92^2 =0.4936\\) and \\(\\mbox{Var}(Y)=0.6691\\). So, \\[\n\\rho=\\frac{0.0076}{\\sqrt{0.4936*0.6691}}=0.013\n\\]\nWith such a low \\(\\rho\\), we would say that \\(X\\) and \\(Y\\) are only slightly positively correlated.\n\\[\n\\mbox{Var}(X+Y)=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=0.4936+0.6691+2*0.0076=1.178\n\\]",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#independence",
    "href": "Multivariate-Expectation.html#independence",
    "title": "15  Multivariate Expectation",
    "section": "15.6 Independence",
    "text": "15.6 Independence\nTwo random variables \\(X\\) and \\(Y\\) are said to be independent if their joint pmf/pdf is the product of their marginal pmfs/pdfs: \\[\nf_{X,Y}(x,y)=f_X(x)f_Y(y)\n\\]\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\mbox{Cov}(X,Y) = 0\\). The converse is not necessarily true, however because they could have a non-linear relationship.\nFor a discrete distribution, you must check that each cell, joint probabilities, are equal to the product of the marginal probability. Back to our joint pmf from above:\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\nThe marginal pmf of \\(X\\) is\n\\[\nf_X(x) = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\] The marginal pmf of \\(Y\\) is\n\\[\nf_Y(y) = \\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\] Checking the first cell, we immediately see that \\(f_{X,Y}(x=0,y=0) \\neq f_{X}(x=0)\\cdot f_{Y}(y=0)\\) so \\(X\\) and \\(Y\\) are not independent.\nAn easy way to determine if continuous variables are independent is to first check that the domain only contains constants, it is rectangular, and second that the joint pdf can be written as a product of a function of \\(X\\) only and a function of \\(Y\\) only.\nThus for our examples above even though the domains were rectangular, in \\(f(x,y)=xy\\), \\(X\\) and \\(Y\\) were independent while \\(f(x,y)=x+y\\) they were not.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Multivariate-Expectation.html#conditional-expectation",
    "href": "Multivariate-Expectation.html#conditional-expectation",
    "title": "15  Multivariate Expectation",
    "section": "15.7 Conditional expectation",
    "text": "15.7 Conditional expectation\nAn important idea in graph theory, network analysis, Bayesian networks, and queuing theory is conditional expectation. We will only briefly introduce the ideas here so that you have a basic understanding. This does not imply it is not an important topic.\nLet’s start with a simple example to illustrate the ideas.\n\nExample:\nSam will read either one chapter of his history book or one chapter of his philosophy book. If the number of misprints in a chapter of his history book is Poisson with mean 2 and if the number of misprints in a chapter of his philosophy book is Poisson with mean 5, then assuming Sam is equally likely to choose either book, what is the expected number of misprints that Sam will find?\n\nNote: in the next chapter we are working with transformations and could attack the problem using that method.\nFirst let’s use simulation to get an idea what value the answer should be and then use algebraic techniques and definitions we have learned in this book.\nSimulate 5000 reads from the history book and 5000 from philosophy and combine:\n\nset.seed(2011)\nmy_data&lt;-data.frame(misprints=c(rpois(5000,2),rpois(5000,5)))\n\n\nhead(my_data)\n\n  misprints\n1         1\n2         1\n3         2\n4         1\n5         2\n6         4\n\n\n\ndim(my_data)\n\n[1] 10000     1\n\n\nFigure @ref(fig:hist151-fig) is a histogram of the data.\n\ngf_histogram(~misprints,data=my_data,breaks=seq(-0.5, 15.5, by=1)) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x=\"Number of Misprints\")\n\n\n\n\nMisprints from combined history and philosphy books.\n\n\n\n\nOr as a bar chart in Figure @ref(fig:bar151-fig)\n\ngf_bar(~misprints,data=my_data)%&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x=\"Number of Misprints\")\n\n\n\n\nMisprints from combined history and philosphy books as a bar chart.\n\n\n\n\nAnd now find the average.\n\nmean(~misprints,data=my_data)\n\n[1] 3.4968\n\n\nNow for a mathematical solution. Let \\(X\\) stand for the number of misprints and \\(Y\\) for the book, to make \\(Y\\) a random variable let’s call 0 history and 1 philosophy. Then \\(E[X|Y]\\) is the expected number of misprints given the book. This is a conditional expectation. For example \\(E[X|Y=0]\\) is the expected number of misprints in the history book.\nNow here is the tricky part, without specifying a value of \\(Y\\), called a realization, this expectation function is a random variable that depends on \\(Y\\). In other words, if we don’t know the book, the expected number of misprints depends on \\(Y\\) and thus is a random variable. If we take the expected value of this random variable we get\n\\[E[X]=E[E[X|Y]]\\]\nThe inner expectation in the right hand side is for the conditional distribution and the outer is for the marginal with respect to \\(Y\\). This seems confusing, so let’s go back to our example.\n\\[E[X]=E[E[X|Y]]\\] \\[=E[X|Y=0] \\cdot \\mbox{P}(Y=0)+E[X|Y=1] \\cdot \\mbox{P}(Y=1)\\] \\[=2*\\frac{1}{2}+5*\\frac{1}{2}=\\frac{7}{2}=3.5\\] These ideas are going to be similar for continuous random variables. There is so much more that you can do with conditional expectations, but these are beyond the scope of the book.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "Transformations.html",
    "href": "Transformations.html",
    "title": "16  Transformations",
    "section": "",
    "text": "16.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Transformations.html#objectives",
    "href": "Transformations.html#objectives",
    "title": "16  Transformations",
    "section": "",
    "text": "Determine the distribution of a transformed discrete random variable using appropriate methods, and use it to calculate probabilities.\nDetermine the distribution of a transformed continuous random variable using appropriate methods, and use it to calculate probabilities.\nDetermine the distribution of a transformation of multivariate random variables using simulation, and use it to calculate probabilities.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Transformations.html#transformations",
    "href": "Transformations.html#transformations",
    "title": "16  Transformations",
    "section": "16.2 Transformations",
    "text": "16.2 Transformations\nThroughout our coverage of random variables, we have mentioned transformations of random variables. These have been in the context of linear transformations. We have discussed expected value and variance of linear transformations. Recall that \\(\\mbox{E}(aX+b)=a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\).\nIn this chapter, we will discuss transformations of random variables in general, beyond the linear case.\n\n16.2.1 Transformations of discrete random variables\nLet \\(X\\) be a discrete random variable and let \\(g\\) be a function. The variable \\(Y=g(X)\\) is a discrete random variable with pmf:\n\\[\nf_Y(y)=\\mbox{P}(Y=y)=\\sum_{\\forall x: g(x)=y}\\mbox{P}(X=x)=\\sum_{\\forall x: g(x)=y}f_X(x)\n\\]\nAn example would help since the notation can be confusing.\n\nExample:\nSuppose \\(X\\) is a discrete random variable with pmf: \\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.05, & x=-2 \\\\\n0.10, & x=-1 \\\\\n0.35, & x=0 \\\\\n0.30, & x=1 \\\\\n0.20, & x=2 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\n\nFind the pmf for \\(Y = X^2\\).\nIt helps to identify the support of \\(Y\\), that is where \\(f_{Y}(y)&gt;0\\). Since the support of \\(X\\) is \\(S_X=\\{-2,-1,0,1,2\\}\\), the support of \\(Y\\) is \\(S_Y=\\{0,1,4\\}\\).\n\\[\nf_Y(0)=\\sum_{x^2=0}f_X(x)=f_X(0)=0.35\n\\]\n\\[\nf_Y(1)=\\sum_{x^2=1}f_X(x)=f_X(-1)+f_X(1)=0.1+0.3=0.4\n\\]\n\\[\nf_Y(4)=\\sum_{x^2=4}f_X(x)=f_X(-2)+f_X(2)=0.05+0.2=0.25\n\\]\nSo,\n\\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.4, & y=1 \\\\\n0.25, & y=4 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nIt also helps to confirm that these probabilities add to one, which they do. This is the pmf of \\(Y=X^2\\).\nThe key idea is to find the support of the new random variable and then go back to the original random variable and sum all the probabilities that get mapped into that new support element.\n\n\n16.2.2 Transformations of continuous random variables\nThe methodology above will not work directly in the case of continuous random variables. This is because in the continuous case, the pdf, \\(f_X(x)\\), represents density and not probability.\n\n\n16.2.3 Simulation\nWe can get an estimate of the distribution by simulating the random variable \\(Y\\).\n\nExample:\nFrom an earlier chapter, let \\(X\\) be a continuous random variable with \\(f_X(x) = 2x\\), where \\(0\\leq x\\leq 1\\). Find an approximation to the distribution of \\(Y = \\ln(X)\\) using simulation.\n\nWe’ll first generate random samples from \\(X\\). Then, we’ll transform \\(X\\) to \\(Y\\), and plot the approximate distribution.\nLet’s start by generating random samples from \\(X\\). We can do this using a method called rejection sampling. Rejection sampling involves generating candidate points from a proposal distribution (usually a uniform distribution, with values between 0 and 1) and accepting them with a certain probability that ensures the desired distribution. To determine the probability of rejection, we essentially check whether a second random number \\(u\\) is less than or equal to a specific function value. For example, we want values where \\(y = 2x\\), so we’ll check whether \\(u\\leq 2x\\). An alternative method for this simulation uses the inverse transform method, which requires deriving the cdf.1 Because we have de-emphasized the cdf in this book, we’ll focus on the rejection sampling method.\n\nset.seed(1107)\n\nnum_sim &lt;- 10000\n\n# select a candidate value between 0 and 1 for x\n# then, select another value from 0 to the max of the pdf\n#   This is our acceptance value\n# generate more points than needed (10000*2) to ensure \n#   enough accepted samples\ncandidates &lt;- runif(num_sim*2, 0, 1)\nu &lt;- runif(num_sim*2, 0, 2)\n\n# create a tibble for candidates and acceptance probabilities\ndf &lt;- tibble(candidate = candidates, u = u)\n\n# filter out accepted samples based on the acceptance criterion\naccepted_samples &lt;- df %&gt;% \n  filter(u &lt;= 2*candidate) %&gt;% \n  slice_head(n = num_sim) %&gt;%   # ensure exactly n samples \n  pull(candidate)         # use only the accepted candidate values \n\nNow, we’ll transform the values of \\(X\\) to \\(Y\\) using \\(Y = \\ln(X)\\).\n\nY_vals &lt;- log(accepted_samples)\n\nNow we can examine the simulated values using inspect() or another favorite function.\n\ninspect(Y_vals)\n\n# A tibble: 1 × 10\n  class     min     Q1 median     Q3        max   mean    sd     n missing\n* &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;\n1 numeric -4.91 -0.696 -0.347 -0.142 -0.0000466 -0.501 0.504 10000       0\n\n\nFinally, we can plot the approximate distribution. Figure 16.1 is the density plot of the transformed random variable \\(Y\\) from the simulation. We can see that \\(-\\infty &lt; y\\leq 0\\) and the density is tight near zero but skewed to the left.\n\ngf_density(~Y_vals, xlab = \"Y\", ylab = \"Density\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 16.1: The density plot of expression Y, where Y = ln(X).\n\n\n\n\n\n\n\n16.2.4 The pdf method\nWe are interested in the transformed variable \\(Y = g(x)\\). In many cases, we want to find the pdf of \\(Y\\) and use it to calculate probabilities. As long as \\(g^{-1}(x)\\) is differentiable, we can use the following pdf method to directly obtain the pdf of \\(Y\\):\n\\[ f_Y(y) = f_X(g^{-1}(y))\\bigg| \\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y}  g^{-1}(y) \\bigg| \\]\nNote that in some texts, the portion of this expression \\(\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} g^{-1}(y)\\) is sometimes referred to as the Jacobian. Note that we need to take the absolute value of the transformation function \\(g(x)\\). The reason for this is left to the interested learner.2\n\nExercise:\nLet \\(X\\sim \\textsf{Unif}(0,1)\\) and let \\(Y=X^2\\). Find the pdf of \\(Y\\) using the pdf method.\n\nSince \\(X\\) has the uniform distribution, we know that \\(f_X(x)=1\\) for \\(0\\leq x \\leq 1\\). Also, \\(g(x)=x^2\\) and \\(g^{-1}(y)=\\sqrt{y}\\), which is differentiable. So,\n\\[ f_Y(y)=f_X(\\sqrt{y})\\bigg|\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} \\sqrt{y}\\bigg| = \\frac{1}{2\\sqrt{y}} \\]\n\n\n16.2.5 The cdf method - OPTIONAL\nThe cdf method is one of several methods that can be used for transformations of continuous random variables. The idea is to find the cdf of the new random variable and then find the pdf by way of the fundamental theorem of calculus. However, we continue to de-emphasize the cdf in this book, so we will leave the cdf method to the interested learner.\n\n\n\n\n\n\nDetails of the cdf method\n\n\n\nSuppose \\(X\\) is a continuous random variable with cdf \\(F_X(x)\\). Let \\(Y=g(X)\\). We can find the cdf of \\(Y\\) as:\n\\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X)\\leq y)=\\mbox{P}(X\\leq g^{-1}(y))=F_X(g^{-1}(y)) \\]\nTo get the pdf of \\(Y\\) we would need to take the derivative of the cdf. Note that \\(g^{-1}(y)\\) is the function inverse while \\(g(y)^{-1}\\) is the multiplicative inverse.\nThis method requires the transformation function to have an inverse. Sometimes we can break the domain of the original random variables into regions where an inverse of the transformation function exists.\n\nExample: Let \\(X\\sim \\textsf{Unif}(0,1)\\) and let \\(Y=X^2\\). Find the pdf of \\(Y\\).\n\nBefore we start, let’s think about the distribution of \\(Y\\). We are randomly taking numbers between 0 and 1 and then squaring them. Squaring a positive number less than 1 makes it even smaller. We thus suspect the pdf of \\(Y\\) will have larger density near 0 than 1. The shape is hard to determine so let’s do some math.\nSince \\(X\\) has the uniform distribution, we know that \\(f_X(x)\\) and \\(F_X(x)=x\\) for \\(0\\leq x \\leq 1\\). So, \\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(X^2\\leq y)=\\mbox{P}(X\\leq \\sqrt{y})=F_X\\left(\\sqrt{y}\\right)=\\sqrt{y} \\]\nTaking the derivative of this yields: \\[ f_Y(y)=\\frac{1}{2\\sqrt{y}} \\]\nfor \\(0 &lt; y \\leq 1\\) and 0 otherwise. Notice we can’t have \\(y=0\\) since we would be dividing by zero. This is not a problem since we have a continuous distribution. We could verify this a proper pdf by determining if the pdf integrates to 1 over the domain: \\[ \\int_0^1 \\frac{1}{2\\sqrt{y}} \\,\\mathrm{d}y = \\sqrt{y}\\bigg|_0^1 = 1 \\] We can also do this using R but we first have to create a function that can take vector input.\n\ny_pdf &lt;- function(y) {1/(2*sqrt(y))}\n\n\ny_pdf&lt;- Vectorize(y_pdf)\n\n\nintegrate(y_pdf, 0, 1)\n\n1 with absolute error &lt; 2.9e-15\n\n\nNotice that since the domain of the original random variable was non-negative, the squared function had an inverse.\nThe pdf of the random variable \\(Y\\) is plotted in Figure 16.2.\n\ngf_line(y_pdf(seq(0.01, 1, 0.01)) ~ seq(0.01, 1, 0.01),\n        xlab = \"Y\", ylab = expression(f(y))) %&gt;%   \n  gf_theme(theme_bw())\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the density is much larger at we approach 0.\n\n\n\n\n16.2.6 Multivariate Transformations\nFor the discrete case when we have the joint pmf, the process is similar to what we learned above if the transformation is to a univariate random variable. For continuous random variables, the mathematics are a little more difficult so we will just use simulation.\nHere’s the scenario. Suppose \\(X\\) and \\(Y\\) are independent random variables, both uniformly distributed on \\([5,6]\\). \\[\nX\\sim \\textsf{Unif}(5,6)\\hspace{1.5cm} Y\\sim \\textsf{Unif}(5,6)\n\\]\nLet \\(X\\) be your arrival time for dinner and \\(Y\\) your friends arrival time. We picked 5 to 6 because this is the time in the evening we want to meet. Also assume you both travel independently.\nDefine \\(Z\\) as a transformation of \\(X\\) and \\(Y\\) such that \\(Z=|X-Y|\\). Thus \\(Z\\) is the absolute value of the difference between your arrival times. The units for \\(Z\\) are hours. We would like to explore the distribution of \\(Z\\). We could do this via Calculus III (multivariable calculus) methods, but we will use simulation instead.\nWe can use R to obtain simulated values from \\(X\\) and \\(Y\\) (and thus find \\(Z\\)).\n\nExercise:\nFirst, simulate 100,000 observations from the uniform distribution with parameters 5 and 6. Assign those random observations to a variable. Next, repeat that process, assigning those to a different variable. These two vectors represent your simulated values from \\(X\\) and \\(Y\\). Finally, obtain your simulated values of \\(Z\\) by taking the absolute value of the difference.\nTry to complete the code on your own before looking at the code below.\n\n\nset.seed(354)\n\n# generate two values between 5 and 6 from a random uniform distribution\n# do this 100,000 times\nresults &lt;- do(100000)*abs(diff(runif(2, 5, 6)))\n\n\nhead(results)\n\n         abs\n1 0.03171229\n2 0.77846706\n3 0.29111599\n4 0.06700434\n5 0.08663187\n6 0.40622840\n\n\nFigure 16.3 is a plot of the estimated density of the transformation.\n\nresults %&gt;%\n  gf_density(~abs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"|X-Y|\",y=\"\")\n\n\n\n\n\n\n\nFigure 16.3: The density of the absolute value of the difference in uniform random variables.\n\n\n\n\n\nFigure 16.4 is a plot of the estimated density of the transformation as a histogram.\n\nresults %&gt;%\n  gf_histogram(~abs)%&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"|X-Y|\",y=\"\")\n\n\n\n\n\n\n\nFigure 16.4: Histogram of the absolute value of the difference in random variables.\n\n\n\n\n\n\ninspect(results)\n\n\nquantitative variables:  \n  name   class          min       Q1    median        Q3       max     mean\n1  abs numeric 1.265667e-06 0.133499 0.2916012 0.4990543 0.9979459 0.332799\n         sd      n missing\n1 0.2358863 100000       0\n\n\n\nExercise:\nNow suppose whomever arrives first will only wait 5 minutes and then leave. What is the probability you eat together?\n\n\ndata.frame(results) %&gt;%\n  summarise(mean(abs&lt;=5/60))\n\n  mean(abs &lt;= 5/60)\n1           0.15966\n\n\n\nExercise:\nHow long should the first person wait so that there is at least a 50% probability of you eating together?\n\nWe leave the solution to the interested learner.3",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Transformations.html#footnotes",
    "href": "Transformations.html#footnotes",
    "title": "16  Transformations",
    "section": "",
    "text": "To sample from a non-uniform distribution using inverse transform sampling, we generate a uniform random variable \\(U\\) on the interval \\([0, 1]\\). Then, we use the cdf of \\(X\\) to find the inverse, \\(X = F^{-1}(U)\\). For \\(f(x) = 2x\\), the cdf is \\(F(x) = x^2\\) and the inverse cdf is \\(F^{-1}(u) = \\sqrt{u}\\). So, we take the square root of the random uniform values generated to get values of \\(X\\). Finally, we transform those values to \\(Y\\). In this case, we take the natural logarithm, \\(Y = \\ln(X)\\). We can then plot and analyze the distribution of \\(Y\\).↩︎\nWe need to take the absolute value of the transformation function \\(g(x)\\) because if it is a decreasing function, we have\n\\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X) \\leq y)=\\mbox{P}(X \\geq g^{-1}(y))= 1 - F_X(g^{-1}(y)) \\]↩︎\nLet’s write a function to find the cdf.\n\nz_cdf &lt;- function(x) {mean(results$abs &lt;= x)}\n\n\nz_cdf &lt;- Vectorize(z_cdf)\n\nNow test for 5 minutes to make sure our function is correct since we determined above that this value should be 0.15966.\n\nz_cdf(5/60)\n\n[1] 0.15966\n\n\nLet’s plot to see what the cdf looks like.\n\ngf_line(z_cdf(seq(0, 1, 0.01)) ~ seq(0, 1, 0.01),\n        xlab = \"Time Difference\", ylab = \"CDF\") %&gt;%   \n  gf_theme(theme_bw())\n\n\n\n\n\n\n\n\nIt looks like somewhere around 15 minutes, a quarter of an hour. But we will find a better answer by finding the root. In the code that follows we want to find where the cdf equals 0.5. The function uniroot() solves the given equations for roots so we want to put in the cdf minus 0.5. In other words, uniroot() solves \\(f(x)=0\\) for x.\n\nuniroot(function(x) z_cdf(x) - 0.5, c(0.25, 35))$root\n\n[1] 0.2916077\n\n\nSo it is actually 0.292 hours, 17.5 minutes. So round up and wait 18 minutes.↩︎",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html",
    "href": "Estimation-Methods.html",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html#objectives",
    "href": "Estimation-Methods.html#objectives",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "Apply the method of moments to estimate parameters or sets of parameters from given data.\nDerive the likelihood function for a given random sample from a distribution.\nDerive a maximum likelihood estimate of a parameter or set of parameters.\nCalculate and interpret the bias of an estimator by analyzing its expected value relative to the true parameter.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html#transitioning-from-probability-models-to-statistical-models",
    "href": "Estimation-Methods.html#transitioning-from-probability-models-to-statistical-models",
    "title": "17  Estimation Methods",
    "section": "17.2 Transitioning from probability models to statistical models",
    "text": "17.2 Transitioning from probability models to statistical models\nWe started this book with descriptive models of data and then moved onto probability models. In these probability models, we have been characterizing experiments and random processes using both theory and simulation. These models are using a model about a random event to make decisions about data. These models are about the population and are used to make decisions about samples and data. For example, suppose we flip a fair coin 10 times, and record the number of heads. The population is the collection of all possible outcomes of this experiment. In this case, the population is infinite, as we could run this experiment repeatedly without limit. If we assume, model, the number of heads as a binomial distribution, we know the exact distribution of the outcomes. For example, we know that exactly 24.61% of the time, we will obtain 5 heads out of 10 flips of a fair coin. We can also use the model to characterize the variance, that is when it does not equal 5 and how much different from 5 it will be. However, these probability models are highly dependent on the assumptions and the values of the parameters.\nFrom this point on in the book, we will focus on statistical models. Statistical models describe one or more variables and their relationships. We use these models to make decisions about the population, to predict future outcomes, or both. Often we don’t know the true underlying process; all we have is a sample of observations and perhaps some context. Using inferential statistics, we can draw conclusions about the underlying process. For example, suppose we are given a coin and we don’t know whether it is fair. So, we flip it a number of times to obtain a sample of outcomes. We can use that sample to decide whether the coin could be fair.\nIn some sense, we’ve already explored some of these concepts. In our simulation examples, we have drawn observations from a population of interest and used those observations to estimate characteristics of another population or segment of the experiment. For example, we explored random variable \\(Z\\), where \\(Z=|X - Y|\\) and \\(X\\) and \\(Y\\) were both uniform random variables. Instead of dealing with the distribution of \\(Z\\) directly, we simulated many observations from \\(Z\\) and used this simulation to describe the behavior of \\(Z\\).\nStatistical models and probability models are not separate. In statistical models we find relationships, the explained portion of variation, and use probability models for the remaining random variation. In Figure 17.1, we demonstrate this relationship between the two types of models. In the first part of our studies, we will use univariate data in statistical models to estimate the parameters of a probability model. From there we will develop more sophisticated models to include multivariate models.\n\n\n\n\n\n\n\n\nFigure 17.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we don’t know the underlying process, and must infer based on representative samples.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html#estimation",
    "href": "Estimation-Methods.html#estimation",
    "title": "17  Estimation Methods",
    "section": "17.3 Estimation",
    "text": "17.3 Estimation\nRecall that in probability models, we have complete information about the population and we use that to describe the expected behavior of samples from that population. In statistics we are given a sample from a population about which we know little or nothing.\nIn this chapter, we will discuss estimation. Given a sample, we would like to estimate population parameters. There are several ways to do that. We will discuss two methods: method of moments and maximum likelihood.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html#method-of-moments",
    "href": "Estimation-Methods.html#method-of-moments",
    "title": "17  Estimation Methods",
    "section": "17.4 Method of Moments",
    "text": "17.4 Method of Moments\nRecall earlier we discussed moments. We can refer to \\(\\mbox{E}(X) = \\mu\\) as the first moment or mean. Further, we can refer to \\(\\mbox{E}(X^k)\\) as the \\(k\\)th central moment and \\(\\mbox{E}[(X-\\mu)^k]\\) as the \\(k\\) moment around the mean. The second moment around the mean is also known as variance. It is important to point out that these are POPULATION moments and are typically some function of the parameters of a probability model.\nSuppose \\(X_1,X_2,...,X_n\\) is a sequence of independent, identically distributed random variables with some distribution and parameters \\(\\boldsymbol{\\theta}\\). When provided with a random sample of data, we will not know the population moments. However, we can obtain sample moments. The \\(k\\)th central sample moment is denoted by \\(\\hat{\\mu}_k\\) and is given by \\[\n\\hat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n x_i^k\n\\]\nThe \\(k\\)th sample moment around the mean is denoted by \\(\\hat{\\mu}'_k\\) and is given by \\[\n\\hat{\\mu}'_k=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^k\n\\]\nThe value \\(\\hat{\\mu}\\) is read “mu-hat”. The hat denotes that the value is an estimate.\nWe can use the sample moments to estimate the population moments since the population moments are usually functions of a distribution’s parameters, \\(\\boldsymbol{\\theta}\\). Thus, we can solve for the parameters to obtain method of moments estimates of \\(\\boldsymbol{\\theta}\\).\nThis is all technical, so let’s look at an example.\n\nExample:\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d., independent and identically distributed, sample from a uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), and we don’t know \\(\\theta\\). That is, our data consists of positive random numbers but we don’t know the upper bound. Find the method of moments estimator for \\(\\theta\\), the upper bound.\n\nWe know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{E}(X)=\\frac{a+b}{2}\\). So, in this case, \\(\\mbox{E}(X)={\\theta \\over 2}\\). This is the first population moment. We can estimate this with the first sample moment, which is just the sample mean: \\[\n\\hat{\\mu}_1=\\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x}\n\\]\nOur best guess for the first population moment (\\(\\theta/2\\)) is the first sample moment (\\(\\bar{x}\\)). From a common sense perspective, we are hoping that the sample moment will be close in value to the population moment, so we can set them equal and solve for the unknown population parameter. This is essentially what we were doing in our simulations of probability models. Solving for \\(\\theta\\) yields our method of moments estimator for \\(\\theta\\): \\[\n\\hat{\\theta}_{MoM}=2\\bar{x}\n\\]\nNote that we could have used the second moments about the mean as well. This is less intuitive but still applicable. In this case we know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{Var}(X)=\\frac{(b - a)^2}{12}\\). So, in this case, \\(\\mbox{Var}(X)=\\frac{\\theta ^2}{ 12}\\). We use the second sample moment about the mean \\(\\hat{\\mu}'_2=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^2\\) which is not quite the sample variance. In fact, the sample variance is related to the second sample moment about the mean by \\(\\hat{\\mu}'_2 = s^2 \\frac{n}{n-1}\\). Setting the population moment and sample moment equal and solving we get\n\\[\n\\hat{\\theta}_{MoM}=\\sqrt{\\frac{12n}{n-1}}s\n\\]\nTo decide which is better we need a criteria of comparison. This is beyond the scope of this book, but some common criteria are unbiased and minimum variance.\nThe method of moments can be used to estimate more than one parameter as well. We simply would have to incorporate higher order moments.\n\nExample:\nSuppose we take an i.i.d. sample from the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Find method of moments estimates of \\(\\mu\\) and \\(\\sigma\\).\n\nFirst, we remember that we know two population moments for the normal distribution: \\[\n\\mbox{E}(X)=\\mu \\hspace{1cm} \\mbox{Var}(X)=\\mbox{E}[(X-\\mu)^2]=\\sigma^2\n\\]\nSetting these equal to the sample moments yields: \\[\n\\hat{\\mu}_{MoM}=\\bar{x} \\hspace{1cm} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\nAgain, we notice that the estimate for \\(\\sigma\\) is different from sample standard deviation discussed earlier in the book. The reason for this is a property of estimators called unbiased. Notice that if we treat the data points as random variables then the estimators are random variables. We can then take the expected value of the estimator and if this equals the parameter being estimated, then it is unbiased. Mathematically, this is written \\[\nE(\\hat{\\theta})=\\theta\n\\] Unbiased is not a required property for an estimator, but many practitioners find it desirable. In words, unbiased means that on average the estimator will equal the true value. Sample variance using \\(n-1\\) in the denominator is an unbiased estimate of the population variance.\n\nExercise:\nYou shot 25 free throws and make 21. Assuming a binomial model fits. Find an estimate of the probability of making a free throw.\n\nThere are two ways to approach this problem depending on how we define the random variable. In the first case we will use a binomial random variable, \\(X\\) the number of made free throws in 25 attempts. In this case, we only ran the experiment once and have the observed result of 21. Recall for the binomial \\(E(X)=np\\) where \\(n\\) is the number of attempts and \\(p\\) is the probability of success. The sample mean is 21 since we only have one data point. Using the method of moments, we set the first population mean equal to the first sample mean \\(np=\\frac{\\sum{x_i}}{m}\\), notice \\(n\\) is the number of trials 25 and \\(m\\) is the number of data points 1, or \\(25 \\hat{p} = 21\\). Thus \\(\\hat{p} = \\frac{21}{25}\\).\nA second approach is to let \\(X_i\\) be a single free throw, we have a Bernoulli random variable. This variable takes on the values of 0 if we miss and 1 if we make the free throw. Thus we have 25 data points. For a Bernoulli random variable \\(E(X)=p\\). The sample is \\(\\bar{x} = \\frac{21}{25}\\). Using the method of moments, we set the sample mean equal to the population mean. We have \\(E(X) = \\hat{p} = \\bar{x} = \\frac{21}{25}\\). This is a natural estimate; we estimate our probability of success as the number of made free throws divided by the number of shots. As a side note, this is an unbiased estimator since \\[\nE(\\hat{p})=E\\left( \\sum{\\frac{X_i}{n}} \\right)\n\\]\n\\[\n=  \\sum{E\\left( \\frac{X_i}{n} \\right)}= \\sum{ \\frac{E\\left(X_i\\right)}{n}}=\\sum{\\frac{p}{n}}=\\frac{np}{n}=p\n\\]",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Estimation-Methods.html#maximum-likelihood",
    "href": "Estimation-Methods.html#maximum-likelihood",
    "title": "17  Estimation Methods",
    "section": "17.5 Maximum likelihood",
    "text": "17.5 Maximum likelihood\nRecall that using method of moments involves finding values of the parameters that cause the population moments to be equal to the sample moments. Solving for the parameters yields method of moments estimates.\nNext we will discuss one more estimation method, maximum likelihood estimation. In this method, we are finding values of parameters that would make the observed data most “likely”. In order to do this, we first need to introduce the likelihood function.\n\n17.5.1 Likelihood Function\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d. random sample from a distribution with mass/density function \\(f_{X}(x;\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) are the parameters. Let’s take a second to explain this notation. We are using a bold symbol for \\(\\boldsymbol{\\theta}\\) to indicate it is a vector, that it can be one or more values. However, in the pmf/pdf \\(x\\) is not bold since it is a scalar variable. In our probability models we know \\(\\boldsymbol{\\theta}\\) and then use to model to make decision about the random variable \\(X\\).\nThe likelihood function is denoted as \\(L(\\boldsymbol{\\theta};x_1,x_2,...,x_n) = L(\\boldsymbol{\\theta};\\boldsymbol{x})\\). Now we have multiple instances of the random variable, we use \\(\\boldsymbol{x}\\). Since our random sample is i.i.d., independent and identically distributed, we can write the likelihood function as a product of the pmfs/pdfs: \\[\nL(\\boldsymbol{\\theta};\\boldsymbol{x})=\\prod_{i=1}^n f_X(x_i;\\boldsymbol{\\theta})\n\\]\nThe likelihood function is really the pmf/pdf except instead of the variables being random and the parameter(s) fixed, the values of the variable are known and the parameter(s) are unknown. A note on notation, we are using the semicolon in the pdf and likelihood function to denote what is known or given. In the pmf/pdf the parameters are known and thus follow the semicolon. The opposite is the case in the likelihood function.\nLet’s do an example to help understand these ideas.\n\nExample:\nSuppose we are presented with a coin and are unsure of its fairness. We toss the coin 50 times and obtain 18 heads and 32 tails. Let \\(\\pi\\) be the probability that a coin flip results in heads, we could use \\(p\\) but we are getting you used to the two different common ways to represent a binomial parameter. What is the likelihood function of \\(\\pi\\)?\n\nThis is a binomial process, but each individual coin flip can be thought of as a Bernoulli experiment. That is, \\(x_1,x_2,...,x_{50}\\) is an i.i.d. sample from \\(\\textsf{Binom}(1,\\pi)\\) or, in other words, \\(\\textsf{Bernoulli}(\\pi)\\). Each \\(x_i\\) is either 1 or 0. The pmf of \\(X\\), a Bernoulli random variable, is simply: \\[\nf_X(x;\\pi)= \\binom{1}{x} \\pi^x(1-\\pi)^{1-x} = \\pi^x(1-\\pi)^{1-x}\n\\]\nNotice this makes sense\n\\[\nf_X(1)=P(X=1)= \\pi^1(1-\\pi)^{1-1}=\\pi\n\\]\nand\n\\[\nf_X(0)=P(X=0)= \\pi^0(1-\\pi)^{1-0}=(1-\\pi)\n\\]\nGeneralizing for any sample size \\(n\\), the likelihood function is: \\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{n} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{\\sum_{i=1}^{n} x_i}(1-\\pi)^{n-\\sum_{i=1}^{n} x_i}\n\\]\nFor our example \\(n=50\\) and the\n\\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{50} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{18}(1-\\pi)^{32}\n\\]\nwhich makes sense because we had 18 successes, heads, and 32 failures, tails. The likelihood function is a function of the unknown parameter \\(\\pi\\).\n\n\n17.5.2 Maximum Likelihood Estimation\nOnce we have a likelihood function \\(L(\\boldsymbol{\\theta},\\boldsymbol{x})\\), we need to figure out which value of \\(\\boldsymbol{\\theta}\\) makes the data most likely. In other words, we need to maximize \\(L\\) with respect to \\(\\boldsymbol{\\theta}\\).\nMost of the time (but not always), this will involve simple optimization through calculus (i.e., take the derivative with respect to the parameter, set to 0 and solve for the parameter). When maximizing the likelihood function through calculus, it is often easier to maximize the log of the likelihood function, denoted as \\(l\\) and often referred to as the “log-likelihood function”: \\[\nl(\\boldsymbol{\\theta};\\boldsymbol{x})= \\log L(\\boldsymbol{\\theta};\\boldsymbol{x})\n\\] Note that since logarithm is one-to-one, onto and increasing, maximizing the log-likelihood function is equivalent to maximizing the likelihood function, and the maximum will occur at the same values of the parameters. We are using log because now we can take the derivative of a sum instead of a product, thus making it much easier.\n\nExample:\nContinuing our example. Find the maximum likelihood estimator for \\(\\pi\\).\n\nRecall that our likelihood function is \\[\nL(\\pi;\\boldsymbol{x})= \\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\n\\]\nFigure 17.2 is a plot of the likelihood function as a function of the unknown parameter \\(\\pi\\).\n\n\n\n\n\n\n\n\nFigure 17.2: Likelihood function for 18 successes in 50 trials\n\n\n\n\n\nBy visual inspection, the value of \\(\\pi\\) that makes our data most likely, maximizes the likelihood function, is something a little less than 0.4, the actual value is 0.36 as indicated by the blue line in Figure 17.2.\nTo maximize by mathematical methods, we need to take the derivative of the likelihood function with respect to \\(\\pi\\). We can do this because the likelihood function is a continuous function. Even though the binomial is a discrete random variable, its likelihood is a continuous function.\nWe can find the derivative of the likelihood function by applying the product rule: \\[\n{\\,\\mathrm{d}L(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi} = \\left(\\sum x_i\\right) \\pi^{\\sum x_i -1}(1-\\pi)^{n-\\sum x_i} + \\pi^{\\sum x_i}\\left(\\sum x_i -n\\right)(1-\\pi)^{n-\\sum x_i -1}\n\\]\nWe could simplify this, set to 0, and solve for \\(\\pi\\). However, it may be easier to use the log-likelihood function: \\[\nl(\\pi;\\boldsymbol{x})=\\log L(\\pi;\\boldsymbol{x})= \\log \\left(\\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\\right) = \\sum x_i \\log \\pi + (n-\\sum x_i)\\log (1-\\pi)\n\\]\nNow, taking the derivative does not require the product rule: \\[\n{\\,\\mathrm{d}l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi}= {\\sum x_i \\over \\pi} - {n-\\sum x_i\\over (1-\\pi)}\n\\]\nSetting equal to 0 yields: \\[\n{\\sum x_i \\over \\pi} ={n-\\sum x_i\\over (1-\\pi)}\n\\]\nSolving for \\(\\pi\\) yields \\[\n\\hat{\\pi}_{MLE}={\\sum x_i \\over n}\n\\]\nNote that technically, we should confirm that the function is concave down at our critical value, ensuring that \\(\\hat{\\pi}_{MLE}\\) is, in fact, a maximum: \\[\n{\\,\\mathrm{d}^2 l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi^2}= {-\\sum x_i \\over \\pi^2} - {n-\\sum x_i\\over (1-\\pi)^2}\n\\]\nThis value is negative for all relevant values of \\(\\pi\\), so \\(l\\) is concave down and \\(\\hat{\\pi}_{MLE}\\) is a maximum.\nIn the case of our example (18 heads out of 50 trials), \\(\\hat{\\pi}_{MLE}=18/50=0.36\\).\nThis seems to make sense. Our best guess for the probability of heads is the number of observed heads divided by our number of trials. That was a great deal of algebra and calculus for what appears to be an obvious answer. However, in more difficult problems, it is not as obvious what to use for a MLE.\n\n\n17.5.3 Numerical Methods - OPTIONAL\nWhen obtaining MLEs, there are times when analytical methods (calculus) are not feasible or not possible. In the Pruim book (Pruim 2011), there is a good example regarding data from Old Faithful at Yellowstone National Park. We leave the example to the interested learner.\n\n\n\n\n\n\nNumerical methods example - Old Faithful\n\n\n\nWe need to load the fastR2 package for this example.\n\nlibrary(fastR2)\n\nThe faithful data set is preloaded into R and contains 272 observations of 2 variables: eruption time in minutes and waiting time until next eruption. If we plot eruption durations, we notice that the distribution appears bimodal (see Figure 17.3).\n\nfaithful %&gt;% gf_histogram(~eruptions, fill = \"cyan\", color = \"black\") %&gt;%   \n  gf_theme(theme_bw()) %&gt;%   \n  gf_labs(x = \"Duration in minutes\", y = \"Count\") \n#histogram(~eruptions, data = faithful, breaks = 30)\n\n\n\n\n\n\n\n\n\n\n\n\nWithin each section, the distribution appears somewhat bell-curve-ish so we’ll model the eruption time with a mixture of two normal distributions. In this mixture, a proportion \\(\\alpha\\) of our eruptions belong to one normal distribution and the remaining \\(1-\\alpha\\) belong to the other normal distribution. The density function of eruptions is given by: \\[ \\alpha f(x;\\mu_1,\\sigma_1)+(1-\\alpha)f(x;\\mu_2,\\sigma_2) \\]\nwhere \\(f\\) is the pdf of the normal distribution with parameters specified.\nWe have five parameters to estimate: \\(\\alpha, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). Obviously, estimation through differentiation is not feasible and thus we will use numerical methods. This code is less in the spirit of tidyverse but we want you to see the example. Try to work your way through the code below:\n\n# Define function for pdf of eruptions as a mixture of normals \ndmix &lt;- function(x, alpha, mu1, mu2, sigma1, sigma2){   \n  if(alpha &lt; 0) dnorm(x, mu2, sigma2)   \n  if(alpha &gt; 1) dnorm(x, mu1, sigma1)   \n  if(alpha &gt;= 0 && alpha &lt;=1){     \n    alpha*dnorm(x, mu1, sigma1) + (1 - alpha)*dnorm(x, mu2, sigma2)   \n  } \n}\n\nNext write a function for the log-likelihood function. R is a vector based programming language so we send theta into the function as a vector argument.\n\n# Create the log-likelihood function \nloglik &lt;- function(theta,x){   \n  alpha = theta[1]   \n  mu1 = theta[2]   \n  mu2 = theta[3]   \n  sigma1 = theta[4]   \n  sigma2 = theta[5]   \n  density &lt;- function(x){     \n    if(alpha &lt; 0) return (Inf)     \n    if(alpha &gt; 1) return (Inf)     \n    if(sigma1 &lt; 0) return (Inf)     \n    if(sigma2 &lt; 0) return (Inf)     \n    dmix(x, alpha, mu1, mu2, sigma1, sigma2)   \n  }   \n  sum(log(sapply(x, density)))\n}\n\nFind the sample mean and standard deviation of the eruption data to use as starting points in the optimization routine.\n\nm &lt;- mean(faithful$eruptions) \ns &lt;- sd(faithful$eruptions)\n\nUse the function nlmax() to maximize the non-linear log-likelihood function.\n\nmle &lt;- nlmax(loglik, \n             p = c(0.5, m - 1, m + 1, s, s), \n             x = faithful$eruptions)$estimate \nmle\n\n[1] 0.3484040 2.0186065 4.2733410 0.2356208 0.4370633\n\n\nSo, according to our MLEs, about 34.84% of the eruptions belong to the first normal distribution (the one on the left). Furthermore the parameters of that first distribution are a mean of 2.019 and a standard deviation of 0.236. Likewise, 65.16% of the eruptions belong to the second normal with mean of 4.27 and standard deviation of 0.437.\nPlotting the density atop the histogram shows a fairly good fit:\n\ndmix2 &lt;- function(x) dmix(x, mle[1], mle[2], mle[3], mle[4], mle[5]) \n#y_old&lt;-dmix2(seq(1,6,.01)) \n#x_old&lt;-seq(1,6,.01) \n#dens_data&lt;-data.frame(x=x_old,y=y_old) \n#faithful%&gt;% \n#gf_histogram(~eruptions,fill=\"cyan\",color = \"black\") %&gt;% \n#  gf_curve(y~x,data=dens_data)%&gt;% \n#  gf_theme(theme_bw()) %&gt;% \n#  gf_labs(x=\"Duration in minutes\",y=\"Count\")  \nhist(faithful$eruptions, breaks = 40, freq = F, main = \"\", \n     xlab = \"Duration in minutes.\") \ncurve(dmix2, from = 1, to = 6, add = T)\n\n\n\n\nHistogram of eruption duration with estimated mixture of normals plotted on top.\n\n\n\n\nThis is a fairly elaborate example but it is cool. You can see the power of the method and the software.\n\n\n\n\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html",
    "href": "Inference-Case-Study.html",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#objectives",
    "href": "Inference-Case-Study.html#objectives",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "Using bootstrap methods, obtain and interpret a confidence interval for an unknown parameter, based on a random sample.\nConduct a hypothesis test using a randomization test, to include all 4 steps.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#introduction",
    "href": "Inference-Case-Study.html#introduction",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nWe now have the foundation to move on to statistical modeling, both inferential and prediction. First we will begin with inference, where we use the ideas of estimation and the variance of estimates to make decisions about the population. We will also briefly introduce the ideas of prediction. Then in the final block of material, we will examine some common linear models and use them for both prediction and inference.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#foundation-for-inference",
    "href": "Inference-Case-Study.html#foundation-for-inference",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.3 Foundation for inference",
    "text": "18.3 Foundation for inference\nSuppose a professor randomly splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product (e.g., iPad, iPod, iPhone, AirPods, Apple Watch, etc.) on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)?\nWhile the proportions would probably be close to each other, they are probably not exactly the same. We would probably observe a small difference due to chance.\n\nExercise:\nIf we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?1\n\nStudying randomness of this form is a key focus of statistical modeling. In this block, we’ll explore this type of randomness in the context of several applications, and we’ll learn new tools and ideas that can be applied to help make decisions from data.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#case-study-gender-discrimination",
    "href": "Inference-Case-Study.html#case-study-gender-discrimination",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.4 Case study: gender discrimination",
    "text": "18.4 Case study: gender discrimination\nWe consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.2 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?”\n\n18.4.1 Variability within data\nThe participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects (the bank supervisors).\n\nExercise:\nIs this an observational study or an experiment? How does the type of study impact what can be inferred from the results?3\n\nFor each supervisor, we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in the table below, we would like to evaluate whether females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against.\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Decision}\\\\\n& & \\mbox{Promoted} & \\mbox{Not Promoted} & \\mbox{Total}  \\\\\n& \\hline \\mbox{male} & 21 & 3 & 24  \\\\\n\\textbf{Gender}& \\mbox{female} & 14 & 10 & 24  \\\\\n& \\mbox{Total} & 35 & 13 & 48  \\\\\n\\end{array}\n\\]\n\nThought Question:\nStatisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against?\n\nThe large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. Most people come to this conclusion because they think these sample statistics are the actual population parameters. We cannot yet be sure if the observed difference represents discrimination or is just from random variability. Generally, there is fluctuation in sample data; if we conducted the experiment again, we would likely get different values. We also wouldn’t expect the sample proportions for males and females to be exactly equal, even if the truth was that the promotion decisions were independent of gender. To make a decision, we must understand the random variability and use it to compare with the observed difference.\n\n\n18.4.2 The bootstrap\nTypically our available data consists of a single sample from a population. From that sample, we calculate a statistic of interest, such as the mean or median or a proportion. In a perfect world, we would repeatedly take samples from the population and compute our statistic of interest with each new sample. We would do this until we had enough estimates to get a good understanding of the variability of our original estimate. In reality, resampling from the population is often not possible due to the time and monetary cost associated with taking such samples. For example, most national polls conducted by companies like Gallup involve interviewing around 1,000 U.S. adults weekly or even daily. Interviewing this number of adults requires many more contacts be made due to usually high nonresponse rates. Thus, we often need a different solution to get a sense of the variability of an estimate.\nOne way to estimate the random variability in a sample is to use the bootstrap. Bootstrapping allows us to simulate the sampling distribution by resampling with replacement from the original sample. We then calculate and record the statistic of interest for that bootstrap sample and repeat the process many times.\nLet’s use the bootstrap to estimate the variability of the difference in promotion rates for the gender discrimination study. First let’s import the data.\n\ndiscrim &lt;- read_csv(\"data/discrimination_study.csv\")\n\nLet’s inspect the data set.\n\ninspect(discrim)\n\n\ncategorical variables:  \n      name     class levels  n missing\n1   gender character      2 48       0\n2 decision character      2 48       0\n                                   distribution\n1 female (50%), male (50%)                     \n2 promoted (72.9%), not_promoted (27.1%)       \n\n\nLet’s look at a table of the data, showing gender versus decision.\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   not_promoted promoted Total\n  female           10       14    24\n  male              3       21    24\n  Total            13       35    48\n\n\nLet’s do some categorical data cleaning. To get the tally() results to look like our initial table, we need to change the variables from characters to factors and reorder the levels. By default, factor levels are ordered alphabetically, but we want promoted and male to appear as the first levels in the table.\nWe will use mutate_if() to convert character variables to factors and fct_relevel() to reorder the levels.\n\ndiscrim &lt;- discrim %&gt;%   \n  mutate_if(is.character, as.factor) %&gt;%   \n  mutate(gender = fct_relevel(gender, \"male\"),          \n         decision = fct_relevel(decision, \"promoted\"))\n\n\nhead(discrim)\n\n# A tibble: 6 × 2\n  gender decision    \n  &lt;fct&gt;  &lt;fct&gt;       \n1 female not_promoted\n2 female not_promoted\n3 male   promoted    \n4 female promoted    \n5 female promoted    \n6 female promoted    \n\n\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   promoted not_promoted Total\n  male         21            3    24\n  female       14           10    24\n  Total        35           13    48\n\n\nThe table shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). This observed difference is what we call a point estimate of the true effect.\n\nobs &lt;- diffprop(decision ~ gender, data = discrim)\nobs\n\n  diffprop \n-0.2916667 \n\n\nNow that we have the data in the form that we want, we are ready to generate bootstrap samples. We essentially assign an equal probability to each observation in the original sample and sample with replacement, ensuring each bootstrap sample is the same size as the original sample. In R, we use the resample() function from the mosaic package to sample rows of the data with replacement. This generates a new data frame with the same number of rows as the original. Some rows may be duplicated and others may be missing.\nWe’ll resample the rows of discrim, grouped by gender. This maintains the same number of male and female personnel files in the resample.\n\nset.seed(627)\nresample(discrim, groups = gender)\n\n# A tibble: 48 × 3\n   gender decision     orig.id\n   &lt;fct&gt;  &lt;fct&gt;        &lt;chr&gt;  \n 1 male   promoted     24     \n 2 male   promoted     17     \n 3 male   promoted     3      \n 4 male   promoted     35     \n 5 male   not_promoted 31     \n 6 male   promoted     8      \n 7 male   promoted     37     \n 8 male   not_promoted 31     \n 9 male   promoted     24     \n10 male   promoted     8      \n# ℹ 38 more rows\n\n\nRows 5, 9, 10, 15, 22, 23, 31, 35, 36, 40, and 42 appear at least twice. Several other rows did not appear at all. This is a single bootstrap sample of the data.\nWe now calculate a point estimate, a statistic of interest, on the bootstrap sample. In our gender discrimination study, our statistic of interest is the difference in promotion rates. We can use the diffprop() function to calculate the difference in promotion rates for a single bootstrap sample.\n\nset.seed(627)\ndiffprop(decision ~ gender, data = resample(discrim, groups = gender))\n\n  diffprop \n-0.3333333 \n\n\nWe repeat this process many times. The collection of sample statistics comprises a bootstrap distribution of the sample statistic. This procedure typically works quite well, provided the original sample is representative of the population. More details on the advantages and disadvantages of the bootstrap are discussed in Chapter 20.\n\nset.seed(630)\nboot_results &lt;- do(10000)*diffprop(decision ~ gender, \n                                   data = resample(discrim, groups = gender))\n\n\n\n\n\n\n\n\n\nFigure 18.1: The sampling distribution of the difference in promotion rates approximated using a bootstrap distribution.\n\n\n\n\n\nFigure 18.1 displays the bootstrap distribution, which is centered at -29.2%. This is the observed difference in promotion rates from our original sample, show as a black vertical line. The distribution also appears bell-shaped and symmetric.\nNow that we have completed the bootstrap procedure, we can use the bootstrap distribution to estimate the standard error of the statistic of interest, the difference in promotion rates. One way to do this is to look at our favorite summary statistics using the favstats() function.\n\nfavstats(~diffprop, data = boot_results)\n\n   min     Q1     median         Q3       max       mean        sd     n\n -0.75 -0.375 -0.2916667 -0.2083333 0.1666667 -0.2922125 0.1193555 10000\n missing\n       0\n\n\nThe standard deviation displayed by favstats() is what we refer to as the standard error of the difference in promotion rates. From here, the bootstrap distribution can be used to build a confidence interval for the population parameter. The cdata() function can be used to calculate the middle 95% of the bootstrap distribution. That is, the function finds the 2.5 and 97.5 percentiles of the bootstrap distribution using the code below.\n\ncdata(~diffprop, data = boot_results, p = 0.95)\n\n          lower       upper central.p\n2.5% -0.5416667 -0.04166667      0.95\n\n\nThis is the percentile method for finding a 95% confidence interval using the bootstrap distribution. The qdata() function, which gives the quantiles corresponding to specified probabilities, results in the same 95% confidence interval. Other confidence levels can be specified by simply changing the value of the p argument in either function.\n\nqdata(~diffprop, data = boot_results, p = c(0.025, 0.975))\n\n       2.5%       97.5% \n-0.54166667 -0.04166667 \n\n\nWe say that we are 95% confident that the true difference in promotion rates between males and females is between -52.9% and -5.6%, with females being promoted less often.\n\nExercise: What do you think it means for the 95% confidence interval to not include zero? Do you think this provides evidence of a difference between promotion rates in males and females?\n\n\n\n18.4.3 Hypothesis testing\nWe now have a better understanding of the variability in our statistic of interest, the difference in promotion rates. We used our bootstrap distribution to estimate the standard error of the statistic and to calculate a confidence interval. Recall that we did this to examine whether the observed difference in promotion rates represents discrimination or comes just from random variability.\nThis question is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. The point estimate of the difference, 29.2%, is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance. We label these two competing claims, \\(H_0\\) and \\(H_A\\):\n\\(H_0\\): Null hypothesis. The variables gender and decision are independent. They have no relationship, and the observed difference between the proportion of males and females who were promoted, 29.2%, was due to chance.\n\\(H_A\\): Alternative hypothesis. The variables gender and decision are not independent. The difference in promotion rates of 29.2% was not due to chance, and equally qualified females are less likely to be promoted than males.\n\n\n\n\n\n\nHypothesis testing\n\n\n\nThese hypotheses are part of what is called a hypothesis test. A hypothesis test is a statistical technique used to evaluate competing claims using data. Often times, the null hypothesis takes a stance of no difference or no effect. If the null hypothesis and the data notably disagree, then we will reject the null hypothesis in favor of the alternative hypothesis.\n\n\nWhat would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%.\nConsider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files.\nWe will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Again, we will do this by determining how much difference in promotion rates would happen by random variation and compare this with the observed difference. We will make a decision based on probability considerations.\n\n\n18.4.4 Simulating the study\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   promoted not_promoted Total\n  male         21            3    24\n  female       14           10    24\n  Total        35           13    48\n\n\nThe table of data shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers’ decisions were independent of gender, that is the null hypothesis is true. Then, if we conducted the experiment again with a different random assignment of files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers’ decisions had been independent of gender but the files had been distributed differently.4 We will walk through the steps next.\nTo think about this simulation, imagine we actually had the personnel files. We thoroughly shuffle 48 personnel files, 24 labeled male and 24 labeled female, and deal these files into two stacks. We will deal 35 files into the first stack, which will represent the 35 supervisors who recommended promotion. The second stack will have 13 files, representing the 13 supervisors who recommended against promotion. That is, we keep the same number of files in the promoted and not_promoted categories, and imagine simply shuffling the male and female labels around. Remember that the files are identical except for the listed gender. This simulation then assumes that gender is not important and, thus, we can randomly assign the files to any of the supervisors. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female candidates who were promoted. Since we don’t actually physically have the files, we will do this shuffle via computer code.\nThe following code shows the results of such a simulation:\n\nset.seed(101)\ntally(~shuffle(gender) + decision, discrim, margins = TRUE)\n\n               decision\nshuffle(gender) promoted not_promoted Total\n         male         18            6    24\n         female       17            7    24\n         Total        35           13    48\n\n\nThe shuffle() function randomly rearranges the gender column while keeping the decision column the same. It is really a sampling without replacement. That is, we randomly sample 35 personnel files to be promoted and the other 13 personnel files are not_promoted.\n\nExercise: What is the difference in promotion rates between the two simulated groups? How does this compare to the observed difference, 29.2%, from the actual study?5\n\nCalculating by hand will not help in a simulation, so we must write a function or use an existing one. We will use diffprop() from the mosaic package. The code to find the difference for the original data is:\n\n(obs &lt;- diffprop(decision ~ gender, data = discrim))\n\n  diffprop \n-0.2916667 \n\n\nNotice that we are subtracting the proportion of males promoted from the proportion of females promoted. This does not impact our results as this is an arbitrary decision. We just need to be consistent in our analysis. If we prefer to use positive values, we can adjust the order easily.\n\ndiffprop(decision ~ fct_relevel(gender, \"female\"), data = discrim)\n\n diffprop \n0.2916667 \n\n\nNotice what we have done here; we developed a single value metric to measure the relationship between gender and decision. This single value metric is called the test statistic. We could have used a number of different metrics, to include just the difference in number of promoted males and females. The key idea in hypothesis testing is that once you decide on a test statistic, you need to find the distribution of that test statistic, assuming the null hypothesis is true. Since the randomization of files in this simulation is independent of the promotion decisions, any difference in the two fractions is entirely due to chance.\n\n\n18.4.5 Checking for independence\nWe computed one possible difference under the null hypothesis in the exercise above, which represents one difference due to chance. Repeating the simulation, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. That is, we want the distribution of the difference if there really is no relationship between gender and the promotion decision. This is also known as the null distribution. We are using a simulation when there is actually a finite number of permutations of the gender label. From Chapter 8 on counting, we have 48 labels of which 24 are male and 24 are female. Thus the total number of ways to arrange the labels differently is:\n\\[\n\\frac{48!}{24!\\cdot24!} \\approx 3.2 \\cdot 10^{13}\n\\]\n\nfactorial(48) / (factorial(24)*factorial(24))\n\n[1] 3.22476e+13\n\n\nAs is often the case, the number of all possible permutations is too large to find by hand or even via code. Thus, we will use a simulation to generate a subset of all possible permutations, to approximate the permutation test. Using simulation in this way is called a randomization test.\nLet’s simulate the experiment and plot the simulated values of the difference in the proportions of male and female files recommended for promotion.\n\nset.seed(2022)\nresults &lt;- do(10000)*diffprop(decision ~ shuffle(gender), data = discrim)\n\nIn Figure 18.2, we will insert a vertical line at the value of our observed difference.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = -0.2916667 ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Difference in proportions\", y = \"Counts\",\n          title = \"Gender discrimination in hiring permutation test\",\n          subtitle = \"Test statistic is difference in promotion for female and male\")\n\n\n\n\n\n\n\nFigure 18.2: Distribution of test statistic\n\n\n\n\n\nNote that the distribution of these simulated differences is centered around 0 and is roughly symmetrical. It is centered on zero because we simulated differences in a way that made no distinction between men and women. This makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation under the assumption of the null hypothesis. The histogram also looks like a normal distribution; this is not a coincidence, but a result of the Central Limit Theorem, which we will learn about later in this block.\n\nExample:\nHow often would you observe a difference as extreme as -29.2% (-0.292) according to the figure? (Often, sometimes, rarely, or never?)\n\nIt appears that a difference as extreme as -29.2% due to chance alone would only happen rarely. We can estimate the probability using the results object.\n\nresults %&gt;%\n  summarise(p_value = mean(diffprop &lt;= obs))\n\n  p_value\n1  0.0257\n\n\nIn our simulations, only 2.6% of the simulated test statistics were less than or equal to the observed test statistic, as or more extreme relative to the null hypothesis. Such a low probability indicates that observing such a large difference in proportions from chance alone is rare. This probability is known as a \\(p\\)-value. The \\(p\\)-value is a conditional probability, the probability of the observed value or more extreme given that the null hypothesis is true.\nThe observed difference of -29.2% is a rare (low probability) event if there truly is no impact from listing gender in the personnel files. When we conduct formal studies, we reject a skeptical position (\\(H_0\\)) if the data strongly conflict with that position.6\nIn our analysis, we determined that there was only a ~ 2% probability of obtaining a test statistic where the difference between female and male promotion proportions was 29.2% or larger assuming gender had no impact. So we conclude the data provide sufficient evidence of gender discrimination against women by the supervisors. In this case, we reject the null hypothesis in favor of the alternative hypothesis.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#from-bootstrap-to-hypothesis-testing-key-insights",
    "href": "Inference-Case-Study.html#from-bootstrap-to-hypothesis-testing-key-insights",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.5 From Bootstrap to Hypothesis Testing: Key Insights",
    "text": "18.5 From Bootstrap to Hypothesis Testing: Key Insights\nBoth the hypothesis test and the bootstrap method provide insights into the difference in promotion rates. The bootstrap method offers a confidence interval, giving a range of plausible values, while the hypothesis test provides a p-value indicating the strength of evidence against the null hypothesis. Both methods led us to the same conclusion. The bootstrap confidence interval was entirely negative, not including zero, which led us to conclude that the true difference in proportions is different from zero, with females being promoted less often. The hypothesis test provided sufficient evidence to reject the null hypothesis, concluding that the observed difference in promotion rates was very unlikely to result from chance alone and that, instead, there is sufficient evidence of discrimination due to gender. While many researchers seem to prefer hypothesis testing and the associated \\(p\\)-values, confidence intervals provide a range of plausible values while also equipping us to make decisions. Together, these two methods enhance our understanding and help us make informed decisions based on the data.\n\nExercise: What is the difference between the sampling we conduct for the bootstrap procedure and the sampling we perform in a hypothesis test with simulation? Explain it in your own words.7\n\nStatistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur.\nLet’s summarize what we did in this case study. We had a research question and some data to test the question. We then performed a bootstrap and a hypothesis test. In the bootstrap, we sampled with replacement from the original sample, calculated a statistic of interest, and repeated this process many times so that we could generate a bootstrap distribution. We could then estimate the standard error of the statistic and calculate confidence intervals.\nIn the hypothesis test, we performed a randomization test to make a decision, following these four steps:\n\nState the null and alternative hypotheses.\nCompute a test statistic (statistic of interest).\nDetermine the \\(p\\)-value.\nDraw a conclusion.\n\nWhen creating a randomization distribution, we attempted to satisfy three guiding principles.\n\nBe consistent with the null hypothesis.\nWe need to simulate a world in which the null hypothesis is true. If we don’t do this, we won’t be testing our null hypothesis. In our problem, we assumed gender and promotion were independent.\nUse the data in the original sample.\nThe original data should shed light on some aspects of the distribution that are not determined by the null hypothesis. For our problem, we used the difference in promotion rates. The data does not give us the distribution direction, but it gives us an idea that there is a large difference.\nReflect the way the original data were collected.\nThere were 48 files and 48 supervisors. A total of 35 files were recommended for promotion. We keep this the same in our simulation.\n\nThe remainder of this block expands on the ideas of this case study.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Inference-Case-Study.html#footnotes",
    "href": "Inference-Case-Study.html#footnotes",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "We would be assuming that these two variables, side of the room and use of Apple product, are independent, meaning they are unrelated.↩︎\nRosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.↩︎\nThe study is an experiment, as subjects were randomly assigned a male personnel file or a female personnel file. Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.↩︎\nThe test procedure we employ in this section is formally called a permutation test.↩︎\n\\(18/24 - 17/24 = 0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups.↩︎\nThis reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎\nIn the bootstrap, we sample with replacement from the original data. We do this by sampling entire rows from the original data, so that the original observations remain intact. In our bootstrap sample, some observations may appear more than once while others may be left out. Thus, the marginal counts (e.g., the number of individuals promoted or not promoted) may differ from those in the original data. In our gender discrimination example, we grouped by gender in our resample. This ensures that the number of male and female personnel files remains constant even though the number of individuals promoted or not promoted, within those groups, may differ from the original data set.\nIn a randomization test, the marginal counts remain fixed but we randomize from the original observations in a manner that ignores the observed relationship between the explanatory and response variables. Imagine having 35 slots for individuals being promoted and 13 slots for individuals not being promoted. We shuffle and distribute male and female files into the slots for promoted and not promoted. In this process, we ignore the observed promotion status for each file as we place files into the slots. Thus, it is possible that the samples in our randomization test include observations that did not exist in the original data.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html",
    "href": "Sampling-Distributions.html",
    "title": "19  Sampling Distributions",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#objectives",
    "href": "Sampling-Distributions.html#objectives",
    "title": "19  Sampling Distributions",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as point estimate, parameter, sampling error, bias, sampling distribution, and standard error, and construct examples to demonstrate their proper use in context.\nConstruct a sampling distribution for various statistics, including the sample mean, using R.\nUsing a sampling distribution, make decisions about the population. In other words, understand the effect of sampling variation on our estimates.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#decision-making-under-uncertainty",
    "href": "Sampling-Distributions.html#decision-making-under-uncertainty",
    "title": "19  Sampling Distributions",
    "section": "19.2 Decision making under uncertainty",
    "text": "19.2 Decision making under uncertainty\nAt this point, it is useful to take a look at where we have been in this book and where we are going. We did this in the case study, but we want to discuss it again in a little more detail. We first looked at descriptive models to help us understand our data. This also required us to get familiar with software. We learned about graphical summaries, data collection methods, and summary metrics.\nNext we learned about probability models. These models allowed us to use assumptions and a small number of parameters to make statements about data (a sample) and also to simulate data. We found that there is a close tie between probability models and statistical models. In our first efforts at statistical modeling, we started to use data to create estimates for parameters of a probability model.\nNow we are moving more in depth into statistical models. This is going to tie all the ideas together. We are going to use data from a sample and ideas of randomization to make conclusions about a population. This will require probability models, descriptive models, and some new ideas and terminology. We will generate point estimates for a metric designed to answer the research question and then find ways to determine the variability of the metric. In Figure 19.1, we demonstrate this relationship between probability and statistical models.\n\n\n\n\n\n\n\n\nFigure 19.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen in a sample if we know the underlying process; in statistics, we don’t know the underlying process, and must infer about the population based on representative samples.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#introduction",
    "href": "Sampling-Distributions.html#introduction",
    "title": "19  Sampling Distributions",
    "section": "19.3 Introduction",
    "text": "19.3 Introduction\nCompanies such as Pew Research frequently conduct polls as a way to understand the state of public opinion or knowledge on many topics, including politics, scientific understanding, brand recognition, and more. The ultimate goal in taking a poll is generally to use the responses to estimate the opinion or knowledge of the broader population.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#point-estimates-and-error",
    "href": "Sampling-Distributions.html#point-estimates-and-error",
    "title": "19  Sampling Distributions",
    "section": "19.4 Point estimates and error",
    "text": "19.4 Point estimates and error\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by \\(p\\), and we often refer to the sample proportion as \\(\\hat{p}\\) (pronounced p-hat). Unless we collect responses from every individual in the population, \\(p\\) remains unknown, and we use \\(\\hat{p}\\) as our estimate of \\(p\\). The difference we observe from the poll versus the parameter is called the error in the estimate. Generally, the error consists of two aspects: sampling error and bias.\nSampling error, sometimes called sampling uncertainty, describes how much an estimate will tend to vary from one sample to the next. For instance, the estimate from one sample might be 1% too low while in another it may be 3% too high. Much of statistics, including much of this book, is focused on understanding and quantifying sampling error, and we will find it useful to consider a sample’s size to help us quantify this error; the sample size is often represented by the letter \\(n\\).\nBias describes a systematic tendency to over- or under-estimate the true population value. For example, if we were taking a student poll asking about support for a new college stadium, we’d probably get a biased estimate of the stadium’s level of student support by wording the question as, Do you support your school by supporting funding for the new stadium? We try to minimize bias through thoughtful data collection procedures, which were discussed previously and are the topic of many other books.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#understanding-the-variability-of-a-point-estimate",
    "href": "Sampling-Distributions.html#understanding-the-variability-of-a-point-estimate",
    "title": "19  Sampling Distributions",
    "section": "19.5 Understanding the variability of a point estimate",
    "text": "19.5 Understanding the variability of a point estimate\nSuppose the proportion of American adults who support the expansion of solar energy is \\(p = 0.88\\), which is our parameter of interest. If we were to take a poll of 1000 American adults on this topic, the estimate would not be perfect, but how close might we expect the sample proportion in the poll would be to 88%? We want to understand, how does the sample proportion \\(\\hat{p}\\) behave when the true population proportion is 0.88.1 Let’s find out! We can simulate responses we would get from a simple random sample of 1000 American adults, which is only possible because we know the actual support for expanding solar energy is 0.88. Here’s how we might go about constructing such a simulation:\n\nThere were about 250 million American adults in 2018. On 250 million pieces of paper, write “support” on 88% of them and “not” on the other 12%.\nMix up the pieces of paper and pull out 1000 pieces to represent our sample of 1000 American adults.\nCompute the fraction of the sample that say “support”.\n\nAny volunteers to conduct this simulation? Probably not. Running this simulation with 250 million pieces of paper would be time-consuming and very costly, but we can simulate it using computer code.\nFor the purposes of this book, we’ll create a population with only 2.5 million individuals. In this simulation, the sample gave a point estimate of \\(\\hat{p}_1 = 0.893\\). We know the population proportion for the simulation was \\(p = 0.88\\), so we know the estimate had an error of \\(0.893 - 0.88 = +0.013\\).\n\n# 1. Create a set of 2.5 million entries, \n#    where 88% of them are \"support\" and 12% are \"not\". \npop_size &lt;- 2500000\npossible_entries &lt;- c(rep(\"support\", 0.88 * pop_size), \n                      rep(\"not\", 0.12 * pop_size))\n\n\n# 2. Sample 1000 entries without replacement.\nset.seed(2024)\nsampled_entries &lt;- sample(possible_entries, size = 1000)\n\n# 3. Compute p-hat: count the number that are \"support\", then divide \n#    by the sample size.\nsum(sampled_entries == \"support\") / 1000\n\n[1] 0.893\n\n\nOne simulation isn’t enough to get a great sense of the distribution of estimates we might expect in the simulation, so we should run more simulations. In a second simulation, we get \\(\\hat{p}_2 = 0.885\\), which has an error of \\(+0.005\\). In another, \\(\\hat{p}_3 = 0.878\\) for an error of \\(-0.002\\). And in another, an estimate of \\(\\hat{p}_4 = 0.859\\) with an error of \\(-0.021\\). With the help of a computer, we’ve run the simulation 10,000 times and created a histogram of the results from all 10,000 simulations in Figure 19.2.\n\n# 4. Repeat steps 2 and 3 10,000 times.\nset.seed(2024)\nsolar_results &lt;- do(10000)*sum(sample(possible_entries, \n                                      size = 1000) == \"support\") / 1000\n\n\nsolar_results %&gt;% \n  gf_histogram(~sum, color = \"black\", fill = \"cadetblue1\") %&gt;% \n  gf_vline(xintercept = mean(solar_results$sum)) %&gt;% \n  gf_text(label = \"mean = 0.880\", x = 0.888, y = 1400) %&gt;% \n  gf_labs(x = \"Proportion of support\", y = \"Number of simulations\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 19.2\n\n\n\n\n\nThis distribution of a statistic, in this case the sample proportion, is called a sampling distribution. We can characterize this sampling distribution as follows:\nCenter. The center of the distribution is \\(\\bar{x}_p = 0.880\\), which is the same as the parameter. Notice that the simulation mimicked a simple random sample of the population, which is a straightforward sampling strategy that helps avoid sampling bias.\nSpread. The standard deviation of the distribution is \\(s_{\\hat{p}} = 0.010\\). When we’re talking about a sampling distribution or the variability of a point estimate, we typically use the term standard error rather than standard deviation.\nShape. The distribution is symmetric and bell-shaped, and it resembles a normal distribution.\nThese \ffindings are encouraging! When the population proportion is \\(p = 0.88\\) and the sample size is \\(n = 1000\\), the sample proportion \\(\\hat{p}\\) tends to give a pretty good estimate of the population proportion. We also have the interesting observation that the histogram resembles a normal distribution.\n\n\n\n\n\n\nSampling distributions are never observed, but we keep them in mind\n\n\n\nIn real-world applications, we never actually observe the sampling distribution, yet it is useful to always think of a point estimate as coming from such a hypothetical distribution. Understanding the sampling distribution will help us characterize and make sense of the point estimates that we do observe.\n\n\n\nExercise: If we used a much smaller sample size of \\(n = 50\\), would you guess that the standard error for \\(\\hat{p}\\) would be larger or smaller than when we used \\(n = 1000\\)?\n\nIntuitively, it seems like more data is better than less data, and generally that is correct! The typical error when \\(p = 0.88\\) and \\(n = 50\\) would be larger than the error we would expect when \\(n = 1000\\).\nThe above exercise highlights an important property we will see again and again: a bigger sample tends to provide a more precise point estimate than a smaller sample.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#sampling-distributions-case-study-rolling-the-dice",
    "href": "Sampling-Distributions.html#sampling-distributions-case-study-rolling-the-dice",
    "title": "19  Sampling Distributions",
    "section": "19.6 Sampling distributions case study: rolling the dice",
    "text": "19.6 Sampling distributions case study: rolling the dice\nLet’s consider a standard six-sided die and define a random variable \\(X\\) as the result of a single die roll. The sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}\\) and each outcome has an equal probability of \\(1/6\\). The expected value of \\(X\\) is easily found to be \\(E(X) = 3.5\\).2\nWe will take a sample by rolling the die \\(n = 5\\) times. We will then compute the mean of each sample. We can easily set up this process in R. Our first sample is \\(\\{2, 5, 5, 4, 1\\}\\) with a mean value of \\(\\bar{x}_1 = 3.4\\).\n\n# 1. Create a population/sample space of the six values \n#    on a standard six-sided die\npossible_values &lt;- c(1, 2, 3, 4, 5, 6)\n\n\n# 2. Sample 5 entries with replacement.\nset.seed(2024)\nsampled_values &lt;- sample(possible_values, size = 5, replace = T)\nsampled_values\n\n[1] 2 5 5 4 1\n\n\n\n# 3. Compute the mean value.\nmean(sampled_values)\n\n[1] 3.4\n\n\nAgain, one simulation isn’t enough to understand the distribution of means we might expect, so we should run more simulations. In a few further simulations, we get \\(\\bar{x}_2 = 3\\), \\(\\bar{x}_3 = 4.8\\), \\(\\bar{x}_4 = 2.4\\), and \\(\\bar{x}_5 = 2.6\\). With the help of a computer, we’ve run the simulation 10,000 times and created a histogram of the results from all 10,000 simulations in Figure 19.3.\n\n# 4. Repeat steps 2 and 3 10,000 times.\nset.seed(2024)\ndice_results &lt;- do(10000)*mean(sample(possible_values, \n                                      size = 5, replace = T))\n\n\ndice_results %&gt;% \n  gf_histogram(~mean, color = \"black\", fill = \"cadetblue1\", binwidth = 0.2) %&gt;% \n  gf_vline(xintercept = mean(dice_results$mean)) %&gt;% \n  gf_lims(y = c(0, 1100)) %&gt;% \n  gf_text(label = \"mean = 3.499\", x = 4, y = 1050) %&gt;% \n  gf_labs(x = \"Mean die value\", y = \"Number of simulations\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 19.3\n\n\n\n\n\nThis distribution of sample means is another example of a sampling distribution. We can characterize this sampling distribution as follows:\nCenter. The center of the distribution is \\(\\bar{x} = 3.499\\), which is almost exactly the same as the parameter value of \\(\\mu = 3.5\\).\nSpread. The standard deviation of the distribution is \\(s_{\\bar{x}} = 0.764\\). Because this is the standard deviation of the point estimate \\(\\bar{x}\\), we call it the standard error of \\(\\bar{x}\\).\nShape. The distribution is symmetric and bell-shaped, and it again resembles a normal distribution.\nIn contrast to our first example of support for solar energy, we have a population mean of \\(\\mu = 3.5\\) and a much smaller sample size of \\(n = 5\\). Even so, the sample mean is a good estimate of the population mean and the histogram of sample mean values again looks similar to that of a normal distribution.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#sampling-distributions-case-study-gerrymandering",
    "href": "Sampling-Distributions.html#sampling-distributions-case-study-gerrymandering",
    "title": "19  Sampling Distributions",
    "section": "19.7 Sampling distributions case study: gerrymandering",
    "text": "19.7 Sampling distributions case study: gerrymandering\nGerrymandering is the practice of drawing the boundaries of electoral districts in a way that gives one political party an unfair advantage over others. This manipulation can influence the outcomes of elections, allowing the controlling party to maintain or increase its power, even if it doesn’t have majority support among the voters.\nTo get a real sense of how gerrymandering works, try it out yourself! Use this simulation to practice gerrymandering, but stop after the 6th challenge.\nAt this point, you might be wondering: Why can’t we just make everything proportional? For example, if 60% of people in a state support Republicans, let’s just make 6 out of their 10 representatives Republican!” Unfortunately, it’s not so simple.\nLet’s consider the state of Idaho. Idaho has two congressional districts, so the state gets two House representatives. In a typical election, Democrats earn 34% and Republicans earn 66% of major party votes. Both Idaho representatives are almost always Republicans (Alarm Redistricting 2024).\nFigure 19.4 displays the vote patterns broken down by census tracts of roughly equal population size.\n\n\n\n\n\n\nFigure 19.4: Districting map of the state of Idaho.\n\n\n\n\nExercise: To give the most proportional representation, how many of the two representatives should be Republican? How many Democrat?\n\nBecause 34% and 66% are closer to 50% than they are to 0% or 100%, the most proportional representation would come from one Republican and one Democrat representative.\nIn order to give one seat to a Democrat though, we would have to make a district that looks like a snake, winding its way from the southwest corner north towards Boise and then east towards the other blue patches. Even if we wanted to make this district, it may not be possible (since all districts have to have roughly equal population sizes). Many states, including Idaho have constraints on how districts can be drawn. In Idaho, districts must be contiguous, have equal populations, be geographically compact, preserve county and municipality boundaries as much as possible, not be drawn to favor party or incumbents, and connect counties based on highways. Plus, representing a “snake” district isn’t ideal; because of the strange layout, people in different parts of the district may face different issues and have highly different desires. This would lead to less than ideal representation in government.\n\n19.7.1 The statistician’s answer: simulation\nBetween balancing the need for party representation (proportionality) and the need for geographic representation (no weird shapes), the Supreme Court has struggled to set a consistent standard for evaluating maps. So, in 2019, a group of mathematicians got together and wrote a Supreme Court brief that proposed a new simulation method for map evaluation:\n\nHave a computer draw a bunch of randomly simulated maps under two rules: a) districts have equal population sizes, and b) no weird shapes.\nUsing prior election data, estimate how well Democrats and Republicans would do in each of the randomly drawn maps.\nSee if the map drawn by the actual state legislature is unusual (in terms of projected election outcomes) compared to the randomly simulated maps.\n\nBy having a computer (with no internal party preference and with rules for shapes) draw random maps, we balance the needs for both proportionality and geography!\nSkew the Script (Skew the Script 2024) developed a simulation applet to help determine if a state gerrymandered its Congressional districts, through the power of simulation. Powered by the ALARM redist package3, the simulation divides states into randomly drawn districts of equal population size. The simulations also follow some rules about “no weird shapes” and the details for that requirements are available through the ALARM Redistricting project (Alarm Redistricting, n.d.). Then, it uses prior election data to find the political lean of the simulated districts. For the state of Idaho, the analysis is based on data from the 2016 and 2020 Presidential elections, 2016 and 2020 US Senate elections, and the 2018 Governor, Attorney General, and Secretary of State elections.\nLet’s perform an analysis for Virginia. During redistricting, Virginia’s state legislature was controlled by Republicans. Did the legislature draw an unfair map?\n\n\n\n\n\n\nFigure 19.5\n\n\n\nWe get started by selecting the state of Virginia in the applet. The actual Virginia map of Congressional districts is displayed for us in the applet and in Figure 19.5. Out of 11 total districts, seven are Democrat-leaning. Let’s add one sample. Based on the redistricting rules (e.g., districts have equal populations and no weird shapes), the app provides a single simulated map shown in Figure 19.6.\n\n\n\n\n\n\nFigure 19.6\n\n\n\nThe applet also plots the number of Democrat-leaning districts, in this case seven, for us on a bar chart. We can continue to add one, three, ten, 30, or 100 sample redistricting maps to these results by clicking the appropriate button in the applet. When generating three or more samples at a time, the applet displays the last three simulated redistricting maps for comparison.\nAfter adding 1,000 samples (for a total of 1,001 samples), we have results that look similar to those shown in Figure 19.7. What we’ve done here is create a sampling distribution for the number of Democrat-leaning districts in the state of Virginia by sampling 1,001 redistricting maps.4 In the applet, we’re able to compare the actual number of Democrat-leaning districts to our sampling distribution. Virginia appears to have a fair map. The legislature’s map projects seven Democrat-leaning districts, and the vast majority of simulations ($664/1001 = 66.3%) also had seven Democrat-leaning districts or fewer. So, the actual Virginia map is not unusual. There is little evidence that Republicans gerrymandered the state.\n\n\n\n\n\n\n\n\nFigure 19.7\n\n\n\n\n\n\nExercise: Using the applet, perform an analysis and comment on the fairness of Illinois (Democrat-controlled legislature) and Texas (Republican-controlled legislature). Then, try your own state or another state of interest (note: states with one house member don’t go through redistricting).5\n\nIn its most recent landmark decision on gerrymandering (Rucho v. Common Cause), the Supreme Court abdicated to the legislature, saying that the courts could not make decisions on “political questions outside the remit of these courts.” In other words, if change is to happen, state legislatures or Congress have to pass laws to limit gerrymandering.\n\nExercise: Write such a law, using the simulation method described in this lesson. Specifically, fill in the blank: “If your map has more Democrat/Republican leaning districts than __% of simulations, it is unfair and should be rejected.” What number would you choose? Why?6\n\nIn the next few chapters, we’ll learn how to formalize these ideas and implement the four steps of hypothesis testing that we first glimpsed in the Inference Case Study in Chapter 18.\n\n\n\n\nAlarm Redistricting. 2024. “Idaho Congressional Districts 2020.” https://alarm-redist.org/fifty-states/ID_cd_2020/.\n\n\n———. n.d. “ALARM Project - Redistricting.” https://alarm-redist.org/.\n\n\nSkew the Script. 2024. “Sampling Distributions.” https://skewthescript.org/ap-stats-curriculum/part-6.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Sampling-Distributions.html#footnotes",
    "href": "Sampling-Distributions.html#footnotes",
    "title": "19  Sampling Distributions",
    "section": "",
    "text": "We haven’t actually conducted a census to measure this value perfectly. However, a very large sample has suggested the actual level of support is about 88%.↩︎\n\\(E(X) = 1*{1\\over 6} + 2*{1\\over 6} + 3*{1\\over 6} + 4*{1\\over 6} + 5*{1\\over 6} + 6*{1\\over 6} = 3.5\\)↩︎\nThe ALARM redist package in R enables researchers to sample redistricting plans from a pre-specified target distribution. Essentially, the authors of the package simulated a large number of possible redistricting maps. The package supports various constraints in the redistricting process, such as geographic compactness and population parity requirements, just like those requirements for any redistricting done in the state of Idaho.↩︎\nIn fact, we’ve actually created a null distribution by sampling 1,001 possible fair redistricting maps. This null distribution is developed by sampling under the assumption of fairness, the null hypothesis in this scenario.↩︎\nIllinois is heavily gerrymandered: zero of the simulations have as many Democrat-leaning districts as the actual map. In Illinois’ enacted map, there are weirdly shaped districts that “carve out” Democrat-leaning areas. Texas is gerrymandered the opposite way: zero of the simulations have as few Democrat-leaning districts as the actual map. One district connects the east side of San Antonio with the east side of Austin, condensing Democrats in a long, skinny district across two metropolitan areas.↩︎\nThis question is building towards a hypothesis test. If the proportion of districts more extreme than the drawn map (the “p-value”) is lower than 5% (the “alpha level”), we reject the map (reject the null hypothesis that the map was fair). Of course, the analogy to hypothesis tests isn’t perfect, but it’s all about building intuition.\nWe typically use 5% as the alpha level in hypothesis tests, but it’s okay for your answer to vary a little - as long as you have a relatively low number and use the logic of finding a percent that would be “surprising” or “unusual”.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html",
    "href": "Bootstrap.html",
    "title": "20  Bootstrap",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#objectives",
    "href": "Bootstrap.html#objectives",
    "title": "20  Bootstrap",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as sampling distribution, bootstrapping, bootstrap distribution, resample, sampling with replacement, and standard error, and construct examples to demonstrate their proper use in context.\nApply the bootstrap technique to estimate the standard error of a sample statistic.\nUtilize bootstrap methods to construct and interpret confidence intervals for unknown parameters from random samples.\nAnalyze and discuss the advantages, disadvantages, and underlying assumptions of bootstrapping for constructing confidence intervals.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#computational-vs.-mathematical-inference-methods",
    "href": "Bootstrap.html#computational-vs.-mathematical-inference-methods",
    "title": "20  Bootstrap",
    "section": "20.2 Computational vs. mathematical inference methods",
    "text": "20.2 Computational vs. mathematical inference methods\nWe are going to be using data from a sample of the population to make decisions about the population. There are many approaches and techniques for this. In this book, we will be introducing and exploring different approaches; we are establishing foundations. As you can imagine, these ideas are varied, subtle, and at times difficult. We will just be exposing you to the foundational ideas. We want to make sure you understand that to become an accomplished practitioner, you must master the fundamentals and continue to learn the advanced ideas.\nHistorically, there have been two approaches to statistical decision making, hypothesis testing and confidence intervals. At their mathematical foundation, they are equivalent, but, sometimes in practice, they offer different perspectives on the problem. We will learn about both of these approaches.\nThe engines that drive the numeric results of a decision making model are either mathematical or computational. In reality, computational methods have mathematics behind them, and mathematical methods often require computer computations. The real distinction between them is the assumptions we are making about our population. Mathematical solutions typically have stricter assumptions, thus leading to a tractable mathematical solution to the sampling distribution of the test statistic, while computational models relax assumptions but may require extensive computational power. Like all problems, there is a trade off to consider when trying to choose which approach is better. There is no one universal best method. Some methods perform better in certain contexts. It is important to understand that computational methods such as the bootstrap are NOT all you need to know.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#bootstrapping",
    "href": "Bootstrap.html#bootstrapping",
    "title": "20  Bootstrap",
    "section": "20.3 Bootstrapping",
    "text": "20.3 Bootstrapping\nBootstrapping1 is a powerful statistical method that allows us to estimate the sampling distribution of a statistic by resampling with replacement from the original data. This technique is particularly useful when the theoretical distribution of a statistic is complex or unknown.\nThe theory required to quantify the uncertainty of a statistic such as the sample median is complex. In an ideal world, we would sample data from the population and compute the median with this sample. Then we could sample again from the population and recompute the media with this new sample. Then we could do it again. And again. And so on until we get enough median estimates that we have a good sense of the precision of our original estimate. This is an ideal world where sampling data is free or extremely inexpensive. That is rarely the case, which poses a challenge to this “resample from the population” approach.\nHowever, we can sample from the sample we already have. Bootstrapping allows us to simulate the sampling distribution by resampling from the original sample. Suppose \\(x_1, x_2, ..., x_n\\) is an i.i.d. random sample from the population. First, we define the empirical distribution function of \\(X\\) by assigning an equal probability to each \\(x_i\\). Then, we sample from this empirical probability mass function. Thus, we are treating our sample as a discrete uniform random variable, and sampling from our original sample with replacement.\nThe general procedure for bootstrapping is to sample with replacement from the original sample, calculate and record the sample statistic for that bootstrapped sample, then repeat the process many times. The collection of sample statistics comprises a bootstrap distribution of the sample statistic. Generally, this procedure works quite well, provided the original sample is representative of the population. Otherwise, any bias or misrepresentation is simply amplified throughout the bootstrap process. Further, for very small sample sizes, bootstrap distributions become “choppy” and hard to interpret. Thus, in small sample cases, we must use permutation or mathematical methods to determine the sampling distribution.\nOnce we have completed the procedure, the bootstrap distribution can be used to build a confidence interval for the population parameter or estimate the standard error.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#bootstrap-example",
    "href": "Bootstrap.html#bootstrap-example",
    "title": "20  Bootstrap",
    "section": "20.4 Bootstrap example",
    "text": "20.4 Bootstrap example\nTo help us understand the bootstrap, let’s use an example of a single mean. We would like to estimate the mean height of students at a local college. We collect a sample of size 50 (stored in the vector heights below).\n\nExercise Using the bootstrap method, generate an approximation of the sampling distribution for the true mean height, \\(\\mu\\). Use it to estimate the variability of the mean and find 95% confidence intervals for \\(\\mu\\).\n\n\nheights &lt;- c(62.0, 73.8, 59.8, 66.9, 75.6, 63.3, 64.0, 63.1, 65.0, 67.2, \n             73.0, 62.3, 60.8, 65.7, 60.8, 65.8, 63.3, 54.9, 67.8, 65.1, \n             74.8, 75.0, 77.8, 73.7, 74.3, 68.4, 77.5, 77.9, 66.5, 65.5, \n             71.7, 75.9, 81.7, 76.5, 77.8, 75.0, 64.6, 59.4, 60.7, 69.2, \n             78.2, 65.7, 69.6, 80.0, 67.6, 73.0, 65.3, 67.6, 66.2, 69.6)\n\n\n20.4.1 Summary statistics\nLet’s look at the data. Figure 26.1 and Figure 20.2 show a boxplot and density plot of the heights, respectively.\n\n\n\n\n\n\n\n\nFigure 20.1: Boxplot of heights of local college students.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.2: Density plot of heights of local college students.\n\n\n\n\n\nThe distribution of heights looks bimodal. There are likely both male and female students in this sample, and thus we have two different population distributions of heights. We can also use favstats() to examine some of our favorite statistics.\n\nfavstats(~heights)\n\n  min   Q1 median     Q3  max   mean       sd  n missing\n 54.9 64.7   67.6 74.675 81.7 68.938 6.345588 50       0\n\n\nIf we want to use the tidyverse, we must convert heights to a data frame.\n\nheights &lt;- tibble(height = heights)\n\n\nhead(heights)\n\n# A tibble: 6 × 1\n  height\n   &lt;dbl&gt;\n1   62  \n2   73.8\n3   59.8\n4   66.9\n5   75.6\n6   63.3\n\n\n\n\n20.4.2 Generating a bootstrap distribution\nThe idea behind the bootstrap is that we will get an estimate of the distribution for the statistic of interest (also called the sampling distribution) by sampling the original data with replacement. We must sample under the same scheme with which the original data was collected. In R, we will use the resample() function from the mosaic package. There are entire packages dedicated to resampling, such as boot, and there is a great deal of information about these types of packages online for the interested learner.\nWhen applied to a dataframe, the resample() function samples rows with replacement to produce a new data frame with the same number of rows as the original. It’s important to note that the rows from the original data set remain intact. However, some rows may be duplicated and others may be missing entirely from the bootstrap sample.\nTo illustrate, let’s use resample() on the first 10 positive integers.\n\nset.seed(305)\nresample(1:10)\n\n [1] 8 7 8 1 4 4 2 2 6 9\n\n\nNotice that observations 2, 4, and 8 appear at least twice. Observations 3 and 5 do not appear at all. This is a single bootstrap replicate of the data.\nWe then calculate a point estimate, or statistic of interest, on the bootstrap replicate. We repeat this process a large number of times (e.g., 1,000 or 10,000 times). The collection of the point estimates is called the bootstrap distribution.\nFor the sample mean, ideally, the bootstrap distribution should be unimodal, roughly symmetric, and centered at the original estimate.\nLet’s generate the bootstrap distribution for the mean using our heights data.\n\nset.seed(2115)\nboot_results &lt;- do(1000)*mean(~height, data = resample(heights))\n\nThe first few rows of the results are:\n\nhead(boot_results)\n\n    mean\n1 68.390\n2 68.048\n3 67.732\n4 68.534\n5 70.980\n6 68.424\n\n\nThe do() function, by default, gives the column name of the last function used, in this case mean(). This is an unfortunate name as it can lead to some confusion.\nFigure 20.3 is a plot of the bootstrap distribution. The observed mean from our original sample is shown as a black line.\n\n\n\n\n\n\n\n\nFigure 20.3: The sampling distribution of the mean approximated using a bootstrap distribution.\n\n\n\n\n\n\n\n20.4.3 Estimating the variability of a statistic\nThe standard deviation of our statistic, the mean in this case, is also known as the standard error. Functions like favstats() or sd() can be used to calculate the standard error from the bootstrap distribution.\n\nfavstats(~mean, data = boot_results)\n\n    min      Q1 median    Q3  max     mean        sd    n missing\n 65.684 68.3915 68.976 69.55 72.3 68.96724 0.9040555 1000       0\n\n\n\nsd(~mean, data = boot_results)\n\n[1] 0.9040555\n\n\nHere we see the standard error of the mean is approximately 0.904.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#confidence-intervals",
    "href": "Bootstrap.html#confidence-intervals",
    "title": "20  Bootstrap",
    "section": "20.5 Confidence intervals",
    "text": "20.5 Confidence intervals\nA point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. We just showed how to estimate the variability in a point estimate by generating a bootstrap distribution and using it to estimate the standard error of the statistic. In addition to supplying a point estimate of a parameter, the next logical step would be to provide a plausible range of values for the parameter.\n\n20.5.1 Capturing the population parameter\nA plausible range of values for the population parameter is called a confidence interval. Using only a point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish.\nIf we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values – a confidence interval – we have a good shot at capturing the parameter.\n\nExercise: If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?2\n\n\n\n20.5.2 Constructing a confidence interval\nGenerally, what you should know about building confidence intervals is laid out in the following steps:\n\nIdentify the parameter you would like to estimate (e.g., the population mean, \\(\\mu\\)).\nIdentify a good estimate for that parameter (e.g., the sample mean, \\(\\bar{X}\\)).\nDetermine the distribution of your estimate, or a function of your estimate. One way to do this is by generating the bootstrap distribution of your estimate.\nUse this distribution to obtain a range of feasible values (a confidence interval) for the parameter.\n\n\n20.5.2.1 Bootstrap percentile method\nWe can now calculate a confidence interval using the bootstrap percentile method, where we examine the bootstrap distribution and find the values corresponding to percentiles of interest3 to construct a confidence interval.\nFor a 95% confidence interval, we simply want the middle 95% of the bootstrap distribution. That is, we want the 2.5 and 97.5 percentiles of the bootstrap distribution. The function cdata() makes this easy for us by specifying the probability for the confidence interval desired.\n\ncdata(~mean, data = boot_results, p = 0.95)\n\n       lower   upper central.p\n2.5% 67.2197 70.7964      0.95\n\n\nWe can also use the qdata() function, which gives the values corresponding to the specified percentiles.\n\nqdata(~mean, data = boot_results, p = c(0.025, 0.975))\n\n   2.5%   97.5% \n67.2197 70.7964 \n\n\nBoth functions result in the same 95% confidence interval.\n\n\n20.5.2.2 Using the bootstrap standard error\nSince the bootstrap distribution is roughly symmetric and bell-shaped, we can use an approximation similar to the empirical rule (68-95-99.7 rule) which states that approximately 95% of data falls within two standard deviations of the mean. In this situation, the standard deviation of the bootstrap distribution is the bootstrap standard error of the sample mean.\nFor an approximate 95% confidence interval, we use\n\\[\n\\text{bootstrap mean} \\pm 2 \\times \\text{standard error}\n\\]\n\nxbar &lt;- mean(boot_results$mean)\nSE &lt;- sd(boot_results$mean)\nxbar + c(-2, 2)*SE\n\n[1] 67.15913 70.77535\n\n\nWe could of course use tidyverse as well, but we must change the column name so that the mean column and the mean() function don’t conflict with each other.\n\nboot_results %&gt;% \n  mutate(stat = mean) %&gt;%      # rename the mean column as stat\n  summarize(mean = mean(stat), stderr = sd(stat), \n            lower = mean + -2*stderr, upper = mean + 2*stderr)\n\n      mean    stderr    lower    upper\n1 68.96724 0.9040555 67.15913 70.77535\n\n\nThis interval is very similar to the one found with the percentile method, but the standard error method can only be used when the bootstrap distribution is approximately symmetric and bell-shaped. You can learn more about bootstrap confidence intervals online.4",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#non-standard-sample-statistics",
    "href": "Bootstrap.html#non-standard-sample-statistics",
    "title": "20  Bootstrap",
    "section": "20.6 Non-standard sample statistics",
    "text": "20.6 Non-standard sample statistics\nOne of the huge advantages of simulation-based methods is the ability to build confidence intervals for parameters whose estimates don’t have known sampling distributions, or when the distributions are difficult to derive. In essence, the bootstrap method allows us estimate the sampling distribution and standard error of any statistic of interest.\n\n20.6.1 Sample median\nConsider the heights data again. The observed sample median is 67.6. We would like to know the true median student height and generate a confidence interval for the estimate. However, we have no idea of the sampling distribution of the median. We can use bootstrapping to obtain an empirical distribution of the median.\n\nExercise:\nFind a 90% confidence interval for the median height of students at a local college.\n\n\nset.seed(427)\nboot_results &lt;- do(10000)*median(~height, data = resample(heights))\n\n\n\n\n\n\n\n\n\nFigure 20.4: The sampling distribution of the median approximated using a bootstrap distribution.\n\n\n\n\n\nNow that we have the bootstrap distribution, we can generate a confidence interval using the percentile method.\n\ncdata(~median, data = boot_results, p = 0.90)\n\n   lower upper central.p\n5%  65.8 70.65       0.9\n\n\nWe are 90% confident that the median student height is between 65.8 and 70.65 inches.\n\n\n20.6.2 Sample interquartile range\nRemember, the bootstrap method allows us estimate the sampling distribution and standard error of any statistic of interest. So, let’s consider an even less commonly used statistic: the interquartile range (IQR). The interquartile range is calculated as the difference in the third and first quartile, \\(IQR = Q_3 - Q_1\\). It represents the middle 50% of the data. We can calculate this value using information from favstats() or by using the iqr() function.\n\nfavstats(~height, data = heights)\n\n  min   Q1 median     Q3  max   mean       sd  n missing\n 54.9 64.7   67.6 74.675 81.7 68.938 6.345588 50       0\n\n\n\niqr(~height, data = heights)\n\n[1] 9.975\n\n\nThe observed sample IQR is 9.975. We would like to know the true interquartile range of student height and generate a confidence interval for the estimate. We can use bootstrapping to estimate the sampling distribution and standard error of the interquartile range.\n\nset.seed(801)\nboot_results &lt;- do(10000)*iqr(~height, data = resample(heights))\n\n\n\n\n\n\n\n\n\nFigure 20.5: The sampling distribution of the IQR approximated using a bootstrap distribution.\n\n\n\n\n\nNow that we have the bootstrap distribution, we can generate a confidence interval using the percentile method.\n\ncdata(~iqr, data = boot_results, p = 0.95)\n\n     lower upper central.p\n2.5%   6.5 12.35      0.95\n\n\nWe are 95% confident that the true interquartile range of student height is between 6.5 and 12.35 inches.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#summary-of-the-bootstrap",
    "href": "Bootstrap.html#summary-of-the-bootstrap",
    "title": "20  Bootstrap",
    "section": "20.7 Summary of the bootstrap",
    "text": "20.7 Summary of the bootstrap\nThe key idea behind the bootstrap is that we estimate the population with the sample. This is called the plug in principle: if a value is unknown, then substitute an estimate of the value. We can then generate new samples from this population estimate. The bootstrap does not improve the accuracy of the original estimate. In fact, the bootstrap distribution is centered on the original sample estimate. Instead, we only get information about the variability of the sample estimate. We are then estimating the standard error and/or calculating confidence intervals.\nSome people are suspicious of the bootstrap because we are using the data over and over again. But remember, we are just getting estimates of variability. In traditional statistics, when we calculate the sample standard deviation, we are using the sample mean. Thus, we are using the data twice. Always think of the bootstrap as providing a way to find the variability in an estimate.\n\n20.7.1 Frequently asked questions\n\nAre there more types of bootstrap techniques?\nYes! There are many excellent bootstrap techniques. We have only chosen to present two bootstrap techniques that could be explained in a single chapter and that are also reasonably reliable. There are many adjustments that can be made to speed up and improve accuracy. Packages such as resample and boot are more appropriate for these situations.\nI’ve heard the percentile bootstrap is very robust.\nIt is a commonly held belief that the percentile bootstrap is a robust bootstrap method. That is false. The percentile method is one of the least reliable bootstrap methods. However, it is easy to use and understand and can give a first attempt at a solution before more accurate methods, like the standard error method, are used. We’ll learn more about the standard error method in a later chapter.\nI should use 1,000 replicates in my bootstrap and permutation tests, right?\nThe randomization and bootstrap distributions involve a random component from the sampling process, and thus \\(p\\)-values and confidence intervals computed from the same data will vary. The amount of this Monte Carlo variability depends on the number of replicates used to create the randomization or bootstrap distribution. It is important that we not use too few, as this will introduce too much random noise into \\(p\\)-value and confidence interval calculations. But each replicate costs time, and the marginal gain for each additional replicate decreases as the number of replicates increases. There is little reason to use millions of replicates (unless the goal is to estimate very small \\(p\\)-values). We generally use roughly 1,000 replicates for routine or preliminary work, and increase this to 10,000 replicates when we want to reduce the effects of Monte Carlo variability.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Bootstrap.html#footnotes",
    "href": "Bootstrap.html#footnotes",
    "title": "20  Bootstrap",
    "section": "",
    "text": "The term bootstrapping comes from the phrase “pulling oneself up by one’s bootstraps,” which is credited to the 18th century story of Baron Munchausen who supposedly pulled himself out of a swamp by tugging on his own bootstraps. It implies performing an apparently impossible task. The bootstrap method was created by Bradley Efron in 1979 and was named in this way because it relies on the data itself to generate estimates of sampling distributions.↩︎\nIf we want to be more certain we will capture the fish, we should use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter. A higher level of confidence implies a wider interval.↩︎\nThe values corresponding to the percentiles of interest are also known as quantiles.↩︎\nAlso, see resources like Statistical Inference via Data Science by Chester Ismay and Albert Kim.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html",
    "href": "Hypothesis-Testing-Simulation.html",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "",
    "text": "21.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html#objectives",
    "href": "Hypothesis-Testing-Simulation.html#objectives",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as null hypothesis, alternative hypothesis, test statistic, p-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type I error, type II error, false positive, false negative, null distribution, and sampling distribution, and construct examples to demonstrate their proper use in context.\nApply and evaluate all four steps of a hypothesis test using randomization methods: formulating hypotheses, calculating a test statistic, determining the p-value through randomization, and making a decision based on the test outcome.\nAnalyze and discuss the concepts of decision errors (type I and type II errors), the differences between one-sided and two-sided tests, and the impact of choosing a significance level. Evaluate how these factors influence the conclusions and reliability of hypothesis tests and their practical implications in statistical decision-making.\nAnalyze how confidence intervals and hypothesis testing complement each other in making statistical inferences.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html#introduction",
    "href": "Hypothesis-Testing-Simulation.html#introduction",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "21.2 Introduction",
    "text": "21.2 Introduction\nIn this chapter we will introduce hypothesis testing. It is really an extension of the case study. We will put more emphasis on terms and core concepts. In this chapter, we will use a computational solution but this will lead us into thinking of mathematical solutions.1 The role of the analyst is always key regardless of the perceived power of the computer. The analyst must take the research question and translate it into a numeric metric for evaluation. The analyst must decide on the type of data and its collection to evaluate the question. The analyst must evaluate the variability in the metric and determine what that means in relation to the original research question. The analyst must propose an answer.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "href": "Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "21.3 Hypothesis testing",
    "text": "21.3 Hypothesis testing\nWe will continue to emphasize the ideas of hypothesis testing through a data-driven example, but also via analogy to the US court system. So let’s begin our journey.\n\nExample:\nYou are annoyed by TV commercials. You suspect that there were more commercials in the basic TV channels, typically the local area channels, than in the premium channels you pay extra for. To test this claim, hypothesis, you want to collect some data and draw a conclusion. How would you collect this data?\n\nHere is one approach: we watch 20 random half hour shows of live TV. Ten of those hours are basic TV and the other ten are premium. In each case, you record the total length of commercials during each show.\n\nExercise: Is this enough data? You decide to have your friends help you, so you actually only watched 5 hours and got the rest of the data from your friends. Is this a problem?\n\nWe cannot determine if this is enough data without some type of subject matter knowledge. First, we need to decide what metric to use to determine if a difference exists (more to come on this). Second, we need to decide how big of a difference, from a practical standpoint, is of interest. Is a loss of 1 minute of TV show enough to say there is a difference? How about 5 minutes? These are not statistical questions, but depend on the context of the problem and often require subject matter expertise to answer. Often, data is collected without thought to these considerations. There are several methods that attempt to answer these questions. They are loosely called sample size calculations. This book will not focus on sample size calculations and will leave it to the reader to learn more from other sources. For the second exercise question, the answer depends on the protocol and operating procedures used. If your friends are trained on how to measure the length of commercials, what counts as an ad, and their skills are verified, then it is probably not a problem to use them to collect data. Consistency in measurement is the key.\nThe file ads.csv contains the data. Let’s read the data into R and start to summarize. Remember to load the appropriate R packages.\n\nads &lt;- read_csv(\"data/ads.csv\")\n\n\nads\n\n# A tibble: 10 × 2\n   basic premium\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  6.95    3.38\n 2 10.0     7.8 \n 3 10.6     9.42\n 4 10.2     4.66\n 5  8.58    5.36\n 6  7.62    7.63\n 7  8.23    4.95\n 8 10.4     8.01\n 9 11.0     7.8 \n10  8.52    9.58\n\n\n\nglimpse(ads)\n\nRows: 10\nColumns: 2\n$ basic   &lt;dbl&gt; 6.950, 10.013, 10.620, 10.150, 8.583, 7.620, 8.233, 10.350, 11…\n$ premium &lt;dbl&gt; 3.383, 7.800, 9.416, 4.660, 5.360, 7.630, 4.950, 8.013, 7.800,…\n\n\nNotice that this data may not be tidy; what does each row represent and is it a single observation? We don’t know how the data was obtained, but if each row represents a different friend who watches one basic and one premium channel, then it is possible this data is tidy. We want each observation to be a single TV show, so let’s clean up, or tidy, our data. Remember to ask yourself “What do I want R to do?” and “What does it need to do this?” We want one column that specifies the channel type and another to specify the total length of the commercials.\nWe need R to put, or pivot, the data into a longer form. We need the function pivot_longer(). For more information type vignette(\"pivot\") at the command prompt in R.\n\nads &lt;- ads %&gt;%\n  pivot_longer(cols = everything(), names_to = \"channel\", values_to = \"length\")\nads\n\n# A tibble: 20 × 2\n   channel length\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 basic     6.95\n 2 premium   3.38\n 3 basic    10.0 \n 4 premium   7.8 \n 5 basic    10.6 \n 6 premium   9.42\n 7 basic    10.2 \n 8 premium   4.66\n 9 basic     8.58\n10 premium   5.36\n11 basic     7.62\n12 premium   7.63\n13 basic     8.23\n14 premium   4.95\n15 basic    10.4 \n16 premium   8.01\n17 basic    11.0 \n18 premium   7.8 \n19 basic     8.52\n20 premium   9.58\n\n\nLooks good. Let’s summarize the data.\n\ninspect(ads)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 channel character      2 20       0\n                                   distribution\n1 basic (50%), premium (50%)                   \n\nquantitative variables:  \n    name   class   min     Q1 median      Q3    max    mean       sd  n missing\n1 length numeric 3.383 7.4525  8.123 9.68825 11.016 8.03215 2.121412 20       0\n\n\nThis summary is not what we want, since we want to break it down by channel type.\n\nfavstats(length ~ channel, data = ads)\n\n  channel   min      Q1 median       Q3    max   mean       sd  n missing\n1   basic 6.950 8.30375  9.298 10.30000 11.016 9.2051 1.396126 10       0\n2 premium 3.383 5.05250  7.715  7.95975  9.580 6.8592 2.119976 10       0\n\n\n\nExercise: Visualize the data using a boxplot.\n\n\nads %&gt;%\n  gf_boxplot(channel ~ length) %&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\n\nIt appears that the premium channels are skewed to the left. A density plot may help us compare the distributions and see the skewness, Figure 21.1.\n\nads %&gt;%\n  gf_dens(~length, color = ~channel)%&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Density\", color = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 21.1: Commercial length broken down by channel type.\n\n\n\n\n\nFrom this data, it looks like there is a difference between the two type of channels, but we must put the research question into a metric that will allow us to reach a decision. We will do this in a hypothesis test. As a reminder, the steps are\n\nState the null and alternative hypotheses.\n\nCompute a test statistic.\n\nDetermine the \\(p\\)-value.\n\nDraw a conclusion.\n\nBefore doing this, let’s visit an example of hypothesis testing that has become common knowledge for us, the US criminal trial system. We could also use the cadet honor system. This analogy allows us to remember and apply the steps.\n\n21.3.1 Hypothesis testing in the US court system\nA US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.\nThe jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).\nJurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty.\nThis is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.\nThere are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.\nNow back to our problem.\n\n\n21.3.2 Step 1- State the null and alternative hypotheses\nThe first step is to translate the research question into hypotheses. As a reminder, our research question is do premium channels have less ad time than basic channels? In collecting the data, we already decided the total length of time of commercials in a 30 minute show was the correct data for answering this question. We believe that premium channels have less commercial time. However, the null hypothesis, the straw man, has to be the default case that makes it possible to generate a sampling distribution.\n\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different.\n\nThese hypotheses are vague. What does it mean for two distributions to be different and how do we measure and summarize this? Let’s move to the second step and then come back and modify our hypotheses. Notice that the null hypothesis states the distributions are the same. When we generate our sampling distribution of the test statistic, we will do so under this null hypothesis.\n\n\n21.3.3 Step 2 - Compute a test statistic.\n\nExercise:\nWhat type of metric could we use to test for a difference in the distributions of commercial lengths between the two types of channels?\n\nThere are many ways for the distributions of lengths of commercials to differ. The easiest is to think of the summary statistics such as mean, median, standard deviation, or some combination of all of these. Historically, for mathematical reasons, it has been common to look at differences in measures of centrality, mean or median. The second consideration is what kind of difference? For example, should we consider a ratio or an actual difference (subtraction)? Again for historical reasons, the difference in means has been used as a measure. To keep things interesting, and to force those with some high school stats experience to think about this problem differently, we are going to use a different metric than has historically been used and taught. This also requires us to write some of our own code. Later, we will ask you to complete the same analysis with a different test statistic, either with your own code or using code from the mosaic package.\nOur metric is the ratio of the median length of commercials in basic channels to premium. Thus, our hypotheses are now:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nFirst, let’s calculate the median length of commercials, by channel type, for our data.\n\nmedian(length ~ channel, data = ads) \n\n  basic premium \n  9.298   7.715 \n\n\nSo, the ratio of median lengths is\n\nmedian(length ~ channel, data = ads)[1] / median(length ~ channel, data = ads)[2]\n\n   basic \n1.205185 \n\n\nLet’s put the calculation of the ratio into a function.\n\nmetric &lt;- function(x){\n  temp &lt;- x[1] / x[2]\n  names(temp) &lt;- \"test_stat\"\n  return(temp)\n}\n\n\nmetric(median(length ~ channel, data = ads))\n\ntest_stat \n 1.205185 \n\n\nNow, let’s save the observed value of the test statistic in an object.\n\nobs &lt;- metric(median(length ~ channel, data = ads))\nobs\n\ntest_stat \n 1.205185 \n\n\nHere is what we have done; we needed a single number metric to use in evaluating the null and alternative hypotheses. The null hypothesis is that the commercial lengths for the two channel types have the same distribution and the alternative is that they don’t. To measure the alternative hypothesis, we decided to use a ratio of the medians. If the ratio is close to 1, then the medians are not different. There may be other ways in which the distributions are different but we have decided on the ratio of medians for this example.\n\n\n21.3.4 Step 3 - Determine the \\(p\\)-value.\nAs a reminder, the \\(p\\)-value is the probability of our observed test statistic or something more extreme, given the null hypothesis is true. Since our null hypothesis is that the distributions are the same, we can use a randomization test. We will shuffle the channel labels since under the null hypothesis, they are irrelevant. Here is the code for one randomization.\n\nset.seed(371)\nmetric(median(length ~ shuffle(channel), data = ads))\n\ntest_stat \n0.9957097 \n\n\nLet’s generate the empirical sampling distribution of the test statistic we developed. We will perform 1,000 simulations now.\n\nset.seed(371)\nresults &lt;- do(1000)*metric(median(length ~ shuffle(channel), data = ads))\n\nNext we create a plot of the distribution of the ratio of median commercial lengths in basic and premium channels, assuming they come from the same population, Figure 21.2.\n\nresults %&gt;%\n  gf_histogram(~test_stat) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 21.2: Histogram of the sampling distribution by an approxiamte permutation test.\n\n\n\n\n\nNotice that this distribution is centered on 1 and appears to be roughly symmetrical. The vertical line is our observed value of the test statistic. It seems to be in the tail, and is larger than expected if the channels came from the same distribution. Let’s calculate the \\(p\\)-value.\n\nresults %&gt;%\n  summarise(p_value = mean(test_stat &gt;= obs))\n\n  p_value\n1   0.026\n\n\nBefore proceeding, we have a technical question: Should we include the observed data in the calculation of the \\(p\\)-value?\nThe answer is that most people would conclude that the original data is one of the possible permutations and thus include it. This practice will also ensure that the \\(p\\)-value from a randomization test is never zero. In practice, this simply means adding 1 to both the numerator and denominator. The mosaic package has done this for us with the prop1() function.\n\nprop1(~(test_stat &gt;= obs), data = results)\n\n prop_TRUE \n0.02697303 \n\n\nThe test we performed is called a one-sided test because we only checked if the median length for basic channels is larger than that for premium channels. In this case of a one-sided test, more extreme meant a ratio much bigger than 1. A two-sided test is also common, in fact it is more common, and is used if we did not apriori think one channel type had longer commercials than the other. In this case, we find the \\(p\\)-value by doubling the single-sided value. This is because more extreme could have happened in either tail of the sampling distribution.\n\n\n21.3.5 Step 4 - Draw a conclusion\nOur research question – do premium channels have less ad time than basic channels? – was framed in context of the following hypotheses:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nIn our simulations, less than 2.7% of the simulated test statistics were greater than or equal (more extreme relative to the null hypothesis) to the observed test statistic. That is, the observed ratio of 1.2 is a rare event if the distributions of commercial lengths for premium and basic channels truly are the same. By chance alone, we would only expect an observed ratio this large to occur less than 3 in 100 times. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we reject \\(H_0\\) and conclude there is evidence that the median length of basic channel ads is bigger than that for premium channels.\nThe less than 3-in-100 chance is the \\(p\\)-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.\nWhen the \\(p\\)-value is small, i.e. less than a previously set threshold, we say the results are statistically significant2. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The threshold, called the significance level and often represented by the Greek letter \\(\\alpha\\), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application. Using a significance level of \\(\\alpha = 0.05\\) in the TV channel study, we can say that the data provided statistically significant evidence against the null hypothesis.\n\nWe say that the data provide statistically significant evidence against the null hypothesis if the \\(p\\)-value is less than some reference value, usually \\(\\alpha=0.05\\).\n\nIf the null hypothesis is true, unknown to us, the significance level \\(\\alpha\\) defines the probability that we will make a Type 1 Error. We will define decision errors in the next section.\n\nSide note: What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye – good job! There are many video clips that explain the use of 0.05. Sometimes it’s also a good idea to deviate from the standard. It really depends on the risk that the decision maker wants to accept in terms of the two types of decision errors.\n\n\nExercise:\nUse our \\(p\\)-value and a significance level of 0.05 to make a decision.\n\nBased on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed ratio of medians is 0.027. Since this is less than our significance level of \\(\\alpha = 0.05\\), we reject the null in favor of the alternative that the basic channel has longer commercials.\n\n\n21.3.6 Decision errors\nHypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\nThere are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\nExample:\nIn a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 error represent in this context? What does a Type 2 error represent?\n\nIf the court makes a Type 1 error, this means the defendant is truly innocent (\\(H_0\\) true) but is wrongly convicted. A Type 2 error means the court failed to reject \\(H_0\\) (i.e. failed to convict the person) when she was in fact guilty (\\(H_A\\) true).\n\nExercise:\nConsider the commercial length study where we concluded basic channels had longer commercials than premium channels. What would a Type 1 error represent in this context?3\n\n\nExercise:\nHow could we reduce the Type 1 error rate in US courts? What influence would this have on the Type 2 error rate?\n\nTo lower the Type 1 error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 errors.\n\nExercise:\nHow could we reduce the Type 2 error rate in US courts? What influence would this have on the Type 1 error rate?\n\nTo lower the Type 2 error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 error rate.\n\nExercise: Think about the cadet honor system, its metric of evaluation, and the impact on the types of decision errors.\n\nThese exercises provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type for a given amount of data, information.\n\n\n21.3.7 Choosing a significance level\nChoosing a significance level for a test is important in many contexts, and the traditional level is \\(\\alpha = 0.05\\). However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.\nIf making a Type 1 error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01 or 0.001). Under this scenario, we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\).\nIf making a Type 2 error is relatively more dangerous or much more costly than a Type 1 error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null hypothesis is actually false.\nThe significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 error.\n\n\n21.3.8 Introducing two-sided hypotheses\nSo far we have explored whether women were discriminated against and whether commercials were longer depending on the type of channel. In these two case studies, we’ve actually ignored some possibilities:\n\nWhat if men are actually discriminated against?\n\nWhat if ads on premium channels are actually longer?\n\nThese possibilities weren’t considered in our hypotheses or analyses. This may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our worldview:\n\nFraming an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.\n\nIf we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!\n\nThe previous hypothesis tests we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in a single direction, but usually we want to consider all possibilities. To do so, let’s discuss two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone cardiopulmonary resuscitation, CPR.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "href": "Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "21.4 Two-sided hypothesis test",
    "text": "21.4 Two-sided hypothesis test\nIt is important to distinguish between a two-sided hypothesis test and a one-sided hypothesis test. In a two-sided test, we are concerned with whether or not the population parameter could take a particular value. For parameter \\(\\theta\\), a set of two-sided hypotheses looks like:\n\\[\nH_0: \\theta=\\theta_0 \\hspace{1.5cm} H_1: \\theta \\neq \\theta_0\n\\] where \\(\\theta_0\\) is a particular value the parameter could take on.\nIn a one-sided test, we are concerned with whether a parameter exceeds or does not exceed a specific value. A set of one-sided hypotheses looks like:\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &gt; \\theta_0\n\\] or\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &lt; \\theta_0\n\\]\nIn some texts, one-sided null hypotheses include an inequality (\\(H_0 \\leq \\theta_0\\) or \\(H_0 \\geq \\theta_0\\)). We have already demonstrated one-sided tests and, in the next example, we will use a two-sided test.\n\n21.4.1 CPR example\nCardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.\nHere we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.4 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\n\n\n21.4.2 Step 1 - State the null and alternative hypotheses\n\nExercise: Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).\n\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\\(H_0\\): Blood thinners do not have an overall survival effect; survival rate is independent of experimental treatment group. \\(p_c - p_t = 0\\).\n\\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\).\nNotice here that we accelerated the process by already defining our test statistic, our metric, in the hypothesis. It is the difference in survival rates for the control and treatment groups. This is a similar metric to what we used in the case study. We could use others but this will allow us to use functions from the mosaic package and will also help us to understand metrics for mathematically derived sampling distributions.\nThere were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are in the file blood_thinner.csv.\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\nthinner\n\n# A tibble: 90 × 2\n   group     outcome \n   &lt;chr&gt;     &lt;chr&gt;   \n 1 treatment survived\n 2 control   survived\n 3 control   died    \n 4 control   died    \n 5 control   died    \n 6 treatment survived\n 7 control   died    \n 8 control   died    \n 9 treatment died    \n10 treatment survived\n# ℹ 80 more rows\n\n\nLet’s put the data in a table.\n\ntally(~group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\n\n21.4.3 Step 2 - Compute a test statistic.\nThe test statistic we have selected is the difference in survival rate between the control group and the treatment group. The following R command finds the observed proportions.\n\ntally(outcome ~ group, data = thinner, margins = TRUE, format = \"proportion\")\n\n          group\noutcome    control treatment\n  died        0.78      0.65\n  survived    0.22      0.35\n  Total       1.00      1.00\n\n\nNotice the formula we used to get the correct variable in the column for the summary proportions.\nThe observed test statistic can now be found.5\n\nobs &lt;- diffprop(outcome ~ group, data = thinner)\nobs\n\ndiffprop \n   -0.13 \n\n\nBased on the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. We wonder if this difference is easily explainable by chance.\n\n\n21.4.4 Step 3 - Determine the \\(p\\)-value.\nAs we did in the previous two studies, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning treatment and control stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences. This is our empirical sampling distribution, the distribution of differences simulated under the null hypothesis.\n\nset.seed(655)\nresults &lt;- do(10000)*diffprop(outcome ~ shuffle(group), data = thinner)\n\nFigure 21.3 is a histogram of the estimated sampling distribution.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 21.3: Histogram of the estimated sampling distribution.\n\n\n\n\n\nNotice how it is centered on zero, the assumption of no difference. Also notice that it is unimodal and symmetric. We will use this when we develop mathematical sampling distributions. We now calculate the proportion of simulated differences that are less than or equal to the observed difference.\n\nprop1(~(diffprop &lt;= obs), data = results)\n\nprop_TRUE \n0.1283872 \n\n\nThe left tail area is about 0.128. (Note: it is only a coincidence that our \\(p\\)-value is approximately 0.13 and we also have \\(\\hat{p}_c - \\hat{p}_t= -0.13\\).) However, contrary to how we calculated the \\(p\\)-value in previous studies, the \\(p\\)-value of this test is not 0.128!\nThe \\(p\\)-value is defined as the probability we observe a result at least as favorable to the alternative hypothesis as the observed result (i.e. the observed difference). In this case, any differences greater than or equal to 0.13 would provide equally strong evidence favoring the alternative hypothesis as differences less than or equal to -0.13. A difference of 0.13 would correspond to 13% higher survival rate in the treatment group than the control group.\nThere is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase or decrease the risk of death in patients who undergo CPR before arriving at the hospital.6\nFor a two-sided test, we take the single tail (in this case, 0.128) and double it to get the \\(p\\)-value: 0.256.\n\n\n21.4.5 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment.\n\nDefault to a two-sided test We want to be rigorous and keep an open mind when we analyze data and evidence. In general, you should default to using a two-sided test when conducting hypothesis tests. Use a one-sided hypothesis test only if you truly have interest in only one direction.\n\n\nComputing a \\(p\\)-value for a two-sided test\nFirst compute the \\(p\\)-value for one tail of the distribution, then double that value to get the two-sided \\(p\\)-value. That’s it!\n\nIt is never okay to change two-sided tests to one-sided tests after observing the data.\n\nHypothesis tests should be set up before seeing the data\nAfter observing data, it can be tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses and the significance level should be set up before observing the data.\n\n\n\n21.4.6 How to use a hypothesis test\nThis is a summary of the general framework for using hypothesis testing. These are the same steps as before, with just slightly different wording.\n\nFrame the research question in terms of hypotheses.\nHypothesis tests are appropriate for research questions that can be summarized in two competing hypotheses. The null hypothesis (\\(H_0\\)) usually represents a skeptical perspective or a perspective of no difference. The alternative hypothesis (\\(H_A\\)) usually represents a new view or a difference.\nCollect data with an observational study or experiment.\nIf a research question can be formed into two hypotheses, we can collect data to run a hypothesis test. If the research question focuses on associations between variables but does not concern causation, we would run an observational study. If the research question seeks a causal connection between two or more variables, then an experiment should be used if possible.\nAnalyze the data.\nChoose an analysis technique appropriate for the data and identify the \\(p\\)-value. So far, we’ve only seen one analysis technique: randomization. We’ll encounter several new methods suitable for many other contexts.\nForm a conclusion.\nUsing the \\(p\\)-value from the analysis, determine whether the data provide statistically significant evidence against the null hypothesis. Also, be sure to write the conclusion in plain language so casual readers can understand the results.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-Simulation.html#footnotes",
    "href": "Hypothesis-Testing-Simulation.html#footnotes",
    "title": "21  Hypothesis Testing with Simulation",
    "section": "",
    "text": "In our opinion, this is how things developed historically. However, since computational tools prior to machine computers, humans in most cases, were limited and expensive, there was a shift to mathematical solutions. The relatively recent increase and availability in machine computational power has led to a shift back to computational methods. Thus, some people think mathematical methods predate computational methods, but that is not the case.↩︎\nSome experts in the field of statistics are uncomfortable with using a significance level to declare that something is statistically significant. The choice of a significance level (discussed more in a later section) can be somewhat arbitrary. If you conduct a hypothesis test with a \\(p\\)-value of 0.051 and chose a significance level of 0.05, are your results really non-significant compared to getting a \\(p\\)-value of 0.049? For this reason, some experts shy away from the term statistically significant and instead describe detectable or discernable effects/differences.↩︎\nMaking a Type 1 error in this context would mean that there is no difference in commercial length between basic and premium channels, despite the strong evidence (the data suggesting otherwise) found in the observational study. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings. Replication is part of the scientific method.↩︎\n“Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎\nObserved control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Observed treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_c - \\hat{p}_t = 0.22 - 0.35 = -0.13\\).↩︎\nRealistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html",
    "href": "Hypothesis-Testing-ProbDist.html",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "22.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#objectives",
    "href": "Hypothesis-Testing-ProbDist.html#objectives",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, p-value, and power, and construct examples to demonstrate their proper use in context.\nApply and evaluate all four steps of a hypothesis test using probability models: formulating hypotheses, calculating a test statistic, determining the p-value through randomization, and making a decision based on the test outcome.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "href": "Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "22.2 Hypothesis testing using probability models",
    "text": "22.2 Hypothesis testing using probability models\nAs a lead into the Central Limit Theorem in Chapter 23 and mathematical sampling distributions, we will look at a class of hypothesis testing where the null hypothesis specifies a probability model. In some cases, we can get an exact answer, and in others, we will use simulation to get an empirical \\(p\\)-value. By the way, a permutation test is an exact test; by this we mean we are finding all possible permutations in the calculation of the \\(p\\)-value. However, since the complete enumeration of all permutations is often difficult, we approximate it with randomization, simulation. Thus, the \\(p\\)-value from a randomization test is an approximation of the exact (permutation) test.\nLet’s use three examples to illustrate the ideas of this chapter.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "href": "Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "22.3 Tappers and listeners",
    "text": "22.3 Tappers and listeners\nHere’s a game you can try with your friends or family. Pick a simple, well-known song. Tap that tune on your desk, and see if the other person can guess the song. In this simple game, you are the tapper, and the other person is the listener.\nA Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.1 In her study, she recruited 120 tappers and 120 listeners into the study. About 50% of the tappers expected that the listener would be able to guess the song. Newton wondered, is 50% a reasonable expectation?\n\n22.3.1 Step 1- State the null and alternative hypotheses\nNewton’s research question can be framed into two hypotheses:\n\\(H_0\\): The tappers are correct, and, in general, 50% of listeners are able to guess the tune. \\(p = 0.50\\)\n\\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners are able to guess the tune. \\(p \\neq 0.50\\)\n\nExercise: Is this a one-sided or two-sided hypothesis test? How many variables are in this model?\n\nThe tappers think that listeners will guess the song correctly 50% of the time, and this is a two-sided test since we don’t know beforehand if listeners will be better or worse than 50%.\nThere is only one variable of interest, whether the listener is correct.\n\n\n22.3.2 Step 2 - Compute a test statistic\nIn Newton’s study, only 42 (we changed the number to make this problem more interesting from an educational perspective) out of 120 listeners (\\(\\hat{p} = 0.35\\)) were able to guess the tune! From the perspective of the null hypothesis, we might wonder, how likely is it that we would get this result from chance alone? That is, what’s the chance we would happen to see such a small fraction if \\(H_0\\) were true and the true correct-guess rate is 0.50?\nNow before we use simulation, let’s frame this as a probability model. The random variable \\(X\\) is the number of correct guesses out of 120. If the observations are independent and the probability of success is constant (each listener has the same probability of guessing correctly), then we could use a binomial model. We can’t assess the validity of these assumptions without knowing more about the experiment, the subjects, and the data collection. For educational purposes, we will assume they are valid. Thus, our test statistic is the number of successes in 120 trials. The observed value is 42.\n\n\n22.3.3 Step 3 - Determine the \\(p\\)-value\nWe now want to find the \\(p\\)-value as \\(2 \\cdot \\mbox{P}(X \\leq 42)\\) where \\(X\\) is a binomial random variable with \\(p = 0.5\\) and \\(n = 120\\). Again, the \\(p\\)-value is the probability of the observed data or something more extreme, given the null hypothesis is true. Here, the null hypothesis being true implies that the probability of success is 0.50. We will use R to get the one-sided \\(p\\)-value and then double it to get the two-sided \\(p\\)-value for the problem. We selected \\(\\mbox{P}(X \\leq 42)\\) because “more extreme” means the observed values and values further from what you would get if the null hypothesis were true, which is 60 for this problem.\n\n2*pbinom(42, 120, prob = 0.5)\n\n[1] 0.001299333\n\n\nThat is a small \\(p\\)-value.\n\n\n22.3.4 Step 4 - Draw a conclusion\nBased on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0013\\) probability that only 42 or less (or 78 or more) listeners would get correctly. This is the probability of what we observed or something more extreme, given the null hypothesis is true. This probability is much less than 0.05, so we reject the null hypothesis that the listeners are guessing correctly half of the time and conclude that the correct-guess rate rate is different from 50%.\nThis decision region looks like the pmf in Figure 22.1. Any observed values inside the red boundary lines would be consistent with the null hypothesis. That is, any observed values inside the red boundary lines would result in a \\(p\\)-value larger than 0.05. Any values at the red line or more extreme would be in the rejection region, resulting in a \\(p\\)-value smaller than 0.05. We also plotted the observed value in black.\n\ngf_dist(\"binom\", size = 120, prob = 0.5, xlim = c(50, 115)) %&gt;%\n  gf_vline(xintercept = c(48, 72), color = \"red\") %&gt;%\n  gf_vline(xintercept = c(42), color = \"black\") %&gt;%\n  gf_theme(theme_bw) %&gt;%\n  gf_labs(title = \"Binomial pmf\", subtitle = \"Probability of success is 0.5\", \n          y = \"Probability\")\n\n\n\n\n\n\n\nFigure 22.1: Binomial pmf\n\n\n\n\n\n\n\n22.3.5 Repeat using simulation\nWe will repeat the analysis using an empirical (observed from simulated data) \\(p\\)-value. Step 1, stating the null and alternative hypothesis, is the same.\n\n\n22.3.6 Step 2 - Compute a test statistic\nWe will use the proportion of listeners that get the song correct instead of the number of listeners that get it correct. This is a minor change since we are simply dividing by 120.\n\nobs &lt;- 42 / 120\nobs\n\n[1] 0.35\n\n\n\n\n22.3.7 Step 3 - Determine the \\(p\\)-value\nTo simulate 120 games under the null hypothesis where \\(p = 0.50\\), we could flip a coin 120 times. Each time the coin comes up heads, this could represent the listener guessing correctly, and tails would represent the listener guessing incorrectly. For example, we can simulate 5 tapper-listener pairs by flipping a coin 5 times:\n\\[\n\\begin{array}{ccccc}\nH & H & T & H & T \\\\\nCorrect & Correct & Wrong & Correct & Wrong \\\\\n\\end{array}\n\\]\nAfter flipping the coin 120 times, we got 56 heads for a proportion of \\(\\hat{p}_{sim} = 0.467\\). As we did with the randomization technique, seeing what would happen with one simulation isn’t enough. In order to evaluate whether our originally observed proportion of 0.35 is unusual or not, we should generate more simulations. Here, we’ve repeated this simulation 10,000 times:\n\nset.seed(604)\nresults &lt;- rbinom(10000, 120, 0.5) / 120\n\nNote, we could simulate it a number of ways. Here is a way using do() that will look like what we’ve done for other randomization tests.\n\nset.seed(604)\nresults &lt;- do(10000)*mean(sample(c(0, 1), size = 120, replace = TRUE))\n\n\nhead(results)\n\n       mean\n1 0.4250000\n2 0.5250000\n3 0.5916667\n4 0.5000000\n5 0.5250000\n6 0.5083333\n\n\n\nresults %&gt;%\n  gf_histogram(~mean, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = c(obs, 1 - obs), color = \"black\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 22.2: The estimated sampling distribution\n\n\n\n\n\nNotice in Figure 22.2 how the sampling distribution is centered at 0.5 and looks symmetrical.\nThe \\(p\\)-value is found using the prop1 function. In this problem, we really need the observed value to be included to prevent a \\(p\\)-value of zero.\n\n2*prop1(~(mean &lt;= obs), data = results)\n\n prop_TRUE \n0.00119988 \n\n\n\n\n22.3.8 Step 4 - Draw a conclusion\nIn these 10,000 simulations, we see very few results close to 0.35. Based on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0012\\) probability that only 35% or less or 65% or more listeners would get it right. This \\(p\\)-value is much less than 0.05, so we reject that the listeners are guessing correctly half of the time and conclude that the correct-guess rate is different from 50%.\n\nExercise: In the context of the experiment, what is the \\(p\\)-value for the hypothesis test?2\n\n\nExercise:\nDo the data provide statistically significant evidence against the null hypothesis? State an appropriate conclusion in the context of the research question.3",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#golf-balls",
    "href": "Hypothesis-Testing-ProbDist.html#golf-balls",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "22.4 Golf Balls",
    "text": "22.4 Golf Balls\nOur last example will be interesting because the distribution has multiple parameters and a test metric is not obvious at this point.\nThe owners of a residence located along a golf course collected the first 500 golf balls that landed on their property. Most golf balls are labeled with the make of the golf ball and a number, for example “Nike 1” or “Titleist 3”. The numbers are typically between 1 and 4, and the owners of the residence wondered if these numbers are equally likely (at least among golf balls used by golfers of poor enough quality that they lose them in the yards of the residences along the fairway.)\nWe will use a significance level of \\(\\alpha = 0.05\\) since there is no reason to favor one decision error over the other.\n\n22.4.1 Step 1- State the null and alternative hypotheses\nWe think that the numbers are not all equally likely. The question of one-sided versus two-sided is not relevant in this test. You will see this when we write the hypotheses.\n\\(H_0\\): All of the numbers are equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) Or \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\)\n\\(H_A\\): There is some other distribution of the numbers in the population. At least one population proportion is not \\(\\frac{1}{4}\\).\nNotice that we switched to using \\(\\pi\\) instead of \\(p\\) for the population parameter. There is no reason other than to make you aware that both are used.\nThis problem is an extension of the binomial. Instead of two outcomes, there are four outcomes. This is called a multinomial distribution. You can read more about it if you like, but our methods will not make it necessary to learn the probability mass function.\nOut of the 500 golf balls collected, 486 of them had a number between 1 and 4. We will deal with only these 486 golf balls. Let’s get the data from `golf_balls.csv”.\n\ngolf_balls &lt;- read_csv(\"data/golf_balls.csv\")\n\n\ninspect(golf_balls)\n\n\nquantitative variables:  \n    name   class min Q1 median Q3 max     mean       sd   n missing\n1 number numeric   1  1      2  3   4 2.366255 1.107432 486       0\n\n\n\ntally(~number, data = golf_balls)\n\nnumber\n  1   2   3   4 \n137 138 107 104 \n\n\n\n\n22.4.2 Step 2 - Compute a test statistic.\nIf all numbers were equally likely, we would expect to see 121.5 golf balls of each number. This is a point estimate and thus not an actual value that could be realized. Of course, in a sample we will have variation and thus departure from this state. We need a test statistic that will help us determine if the observed values are reasonable under the null hypothesis. Remember that the test statistic is a single number metric used to evaluate the hypothesis.\n\nExercise:\nWhat would you propose for the test statistic?\n\nWith four proportions, we need a way to combine them. This seems tricky, so let’s just use a simple approach. Let’s take the maximum number of balls across all cells of the table and subtract the minimum. This is called the range and we will denote the parameter as \\(R\\). Under the null hypothesis, this should be zero. We could re-write our hypotheses as:\n\\(H_0\\): \\(R=0\\)\n\\(H_A\\): \\(R&gt;0\\)\nNotice that \\(R\\) will always be non-negative, thus this test is one-sided.\nThe observed range is 34, \\(138 - 104\\).\n\nobs &lt;- diff(range(tally(~number, data = golf_balls)))\nobs\n\n[1] 34\n\n\n\n\n22.4.3 Step 3 - Determine the \\(p\\)-value.\nWe don’t know the distribution of our test statistic, so we will use simulation. We will simulate data from a multinomial distribution under the null hypothesis and calculate a new value of the test statistic. We will repeat this 10,000 times and this will give us an estimate of the sampling distribution.\nWe will use the sample() function again to simulate the distribution of numbers under the null hypothesis. To help us understand the process and build the code, we are only initially using a sample size of 12 to keep the printout reasonable and easy to read.\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nNotice this is not using tidyverse coding ideas. We don’t think we need tibbles or data frames so we went with straight nested R code. You can break this code down by starting with the code in the center.\n\nset.seed(3311)\nsample(1:4, size = 12, replace = TRUE)\n\n [1] 3 1 2 3 2 3 1 3 3 4 1 1\n\n\n\nset.seed(3311)\ntable(sample(1:4, size = 12, replace = TRUE))\n\n\n1 2 3 4 \n4 2 5 1 \n\n\n\nset.seed(3311)\nrange(table(sample(1:4, size = 12, replace = TRUE)))\n\n[1] 1 5\n\n\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nWe are now ready to ramp up to the full problem. Let’s simulate the data under the null hypothesis. We are sampling 486 golf balls (instead of 12) with the numbers 1 through 4 on them. Each number is equally likely. We then find the range, our test statistic. Finally we repeat this 10,000 to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(3311)\nresults &lt;- do(10000)*diff(range(table(sample(1:4, size = 486, replace = TRUE))))\n\nFigure 22.3 is a plot of the sampling distribution of the range.\n\nresults %&gt;%\n  gf_histogram(~diff, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Range\", \n          subtitle = \"Multinomial with equal probability\",\n          x = \"Range\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 22.3: Sampling distribution of the range.\n\n\n\n\n\nNotice how this distribution is skewed to the right. The \\(p\\)-value is 0.14.\n\nprop1(~(diff &gt;= obs), data = results)\n\nprop_TRUE \n0.1393861 \n\n\n\n\n22.4.4 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, based on our data, we do not find statistically significant evidence against the claim that the number on the golf balls are equally likely. We can’t say that the proportion of golf balls with each number differs from 0.25.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "href": "Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "22.5 Repeat with a different test statistic",
    "text": "22.5 Repeat with a different test statistic\nThe test statistic we developed was helpful, but it seems weak because we did not use the information in all four cells. So let’s devise a metric that does this. The hypotheses are the same, so we will jump to step 2.\n\n22.5.1 Step 2 - Compute a test statistic.\nIf each number were equally likely, we would have 121.5 balls in each bin. We can find a test statistic by looking at the deviation in each cell from 121.5.\n\ntally(~number, data = golf_balls) - 121.5\n\nnumber\n    1     2     3     4 \n 15.5  16.5 -14.5 -17.5 \n\n\nNow we need to collapse these into a single number. Just adding will always result in a value of 0, why? So let’s take the absolute value and then add the cells together.\n\nobs &lt;- sum(abs(tally(~number, data = golf_balls) - 121.5))\nobs\n\n[1] 64\n\n\nThis will be our test statistic.\n\n\n22.5.2 Step 3 - Determine the \\(p\\)-value.\nWe will use similar code from above with our new metric. Now we sample 486 golf balls with the numbers 1 through 4 on them, and find our test statistic, the sum of the absolute deviations of each cell of the table from the expected count, 121.5. We repeat this process 10,000 times to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(9697)\nresults &lt;- do(10000)*sum(abs(table(sample(1:4, size = 486, replace = TRUE)) - 121.5))\n\nFigure 22.4 is a plot of the sampling distribution of the absolute value of deviations.\n\nresults %&gt;%\n  gf_histogram(~sum, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Absolute Deviations\",\n          subtitle = \"Multinomial with equal probability\",\n          x = \"Absolute deviations\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 22.4: Sampling distribution of the absolute deviations.\n\n\n\n\n\nNotice how this distribution is skewed to the right and our test statistic seems to be more extreme.\n\nprop1(~(sum &gt;= obs), data = results)\n\n prop_TRUE \n0.01359864 \n\n\nThe \\(p\\)-value is 0.014. This value is much smaller than our previous result. The test statistic matters in our decision process as nothing about this problem has changed except the test statistic.\n\n\n22.5.3 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is smaller than 0.05, we reject the null hypothesis. That is, based on our data, we find statistically significant evidence against the claim that the numbers on the golf balls are equally likely. We conclude that the numbers on the golf balls are not all equally likely, or that at least one is different.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#summary",
    "href": "Hypothesis-Testing-ProbDist.html#summary",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "22.6 Summary",
    "text": "22.6 Summary\nIn this chapter, we used probability models to help us make decisions from data. This chapter is different from the randomization section in that randomization had two variables (one of which we could shuffle) and a null hypothesis of no difference. In the case of a single proportion, we were able to use the binomial distribution to get an exact \\(p\\)-value under the null hypothesis. In the case of a \\(2 \\times 2\\) table, we were able to show that we could use the hypergeometric distribution to get an exact \\(p\\)-value under the assumptions of the model.\nWe also found that the choice of test statistic has an impact on our decision. Even though we get valid \\(p\\)-values and the desired Type 1 error rate, if the information in the data is not used to its fullest, we will lose power. Note: power is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\nIn the next chapter, we will learn about mathematical solutions to finding the sampling distribution. The key difference in all these methods is the selection of the test statistic and the assumptions made to derive a sampling distribution.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Hypothesis-Testing-ProbDist.html#footnotes",
    "href": "Hypothesis-Testing-ProbDist.html#footnotes",
    "title": "22  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "This case study is described in Made to Stick: Why Some Ideas Survive and Others Die by Chip and Dan Heath.↩︎\nThe \\(p\\)-value is the chance of seeing the observed data or something more in favor of the alternative hypothesis (something as or more extreme) given that guessing has a probability of success of 0.5. Since we didn’t observe many simulations with even close to just 42 listeners correct, the \\(p\\)-value will be small, around 1-in-1000.↩︎\nThe \\(p\\)-value is less than 0.05, so we reject the null hypothesis. There is statistically significant evidence, and the data provide strong evidence that the chance a listener will guess the correct tune is different from 50%.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html",
    "href": "Central-Limit-Theorem.html",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "",
    "text": "23.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#objectives",
    "href": "Central-Limit-Theorem.html#objectives",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "",
    "text": "Explain the central limit theorem and when it can be used for inference.\nApply the CLT to conduct hypothesis tests using R and interpret the results with an understanding of the CLT’s role in justifying normal approximations.\nAnalyze the relationship between the \\(t\\)-distribution and normal distribution, explain usage contexts, and evaluate how changes in parameters impact their shape and location using visualizations.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#central-limit-theorem",
    "href": "Central-Limit-Theorem.html#central-limit-theorem",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "23.2 Central limit theorem",
    "text": "23.2 Central limit theorem\nWe’ve encountered several research questions and associated hypothesis tests so far in this block of material. While they differ in the settings, in their outcomes, and also in the technique we use to analyze the data, many of them have something in common: for a certain class of test statistics, the general shape of the sampling distribution under the null hypothesis looks like a normal distribution.\n\n23.2.1 Null distribution\nAs a reminder, in the tapping and listening problem, we used the proportion of correct guesses as our test statistic. Under the null hypothesis, we assumed the probability of success was 0.5. The estimate of the sampling distribution of our test statistic is shown in Figure 23.1.\n\n\n\n\n\n\n\n\nFigure 23.1: Sampling distribution of the proportion.\n\n\n\n\n\n\nExercise:\nDescribe the shape of the distribution and note anything that you find interesting.1\n\nIn Figure 23.2, we have overlayed a normal distribution on the histogram of the estimated sampling distribution. This allows us to visually compare a normal probability density curve with the empirical (based on data, or simulation in this case) sampling distribution.\n\n\n\n\n\n\n\n\nFigure 23.2: Sampling distribution of the sample proportion.\n\n\n\n\n\nThis similarity between the empirical and theoretical distributions is not a coincidence, but rather is guaranteed by mathematical theory. This chapter will be a little more notation- and algebra-intensive than the previous chapters. However, the goal is to develop a tool that will help us find sampling distributions for many types of test statistics and, thus, find \\(p\\)-values. This chapter involves classical statistics often taught in AP high school classes, as well as many introductory undergraduate statistics courses. Remember that before the advances of modern computing, these mathematical solutions were all that was available.\n\n\n23.2.2 Theorem - central limit theorem\nTheorem: Let \\(X_1, X_2, ..., X_n\\) be a sequence of i.i.d., independent and identically distributed, random variables from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma &lt; \\infty\\). Then,\n\\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\mu,{\\sigma\\over\\sqrt{n}}\\right)\n\\]\nThere is a lot going on in this theorem. First, notice we are drawing independent samples from the same parent population. The central limit theorem (CLT) does not specify the form of this parent distribution, only that it has a finite variance (\\(\\sigma &lt; \\infty\\)). Second, the CLT tells us that if we form a new random variable that involves the sum of the individual random variables (in this case, the sample mean \\(\\bar{X}\\)), the distribution of that new random variable is approximately normal. In the case of the sample mean, the expected value (the first parameter of the normal distribution) is the same mean as for the parent population. The standard deviation (the second parameter of the normal distribution) is the standard deviation of the parent population divided by the square root of the sample size \\(n\\). Let’s summarize these ideas.\n\nThe process of creating a new random variable from the sum of independent, identically distributed random variables is approximately normal.\nThe approximation to a normal distribution improves with sample size \\(n\\).\nThe mean and variance of the sampling distribution are a function of the mean and variance of the parent population, the sample size \\(n\\), and the form of the new random variable.\n\nIf you go back and review examples, exercises, and homework problems from the previous chapters on hypothesis testing, you will see that we found symmetric, normal-“looking” sampling distributions when we created test statistics that involved the process of summing. One example of a skewed sampling distribution was the golf ball example, where our test statistic was the difference between the maximum and minimum value (and did not involve a summation of all the observations). It is hard to overstate the historical importance of this theorem to the field of inferential statistics and science in general.\nTo get an understanding and some intuition of the central limit theorem, let’s simulate some data and evaluate.\n\n\n23.2.3 Simulating data for the CLT\nFor this section, we are going to use an artificial example where we know the population distribution and parameters. We will repeat sampling from this population distribution many times and plot the distribution of the summary statistic of interest, the sample mean, to demonstrate the CLT. This is purely an educational thought experiment to give ourselves confidence about the validity of the CLT.\nSuppose there is an upcoming election in Colorado and Proposition A is on the ballot. Now suppose that 65% of Colorado voters support Proposition A. We poll a random sample of \\(n\\) Colorado voters. Prior to conducting the sample, we can think about the sample as a sequence of i.i.d. random variables (voters) from the binomial distribution with one trial (vote) in each run and a probability of success (support for the measure) of 0.65. In other words, each random variable will take a value of 1 (support) or 0 (oppose). Figure 23.3 is a plot of the pmf of the parent distribution (\\(\\textsf{Binom}(1,\\, 0.65)\\)):\n\n\n\n\n\n\n\n\nFigure 23.3: Binomial pmf with one trial and probability of success of 0.65.\n\n\n\n\n\nThis is clearly not normally distributed. It is, in fact, discrete. The mean of \\(X\\) is 0.65 and the standard deviation is \\(\\sqrt{0.65(1 - 0.65)} = 0.477\\).\nIn our first simulation, we let the sample size be \\(n = 10\\). This is typically too small for the CLT to apply, but we will still use it as a starting point. In the code below, we will obtain a sample of size 10 from this binomial distribution and record the observed mean \\(\\bar{x}\\), which is a method of moments estimate of the probability of success. We will repeat this process 10,000 times to get an empirical distribution of \\(\\bar{X}\\). (Note that \\(\\bar{X}\\) is a mean of 1s and 0s, and can be thought of as the proportion of voters in the sample that support the measure. Often, the population proportion is denoted as \\(\\pi\\) and the sample proportion is denoted as \\(\\hat{\\pi}\\).)\n\nset.seed(5501)\nresults &lt;- do(10000)*mean(rbinom(10, 1, 0.65))\n\nSince we are summing i.i.d. variables, the sampling distribution of the mean should look like a normal distribution. The mean should be close to 0.65 (the mean from the parent distribution), and the standard deviation should be close to \\(\\sqrt{\\frac{p(1 - p)}{n}} = \\sqrt{\\frac{0.65(1 - 0.65)}{10}} = 0.151\\) (the standard deviation of the parent distribution divided by \\(\\sqrt{n}\\)).\n\nfavstats(~mean, data = results)\n\n min  Q1 median  Q3 max    mean        sd     n missing\n 0.1 0.5    0.7 0.8   1 0.64932 0.1505716 10000       0\n\n\nRemember from our chapters on probability, these results for the mean and standard deviation do not depend on the CLT. They are results from the properties of expectation on independent samples. The distribution of the sample mean (i.e., the shape of the sampling distribution) is approximately normal as a result of the CLT, Figure 23.4.\n\n\n\n\n\n\n\n\nFigure 23.4: Sampling distribution of the sample proportion with sample size of 10.\n\n\n\n\n\nNote that the sampling distribution of the sample mean has a bell-curve shape, but with some skew to the left for this particular small sample size. That is why we state that the approximation improves with sample size.\nAs a way to determine the impact of the sample size on inference to the population, let’s record how often a sample of 10 failed to indicate support for the measure. (How often was the sample proportion less than or equal to 0.5?) Remember, in this artificial example, we know that the population is in favor of the measure, 65% approval. However, if our point estimate is below 0.5, we would be led to believe that the population does not support the measure.\n\nresults %&gt;%\n summarise(low_result = mean(~mean &lt;= 0.5))\n\n  low_result\n1     0.2505\n\n\nEven though we know that 65% of Colorado voters support the measure, a sample of size 10 failed to indicate support 25.05% of the time.\nLet’s take a larger sample. In the code below, we will repeat what we did above but with a sample of size 25. Figure 23.5 plots the sampling distribution.\n\nset.seed(5501)\nresults &lt;- do(10000)*mean(rbinom(25, 1, 0.65))\n\n\n\n\n\n\n\n\n\nFigure 23.5: Sampling distribution of the sample proportion with sample size of 25.\n\n\n\n\n\n\nresults %&gt;%\n summarise(low_result = mean(~mean &lt;= 0.5))\n\n  low_result\n1     0.0623\n\n\nWhen increasing the sample size to 25, the standard deviation of our sample proportion decreased. According to the central limit theorem, it should have decreased to \\(\\sigma/\\sqrt{25} = \\sqrt{\\frac{p(1 - p)}{25}} = 0.095\\). Also, the skew became less severe (the shape became “more normal”). Further, the sample of size 25 failed to indicate support only 6.23% of the time. It reasonably follows that an even larger sample would continue these trends. Figure 23.6 demonstrates these trends.\n\n\n\n\n\n\n\n\nFigure 23.6: Sampling distribution of the sample proportion for different sample sizes.\n\n\n\n\n\n\n\n23.2.4 Summary of example\nIn this example, we knew the true proportion of voters who supported the proposition. Based on that knowledge, we simulated the behavior of the sample proportion. We did this by taking a sample of size \\(n\\), recording the sample proportion (sample mean of 1s and 0s), and repeating that process thousands of times. In reality, we will not know the true underlying level of support; further, we will not take a sample repeatedly, thousands of times, from the parent population. Sampling can be expensive and time-consuming. Thus, we would take one random sample of size \\(n\\), and acknowledge that the resulting sample proportion is but one observation from an underlying normal distribution. We would then determine what values of \\(\\pi\\) (the true unknown population proportion) could reasonably have resulted in the observed sample proportion.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#the-t-distribution",
    "href": "Central-Limit-Theorem.html#the-t-distribution",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "23.3 The \\(t\\)-distribution",
    "text": "23.3 The \\(t\\)-distribution\nPrior to using the CLT in hypothesis testing, we want to discuss other sampling distributions that are based on the CLT or normality assumptions. A large part of theoretical statistics has been about mathematically deriving the distribution of sample statistics. In these methods, we obtain a sample statistic, determine the distribution of that statistic under certain conditions, and then use that information to make a statement about the population parameter. We now discuss a commonly used sampling distribution: the \\(t\\) distribution.\n\n23.3.1 Student’s t\nLet \\(X_1, X_2, ..., X_n\\) be an i.i.d. sequence of random variables, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Recall that the central limit theorem tells us that\n\\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\mu, {\\sigma\\over\\sqrt{n}}\\right)\n\\]\nRearranging, we find that the test statistic on the left-side of the below expression is distributed approximately standard normal (a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\)):\n\\[\n{\\bar{X} - \\mu\\over\\sigma/\\sqrt{n}} \\overset{approx}{\\sim} \\textsf{Norm}(0, 1)\n\\]\nAgain, \\(\\sigma\\) is unknown. Thus, we have to estimate it. We can estimate it with \\(S\\), the sample standard deviation, but now we need to know the distribution of \\({\\bar{X} - \\mu\\over S/\\sqrt{n}}\\). This does not follow the normal distribution.\n\nLemma: Let \\(X_1, X_2, ..., X_n\\) be an i.i.d. sequence of random variables from a normal population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then, \\[\n{\\overline{X} - \\mu\\over S/\\sqrt{n}} \\sim \\textsf{t}(n - 1)\n\\]\n\nThe \\(\\textsf{t}(n - 1)\\) distribution is read as the “\\(t\\)” distribution. The \\(t\\) distribution has one parameter: degrees of freedom. The left-hand side of the expression above \\(\\left({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\right)\\) is referred to as the \\(t\\) statistic, and it tells us how many standard deviations our sample mean is from the population mean.\nThe proof of this lemma is outside the scope of this book, but it is not terribly complicated. It follows from some simple algebra and the fact that the ratio of a standard normal random variable and the square root of a chi-squared random variable, \\(S\\), divided by it’s degrees of freedom follows a \\(t\\) distribution.\nThe \\(t\\) distribution is very similar to the standard normal distribution, but has longer tails. This seems to make sense in the context of estimating \\(\\mu\\) because substituting the sample standard deviation \\(S\\) for the population standard deviation \\(\\sigma\\) adds variability to the random variable.\nFigure 23.7 is a plot of the \\(t\\) distribution, shown as a blue line, and has a bell shape that looks very similar to a normal distribution, show as a red line. However, the tails of the \\(t\\) distribution are thicker, which means observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. When our sample is small, the value \\(s\\) used to compute the standard error \\((s/\\sqrt{n})\\) isn’t very reliable. The extra thick tails of the \\(t\\) distribution are exactly the correction we need to resolve this problem. When the degrees of freedom is about 30 or more, the \\(t\\) distribution is nearly indistinguishable from the normal distribution.\n\ngf_dist(\"norm\", color = \"red\") %&gt;%\n  gf_dist(\"t\", df = 3, color = \"blue\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 23.7: The t distribution.\n\n\n\n\n\n\n\n23.3.2 Important Note\nYou may have noticed an important condition in the lemma above. It was assumed that each \\(X_i\\) in the sequence of random variables was normally distributed. While the central limit theorem has no such normality assumption, the distribution of the \\(t\\) statistic is subject to the distribution of the underlying population. With a large enough sample size, this assumption is not necessary. There is no magic number, but some resources state that as long as \\(n\\) is at least 30-40, the underlying distribution doesn’t matter. This coincides with what we said previously: when the degrees of freedom is about 30 or more, the \\(t\\) distribution is nearly indistinguishable from the normal distribution. For smaller sample sizes, the underlying distribution should be relatively symmetric and unimodal.\nOne advantage of simulation-based inference methods is that these methods do not rely on any such distributional assumptions. However, the simulation-based methods may have smaller power for the same sample size.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#hypothesis-tests-using-the-clt",
    "href": "Central-Limit-Theorem.html#hypothesis-tests-using-the-clt",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "23.4 Hypothesis tests using the CLT",
    "text": "23.4 Hypothesis tests using the CLT\nWe are now ready to reexamine some of our previous examples using the mathematically derived sampling distribution via the CLT.\n\n23.4.1 Body temperature\nWe will repeat the body temperature analysis from Chapter 22 homework, now using the CLT. We will use \\(\\alpha = 0.05\\). Recall that a paper from the American Medical Association2 questioned the long-held belief that the average body temperature of a human is 98.6 degrees Fahrenheit. The authors of the paper believe that the average human body temperature is less than 98.6.\n\n23.4.1.1 Step 1- State the null and alternative hypotheses\n\\(H_0\\): The average body temperature is 98.6; \\(\\mu = 98.6\\)\n\\(H_A\\): The average body temperature is less than 98.6; \\(\\mu &lt; 98.6\\)\n\n\n23.4.1.2 Step 2 - Compute a test statistic.\nThe population variance is unknown, so we will use the \\(t\\) distribution. Remember that\n\\[\n{\\bar{X} - \\mu\\over S/\\sqrt{n}} \\sim \\textsf{t}(n - 1)\n\\] Thus, our test statistic is\n\\[\n\\frac{\\bar{x} - 98.6}{s / \\sqrt{n}}\n\\]\nThe data is available in the file “temperature.csv”.\n\nfavstats(~temperature, data = temperature)\n\n  min   Q1 median   Q3   max     mean        sd   n missing\n 96.3 97.8   98.3 98.7 100.8 98.24923 0.7331832 130       0\n\n\n\ntemperature %&gt;%\n  summarise(mean = mean(temperature), sd = sd(temperature), \n            test_stat = (mean - 98.6) / (sd / sqrt(130)))\n\n# A tibble: 1 × 3\n   mean    sd test_stat\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  98.2 0.733     -5.45\n\n\nRemember, the \\(t\\) statistic tells us how many standard deviations the sample mean is from the population mean (the null hypothesis value). The sample mean of our data is over 5 standard deviations below the null hypothesis mean. We have some assumptions that we will discuss at the end of this problem.\n\n\n23.4.1.3 Step 3 - Determine the \\(p\\)-value.\nWe now want to find the \\(p\\)-value from \\(\\mbox{P}(t \\leq -5.45)\\) on 129 \\((n - 1)\\) degrees of freedom, given the null hypothesis is true. That is, given the true mean human body temperature is 98.6. We will use R to get the one-sided \\(p\\)-value.\n\npt(-5.45, 129)\n\n[1] 1.232178e-07\n\n\nWe could also use the R function t_test(), in which we specify the variable of interest, the data set, the hypothesized mean value, and the alternative hypothesis. Remember to use help(t_test) or ?t_test to access the R documentation for the t_test function.\n\nt_test(~temperature, data = temperature, mu = 98.6, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  temperature\nt = -5.4548, df = 129, p-value = 1.205e-07\nalternative hypothesis: true mean is less than 98.6\n95 percent confidence interval:\n     -Inf 98.35577\nsample estimates:\nmean of x \n 98.24923 \n\n\nYou should notice this \\(p\\)-value is much smaller than the \\(p\\)-value from the method used in homework problem 3 in the last chapter. That is because this test statistic involves more assumptions and uses the data as continuous and not discrete (a positive or negative difference between 98.6 and the observed value).\n\n\n23.4.1.4 Step 4 - Draw a conclusion\nBased on our data, if the true mean human body temperature is 98.6, then the probability of observing a mean of 98.25 or less is only 0.00000012. This is extremely unlikely, so we reject the null hypothesis that the average body temperature is 98.6 and conclude that there is sufficient evidence to say that the true average body temperature is less than 98.6.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#summary-and-rules-of-thumb",
    "href": "Central-Limit-Theorem.html#summary-and-rules-of-thumb",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "23.5 Summary and rules of thumb",
    "text": "23.5 Summary and rules of thumb\nWe have covered a great deal in this chapter. At its core, the central limit theorem is a statement about the distribution of a sum of independent, identically distributed random variables. This sum is approximately normal.\n\n23.5.1 Numerical data\nFirst, we summarize rules of thumb for the use of the CLT and \\(t\\) distribution.\n\nThe central limit theorem works regardless of the underlying distribution. However, if the parent population is highly skewed, then more data is needed. The CLT works well once the sample sizes exceed 30 to 40. If the data is fairly symmetric, then less data is needed.\nWhen estimating the mean and standard error from a sample of numerical data, the \\(t\\) distribution is a little more accurate than the normal distribution. But there is an assumption that the parent population is normally distributed. The \\(t\\) distribution works well even for small samples, as long as the data is close to symmetrical and unimodal.\nFor medium-sized samples, at least 15 data points, the \\(t\\) distribution still works as long as the data is roughly symmetric and unimodal.\nFor large data sets 30-40 or more, the \\(t\\) or the normal distribution can be used, but we suggest always using the \\(t\\) distribution.\n\nNow, let’s discuss the assumptions of the \\(t\\) distribution and how to check them.\n\nIndependence of observations. This is a difficult assumption to verify. If we collect a simple random sample from less than 10% of the population, or if the data are from an experiment or random process, we feel better about this assumption. If the data comes from an experiment, we can plot the data versus time collected to see if there are any patterns that indicate a relationship. A design of experiments course discusses these ideas in more detail.\nObservations come from a nearly normal distribution. This second condition is difficult to verify with small data sets. We often (i) take a look at a plot of the data for obvious departures from the normal distribution, usually in the form of prominent outliers, and (ii) consider whether any previous experiences alert us that the data may not be nearly normal. However, if the sample size is somewhat large, then we can relax this condition. For example, moderate skew is acceptable when the sample size is 30 or more, and strong skew is acceptable when the sample size is about 60 or more.\n\nA typical plot used to evaluate the normality assumption is called the quantile-quantile plot. We form a scatterplot of the empirical quantiles from the data versus exact quantile values from the theoretical distribution. If the points fall along a line, then the data match the distribution. An exact match is not realistic, so we look for major departures from the line.\nFigure 23.8 is our quantile-quantile plot for the body temperature data. The largest value may be an outlier. We may want to verify the data point was entered correctly. The fact that the points are above the line for the larger values and below the line for the smaller values indicates that our data may have longer tails than the normal distribution. There are really only 3 values in the larger quantiles, so in fact, the data may be slightly skewed to the left. This was also indicated by a comparison of the mean and median. However, since we have 130 data points these results do not overly concern us, and should not impact our findings.\n\ngf_qq(~temperature, data = temperature) %&gt;%\n  gf_qqline(~temperature, data = temperature) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 23.8: Q-Q plot for body temperature data.\n\n\n\n\n\nExtreme data points, outliers, can be cause for concern. In later chapters, we will look for ways to detect outliers but we have also seen them in our boxplots. First, outliers are problematic because normal distributions rarely have outliers, so the presence of one may indicate a departure from normality. Second, outliers have a big impact on estimation methods for the mean and standard deviation, whether it is a method of moments or maximum likelihood estimate.\nWe can also check the impacts of the assumptions by using other methods (like those in previous chapters) for the hypothesis test. If all methods give the same conclusion, we can be confident in the results. Another way to check robustness to assumptions is to simulate data from different distributions and evaluate the performance of the test under the simulated data.\n\n\n23.5.2 Binary data\nThe distribution of a binomial random variable or simple scalar transformations of it, such as the proportions of success found by dividing by the sample size, are approximately normal by the CLT. Since binomial random variables are bounded by zero and the number of trials, we have to make sure our probability of success is not close to zero or one. That is, the number of successes is not close to 0 or \\(n\\). A general rule of thumb is that the number of successes and failures be at least 10.\n\n\n23.5.3 Tappers and listeners\nRecall that a Stanford University graduate student conducted an experiment using the tapper-listener game. The tapper picks a well-known song, taps it’s tune, and sees if the listener can guess the song. About 50% of the tappers expected the listener to correctly guess the song. The researcher wanted to determine whether this was a reasonable expectation.\n\n23.5.3.1 Step 1- State the null and alternative hypotheses.\nHere are the two hypotheses:\n\\(H_0\\): The tappers are correct, and generally 50% of the time listeners are able to guess the tune. \\(p = 0.50\\)\n\\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners will be able to guess the tune. \\(p \\neq 0.50\\)\n\n\n23.5.3.2 Step 2 - Compute a test statistic.\nThe test statistic that we want to use is the sample mean \\(\\bar{X}\\). This is the mean of the 1s and 0s for each guess, where a 1 indicates a correct guess and a 0 indicates an incorrect guess. This is a method of moments estimate of the probability of success. These are independent samples from the same binomial distribution, so by the CLT,\n\\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\pi,\\sqrt\\frac{\\pi(1-\\pi)}{n}\\right)\n\\]\nAs we learned, this approximation improves with sample size. As a rule of thumb, most analysts are comfortable with using the CLT for this problem if the number of successes and failures are both 10 or greater.\nIn our study, 42 out of 120 listeners (\\(\\bar{x} = \\hat{p} = 0.35\\)) were able to guess the tune. This is the observed value of the test statistic.\n\n\n23.5.3.3 Step 3 - Determine the \\(p\\)-value.\nWe now want to find the \\(p\\)-value from the one-sided probability \\(\\mbox{P}(\\bar{X} \\leq 0.35)\\), given the null hypothesis is true. That is, given that the true probability of success is 0.50. We will use R to get the one-sided value and then double it since the test is two-sided and the sampling distribution is symmetrical.\n\n2*pnorm(0.35, mean = 0.5, sd = sqrt(0.5*0.5 / 120))\n\n[1] 0.001015001\n\n\nThat is a small \\(p\\)-value and consistent with what we got using both the exact binomial test and the simulated empirical \\(p\\)-values in Chapter 22.\n\nImportant note: In the calculation of the standard deviation of the sampling distribution, we used the null hypothesized value of the probability of success.\n\n\n\n23.5.3.4 Step 4 - Draw a conclusion\nBased on our data, if the listeners were guessing correct 50% of the time, there is about a 1-in-1000 chance that only 42 or less, or 78 or more, listeners would get it right. This is much less than 0.05, so we reject the null hypothesis that the listeners are guessing correctly half of the time. There is sufficient evidence to conclude that the true correct-guess rate is different from 50%.\nNote that R has built in functions to perform this test. If you explore these functions, use help(prop.test) or ?prop.test to learn more. You will find options to improve the performance of the test. You are welcome to and should read about these methods. Again, before computers, researchers spent time optimizing the performance of the asymptotic methods such as the CLT.\nHere is the test of a single proportion for the tapper-listener example using R.\n\nprop.test(x = 42, n = 120, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  42 out of 120\nX-squared = 10.208, df = 1, p-value = 0.001398\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2667083 0.4430441\nsample estimates:\n   p \n0.35 \n\n\nThe \\(p\\)-value is small, reported as \\(0.0014\\). We will study the confidence interval soon, so don’t worry about that part of the output yet. The alternative hypothesis is also listed, and has options for one-sided and two-sided tests.\n\nExercise:\nHow do you conduct a one-sided test? What if the null value were 0.45?3\n\n\npval(prop.test(42, 120, alternative = \"less\", p = 0.45))\n\n  p.value \n0.0174214 \n\n\nThe exact test uses the function binom.test().\n\nbinom.test(42,120)\n\n\n\n\ndata:  42 out of 120\nnumber of successes = 42, number of trials = 120, p-value = 0.001299\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2652023 0.4423947\nsample estimates:\nprobability of success \n                  0.35 \n\n\nThis is the same as the code we used in Chapter 22:\n\n2*pbinom(42, 120, prob = 0.5)\n\n[1] 0.001299333",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Central-Limit-Theorem.html#footnotes",
    "href": "Central-Limit-Theorem.html#footnotes",
    "title": "23  Hypothesis Testing with the Central Limit Theorem",
    "section": "",
    "text": "In general, the distribution is reasonably symmetric. It is unimodal and looks like a normal distribution.↩︎\nMackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), “A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich,” Journal of the American Medical Association, 268, 1578-1580.↩︎\nWe will only extract the \\(p\\)-value in this exercise.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypothesis Testing with the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html",
    "href": "Confidence-Intervals.html",
    "title": "24  Confidence Intervals",
    "section": "",
    "text": "24.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#objectives",
    "href": "Confidence-Intervals.html#objectives",
    "title": "24  Confidence Intervals",
    "section": "",
    "text": "Apply asymptotic methods based on the normal distribution to construct and interpret confidence intervals for unknown parameters.\nAnalyze the relationships between confidence intervals, confidence level, and sample size.\nAnalyze how confidence intervals and hypothesis testing complement each other in making statistical inferences.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#confidence-intervals",
    "href": "Confidence-Intervals.html#confidence-intervals",
    "title": "24  Confidence Intervals",
    "section": "24.2 Confidence intervals",
    "text": "24.2 Confidence intervals\nA point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, the next logical step would be to provide a plausible range of values for the parameter.\n\n24.2.1 Capturing the population parameter\nA plausible range of values for the population parameter is called a confidence interval. Using only a point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish.\nIf we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values – a confidence interval – we have a good shot at capturing the parameter.\n\nExercise: If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?1\n\n\n\n24.2.2 Constructing a confidence interval\nThe general formula for constructing a confidence interval is\n\\[\\text{point estimate} \\ \\pm\\ \\text{critical value}\\times \\text{standard error}\\]\nA point estimate is our best guess for the value of the population parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. The critical value is the number of standard deviations needed for the confidence interval.\nGenerally, what you should know about building confidence intervals is laid out in the following steps:\n\nIdentify the parameter you would like to estimate (e.g., the population mean, \\(\\mu\\)).\nIdentify a good estimate for that parameter (e.g., the sample mean, \\(\\bar{X}\\)).\nDetermine the distribution of your estimate, or a function of your estimate.\nUse this distribution to obtain a range of feasible values (a confidence interval) for the parameter. (For example, if \\(\\mu\\) is the parameter of interest and we are using the CLT, then \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\sim \\textsf{Norm}(0,1)\\). We can solve this equation for \\(\\mu\\) to find a range of feasible values.)\n\n\nConstructing a 95% confidence interval When the sampling distribution of a point estimate can reasonably be modeled as normal, the point estimate we observe will be within 1.96 standard errors of the true value of interest about 95% of the time. Thus, a 95% confidence interval for such a point estimate can be constructed as\n\\[ \\hat{\\theta} \\pm\\ 1.96 \\times SE_{\\hat{\\theta}},\\] where \\(\\hat{\\theta}\\) is our estimate of the parameter and \\(SE_{\\hat{\\theta}}\\) is the standard error of that estimate.\n\nWe can be 95% confident this interval captures the true value of the parameter. The number of standard errors to include in the interval (e.g., 1.96) can be found using the qnorm() function. Note that the qnorm() function calculates the lower tail quantile of a standard normal distribution by default. If we want 0.95 probability in the middle, that leaves 0.025 in each tail. Thus, we use 0.975 in the qnorm() function for a 95% confidence interval.\n\nqnorm(.975)\n\n[1] 1.959964\n\n\n\nExercise:\nCompute the area between -1.96 and 1.96 for a normal distribution with mean 0 and standard deviation 1.\n\n\npnorm(1.96) - pnorm(-1.96)\n\n[1] 0.9500042\n\n\nIn mathematical terms, the derivation of this confidence interval is as follows:\nLet \\(X_1, X_2, ..., X_n\\) be an i.i.d. sequence of random variables, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The central limit theorem tells us that\n\\[\n\\bar{X} \\overset{approx} {\\sim}\\textsf{Norm}\\left(\\mu, {\\sigma\\over\\sqrt{n}}\\right)\n\\]\nand, thus,\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\overset{approx} {\\sim}\\textsf{Norm}(0, 1)\n\\]\nIf the significance level is \\(0\\leq \\alpha \\leq 1\\), then the confidence level is \\(1 - \\alpha\\). The \\(\\alpha\\) value here is the same as the significance level in hypothesis testing. Thus,\n\\[\n\\mbox{P}\\left(-z_{\\alpha/2} \\leq {\\bar{X} - \\mu\\over \\sigma/\\sqrt{n}} \\leq z_{\\alpha/2}\\right) = 1 - \\alpha\n\\]\nwhere \\(z_{\\alpha/2}\\) is such that \\(\\Prob(Z\\geq z_{\\alpha/2}) = \\alpha/2\\), where \\(Z\\sim \\textsf{Norm}(0,1)\\). See Figure 24.1 for an illustration.\n\n\n\n\n\n\n\n\nFigure 24.1: The pdf of a standard normal distribution, showing the idea of how to develop a confidence interval.\n\n\n\n\n\nSo, we know that \\((1 - \\alpha)*100\\%\\) of the time, \\({\\bar{X} - \\mu\\over \\sigma/\\sqrt{n}}\\) will be between \\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\).\nBy rearranging the probability expression above and solving for \\(\\mu\\), we get:\n\\[\n\\mbox{P}\\left(\\bar{X} - z_{\\alpha/2}{\\sigma\\over\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + z_{\\alpha/2}{\\sigma \\over \\sqrt{n}}\\right) = 1 - \\alpha\n\\]\nBe careful with the interpretation of this expression. As a reminder, \\(\\bar{X}\\) is the random variable here. The population mean, \\(\\mu\\), is NOT a variable. It is an unknown parameter. Thus, the above expression is NOT a probabilistic statement about \\(\\mu\\). It is a probabilistic statement about the random variable \\(\\bar{X}\\).\nNonetheless, the above expression gives us a nice interval, a range of “reasonable” values of \\(\\mu\\), given a particular sample.\nA \\((1 - \\alpha)*100\\%\\) confidence interval for the mean is given by:\n\\[\n\\mu \\in \\left(\\bar{x} \\pm z_{\\alpha/2} {\\sigma\\over\\sqrt{n}} \\right)\n\\]\nNotice in this equation that we are using the lower case \\(\\bar{x}\\), the observed sample mean, and thus nothing is random in the interval. Thus, we will not use probabilistic statements about confidence intervals when we calculate numerical values from data for the upper and/or lower limits.\nIn most applications, the most common value of \\(\\alpha\\) is 0.05. In that case, we construct a 95% confidence interval by finding \\(z_{0.05/2} = z_{0.025}\\) or \\(z_{1 - 0.05/2} = z_{0.975}\\), which can be found easily with qnorm():\n\nqnorm(1 - 0.05/2)\n\n[1] 1.959964\n\n\n\nqnorm(.975)\n\n[1] 1.959964\n\n\nNotice that we prefer to use the upper tail probability so that quantile values are positive.\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\nThe value of \\(z_{0.025}\\) is negative; this requires a little more care when calculating the confidence interval due to changing signs. We recommend always using the upper tail probability for this reason.\n\n24.2.2.1 Unknown Variance\nWhen inferring about the population mean, we will usually have to estimate the underlying, population, standard deviation \\(\\sigma\\) as well. We usually estimate \\(\\sigma\\) with \\(S\\), the sample standard deviation. This introduces an extra level of uncertainty to the confidence interval. We found that while \\({\\bar{X} - \\mu \\over \\sigma/\\sqrt{n}}\\) has an approximate normal distribution, \\({\\bar{X} - \\mu\\over S/\\sqrt{n}}\\) follows the \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. This requires the additional assumption that the parent population, the distribution of \\(X\\), is normal.\nThus, when \\(\\sigma\\) is unknown, a \\((1 - \\alpha)*100\\%\\) confidence interval for the mean is given by:\n\\[\n\\mu \\in \\left(\\bar{x} \\pm t_{\\alpha/2, n - 1}{s \\over \\sqrt{n}} \\right)\n\\]\nSimilar to what was done previously, \\(t_{\\alpha/2, n - 1}\\) can be found using the qt() function in R.\nIn practice, if the underlying distribution of \\(X\\) is close to symmetrical and unimodal, we can relax the assumption of normality. Always look at your sample data though. Outliers or skewness can be causes of concern. We can always use other methods to find confidence intervals that don’t require the assumption of normality and compare results. We’ll talk more about some of these methods in the next chapter.\nFor large sample sizes, the choice of using the normal distribution or the \\(t\\) distribution is irrelevant because they are very similar to each other. The \\(t\\) distribution requires us to use the degrees of freedom though, so be careful.\n\n\n\n24.2.3 Body Temperature Example\n\nExample:\nFind a 95% confidence interval for the body temperature data from Chapter 23.\n\nWe need the mean, standard deviation, and sample size from this data. We can find those values using favstats(). The following R code calculates the confidence interval, using the values from favstats() and the expression for a 95% confidence interval given above. Make sure you can follow the code. It can usually help to to run the code line-by-line (i.e., run the first two lines only, then the first three lines only, and so on).\n\ntemperature %&gt;%\n  favstats(~temperature, data = .) %&gt;%\n  select(mean, sd, n) %&gt;%\n  summarise(lower_bound = mean - qt(0.975, 129)*sd/sqrt(n),\n            upper_bound = mean + qt(0.975, 129)*sd/sqrt(n))\n\n  lower_bound upper_bound\n1      98.122    98.37646\n\n\nThe 95% confidence interval for \\(\\mu\\) is \\((98.12, 98.38)\\). We are 95% confident that \\(\\mu\\), the average human body temperature, is in this interval. Alternatively and equally relevant, we could say that 95% of similarly constructed intervals will contain the true mean, \\(\\mu\\). It is important to understand the use of the word confident and not the word probability.\nAs a reminder, we need to be careful with the interpretation of the confidence interval. We have found the interval by calculating numerical values from the observed data, and thus nothing in the interval is random. Thus, we will not make statements about probability once we have calculated the interval. We will only make statements about confidence.\n\nImportant Note: There is a link between hypothesis testing and confidence intervals. A confidence interval provides us with a range of feasible values for the parameter. In hypothesis testing, we are trying to determine whether the null hypothesized value is a feasible value of the parameter. Remember when we used the body temperature data in a hypothesis test, the null hypothesis was \\(H_0\\): The average body temperature is 98.6 degrees Fahrenheit, \\(\\mu = 98.6\\). This null hypothesized value is not inside the interval, so we could reject the null hypothesis with this confidence interval and conclude that 98.6 degrees is not a feasible value of the true mean body temperature.\n\nWe could also use R to find the confidence interval and conduct the hypothesis test. Notice that the connection between confidence intervals and hypothesis testing is reinforced here; we can use the t_test() function to conduct a \\(t\\)-test and to calculate a confidence interval.2 Read about the function t_test() in the help menu to determine why we used the mu option.\n\nt_test(~temperature, data = temperature, mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nWe can use the following code if we only want the interval:\n\nconfint(t_test(~temperature, data = temperature, mu = 98.6))\n\n  mean of x  lower    upper level\n1  98.24923 98.122 98.37646  0.95\n\n\nIn reviewing the hypothesis test for a single mean, you can see how this confidence interval was formed by inverting the test statistic, \\({\\bar{X} - \\mu\\over \\sigma/\\sqrt{n}}\\). As a reminder, the following equation inverts the test statistic.\n\\[\n\\mbox{P}\\left(\\bar{X}-z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\right)=1-\\alpha\n\\]\n\n\n24.2.4 One-sided Intervals\nIf you remember the hypothesis test for body temperature in Chapter @ref(HYPTESTCLT) on the central limit theorem, you may be crying foul. That was a one-sided hypothesis test and we just conducted a two-sided test. So far, we have discussed only “two-sided” intervals. These intervals have an upper and lower bound. Typically, \\(\\alpha\\) is apportioned equally between the two tails. (Thus, we look for \\(z_{\\alpha/2}\\) or \\(z_{1 - \\alpha/2}\\).)\nIn “one-sided” intervals, we only bound the interval on one side, providing only an upper bound or only a lower bound. We construct one-sided intervals when we are concerned with whether a parameter exceeds or stays below some threshold. Building a one-sided interval is similar to building two-sided intervals, but rather than dividing \\(\\alpha\\) into two, we simply apportion all of \\(\\alpha\\) to the relevant side. The difficult part is in determining whether we need an upper bound or lower bound.\nFor the body temperature study, the alternative hypothesis was that the mean body temperature was less than 98.6. We are trying to reject the null hypothesis by showing an alternative that is smaller than the null hypothesized value. Finding the lower limit does not help us because the confidence interval indicates an interval that starts at the lower value and is unbounded above. Let’s just make up some numbers to help illustrate this. Suppose the lower confidence bound is 97.5. All we know is that the true average temperature is 97.5 or greater. This is not helpful in determining whether the true average temperature is less than 98.6. However, if we find an upper confidence bound and the value is 98.1, we know the true average temperature is most likely no larger than this value, and hence, is less than 98.6. This is much more helpful. In our confidence interval, we want to find the largest value the mean could reasonably be and, thus, we want the upper bound.\nWe now repeat the analysis with this in mind.\n\ntemperature %&gt;%\n  favstats(~temperature, data = .) %&gt;%\n  select(mean, sd, n) %&gt;%\n  summarise(upper_bound = mean + qt(0.95, 129)*sd/sqrt(n))\n\n  upper_bound\n1    98.35577\n\n\n\nconfint(t_test(~temperature, data = temperature, alternative = \"less\"))\n\n  mean of x lower    upper level\n1  98.24923  -Inf 98.35577  0.95\n\n\nNotice that the probability in qt() has been adjusted to calculate \\(t_{1 - \\alpha}\\). Additionally, the upper bound in the one-sided interval is smaller than the upper bound in the two-sided interval because all 0.05 is going into the upper tail. See Figure 24.2 for an illustration of a one-sided interval.\n\n\n\n\n\n\n\n\nFigure 24.2: The pdf of a standard normal distribution, showing the idea of how to develop a one-sided confidence interval.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#confidence-intervals-for-one-proportion",
    "href": "Confidence-Intervals.html#confidence-intervals-for-one-proportion",
    "title": "24  Confidence Intervals",
    "section": "24.3 Confidence intervals for one proportion",
    "text": "24.3 Confidence intervals for one proportion\nIn Chapter 22 and Chapter 23, we had an example of a single proportion. A Stanford University graduate student conducted an experiment using the tapper-listener game. The tapper picks a well-known song, taps the tune, and sees if the listener can guess the song. The researcher wanted to know whether a 50% correct-guess rate was a reasonable expectation. There was no second variable to shuffle, so we were unable to use a randomization test with this example. However, we used the CLT and simulation with a binomial probability model to perform hypothesis tests. In this section, we will use a confidence interval to answer our research question, and compare it to the results from hypothesis testing.\nIn the study, 42 out of 120 listeners (\\(\\hat{p} = 0.35\\)) correctly guessed the tune.\n\n24.3.1 Confidence interval for one proportion using the normal model\nThe conditions for applying the normal model were already verified in Chapter @ref(HYPTESTCLT); the number of successes and failures is at least 10 and each guess (observation) is an independent observation from a binomial distribution. Thus, we can proceed to the construction of the confidence interval. Remember, the form of the confidence interval is\n\\[\\text{point estimate} \\ \\pm\\ z^{\\star}\\times SE\\]\nOur point estimate is \\(\\hat{p} = 0.35\\). The standard error is slightly different because we don’t know the true (or hypothesized) value of \\(\\pi\\). Instead, we can use \\(\\hat{p}\\) to estimate \\(\\pi\\).\n\\[SE = \\sqrt{\\hat{\\pi} (1 - \\hat{\\pi}) \\over n}\\]\n\\[SE \\approx \\sqrt{0.35(1 - 0.35) \\over 120} = 0.0435\\]\nThe critical value, the number of standard deviations needed for the confidence interval, is found from the normal quantile.\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nThen, the 95% confidence interval is\n\\[0.35 \\pm 1.96\\times 0.0435 \\qquad \\rightarrow \\qquad (0.265, 0.435)\\]\nWe are 95% confident that the true correct-guess rate in the tapper-listener game is between 26.5% and 43.5%. Since this does not include 50% (or 0.50), we are confident the true correct-guess rate is different from 50%. This supports the results from the hypothesis tests, both of which resulted in \\(p\\)-values around 0.001, leading us to reject the null hypothesis and conclude that the true correct-guess rate was different from 50%.\nOf course, R has built-in functions to perform the hypothesis test and calculate the confidence interval for a single proportion.\n\nprop_test(x = 42, n = 120, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  42 out of 120\nX-squared = 10.208, df = 1, p-value = 0.001398\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2667083 0.4430441\nsample estimates:\n   p \n0.35 \n\n\n\nbinom.test(x = 42, n = 120)\n\n\n\n\ndata:  42 out of 120\nnumber of successes = 42, number of trials = 120, p-value = 0.001299\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2652023 0.4423947\nsample estimates:\nprobability of success \n                  0.35 \n\n\nBoth functions give similar confidence intervals. The documentation for prop_test() and binom.test() provide details on additional options, such as one-sided intervals and different confidence levels.\nThe \\(p\\)-values are a little different than the ones we calculated (0.0012 for the simulation and 0.0010 for the normal-based test) because a correction factor was applied in prop_test() and a different type of interval was used in binom.test(). We leave it to the reader to learn more about these functions and related topics online. The code below calculates the confidence interval without a continuity correction. This confidence interval is closer to what we calculated by hand, and we come to the same conclusion because 50% is not contained in the interval.\n\nprop_test(x = 42, n = 120, p = 0.5, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  42 out of 120\nX-squared = 10.8, df = 1, p-value = 0.001015\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2705190 0.4387868\nsample estimates:\n   p \n0.35",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#confidence-intervals-for-two-proportions",
    "href": "Confidence-Intervals.html#confidence-intervals-for-two-proportions",
    "title": "24  Confidence Intervals",
    "section": "24.4 Confidence intervals for two proportions",
    "text": "24.4 Confidence intervals for two proportions\nThroughout hypothesis testing, we had several examples of two proportions. We tested these problems with a randomization test or using a hypergeometric distribution. But in our chapters and homework, we have not presented the hypothesis test for two proportions using the asymptotic normal distribution, the central limit theorem. In this section, we will use a confidence interval to answer our research question, and compare it to the results from hypothesis testing.\nIn Chapter 21 and Chapter 22, we encountered an experiment that investigated whether providing blood thinners to patients who received CPR for a heart attack helped or hurt survival. It is believed that blood thinners (treatment group) negatively affect internal injuries that may result from CPR, leading to lower survival rates. The results from the experiment, which included 90 patients, are summarized in the R code below. These results are surprising! The point estimate suggests that patients who received blood thinners may have a higher survival rate: \\[p_{\\text{treatment}} - p_{\\text{control}} = 0.13\\].\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\ntally(~ group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\ntally(outcome ~ group, data = thinner, margins = TRUE, format = \"proportion\")\n\n          group\noutcome    control treatment\n  died        0.78      0.65\n  survived    0.22      0.35\n  Total       1.00      1.00\n\n\n\nobs &lt;- diffprop(outcome ~ group, data = thinner)\nobs\n\ndiffprop \n   -0.13 \n\n\nNotice that because R uses the variable names in alphabetic order, we have \\(p_{\\text{control}} - p_{\\text{treatment}} = -0.13\\). This is not a problem. We could fix this by changing the variables to factors and reordering the levels.\n\n24.4.1 Confidence interval for two proportions using the normal model\nThe conditions for applying the normal-based model (i.e., CLT) are met: the number of successes and failures in each group is at least 10 and each observation can be thought of as an independent observation from a hypergeometric distribution. Thus, we can proceed to the construction of the confidence interval. The form of the confidence interval is\n\\[\\text{point estimate} \\ \\pm\\ z^{\\star}\\times SE\\]\nOur point estimate is -0.13. The standard error is different here because we can’t assume the proportion of survival is equal for both groups. We will estimate the standard error with\n\\[SE    = \\sqrt{\\frac{p_{\\text{control}}(1 - p_{\\text{control}})}{n_{\\text{control}}} + \\frac{p_{\\text{treatment}}(1 - p_{\\text{treatment}})}{n_{\\text{treatment}}}}\\]\nwhere \\(p_{\\text{control}}\\) and \\(p_{\\text{treatment}}\\) are the proportion of patients who survive in the control group and treatment group, respectively. Thus, we have\n\\[SE \\approx \\sqrt{\\frac{0.22(1 - 0.22)}{50} + \\frac{0.35(1 - 0.35)}{40}} = 0.0955\\]\nIt is close to the pooled value3 because of the nearly equal sample sizes.\nThe critical value is again found from the normal quantile.\n\nqnorm(.975)\n\n[1] 1.959964\n\n\nThe 95% confidence interval is\n\\[ -0.13 \\pm 1.96\\times 0.0955 \\qquad \\rightarrow \\qquad (-0.317, 0.057)\\]\nWe are 95% confident that the difference in proportions of survival for the control and treatment groups is between -0.317 and 0.057. Because this does include zero, we are not confident they are different. This supports our conclusions from the hypothesis tests; we found a \\(p\\)-value of around 0.26 and failed to reject the null hypothesis that the two proportions were different, or that the difference in the two proportions was different from zero.\nNote that this confidence interval is not an accurate method for smaller samples sizes. This is because the actual coverage rate, the percentage of intervals that contain the true population parameter, will not be the nominal, stated, coverage rate. This means it is not true that 95% of similarly constructed 95% confidence intervals will contain the true parameter. This is because the pooled estimate of the standard error is not accurate for small sample sizes. For the example above, the sample sizes are large enough and the performance of the method should be adequate.\nThe prop_test() function in R, used for the single proportion example, can also be used to calculate the hypothesis test and confidence interval for two proportions.\n\nprop_test(outcome ~ group, data = thinner)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  tally(outcome ~ group)\nX-squared = 1.2801, df = 1, p-value = 0.2579\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07966886  0.33966886\nsample estimates:\nprop 1 prop 2 \n  0.78   0.65 \n\n\nThe \\(p\\)-value is close to the one found in the randomization test, which is an approximation of the exact permutation test, because a correction factor was applied. We again provide code below without the correction factor and get a \\(p\\)-value more similar to that found using the hypergeometric distribution, and a confidence interval more similar to what we calculated above. The confidence interval is a little different because the function used died as its success event, but since zero is contained in the interval, we get the same conclusion.\n\nprop_test(outcome ~ group, data = thinner, correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  tally(outcome ~ group)\nX-squared = 1.872, df = 1, p-value = 0.1712\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.05716886  0.31716886\nsample estimates:\nprop 1 prop 2 \n  0.78   0.65 \n\n\nEssentially, confidence intervals and hypothesis tests serve similar purposes, but answer slightly different questions. A confidence interval gives you a range of feasible values of a parameter, given a particular sample. A hypothesis test tells you whether a specific value is feasible, given a sample. Sometimes you can informally conduct a hypothesis test simply by building an interval and observing whether the hypothesized value is contained in the interval. The disadvantage to this approach is that it does not yield a specific \\(p\\)-value. The disadvantage of the hypothesis test is that it does not give a range of feasible values for the test statistic.\nAs with hypothesis tests, confidence intervals are imperfect. About 1-in-20 properly constructed 95% confidence intervals will fail to capture the parameter of interest. This is a similar idea to our Type 1 error.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#changing-the-confidence-level",
    "href": "Confidence-Intervals.html#changing-the-confidence-level",
    "title": "24  Confidence Intervals",
    "section": "24.5 Changing the confidence level",
    "text": "24.5 Changing the confidence level\nSuppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%; perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want a more narrow interval, we must calculate an interval with lower confidence, such as 90%.\nThe 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a point estimate that comes from a nearly normal distribution:\n\\[\\text{point estimate}\\ \\pm\\ 1.96\\times SE \\]\nThere are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of \\(1.96\\times SE\\), which is also called margin of error, was based on capturing 95% of the data because the estimate is within 1.96 standard errors of the true value about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. Thinking back to the 68-95-99.7 rule for the normal distribution (see the homework in Chapter @ref(CONTNNAMED)) may be helpful here as well.\n\nExercise: If \\(X\\) is a normally distributed random variable, how often will \\(X\\) be within 2.58 standard deviations of the mean?4\n\nTo create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be \\(2.58\\). We can use qnorm() to calculate this value. Remember that qnorm() uses the lower tail probability, so if we want 0.99 probability in the middle of the interval, we need to use 0.995 in the function. Follow the same reasoning to calculate confidence intervals with different confidence levels.\n\nqnorm(0.995)\n\n[1] 2.575829\n\n\nThe normal approximation is crucial to the precision of these confidence intervals, unlike the bootstrap method that allows us to find confidence intervals without the assumption of normality.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#changing-the-sample-size",
    "href": "Confidence-Intervals.html#changing-the-sample-size",
    "title": "24  Confidence Intervals",
    "section": "24.6 Changing the sample size",
    "text": "24.6 Changing the sample size\n\nExercise: By how much do we have to increase the sample size in order to reduce the margin of error by half?5\n\nThe equation for a confidence interval for a mean is\n\\[\n\\bar{x} \\pm t_{\\alpha/2, n - 1}{s \\over \\sqrt{n}}\n\\]\nand the equation for a confidence interval for a proportion is\n\\[\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\hat{\\pi} (1 - \\hat{\\pi}) \\over n}\n\\]\nThe margin of error is the critical value multiplied by the standard error, the right side of each expression. In both equations, we have \\(\\sqrt{n}\\) in the denominator of the standard error. Suppose we calculate a confidence interval based on a sample size of 100. Then, a confidence interval based on a sample size of 200 would have a standard error that is reduced by a factor of \\(1/\\sqrt{2}\\). Compared to the original interval (sample size of 100), a confidence interval based on a sample size of 400 would have a standard error that is reduced by a factor of \\(1/\\sqrt{4} = 1/2\\). As the sample size increases, the standard error (and hence, the margin of error) decreases, and the interval gets more narrow. We can think of this another way: as the sample size increases and we gain more information about the population, our confidence interval gets more precise. A smaller sample size (less information about the population) results in a wider confidence interval.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#interpreting-confidence-intervals",
    "href": "Confidence-Intervals.html#interpreting-confidence-intervals",
    "title": "24  Confidence Intervals",
    "section": "24.7 Interpreting confidence intervals",
    "text": "24.7 Interpreting confidence intervals\nA careful eye might have observed the somewhat awkward language used to describe confidence intervals.\n\nCorrect interpretation:\nWe are XX% confident that the population parameter is between lower bound and upper bound.\n\n\nCorrect interpretation:\nApproximately 95% of similarly constructed intervals will contain the population parameter.\n\nIncorrect language might try to describe the confidence interval as capturing the population parameter with a certain probability. This is one of the most common errors. While it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the interval. Once again, we remind the reader that we do not make probabilistic statements about a realized interval; there are no random components in the interval once we observe data, so there is no probability associated with the calculated interval.\nAnother especially important consideration of confidence intervals is that they only try to capture the population parameter. Our intervals say nothing about the confidence of capturing individual observations, a proportion of the observations, or point estimates. Confidence intervals only attempt to capture population parameters.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Confidence-Intervals.html#footnotes",
    "href": "Confidence-Intervals.html#footnotes",
    "title": "24  Confidence Intervals",
    "section": "",
    "text": "If we want to be more certain we will capture the fish, we should use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter. A higher level of confidence implies a wider interval.↩︎\nWe could also find a bootstrap confidence interval here. Then, if the bootstrap distribution looks like a \\(t\\) distribution, we can use a \\(t\\) interval with the bootstrap estimate of the standard error.↩︎\nWhen the null hypothesis is that \\(p_1 - p_2 = 0\\), we sometimes use the pooled proportion to check the success-failure condition and calculate the standard error. In our example, the pooled proportion is \\[\\hat{p}_{\\text{pooled}} = \\frac{11 + 14}{90} = 0.278\\] The success-failure condition is met and the standard error (using the pooled proportion) is 0.09501. We leave it to the reader to find out more about using pooled standard deviations with two samples.↩︎\nThis is equivalent to asking how often a standard normal variable will be greater than -2.58 but less than 2.58. To determine this probability, look up -2.58 and 2.58 in R using pnorm() (0.0049 and 0.9951). Thus, there is a \\(0.9951-0.0049 \\approx 0.99\\) probability that the unobserved random variable \\(X\\) will be within 2.58 standard deviations of the mean.↩︎\nWe need to multiply the sample size by four in order to reduce the standard error, and hence, the margin of error, by half. This is because \\(\\sqrt{n}\\) is in the denominator of the standard error. If we wanted to reduce the margin of error to one-third of its value, we would need to multiply the sample size by nine.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html",
    "href": "Additional-Hypothesis-Tests.html",
    "title": "25  Additional Hypothesis Tests",
    "section": "",
    "text": "25.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#objectives",
    "href": "Additional-Hypothesis-Tests.html#objectives",
    "title": "25  Additional Hypothesis Tests",
    "section": "",
    "text": "Conduct and interpret a goodness of fit test using both Pearson’s chi-squared and randomization to evaluate the independence between two categorical variables. Evaluate the assumptions for Pearson’s chi-square test.\nAnalyze the relationship between the chi-squared and normal distributions, explain usage contexts, and evaluate the effects of changing degrees of freedom on the chi-squared distribution using visualizations.\nConduct and interpret a hypothesis test for equality of two means and equality of two variances using both permutation and the CLT. Evaluate the assumptions for two-sample t-tests.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#introduction",
    "href": "Additional-Hypothesis-Tests.html#introduction",
    "title": "25  Additional Hypothesis Tests",
    "section": "25.2 Introduction",
    "text": "25.2 Introduction\nThe purpose of the next two chapters is to put all we’ve learned so far in this block into perspective, and to demonstrate a couple of new statistical tests.\nRemember that we have been using data to answer research questions. So far, we can do this with hypothesis tests, using several different methods. There is a close link between these methods, and the steps of hypothesis testing have remained the same no matter which method is used. The key ideas have been to generate a single number metric to use in answering our research question, and then to obtain the sampling distribution of this metric.\nIn obtaining the sampling distribution, we used randomization as an approximation to permutation exact tests, probability models, and mathematical models. Each of these had different assumptions and different areas where they could be applied. In some cases, several methods can be applied to the problem to get a sense of the robustness to different assumptions. For example, if you run a randomization test and a test using the CLT, and they both give you similar results, you can feel better about your decision.\nFinding a single number metric to answer our research question can be difficult. For example, in Chapter 21, we wanted to determine if the distribution of lengths of commercials for premium and basic channels was different. The difference in means has been used for historical reasons, and we now know that is because of the need to use the \\(t\\) distribution. But is this the best way to answer the question? Because we wanted to think creatively, we ended up using the ratio of median lengths as our single number metric. However, there are other ways in which the distribution of lengths of commercials can differ.\nJack Welch was the CEO of GE for years and he made the claim that customers don’t care about the average, but they do care about variability. The average temperature setting of your GE refrigerator could be off and you would adapt. However, if the temperature had great variability, then you would be upset. So metrics that incorporate variability might be good. In Chapter 21, we looked at the length of commercials for basic and premium TV channels. In using a randomization test, we assumed in the null hypothesis that there was no difference in the distributions. However, in the alternative hypothesis, we measured the difference in distributions using only medians, a measure of average. The medians of these two populations could be equal, but the distributions could differ in other ways, like variability. We could conduct a separate test for variances, but we have to be careful about multiple comparisons because, in that case, the Type 1 error is inflated.\nWe also learned that the use of the information in the data impacts the power of the test. In the golf ball example in Chapter 22, using range as our metric did not give the same power as looking at the differences from expected values under the null hypothesis. There is some mathematical theory, called likelihood ratio tests, that leads to better estimators, but it is beyond the scope of this book. What you can do is create a simulation where you generate data from the alternative hypothesis and then measure the power. This will give you a sense of the quality of your metric. We only briefly looked at measuring power in Chapter 22 and will not go further into this idea in this chapter.\nWe will continue this block by examining problems with two variables. In the first case, both variables will be categorical and at least one of the categorical variables will have more than two levels. In the second case, we will examine two variables, where one is numeric and the other is categorical, and the categorical variable has more than two levels.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#other-distributions-for-estimators",
    "href": "Additional-Hypothesis-Tests.html#other-distributions-for-estimators",
    "title": "25  Additional Hypothesis Tests",
    "section": "25.3 Other distributions for estimators",
    "text": "25.3 Other distributions for estimators\nIn Chapter 23, we discussed the \\(t\\) distribution, a different sampling distribution that is based on the CLT or normality assumption. In theoretical statistics, we often mathematically derive the sampling distribution by obtaining a sample statistic, determining the distribution of that statistic under certain conditions, and using that information to make a statement about the population parameter. We now discuss another commonly used sampling distribution: the chi-squared distribution.\n\n25.3.1 Chi-squared\nRecall the central limit theorem tells us that for reasonably large sample sizes, \\(\\bar{X} \\overset{approx}{\\sim} \\textsf{Norm}(\\mu, \\sigma/\\sqrt{n})\\). This expression involves two unknowns: \\(\\mu\\) and \\(\\sigma\\). In the case of binary data, the population variance is a function of the population proportion \\(\\left(Var(X)=\\pi(1-\\pi)\\right)\\), so there is really just one unknown, the population mean \\(\\pi\\). This can be estimated with the sample proportion, \\(p\\).\nIn the case of continuous data, the population mean can be estimated with \\(\\bar{x}\\). The population variance would need to be estimated as well. Let \\(S^2\\) be defined as:\n\\[\nS^2 = {\\sum (X_i - \\bar{X})^2 \\over n - 1}\n\\]\nThis is an unbiased estimate for \\(\\sigma^2\\), meaning that, on average, the estimator above will equal the true value we are trying to estimate (i.e., the true population variance). The sampling distribution of \\(S^2\\) can be found using the following lemma.\n\nLemma: Let \\(X_1, X_2, ..., X_n\\) be an i.i.d. sequence of random variables from a normal population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then,\n\\[\n{(n-1) S^2\\over \\sigma^2} \\sim \\textsf{Chisq}(n - 1)\n\\]\n\nThe \\(\\textsf{Chisq}(n-1)\\) distribution is read as the “chi-squared” distribution (“chi” is pronounced “kye”). This can also be written as \\(\\chi^2 (n-1)\\). The chi-squared distribution has one parameter: degrees of freedom. The chi-squared distribution is used in other contexts such as goodness of fit problems, like the golf ball example from Chapter 22.\nThe proof of this lemma is outside the scope of this book, but it is not terribly complicated. It follows from the fact that the sum of \\(n\\) squared random variables, each with the standard normal distribution, follows the chi-squared distribution with \\(n\\) degrees of freedom.\nThis lemma can be used to draw inferences about \\(\\sigma^2\\). For a particular value of \\(\\sigma^2\\), we know how \\(S^2\\) should behave. So, for a particular value of \\(S^2\\), we can figure out reasonable values of \\(\\sigma^2\\). In practice, one rarely estimates \\(\\sigma\\) for the purpose of inference on \\(\\sigma\\). Typically, we are interested in estimating \\(\\mu\\) and we need to account for the added uncertainty by estimating \\(\\sigma\\) as well.\nThe chi-squared distribution takes on positive values and is right skewed. The degrees of freedom influence the shape and location of the distribution. As the degrees of freedom increase, it becomes more similar to a normal distribution. That is, the distribution becomes more symmetric with larger variability, and the center of the distribution moves to the right. Figure 25.1 demonstrates these trends.\n\n\n\n\n\n\n\n\nFigure 25.1: Chi-square distribution for different degrees of freedom.\n\n\n\n\n\n\n\n25.3.2 Important Note\nJust like for the \\(t\\) distribution, the lemma above assumed that each \\(X_i\\) in the sequence of random variables was normally distributed. While the central limit theorem has no such normality assumption, the distribution of the chi-square statistic is subject to the distribution of the underlying population. With large enough expected counts, this assumption is not necessary. Again, there is no magic number, but some resources state that no expected count should be less than one and no more than 20% of the expected counts should be less than five.\nOne advantage of simulation-based inference methods is that these methods do not rely on any such distributional assumptions. However, the simulation-based methods may have smaller power for the same sample size.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#categorical-data",
    "href": "Additional-Hypothesis-Tests.html#categorical-data",
    "title": "25  Additional Hypothesis Tests",
    "section": "25.4 Categorical data",
    "text": "25.4 Categorical data\nIt is worth spending some time on common approaches to categorical data that you may come across. We have already dealt with categorical data to some extent in this book. We have performed hypothesis tests for \\(\\pi\\), the population proportion of “success” in binary cases (for example, support for a local measure in a vote). This problem had a single variable. Also, the golf ball example involved counts of four types of golf ball. This is considered categorical data because each observation is characterized by a qualitative value (number on the ball). The data are summarized by counting how many balls in a sample belong to each type. This again was a single variable.\nIn another scenario, suppose we are presented with two qualitative variables and would like to know if they are independent. For example, in Chapter 21, we discussed methods for determining whether there was a relationship between blood thinners and survival in patients who received CPR for a heart attack. In this case, we have two categorical variables with two levels each: receiving a blood thinner (treatment vs control) and survival (survived vs died). We have solved this type of problem by looking at a difference in probabilities of survival using randomization and mathematically derived solutions, the CLT. We have also used a hypergeometric distribution to obtain an exact \\(p\\)-value.\nWe will next explore a scenario that involves categorical data with two variables but where at least one variable has more than two levels. However, note that we are only merely scratching the surface in our studies. You could take an entire course on statistical methods for categorical data. This book is giving you a solid foundation to learn more advanced methods.\n\n25.4.1 Health evaluation and linkage to primary care\nThe Health Evaluation and Linkage to Primary Care (HELP) study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care, with the goal of linking them to primary medical care.\nThe HELPrct data set is available in the mosaicData package. There are three substances: alcohol, cocaine, and heroin. We’d like to know if there is evidence that the proportions of use for substance, the primary substance of abuse, differ for males and females.\n\ndata(\"HELPrct\")\n\n\nHELP_sub &lt;- HELPrct %&gt;%\n  select(substance, sex)\n\n\ntally(substance ~ sex, data = HELPrct, format = \"prop\", margins = TRUE)\n\n         sex\nsubstance    female      male\n  alcohol 0.3364486 0.4075145\n  cocaine 0.3831776 0.3208092\n  heroin  0.2803738 0.2716763\n  Total   1.0000000 1.0000000\n\n\n\n\n\n\n\n\n\n\nFigure 25.2: Proportions for primary substance of abuse in the HELP study, faceted by sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.3: The distribution of primary substance of abuse in the HELP study by sex.\n\n\n\n\n\nThe two-way table and Figure 25.2 and Figure 25.3 exhibit modest differences in the primary substance of abuse by sex, but is it statistically significant?\nWe need a test statistic to determine if there is a difference in primary substance of abuse between males and females.\n\n\n25.4.2 Test statistic\nTo help us develop and understand a test statistic, let’s simplify and use a simple theoretical example.\nSuppose we have a 2 x 2 contingency table like the one below.\n\\[\n\\begin{array}{lcc}\n& \\mbox{Response 1} & \\mbox{Response 2} \\\\\n\\mbox{Group 1} & n_{11} & n_{12} \\\\\n\\mbox{Group 2} & n_{21} & n_{22}\n\\end{array}\n\\]\nIf our null hypothesis is that the two variables are independent, a classical test statistic used is the Pearson chi-squared test statistic (\\(X^2\\)). This is similar to the test statistic we used in our golf ball example in Chapter 22. Let \\(e_{ij}\\) be the expected count in the \\(i\\)th row and \\(j\\)th column under the null hypothesis. Then the test statistic is the squared difference of the observed and expected counts, divided by the expected count, summed over both levels of each categorical variable. This expression is shown below:\n\\[\nX^2 = \\sum_{i = 1}^2 \\sum_{j = 1}^2 {(n_{ij} - e_{ij})^2 \\over e_{ij}}\n\\]\nBut how do we find \\(e_{ij}\\), the expected count, for each cell? What do we expect the count to be under \\(H_0\\)?\nTo find the expected counts, we recognize that under \\(H_0\\) (independence), a joint probability is equal to the product of the marginal probabilities. Let \\(\\pi_{ij}\\) be the probability of an outcome occurring in row \\(i\\) and column \\(j\\). In the absence of any other information, our best guess at \\(\\pi_{ij}\\) is \\(\\hat{\\pi}_{ij} = {n_{ij}\\over n}\\), where \\(n_{ij}\\) is the number of observations occurring in row \\(i\\) and column \\(j\\), and \\(n\\) is the total sample size for the entire table. But under the null hypothesis, we have the assumption of independence. Thus, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\) where \\(\\pi_{i+}\\) represents the total probability of occurring in row \\(i\\) and \\(\\pi_{+j}\\) represents the total probability of occurring in column \\(j\\). Note that \\(\\pi_{i+}\\) is estimated by \\(\\hat{\\pi}_{i+}\\) and\n\\[\n\\hat{\\pi}_{i+} = {n_{i+}\\over n},\n\\]\nwhere \\(n_{i+}\\) is the sample size for row \\(i\\). This can be found by summing the sample size for all cells in row \\(i\\).\nThus for our simple 2 x 2 example, we have:\n\\[\n\\hat{\\pi}_{i+} = {n_{i+}\\over n} = {n_{i1}+n_{i2}\\over n}\n\\]\nThus, for Group 1 we would have:\n\\[\n\\hat{\\pi}_{1+} = {n_{1+}\\over n} = {n_{11}+n_{12}\\over n}\n\\]\nSo, under \\(H_0\\), our best guess for \\(\\pi_{ij}\\) is:\n\\[\n\\hat{\\pi}_{ij} = \\hat{\\pi}_{i+}\\hat{\\pi}_{+j} = {n_{i+}\\over n}{n_{+j}\\over n} = {n_{i1}+n_{i2}\\over n}{n_{1j}+n_{2j}\\over n}\n\\]\nContinuing, under \\(H_0\\) the expected cell count is:\n\\[\ne_{ij} = n\\hat{\\pi}_{ij} = n{n_{i+}\\over n}{n_{+j}\\over n} = {n_{i+}n_{+j}\\over n}\n\\]\nThis is essentially found by multiplying the row total (the sample size for row \\(i\\)) by the column total (the sample size for column \\(j\\)) and dividing by the overall table total (the total sample size, \\(n\\)).\n\n\n25.4.3 Extension to larger tables\nThe advantage of using the Pearson chi-squared test statistic is that it can easily be extended to larger contingency tables, the name given to these tables displaying multiple categorical variables. Suppose we are comparing two categorical variables, one with \\(r\\) levels and the other with \\(c\\) levels. Then,\n\\[\nX^2 = \\sum_{i=1}^r \\sum_{j=1}^c {(n_{ij} - e_{ij})^2\\over e_{ij}}\n\\]\nThis is exactly the same as before, except that instead of summing over two levels of each categorical variable, we sum over all \\(r\\) and \\(c\\) levels. That is, we sum over all cells in the contingency table.\nUnder the null hypothesis of independence, the \\(X^2\\) test statistic follows the chi-squared distribution with \\((r-1)\\times(c-1)\\) degrees of freedom.\n\n25.4.3.1 Assumptions\nNote that to use this test statistic, the expected cell counts, each \\(e_{ij}\\), must be reasonably large. In fact, no \\(e_{ij}\\) should be less than one and no more than 20% of the \\(e_{ij}\\)’s should be less than five. If this occurs, we should combine cells or look for a different test.\nThis may all look too abstract, so let’s break it down with an example.\n\n\n\n25.4.4 Test statistic for the HELP example\nLet’s return to the Health Evaluation and Linkage to Primary Care data set. There are two levels of the sex variable, female and male. There are three levels of the substance variable: alcohol, cocaine, and heroin. A two-way contingency table is shown below.\n\ntally(~substance + sex, data = HELPrct)\n\n         sex\nsubstance female male\n  alcohol     36  141\n  cocaine     41  111\n  heroin      30   94\n\n\nTo find the Pearson chi-squared test statistic (\\(X^2\\)), we need to figure out the expected count for each cell, \\(e_{ij}\\), under \\(H_0\\). Recall that under \\(H_0\\), the two variables are independent. It’s helpful to add the row and column totals prior to finding expected counts:\n\ntally(~substance + sex, data = HELPrct, margins = TRUE)\n\n         sex\nsubstance female male Total\n  alcohol     36  141   177\n  cocaine     41  111   152\n  heroin      30   94   124\n  Total      107  346   453\n\n\nUnder the assumption of independence, \\(H_0\\), the expected count for each cell is equal to the row sum multiplied by the column sum divided by the overall table sum. So, the expected count for the cell representing females who abuse alcohol is\n\\[\ne_{11} = {177*107 \\over 453} = 41.81\n\\]\nContinuing in this fashion yields the following table of expected counts:\n\n\n         sex\nsubstance   female      male\n  alcohol 41.80795 135.19205\n  cocaine 35.90287 116.09713\n  heroin  29.28918  94.71082\n\n\nNow, we can find the value of the test statistic, \\(X^2\\):\n\\[\\begin{multline}\nX^2 = {(36 - 41.81)^2 \\over 41.81} + {(141 - 135.19)^2 \\over 135.19} + {(41 - 35.90)^2 \\over 35.90} \\\\ + {(111 - 116.10)^2 \\over 116.10} + {(30 - 29.29)^2 \\over 29.29} + {(94 - 94.71)^2 \\over 94.71}\n\\end{multline}\\]\nAs you can probably tell, \\(X^2\\) is essentially comparing the observed counts with the expected counts under \\(H_0\\). The larger the difference between observed and expected count, the larger the value of \\(X^2\\). It is normalized by dividing by the expected counts since more data in a cell leads to a larger contribution to the sum. Under \\(H_0\\), this statistic follows the chi-squared distribution with \\((r-1)\\times(c-1)\\) degrees of freedom (\\(r\\) is the number of rows and \\(c\\) is the number of columns). In our example, we have \\((3 - 1)\\times (2 - 1) = 2\\) degrees of freedom.\n\n\n25.4.5 Calculate the \\(p\\)-value\nWe can find the Pearson chi-squared test statistic (\\(X^2\\)) and corresponding \\(p\\)-value from the chi-squared distribution in R in a couple of different ways. If we had to enter the data in R, we could simply create vectors of the expected and observed counts.\n\no &lt;- c(36, 141, 41, 111, 30, 94)\ne &lt;- c(41.81, 135.19, 35.90, 116.10, 29.29, 94.71)\nx2 &lt;- sum((o - e)^2 / e)\nx2\n\n[1] 2.02814\n\n\nNote that the chi-squared test statistic is a sum of squared differences. Thus, its distribution, a chi-squared distribution, is skewed right and bounded on the left at zero. A departure from the null hypothesis means larger differences between observed and expected counts, and hence, a larger test statistic value. That is, a value further in the right tail of the distribution. Thus, we find the \\(p\\)-value by calculating the probability in the upper tail of the chi-square distribution. We do this by subtracting the CDF from one.\n\n1 - pchisq(x2, df = 2)\n\n[1] 0.3627397\n\n\nThe large \\(p\\)-value suggests there is not enough evidence to say these two variables are dependent.\nOf course, there is also a built-in function in R that will make the calculations easier. It is chisq.test().\n\ntally(~substance + sex, data = HELPrct)\n\n         sex\nsubstance female male\n  alcohol     36  141\n  cocaine     41  111\n  heroin      30   94\n\n\n\nchisq.test(tally(~substance + sex, data = HELPrct), correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(~substance + sex, data = HELPrct)\nX-squared = 2.0264, df = 2, p-value = 0.3631\n\n\nWe can extract just the table of expected counts:\n\nchisq.test(tally(~substance + sex, data = HELPrct), correct = FALSE)$expected\n\n         sex\nsubstance   female      male\n  alcohol 41.80795 135.19205\n  cocaine 35.90287 116.09713\n  heroin  29.28918  94.71082\n\n\nWe can also extract just the test statistic, which we will for permutation tests:\n\nchisq.test(tally(~substance + sex, data = HELPrct), correct = FALSE)$statistic\n\nX-squared \n 2.026361 \n\n\nWe can calculate just the test statistic by using the chisq() function as well:\n\nchisq(~substance + sex, data = HELPrct)\n\nX.squared \n 2.026361 \n\n\n\n\n25.4.6 Permutation test\nWe will complete our analysis of the HELP data using a randomization, or approximate permutation, test. First, let’s write the hypotheses:\n\\(H_0\\): The variables sex and substance are independent.\n\\(H_a\\): The variables sex and substance are dependent.\nWe will use the chi-squared test statistic as our test statistic for the randomization test. We could use a different test statistic, such as the absolute value function instead of the square function, but then we would need to write a custom function.\nLet’s calculate the observed value of the test statistic:\n\nobs &lt;- chisq(substance ~ sex, data = HELPrct)\nobs\n\nX.squared \n 2.026361 \n\n\nNotice that we can also use chisq(~substance + sex, data = HELPrct) to get the same result.\nNext, we will use a randomization process to find the sampling distribution of our test statistic.\n\nset.seed(2720)\nresults &lt;- do(1000)*chisq(substance ~ shuffle(sex), data = HELPrct)\n\nFigure 25.4 is a visual summary of the results, which helps us to gain some intuition about the \\(p\\)-value. We also plot the theoretical chi-squared distribution as a dark blue overlay. The observed test statistic value is shown as a red line.\n\n\n\n\n\n\n\n\nFigure 25.4: Sampling distribution of chi-squared test statistic from randomization test.\n\n\n\n\n\nWe find the \\(p\\)-value using prop1().\n\nprop1((~X.squared &gt;= obs), data = results)\n\nprop_TRUE \n0.3536464 \n\n\nWe don’t double this value because the chi-squared test is a one-sided test, due to the fact that we squared the differences. A departure from the null hypothesis means larger differences between observed and expected counts, and hence, a larger test statistic value. Thus, we are only interested in the upper (right) tail of the distribution.\nBased on this \\(p\\)-value, we fail to reject the null hypothesis that the variables are independent. This \\(p\\)-value is very similar to the \\(p\\)-value we found in the previous section for the Pearson’s chi-squared test, leading us to the same conclusion.\nNote that in the randomization test, we shuffled the variable sex over many replications and calculated a value of the test statistic for each replication. We did this shuffling because the null hypothesis assumed independence of the two variables. This process led to an empirical estimate of the sampling distribution, shown in Figure 25.4. Earlier in the chapter, using the Pearson’s chi-squared test, under the null hypothesis and the appropriate assumptions, the sampling distribution was a chi-squared distribution, shown by the dark blue line in the previous graph. We used it to calculate the \\(p\\)-value directly.\nNow that we’ve learned how to deal with two categorical variables, where at least one has more than two levels, we also want to consider situations where we have one categorical and one numerical variable.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#numerical-data",
    "href": "Additional-Hypothesis-Tests.html#numerical-data",
    "title": "25  Additional Hypothesis Tests",
    "section": "25.5 Numerical data",
    "text": "25.5 Numerical data\nSometimes we want to compare means across groups. In this case, we have two variables, where one is continuous and the other is categorical. In this section, we will learn how to compare means for two different categories. We can do this using a randomization test, or using the CLT in what is called a two-sample \\(t\\)-test. The hypotheses are:\n\\(H_0\\): The mean outcome is the same for both groups, or the difference in the means between the two groups is zero. In statistical notation, \\(\\mu_1 = \\mu_2\\) or \\(\\mu_1 - \\mu_2 = 0\\), where \\(\\mu_1\\) represents the mean for group 1 and \\(\\mu_2\\) represents the mean for group 2.\n\\(H_A\\): The mean outcome for the two groups is different, or the difference in means is different from zero.\nA two-sample \\(t\\)-test can be used to test for a difference in two means when the following conditions are met:\n\nThe data in each group meet the conditions for using the \\(t\\) distribution, i.e., the observations are independent and come from a nearly normal distribution.\n\nThe two groups are independent of each other.\n\nIn general, the test statistic for a two-sample \\(t\\)-test is:\n\\[\nT = {\\text{point estimate - null value} \\over\\text{standard error}} = {(\\bar{X}_1 - \\bar{X}_2) - 0 \\over \\sqrt{{S_1^2\\over n_1} + {S_2^2\\over n_2}}}\n\\]\nThe point estimate of interest is the difference between the sample means, and the null value is the hypothesized difference between the sample means, i.e., zero.\nThere are variations of this test statistic that use a pooled standard deviation or allow for paired, dependent, data, but these settings are beyond the scope of this book. We leave it to the reader to find out more about those situations as needed, and remind the reader that a lot of data analysis (and the associated code) is done via internet search.\nWhen the null hypothesis (no difference in sample means) is true and the conditions are met, the test statistic \\(T\\) has a \\(t\\) distribution with \\(df \\approx \\min(n_1 - 1, n_2 - 1)\\). More complicated calculations can be done to find the exact degrees of freedom, but this is again beyond the scope of this book. We leave it to the reader to find out more.\nLet’s now turn to an example to illustrate this new statistical test.\n\n25.5.1 MLB batting performance\nWe would like to discern whether there are real differences between the batting performance of baseball players according to their position. We will use a data set mlbbat10 from the openintro package. It is available in the file mlb_obp.csv which has been modified from the original data set to include only those players with more than 200 at bats. The batting performance will be measured by on-base percentage, obp. The on-base percentage roughly represents the fraction of times a player successfully gets on base or hits a home run. There are four positions in the data, but we are only interested in outfielders (OF) and infielders (IF) for now.\nRead the data into R and filter by position.\n\nmlb_obp_subset &lt;- read_csv(\"data/mlb_obp.csv\") %&gt;%\n  filter(position %in% c(\"IF\", \"OF\"))\n\nLet’s review our data:\n\ninspect(mlb_obp_subset)\n\n\ncategorical variables:  \n      name     class levels   n missing\n1 position character      2 274       0\n                                   distribution\n1 IF (56.2%), OF (43.8%)                       \n\nquantitative variables:  \n  name   class   min   Q1 median    Q3   max     mean         sd   n missing\n1  obp numeric 0.174 0.31  0.331 0.353 0.437 0.332719 0.03392522 274       0\n\n\nNext, change the variable position to a factor to make it easier to work with.\n\nmlb_obp_subset &lt;- mlb_obp_subset %&gt;%\n  mutate(position = as.factor(position))\n\nLet’s look at summary statistics of the on-base percentage by position.\n\nfavstats(obp ~ position, data = mlb_obp_subset)\n\n  position   min      Q1 median      Q3   max     mean         sd   n missing\n1       IF 0.174 0.30800 0.3270 0.35275 0.437 0.331526 0.03709504 154       0\n2       OF 0.265 0.31475 0.3345 0.35300 0.411 0.334250 0.02944394 120       0\n\n\nThe means for both groups are pretty similar to each other.\n\nExercise: The null hypothesis under consideration is the following: \\(\\mu_{OF} = \\mu_{IF}\\) or \\(\\mu_{OF} - \\mu_{IF} = 0\\). Write the null and corresponding alternative hypotheses in plain language.1\n\n\nExercise: If we have all the data for the 2010 season, why do we need a hypothesis test? What is the population of interest?2\n\n\nExercise:\nConstruct side-by-side boxplots.\n\nFigure 25.5 shows the side-by-side boxplots for infielders and outfielders.\n\n\n\n\n\n\n\n\nFigure 25.5: Boxplots of on-base percentage by position played.\n\n\n\n\n\nWe must verify the assumptions before proceeding with a two-sample \\(t\\)-test. We can assume that MLB players in 2010 are a representative sample of MLB players from other years. Also, the sample size for both groups is over 100, so we feel good about independence of observations within groups. There is no reason to believe that the data across groups, infielders versus outfielders, are not independent of each other. Figure 25.6 and Figure 25.7 show density curves and quantile-quantile plots for the on-base percentage by position played. There are no major outliers or other causes for concern. The data within each group appears nearly normal.\n\n\n\n\n\n\n\n\nFigure 25.6: Density curves of on-base percentage by position played.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.7: Quantile-quantile plots of on-base percentage by position played.\n\n\n\n\n\nWe can now move forward with a two-sample \\(t\\)-test.\n\n25.5.1.1 Test statistic\nOur test statistic can be found using the summary statistics that we calculated earlier.\n\nfavstats(obp ~ position, data = mlb_obp_subset)\n\n  position   min      Q1 median      Q3   max     mean         sd   n missing\n1       IF 0.174 0.30800 0.3270 0.35275 0.437 0.331526 0.03709504 154       0\n2       OF 0.265 0.31475 0.3345 0.35300 0.411 0.334250 0.02944394 120       0\n\n\nWe’ll treat infielders as group 1, subtracting their mean on-base percentage from that of outfielders. Thus, our observed test statistic is\n\\[\nT = {(\\bar{X}_{OF} - \\bar{X}_{IF}) - 0 \\over \\sqrt{{S_{OF}^2\\over n_{OF}} + {S_{IF}^2\\over n_{IF}}}} = {(0.3343 - 0.3315) - 0 \\over \\sqrt{{0.0294^2\\over 120} + {0.0371^2\\over 154}}}\n\\]\nWe can also calculate this using R.\n\ns &lt;- sd(obp ~ position, data = mlb_obp_subset)\nobs &lt;- (diffmean(obp ~ position, data = mlb_obp_subset) - 0) / sqrt(s[2]^2/120 + s[1]^2/154)\nobs\n\n diffmean \n0.6776293 \n\n\nSince the conditions for the \\(t\\) distribution are met, under the null hypothesis, our test statistic has a \\(t\\) distribution with \\(\\min(120 - 1, 154 - 1) = 119\\) degrees of freedom.\n\n\n25.5.1.2 Calculate the p-value.\nWe find the p-value for the two-sample \\(t\\)-test by calculating the probability in the right tail of a \\(t\\) distribution and doubling it.\n\n2*(1 - pt(obs, df = 119))\n\n diffmean \n0.4993221 \n\n\n\n\n25.5.1.3 Draw a conclusion.\nWith such a large \\(p\\)-value, we fail to reject the null hypothesis that the difference in means is zero. We do not have sufficient evidence to say that the mean on-base percentage for infielders and outfielders differs.\n\n\n\n25.5.2 Two-sample \\(t\\)-test in R\nAs may be expected, we can also use the built-in R function t_test() that was previously discussed in Chapter 23.\nHere, we specify a formula with the numerical variable on the left and the categorical variable on the right. Remember to use help(t_test) or ?t_test to access the R documentation for the t_test function.\n\nt_test(obp ~ position, data = mlb_obp_subset)\n\n\n    Welch Two Sample t-test\n\ndata:  obp by position\nt = -0.67763, df = 271.9, p-value = 0.4986\nalternative hypothesis: true difference in means between group IF and group OF is not equal to 0\n95 percent confidence interval:\n -0.01063818  0.00519013\nsample estimates:\nmean in group IF mean in group OF \n        0.331526         0.334250 \n\n\nOur \\(p\\)-value of 0.499 is very similar to the one we got using the pt() function. You should notice that the test statistic, \\(t\\), reported here is negative but has the same number value as our test statistic from before. The t_test function subtracted outfielders from infielders, but the \\(p\\)-value is practically the same because of the symmetry of the \\(t\\) distribution. If desired, we could reorder the levels of the position variable so that the results from t_test() more closely match our results from finding the \\(p\\)-value directly. You should also notice that the degrees of freedom are much different than the 119 we used previously. The t_test function applies a Welch correction to the degrees of freedom (hence, the “Welch Two Sample t-test” title on the output above) in some situations. We leave the reader to investigate additional options in the t_test() function, as discussions of special cases of the two-sample \\(t\\)-test are beyond the scope of this book.\n\n\n25.5.3 Randomization test\nWe can repeat the same analysis using a randomization test. We will first use the difference of means as our test statistic, and then, for interest, we’ll use the \\(t\\) statistic as well.\n\n25.5.3.1 Test statistic - difference of means\nLet’s calculate the observed test statistic.\n\nobs &lt;- diffmean(obp ~ position, data = mlb_obp_subset)\nobs\n\n   diffmean \n0.002724026 \n\n\n\n\n25.5.3.2 Calculate the p-value\nNow, we’ll conduct the randomization test. Under the null hypothesis, there is no difference in mean on-base percentage between infielders and outfielders. That is, the position label for each player can be shuffled around, and we should still end up with roughly equal mean on-base percentages. We’ll do this many times and plot the results.\n\nset.seed(807)\nresults &lt;- do(1000)*diffmean(obp ~ shuffle(position), data = mlb_obp_subset)\n\nFigure 25.8 is a plot of the sampling distribution of the difference of means from the randomization test. The observed test statistic is shown as a red line.\n\n\n\n\n\n\n\n\nFigure 25.8: The sampling distribution of the randomization test using the difference of means.\n\n\n\n\n\nThe two-sided \\(p\\)-value is\n\n2*prop1(~(diffmean &gt;= obs), results)\n\nprop_TRUE \n0.4895105 \n\n\nThis is very similar to the \\(p\\)-value we obtained using the two-sample \\(t\\)-test.\n\n\n25.5.3.3 Test statistic - \\(t\\)\nNow, let’s repeat the analysis, but use the \\(t\\) statistic as our test statistic. We’ll use the following code to extract the \\(t\\) statistic from the t_test() output and find our observed test statistic.\n\nobs &lt;- t_test(obp ~ position, data = mlb_obp_subset)$statistic\nobs\n\n         t \n-0.6776293 \n\n\n\n\n25.5.3.4 Calculate the p-value\nNow, we’ll conduct the randomization test.\n\nset.seed(526)\nresults &lt;- do(1000)*t_test(obp ~ shuffle(position), data = mlb_obp_subset)$statistic\n\nFigure 25.9 is a plot of the sampling distribution of the \\(t\\) statistic from the randomization test. The observed test statistic is shown as a red line. A \\(t\\) distribution is overlaid as a dark blue line.\n\n\n\n\n\n\n\n\nFigure 25.9: The sampling distribution of the randomization test using the t-statistic.\n\n\n\n\n\nThe two-sided \\(p\\)-value is\n\n2*prop1(~(t &lt;= obs), results)\n\nprop_TRUE \n0.5954046 \n\n\nYou should notice that the \\(p\\)-value for the two-sample \\(t\\)-test is smaller than the \\(p\\)-value obtained here. That is because the \\(p\\)-value for the two-sample \\(t\\)-test involves more assumptions. Still, our results here are consistent with those using the difference of means randomization test and the two-sample \\(t\\)-test.\nIn the next chapter, we will learn how to use the \\(F\\) statistic and ANOVA to test whether there are differences in more than two means simultaneously.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Additional-Hypothesis-Tests.html#footnotes",
    "href": "Additional-Hypothesis-Tests.html#footnotes",
    "title": "25  Additional Hypothesis Tests",
    "section": "",
    "text": "\\(H_0\\): The average on-base percentage for infielders and outfielders is equal, or the average difference between on-base percentage for infielders and outfielders is zero. \\(H_A\\): The average on-base percentage for infielders and outfielders is different.↩︎\nIf we are only making decisions or claims about the 2010 season, we do not need hypothesis testing. We can simply use summary statistics. However, if we want to generalize to other years or other leagues, then we need to conduct a hypothesis test.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Additional Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html",
    "href": "Analysis-of-Variance.html",
    "title": "26  Analysis of Variance",
    "section": "",
    "text": "26.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html#objectives",
    "href": "Analysis-of-Variance.html#objectives",
    "title": "26  Analysis of Variance",
    "section": "",
    "text": "Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the \\(F\\) distribution. Evaluate the assumptions of ANOVA.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html#introduction",
    "href": "Analysis-of-Variance.html#introduction",
    "title": "26  Analysis of Variance",
    "section": "26.2 Introduction",
    "text": "26.2 Introduction\nIn the last chapter, we learned about the chi-squared distribution, and used both mathematically derived tests (Pearson’s chi-squared test) and randomization tests to determine whether two categorical variables are independent. We also examined settings with one categorical and one numerical variable, testing for equality of means and equality of variances in two samples.\nIn this chapter, we will learn how to compare more than two means simultaneously.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html#comparing-more-than-two-means",
    "href": "Analysis-of-Variance.html#comparing-more-than-two-means",
    "title": "26  Analysis of Variance",
    "section": "26.3 Comparing more than two means",
    "text": "26.3 Comparing more than two means\nIn contrast to last chapter, we now want to compare means across more than two groups. Again, we have two variables, where one is continuous (numerical) and the other is categorical. We might initially think to do pairwise comparisons, such as two sample t-tests, as a solution. Suppose we have three groups for which we want to compare means. We might be tempted to compare the first mean with the second, then the first mean with the third, and then finally compare the second and third means, for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, the Type 1 error is inflated and it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations.\nIn this chapter, we will learn a new method called analysis of variance (ANOVA) and a new test statistic called the \\(F\\) statistic. ANOVA uses a single hypothesis test to determine whether the means across many groups are equal. The hypotheses are:\n\\(H_0\\): The mean outcome is the same across all groups. In statistical notation, \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) where \\(\\mu_i\\) represents the mean of the outcome for observations in category \\(i\\).\n\\(H_A\\): At least one mean is different.\nGenerally we must check three conditions on the data before performing ANOVA with the \\(F\\) distribution:\n\nthe observations are independent within and across groups,\n\nthe data within each group are nearly normal, and\n\nthe variability across the groups is about equal.\n\nWhen these three conditions are met, we may perform an ANOVA, using the \\(F\\) distribution, to determine whether the data provide strong evidence against the null hypothesis that all the group means, \\(\\mu_i\\), are equal.\n\n26.3.1 MLB batting performance\nLet’s revisit the MLB batting performance example. We would like to discern whether there are real differences between the batting performance of baseball players according to their position. We will now consider all four positions from the dataset: outfielder (OF), infielder (IF), designated hitter (DH), and catcher (C). The data is available in the mlb_obp.csv file. As a reminder, batting performance is measured by on-base percentage.\nRead the data into R.\n\nmlb_obp &lt;- read_csv(\"data/mlb_obp.csv\")\n\nLet’s review our data:\n\ninspect(mlb_obp)\n\n\ncategorical variables:  \n      name     class levels   n missing\n1 position character      4 327       0\n                                   distribution\n1 IF (47.1%), OF (36.7%), C (11.9%) ...        \n\nquantitative variables:  \n  name   class   min    Q1 median     Q3   max     mean         sd   n missing\n1  obp numeric 0.174 0.309  0.331 0.3545 0.437 0.332159 0.03570249 327       0\n\n\nNext, change the variable position to a factor to give us greater control.\n\nmlb_obp &lt;- mlb_obp %&gt;%\n  mutate(position = as.factor(position))\n\nLet’s look at summary statistics of the on-base percentage by position, this time considering all four positions.\n\nfavstats(obp ~ position, data = mlb_obp)\n\n  position   min      Q1 median      Q3   max      mean         sd   n missing\n1        C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175  39       0\n2       DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669  14       0\n3       IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154       0\n4       OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120       0\n\n\nThe means for each group are pretty similar to each other.\n\nExercise: The null hypothesis under consideration is the following: \\(\\mu_{OF} = \\mu_{IF} = \\mu_{DH} = \\mu_{C}\\). Write the null and corresponding alternative hypotheses in plain language.1\n\n\nExercise:\nConstruct side-by-side boxplots.\n\nFigure 26.1 shows the side-by-side boxplots for all four positions.\n\n\n\n\n\n\n\n\nFigure 26.1: Boxplots of on-base percentage by position played.\n\n\n\n\n\nThe largest difference between the sample means appears to be between the designated hitter and the catcher positions. Consider again the original hypotheses:\n\\(H_0\\): \\(\\mu_{OF} = \\mu_{IF} = \\mu_{DH} = \\mu_{C}\\)\n\\(H_A\\): The average on-base percentage (\\(\\mu_i\\)) varies across some (or all) groups.\n\nThought question: Why might it be inappropriate to run the test by simply estimating whether the difference of \\(\\mu_{DH}\\) and \\(\\mu_{C}\\) is statistically significant at an \\(\\alpha = 0.05\\) significance level?\n\nThe primary issue here is that we are inspecting the data before picking the groups that will be compared. It is inappropriate to examine all data by eye (informal testing) and only afterwards decide which parts to formally test. This is called data snooping or data fishing. Naturally, we would pick the groups with the largest differences for the formal test, leading to an inflation in the Type 1 error rate. To understand this better, let’s consider a slightly different problem.\nSuppose we are to measure the aptitude for students in 20 classes of a large elementary school at the beginning of the year. In this school, all students are randomly assigned to classrooms, so any differences we observe between the classes at the start of the year are completely due to chance. However, with so many groups, we will probably observe a few groups that look rather different from each other. If we select only the classes that look really different, we will probably make the wrong conclusion that the assignment wasn’t random. While we might only formally test differences for a few pairs of classes, we informally evaluated the other classes by eye before choosing the most extreme cases for a comparison.\nIn the next section, we will learn how to use the \\(F\\) statistic and ANOVA to test whether observed differences in means could have happened just by chance, even if there was no true difference in the respective population means.\n\n\n26.3.2 Analysis of variance (ANOVA) and the F test\nThe method of analysis of variance (ANOVA) in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? This question is different from earlier testing procedures because we will simultaneously consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. We call this variability the mean square between groups (\\(MSG\\)), and it has an associated degrees of freedom, \\(df_{G} = k - 1\\), when there are \\(k\\) groups. The \\(MSG\\) can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn’t be too large. We typically use software to find the \\(MSG\\); however, the mathematical derivation follows. Let \\(\\bar{x}_i\\) represent the mean outcome for observations in group \\(i\\), and let \\(\\bar{x}\\) represent the mean outcome across all groups. Then, the mean square between groups is computed as\n\\[\nMSG = \\frac{1}{df_{G}}SSG = \\frac{1}{k - 1}\\sum_{i = 1}^{k} n_{i}\\left(\\bar{x}_{i} - \\bar{x}\\right)^2,\n\\]\nwhere \\(SSG\\) is called the sum of squares between groups and \\(n_{i}\\) is the sample size of group \\(i\\).\nThe mean square between the groups is, on its own, quite useless in a hypothesis test. We need a benchmark value for how much variability is expected among the sample means, if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the mean squared error (\\(MSE\\)), which has an associated degrees of freedom of \\(df_E = n - k\\). It is helpful to think of the \\(MSE\\) as a measure of the variability within the groups. To find the \\(MSE\\), the sum of squares total (\\(SST\\))} is computed as\n\\[SST = \\sum_{i = 1}^{n} \\left(x_{i} - \\bar{x}\\right)^2,\\]\nwhere the sum is over all observations in the data set. Then we compute the sum of squared errors (\\(SSE\\)) in one of two equivalent ways:\n\\[\nSSE = SST - SSG = (n_1 - 1)s_1^2 + (n_2 - 1)s_2^2 + \\cdots + (n_k - 1)s_k^2,\n\\]\nwhere \\(s_i^2\\) is the sample variance (square of the standard deviation) of the observations in group \\(i\\). Then the \\(MSE\\) is the standardized form of \\(SSE\\):\n\\[MSE = \\frac{1}{df_{E}}SSE\\]\nWhen the null hypothesis is true, any differences among the sample means are only due to chance, and the \\(MSG\\) and \\(MSE\\) should be about equal. For ANOVA, we examine the ratio of \\(MSG\\) and \\(MSE\\) in the \\(F\\) test statistic:\n\\[F = \\frac{MSG}{MSE}\\]\nThe \\(MSG\\) represents a measure of the between-group variability, and \\(MSE\\) measures the variability within each of the groups. Using a randomization test, we could also look at the difference in the mean squared errors as a test statistic instead of the ratio.\nWe can use the \\(F\\) statistic to evaluate the hypotheses in what is called an F test. A \\(p\\)-value can be computed from the \\(F\\) statistic using an \\(F\\) distribution, which has two associated parameters: \\(df_{1}\\) and \\(df_{2}\\). For the \\(F\\) statistic in ANOVA, \\(df_{1} = df_{G}\\) and \\(df_{2}= df_{E}\\). The \\(F\\) statistic is really a ratio of chi-squared random variables.\nThe \\(F\\) distribution, shown in Figure 26.2, takes on positive values and is right skewed, like the chi-squared distribution. The larger the observed variability in the sample means (\\(MSG\\)) relative to the within-group observations (\\(MSE\\)), the larger \\(F\\) will be and the stronger the evidence against the null hypothesis. Because larger values of \\(F\\) represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a \\(p\\)-value.\n\n\n\n\n\n\n\n\nFigure 26.2: The F distribution.\n\n\n\n\n\n\nThe \\(F\\) statistic and the \\(F\\) test\nAnalysis of variance (ANOVA) is used to test whether the mean outcome differs across two or more groups. ANOVA uses a test statistic \\(F\\), which represents a standardized ratio of variability in the sample means (across the groups), relative to the variability within the groups. If \\(H_0\\) is true and the model assumptions are satisfied, the statistic \\(F\\) follows an \\(F\\) distribution with parameters \\(df_{1} = k - 1\\) and \\(df_{2} = n - k\\). The upper tail of the \\(F\\) distribution is used to represent the \\(p\\)-value.\n\n\n26.3.2.1 ANOVA\nWe will use R to perform the calculations for the ANOVA. But let’s check our assumptions first.\nThere are three conditions we must check before conducting an ANOVA: 1) all observations must be independent, 2) the data in each group must be nearly normal, and 3) the variance within each group must be approximately equal.\n\nIndependence\nAll observations must be independent. More specifically, observations must be independent within and across groups. If the data are a simple random sample from less than 10% of the population, this assumption is reasonable. For processes and experiments, we must carefully consider whether the data may be independent (e.g., no paired data). In our MLB data, the data were not sampled; they consist of all players from the 2010 season with at least 200 at bats. However, there are not obvious reasons why independence would not hold for most or all observations, given our intended population is all MLB seasons (or something similar). This requires a bit of hand waving, but remember that independence is often difficult to assess.\n\n\nApproximately normal\nAs with one- and two-sample testing for means, the normality assumption is especially important when the sample size is quite small. When we have larger data sets (and larger groups) and there are no extreme outliers, the normality condition is not usually a concern. The normal probability plots (quantile-quantile plots) for each group of the MLB data are shown below; there is some deviation from normality for infielders, but this isn’t a substantial concern since there are over 150 observations in that group and the outliers are not extreme. Sometimes in ANOVA, there are so many groups or so few observations per group that checking normality for each group isn’t reasonable. One solution is to combine the groups into one set of data. First, calculate the residuals of the baseball data, which are calculated by taking the observed values and subtracting the corresponding group means. For example, an outfielder with OBP of 0.435 would have a residual of \\(0.435 - \\bar{x}_{OF} = 0.082\\). Then, to check the normality condition, create a normal probability plot using all the residuals simultaneously.\n\nFigure 26.3 is the quantile-quantile plot to assess the normality assumption.\n\n\n\n\n\n\n\n\nFigure 26.3: Quantile-quantile plot for two-sample test of means.\n\n\n\n\n\n\nConstant variance\nThe final assumption is that the variance within the groups is about equal from one group to the next. This assumption is important to check, especially when the samples sizes vary widely across the groups. The constant variance assumption can be checked by examining a side-by-side box plot of the outcomes across the groups, which we did previously in Figure @ref(fig:box231-fig). In this case, the variability is similar across the four groups but not identical. We also see in the output of favstats that the standard deviation varies a bit from one group to the next. Whether these differences are from natural variation is unclear, so we should report the uncertainty of meeting this assumption when the final results are reported. The permutation test does not have this assumption and can be used as a check on the results from the ANOVA.\n\nIn summary, independence is always important to an ANOVA analysis, but is often difficult to assess. The normality condition is very important when the sample sizes for each group are relatively small. The constant variance condition is especially important when the sample sizes differ between groups.\nLet’s write the hypotheses for the MLB example again:\n\\(H_0\\): The average on-base percentage is equal across the four positions.\n\\(H_A\\): The average on-base percentage varies across some (or all) groups.\nThe test statistic is the ratio of the between-groups variance (mean square between groups, \\(MSG\\)) and the pooled within-group variance (mean squared error, \\(MSE\\)). We perform ANOVA in R using the aov() function, and use the summary() function to extract the most important information from the output.\n\nsummary(aov(obp ~ position, data = mlb_obp))\n\n             Df Sum Sq  Mean Sq F value Pr(&gt;F)\nposition      3 0.0076 0.002519   1.994  0.115\nResiduals   323 0.4080 0.001263               \n\n\nThis table contains all the information we need. It has the degrees of freedom, sums of squares, mean squared errors, \\(F\\) test statistic, and \\(p\\)-value. The test statistic \\(\\left(\\frac{MSG}{MSE}\\right)\\) is 1.994, \\(\\frac{0.002519}{0.001263} = 1.994\\). The \\(p\\)-value is larger than 0.05, indicating the evidence is not strong enough to reject the null hypothesis at a significance level of 0.05. That is, the data do not provide strong evidence that the average on-base percentage varies by player’s primary field position.\nThe calculation of the \\(p\\)-value can also be done by finding the probability associated with the upper tail of the \\(F\\) distribution:\n\npf(1.994, 3, 323, lower.tail = FALSE)\n\n[1] 0.1147443\n\n\nFigure 26.4 is a plot of the \\(F\\) distribution with the observed \\(F\\) test statistic shown as a red line.\n\n\n\n\n\n\n\n\nFigure 26.4: The F distribution.\n\n\n\n\n\n\n\n26.3.2.2 Permutation test\nWe can repeat the same analysis using a permutation test. We will first run it using a ratio of variances (mean squares) and then, for interest, as a difference in variances.\nWe need a way to extract the mean squares from the output. The broom package contains a function called tidy() that cleans up output from functions and makes them into data frames.\n\nlibrary(broom)\n\n\naov(obp ~ position, data = mlb_obp) %&gt;%\n  tidy()\n\n# A tibble: 2 × 6\n  term         df   sumsq  meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 position      3 0.00756 0.00252      1.99   0.115\n2 Residuals   323 0.408   0.00126     NA     NA    \n\n\nLet’s summarize the values in the meansq column and develop our test statistic, the ratio of mean squares. We could just pull the statistic (using the pull() function) but we want to be able to generate a different test statistic, the difference of mean squares, as well.\n\naov(obp ~ position, data = mlb_obp) %&gt;%\n  tidy() %&gt;%\n  summarize(stat = meansq[1] / meansq[2]) \n\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.99\n\n\nNow we are ready. First, get our observed test statistic using pull().\n\nobs &lt;- aov(obp ~ position, data = mlb_obp) %&gt;%\n  tidy() %&gt;%\n  summarize(stat = meansq[1] / meansq[2]) %&gt;%\n  pull()\nobs\n\n[1] 1.994349\n\n\nLet’s put our test statistic into a function that includes shuffling the position variable.\n\nf_stat &lt;- function(x){\n  aov(obp ~ shuffle(position), data = x) %&gt;%\n  tidy() %&gt;%\n  summarize(stat = meansq[1] / meansq[2]) %&gt;%\n  pull()\n}\n\nNow, we run our function.\n\nset.seed(5321)\nf_stat(mlb_obp)\n\n[1] 0.1185079\n\n\nNext, we run the randomization test using the do() function. There is an easier way to do all of this work with the purrr package but we will continue with the work we have started.\n\nset.seed(5321)\nresults &lt;- do(1000)*(f_stat(mlb_obp))\n\nThe above code is slow in executing because the tidyverse functions inside our f_stat() function are slow.\nFigure 26.5 is a plot of the sampling distribution from the randomization test. The \\(F\\) distribution is overlaid as a dark blue curve.\n\n\n\n\n\n\n\n\nFigure 26.5: The sampling distribution of the ratio of variances randomization test statistic.\n\n\n\n\n\nThe \\(p\\)-value is\n\nprop1(~(result &gt;= obs), results)\n\nprop_TRUE \n0.0959041 \n\n\nThis is a similar \\(p\\)-value to the ANOVA output.\nNow, let’s repeat the analysis but use the difference in variance (mean squares) as our test statistic. We’ll define a new function that calculates the test statistic and then run the randomization test.\n\nf_stat2 &lt;- function(x){\n  aov(obp ~ shuffle(position), data = x) %&gt;%\n  tidy() %&gt;%\n  summarize(stat = meansq[1] - meansq[2]) %&gt;%\n  pull(stat)\n}\n\n\nset.seed(5321)\nresults &lt;- do(1000)*(f_stat2(mlb_obp))\n\nFigure 26.6 is the plot of the sampling distribution of the difference in variances.\n\n\n\n\n\n\n\n\nFigure 26.6: The sampling distribution of the difference in variances randomization test statistic.\n\n\n\n\n\nWe need the observed test statistic in order to calculate a \\(p\\)-value.\n\nobs &lt;- aov(obp ~ position, data = mlb_obp) %&gt;%\n  tidy() %&gt;%\n  summarize(stat = meansq[1] - meansq[2]) %&gt;%\n  pull(stat)\nobs\n\n[1] 0.001255972\n\n\nThe \\(p\\)-value is\n\nprop1(~(result &gt;= obs), results)\n\nprop_TRUE \n0.0959041 \n\n\nAgain, we find a similar \\(p\\)-value.\n\nExercise: If we reject the null hypothesis in the ANOVA test, we know that at least one mean is different but we don’t know which one(s). How would you approach answering the question of which means are different?",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html#culminating-example",
    "href": "Analysis-of-Variance.html#culminating-example",
    "title": "26  Analysis of Variance",
    "section": "26.4 Culminating example",
    "text": "26.4 Culminating example\nTo bring together all the ideas we have learned so far in this block, we will work an example of testing for a difference of two means. In our opinion, the easiest method to understand is the permutation test and the most difficult is the one based on the mathematical derivation, because of the assumptions necessary to get a mathematical solution for the sampling distribution. We will also introduce how to use the bootstrap to get a confidence interval.\n\n26.4.1 HELP example\nLet’s return to the Health Evaluation and Linkage to Primary Care data set, HELPrct in the mosaicData package. Previously, we looked at whether there was a difference in substance of abuse between males and females.\nWe are now interested in whether there is a difference between male and female ages. Let’s first take a look at the data to refresh our memory.\n\ndata(\"HELPrct\")\n\n\nHELP_sub &lt;- HELPrct %&gt;%\n  select(age, sex)\n\n\nfavstats(age ~ sex, data = HELP_sub)\n\n     sex min Q1 median   Q3 max     mean       sd   n missing\n1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\n\n\n\nHELP_sub %&gt;%\n  gf_boxplot(age ~ sex) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Gender\", y = \"Age (years)\")\n\n\n\n\nThe distribution of age in the HELP study by gender.\n\n\n\n\n\nHELP_sub %&gt;%\n  gf_dhistogram(~age|sex, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Age\", y = \"\")\n\n\n\n\nThe distribution of age in the HELP study by gender.\n\n\n\n\nFigures @ref(fig:box253-fig) and @ref(fig:hist254-fig) indicate there might be a slight difference in the means, but is it statistically significant?\n\n\n26.4.2 Randomization test\nThe randomization test (approximation of a permutation test) is ideally suited for a hypothesis test. So, we will conduct this first and then see if we can generate a confidence interval.\nThe hypotheses are:\n\\(H_0\\): There is no difference in average age for men and women in the detoxification unit. In statistical notation: \\(\\mu_{male} - \\mu_{female} = 0\\), where \\(\\mu_{female}\\) represents the mean age for female inpatients and \\(\\mu_{male}\\) represents the mean age for male inpatients.\n\\(H_A\\): There is some difference in average age for men and women in the detoxification unit (\\(\\mu_{male} - \\mu_{female} \\neq 0\\)).\nLet’s perform a randomization test. The mean age by sex is again shown below.\n\nfavstats(age ~ sex, data = HELP_sub)\n\n     sex min Q1 median   Q3 max     mean       sd   n missing\n1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\n\n\nWe calculate the observed difference in the mean ages. Notice we are subtracting the mean female age from the mean male age.\n\nobs_stat &lt;- diffmean(age ~ sex, data = HELP_sub)\nobs_stat\n\n  diffmean \n-0.7841284 \n\n\nUnder the null hypothesis, there is no difference in the average age for men and women. So, we shuffle the sex labels around to generate a sampling distribution under the null.\n\nset.seed(345)\nresults &lt;- do(10000)*diffmean(age ~ shuffle(sex), data = HELP_sub)\n\n\nfavstats(~diffmean, data = results)\n\n       min         Q1     median     Q3      max        mean        sd     n\n -3.378154 -0.5638809 0.01120955 0.5863 3.486224 0.009350908 0.8492454 10000\n missing\n       0\n\n\nThe sampling distribution is centered on the null hypothesized value of 0, more or less, and the standard deviation is 0.849. This is an estimate of the variability of the difference in mean ages. The observed difference in means is shown as a red line on the sampling distribution in Figure @ref(fig:hist255-fig).\n\nresults %&gt;%\n  gf_histogram(~diffmean, color = \"black\", fill = \"cyan\") %&gt;%\n  gf_vline(xintercept = obs_stat, color = \"red\") %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Difference of means\", \n          title = \"Sampling distribution of difference of two means\", \n          subtitle = \"Null assumes equal means\")\n\n\n\n\nThe approximate sampling distribution of the difference of means from a bootstrap process.\n\n\n\n\nOur test statistic does not appear to be too extreme. Now, we calculate a two-sided \\(p\\)-value.\n\n2*prop1(~(diffmean &lt;= obs_stat), data = results)\n\nprop_TRUE \n0.3523648 \n\n\nBased on this \\(p\\)-value, we fail to reject the null hypothesis. There is not enough evidence to say that the mean age for men and women is different.\nNow, to construct a confidence interval based on the randomization, we have to be careful and think about this. The object results has the distribution of the difference in means, assuming there is no difference. To get a confidence interval, we want to center this distribution on the observed difference in means and not on zero. We will do this by adding the observed difference in means to the randomization results, and then find the percentile confidence interval.\n\ncdata(~(diffmean + obs_stat), data = results)\n\n         lower     upper central.p\n2.5% -2.449246 0.8789368      0.95\n\n\nWe are 95% confident that the true difference in mean ages between male and female participants in the study is between -2.45 and 0.88. Because 0 is contained in the confidence interval, we fail to reject the null hypothesis. There is not enough evidence to say that the mean age for men and women is different.\nNote that we are assuming the test statistic can be transformed. It turns out that the percentile method for a confidence interval is transformation invariant, so we can perform the transformation of shifting the null sampling distribution by the observed value.\n\n\n26.4.3 Traditional mathematical methods\nUsing the CLT or the \\(t\\) distribution becomes difficult, because we have to find a way to calculate the standard error. There have been many proposed methods, and you are welcome to research them, but we will only present a couple of ideas in this section. Let’s summarize the process for both hypothesis testing and confidence intervals in the case of the difference of two means using the \\(t\\) distribution. You may recall some of the ideas for a two-sample \\(t\\) test (hypothesis test) from Chapter @ref(ADDTESTS).\n\n\n26.4.4 Hypothesis tests\nWhen applying the \\(t\\) distribution for a hypothesis test, we proceed as follows:\n\nState the appropriate hypotheses.\nVerify the conditions for using the \\(t\\) distribution.\nFor a difference of means, when the data are not paired: each sample mean must separately satisfy the one-sample conditions for the \\(t\\) distribution, and the data in each group must also be independent. Just like in the one-sample case, slight skewness will not be a problem for larger sample sizes. We can have moderate skewness and be fine if our sample is 30 or more. We can have extreme skewness if our sample is 60 or more.\nCompute the point estimate of interest, the standard error, and the degrees of freedom.\nCompute the \\(T\\) test statistic and \\(p\\)-value.\nMake a conclusion based on the \\(p\\)-value, and write a conclusion in context of the problem and in plain language so that anyone can understand the results.\n\nWe’ve added the extra step of checking the assumptions here.\n\n\n26.4.5 Confidence intervals\nSimilarly, the following is how we generally compute a confidence interval using the \\(t\\) distribution:\n\nVerify conditions for using the \\(t\\) distribution. (See above.)\nCompute the point estimate of interest, the standard error, the degrees of freedom, and \\(t^{\\star}_{df}\\) (the critical value or number of standard deviations needed for the confidence interval).\nCalculate the confidence interval using the general formula, \\(\\text{point estimate} \\pm\\ t_{df}^{\\star} SE\\).\nPut the conclusions in context of the problem and in plain language, so even non-statisticians can understand the results.\n\nIf the assumptions above are met, each sample mean can itself be modeled using a \\(t\\) distribution. If the samples are independent, then the sample difference of two means, \\(\\bar{x}_1 - \\bar{x}_2\\), can be modeled using the \\(t\\) distribution and the standard error is\n\\[SE_{\\bar{x}_{1} - \\bar{x}_{2}} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\]\nTo calculate the degrees of freedom, we can use statistical software or, conservatively, use the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\).\n\n26.4.5.1 Results\nBack to the HELP data, the men and women were independent of each other. Additionally, the distributions in each population don’t show any clear deviations from normality. There is some slight skewness but the sample size reduces this concern, see Figure @ref(fig:qq256-fig). Finally, within each group we also need independence. If each group represents less than 10% of the population, we are good to go on this. This condition might be difficult to verify.\n\nHELP_sub %&gt;%\n  gf_qq(~age|sex) %&gt;%\n  gf_qqline(~age|sex) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\nThe quantile-quantile plots to check the normality assumption.\n\n\n\n\nThe distribution of males tends to have longer tails than a normal distribution, and the female distribution is skewed to the right. The sample sizes are large enough that this does not worry us. Below are the summary statistics by sex once more.\n\nfavstats(age ~ sex, data = HELP_sub)\n\n     sex min Q1 median   Q3 max     mean       sd   n missing\n1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\n\n\nLet’s find the confidence interval first. We use the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\) as the degrees of freedom: \\(df = 106\\).\n\n(35.47 - 36.25) + c(-1,1)*qt(0.975, 106)*sqrt(7.58^2 / 107 + 7.75^2 / 346)\n\n[1] -2.4512328  0.8912328\n\n\nThis result is very close to the interval we got with the permutation test.\nNow, let’s find the \\(p\\)-value for the hypothesis test.\nThe test statistic is:\n\\[T \\ =\\ \\frac{\\text{point estimate} - \\text{null value}}{SE}\\]\n\\[ = \\frac{(35.47 - 36.25) - 0}{\\sqrt{\\left( \\frac{7.58^2}{107}+ \\frac{7.75^2}{346}\\right)}}\\ =\\ - 0.92976 \\] We again use \\(df = 106\\). The \\(p\\)-value is:\n\n2*pt(-0.92976, 106)\n\n[1] 0.3546079\n\n\nThe results in R are slightly different due to the calculation of the degrees of freedom.\n\nt_test(age ~ sex, data = HELP_sub)\n\n\n    Welch Two Sample t-test\n\ndata:  age by sex\nt = 0.92976, df = 179.74, p-value = 0.3537\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.8800365  2.4482932\nsample estimates:\nmean in group female   mean in group male \n            36.25234             35.46821 \n\n\nNotice that the degrees of freedom is not an integer. This is because it is a weighted average of the two different samples sizes and standard deviations. This method is called the Satterthwaite approximation. Additionally, the confidence interval is slightly different because t_test() subtracted the mean age for men from the mean age for women. Still, the confidence interval and \\(p\\)-value lead us to the same results as before – we fail to reject the null hypothesis.\n\n\n26.4.5.2 Pooled standard deviation\nOccasionally, two populations will have standard deviations that are so similar they can be treated as identical. This is an assumption of equal variance in each group. For example, historical data or a well-understood biological mechanism may justify this strong assumption. In such cases, we can make the \\(t\\) distribution approach slightly more precise by using a pooled standard deviation.\nThe pooled standard deviation of two groups is a way to use data from both samples to better estimate the standard deviation and standard error. If \\(s_1\\) and \\(s_2\\) are the standard deviations of groups 1 and 2, respectively, and there are good reasons to believe that the population standard deviations are equal, then we can obtain an improved estimate of the group variances by pooling their data:\n\\[ s_{pooled}^2 = \\frac{s_1^2\\times (n_1 - 1) + s_2^2\\times (n_2 - 1)}{n_1 + n_2 - 2}\\]\nwhere \\(n_1\\) and \\(n_2\\) are the sample sizes of groups 1 and 2, as before. To use this new estimate, we substitute \\(s_{pooled}^2\\) in place of \\(s_1^2\\) and \\(s_2^2\\) in the standard error formula, and we use an updated formula for the degrees of freedom: \\[df = n_1 + n_2 - 2\\]\nThe benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger degrees of freedom parameter for the \\(t\\) distribution. Both of these changes may permit a more accurate model of the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2\\).\n\nCaution\nPooling standard deviations should be done only after careful research.\n\nA pooled standard deviation is only appropriate when background research indicates the population standard deviations are nearly equal. When the sample size is large and the condition may be adequately checked with data, the benefits of pooling the standard deviations greatly diminishes.\nIn R we can test the difference of two means with equal variance using the var.equal argument.\n\nt_test(age ~ sex, data = HELP_sub, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  age by sex\nt = 0.91923, df = 451, p-value = 0.3585\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.8922735  2.4605303\nsample estimates:\nmean in group female   mean in group male \n            36.25234             35.46821 \n\n\nSince our sample sizes are so large, this did not have a big impact on the results.\n\n\n\n26.4.6 Bootstrap\nFinally, we will construct a confidence interval through the use of the bootstrap. In this problem, we have to be careful and sample within each group. Compare the following two bootstrap samples.\n\nfavstats(age ~ sex, data = resample(HELP_sub))\n\n     sex min Q1 median    Q3 max     mean       sd   n missing\n1 female  21 30     33 38.75  50 34.64706 6.267387 102       0\n2   male  19 29     33 39.00  59 34.74359 7.833066 351       0\n\n\nand\n\nfavstats(age ~ sex, data = resample(HELP_sub, groups = sex))\n\n     sex min Q1 median   Q3 max     mean       sd   n missing\n1 female  22 31     34 39.5  57 35.60748 6.901951 107       0\n2   male  20 31     35 41.0  60 35.94798 8.039227 346       0\n\n\nNotice in the second line of code, we are keeping the groups the same size by sampling within the sex variable.\nNow, let’s get our bootstrap distribution.\n\nset.seed(2527)\nresults &lt;- do(1000)*diffmean(age ~ sex, data = resample(HELP_sub, groups = sex))\n\nFigure @ref(fig:boot257-fig) is our sampling distribution from the bootstrap.\n\nresults %&gt;%\n  gf_histogram(~diffmean, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_theme(theme_classic) %&gt;%\n  gf_labs(x = \"Difference in means\", y = \"\")\n\n\n\n\nBootstrap distribution of the difference in means.\n\n\n\n\n\ncdata(~diffmean, p = 0.95, data = results)\n\n         lower     upper central.p\n2.5% -2.394406 0.8563786      0.95\n\n\nAgain, we find similar results with the percentile confidence interval.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Analysis-of-Variance.html#footnotes",
    "href": "Analysis-of-Variance.html#footnotes",
    "title": "26  Analysis of Variance",
    "section": "",
    "text": "\\(H_0\\): The average on-base percentage is equal across the four positions. \\(H_A\\): The average on-base percentage varies across some (or all) groups. That is, the average on-base percentage for at least one position is different.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html",
    "href": "Linear-Regression-Case-Study.html",
    "title": "27  Linear Regression Case Study",
    "section": "",
    "text": "27.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#objectives",
    "href": "Linear-Regression-Case-Study.html#objectives",
    "title": "27  Linear Regression Case Study",
    "section": "",
    "text": "Using R, generate a linear regression model and use it to produce a prediction model.\nUsing plots, check the assumptions of a linear regression model.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#introduction-to-linear-regression",
    "href": "Linear-Regression-Case-Study.html#introduction-to-linear-regression",
    "title": "27  Linear Regression Case Study",
    "section": "27.2 Introduction to linear regression",
    "text": "27.2 Introduction to linear regression\nLinear regression is often one of the first methods taught in a machine learning course. It is an excellent tool with a wide range of applications. It can be used solely to predict an outcome of interest, a prediction model, and/or be used for inference. In this book, we will mainly focus on its use for inference. Even so, this treatment barely scratches the surface of what can be done. There are entire courses devoted to the interpretation of linear regression models.\nWhen used as a predictive model, linear regression fits into the category of a function approximation method. The parameters of the model, function, are fit using an objective, loss, function and optimization procedure. For linear regression in this book, the loss function will be sum of squared errors which leads to closed form solution using differentiation. In machine learning courses you will learn about different types of loss functions to include penalized or regularized versions as well as different optimization engines. For software such as tidymodels in R or scitkit-learn in python, you will specify the loss function and optimization engine directly.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#case-study-introduction",
    "href": "Linear-Regression-Case-Study.html#case-study-introduction",
    "title": "27  Linear Regression Case Study",
    "section": "27.3 Case study introduction",
    "text": "27.3 Case study introduction\nThe Human Freedom Index is a report that attempts to summarize the idea of “freedom” through a bunch of different variables for many countries around the globe. It serves as a rough objective measure for the relationships between the different types of freedom - whether it’s political, religious, economical or personal freedom - and other social and economic circumstances. The Human Freedom Index is an annually co-published report by the Cato Institute, the Fraser Institute, and the Liberales Institut at the Friedrich Naumann Foundation for Freedom.\nIn this case study, you’ll be analyzing data from Human Freedom Index reports from 2008-2016. Your aim will be to summarize a few of the relationships within the data both graphically and numerically in order to find which variables can help tell a story about freedom. This will be done using the tool of regression.\nAgain, like our previous case studies, this chapter will introduce many of the ideas of the block. Don’t worry if they seem difficult and you feel overwhelmed a bit, we will come back to the ideas in the following chapters.\n\n27.3.1 Load packages\nLet’s load the packages.\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\n\n\n27.3.2 The data and exploratory analysis\nThe data we’re working with is in the file called hfi.csv under the data folder. The name hfi is short for Human Freedom Index.\n\nExercise\nRead the data into R. What are the dimensions of the dataset?\n\n\nhfi&lt;-tibble(read_csv(\"data/hfi.csv\"))\n\n\ndim(hfi)\n\n[1] 1458  123\n\n\nThere are 1458 observations and 123 variables in the data frame.\n\nExercise\nCreate summaries of the first 10 variables in the data. We just don’t want a large printout.\n\n\ninspect(hfi[,1:10])\n\n\ncategorical variables:  \n       name     class levels    n missing\n1  ISO_code character    162 1458       0\n2 countries character    162 1458       0\n3    region character     10 1458       0\n                                   distribution\n1 AGO (0.6%), ALB (0.6%), ARE (0.6%) ...       \n2 Albania (0.6%), Algeria (0.6%) ...           \n3 Sub-Saharan Africa (25.9%) ...               \n\nquantitative variables:  \n                        name   class  min          Q1      median          Q3\n1                       year numeric 2008 2010.000000 2012.000000 2014.000000\n2          pf_rol_procedural numeric    0    4.133333    5.300000    7.389499\n3               pf_rol_civil numeric    0    4.549550    5.300000    6.410975\n4            pf_rol_criminal numeric    0    3.789724    4.575189    6.400000\n5                     pf_rol numeric    0    4.131746    4.910797    6.513178\n6             pf_ss_homicide numeric    0    6.386978    8.638278    9.454402\n7 pf_ss_disappearances_disap numeric    0   10.000000   10.000000   10.000000\n          max        mean       sd    n missing\n1 2016.000000 2012.000000 2.582875 1458       0\n2    9.700000    5.589355 2.080957  880     578\n3    8.773533    5.474770 1.428494  880     578\n4    8.719848    5.044070 1.724886  880     578\n5    8.723094    5.309641 1.529310 1378      80\n6    9.926568    7.412980 2.832947 1378      80\n7   10.000000    8.341855 3.225902 1369      89\n\n\n\nExercise\nCreate a scatter plot to display the relationship between the personal freedom score, pf_score, as the response and pf_expression_control as the predictor. Does the relationship look linear?\n\n\n\n\n\n\n\n\n\nFigure 27.1: A scatterplot of personal freedom versus expression control using the ggformula package.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.2: A scatterplot of personal freedom versus expression control using the ggplot2 package.\n\n\n\n\n\nFigure 27.1 and Figure 27.2 are both scatterplots, we included both to demonstrate both the ggformula and ggplot2 packages. In these figures, the relationship does look linear. Although, we should be uncomfortable using the model at the end points. That is because there are less points at the edge and and linear estimation has larger variance at the endpoints, the predictions at the endpoints is more suspect.\n\nExercise\nThe relationship looks linear, quantify the strength of the relationship with the correlation coefficient.\n\n\nhfi %&gt;%\n  summarise(cor(pf_expression_control, pf_score, use = \"complete.obs\"))\n\n# A tibble: 1 × 1\n  `cor(pf_expression_control, pf_score, use = \"complete.obs\")`\n                                                         &lt;dbl&gt;\n1                                                        0.796\n\n\nThe sample correlation coefficient, indicates a strong positive linear relationship between the variables.\nNote that we set the use argument to “complete.obs” since there are some observations with missing values, NA.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#sum-of-squared-residuals",
    "href": "Linear-Regression-Case-Study.html#sum-of-squared-residuals",
    "title": "27  Linear Regression Case Study",
    "section": "27.4 Sum of squared residuals",
    "text": "27.4 Sum of squared residuals\nIn this section, you will use an interactive function to investigate what we mean by “sum of squared residuals”. You will need to run this function in your console. Running the function also requires that the hfi data set is loaded in your environment, which we did above.\nThink back to the way that we described the distribution of a single variable. Recall that we discussed characteristics such as center, spread, and shape. It’s also useful to be able to describe the relationship of two numerical variables, such as pf_expression_control and pf_score above.\n\nExercise\nLooking at your scatterplot above, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations.\n\nWe would say that there is a strong positive linear relationship between the two variables.\nJust as you’ve used the mean and standard deviation to summarize a single variable, you can summarize the relationship between these two variables by finding the line that best follows their association. Use the following interactive function to select the line that you think does the best job of going through the cloud of points.\nFirst we must remove missing values from the data set and to make the visualization easier, we will just randomly sample 30 of the data points. We included hf_score because we will need it later.\n\nset.seed(4011)\nhfi_sub &lt;- hfi %&gt;%\n  select(pf_expression_control,pf_score,hf_score) %&gt;%\n  drop_na() %&gt;%\n  group_by(pf_expression_control) %&gt;%\n  slice_sample(n=1)\n\nWe used the function slice_sample() to ensure we have unique values of pf_expression_control in our sample.\n\nExercise\nIn your R console, run the code above to create the object hfi_sub. You are going to have to load packages and read in the hfi data set. Then execute the next lines of code. Pick two locations in the plot to create a line. Record the sum of squares.\n\nFirst, run this code chunk to create a function plot_ss() that you will use next.\n\n# Function to create plot and residuals.\nplot_ss &lt;- function (x, y, data, showSquares = FALSE, leastSquares = FALSE) \n{\n    x &lt;- eval(substitute(x), data)\n    y &lt;- eval(substitute(y), data)\n    plot(y ~ x, asp = 1, pch = 16)\n    if (leastSquares) {\n        m1 &lt;- lm(y ~ x)\n        y.hat &lt;- m1$fit\n    }\n    else {\n        cat(\"Click two points to make a line.\")\n        pt1 &lt;- locator(1)\n        points(pt1$x, pt1$y, pch = 4)\n        pt2 &lt;- locator(1)\n        points(pt2$x, pt2$y, pch = 4)\n        pts &lt;- data.frame(x = c(pt1$x, pt2$x), y = c(pt1$y, pt2$y))\n        m1 &lt;- lm(y ~ x, data = pts)\n        y.hat &lt;- predict(m1, newdata = data.frame(x))\n    }\n    r &lt;- y - y.hat\n    abline(m1)\n    oSide &lt;- x - r\n    LLim &lt;- par()$usr[1]\n    RLim &lt;- par()$usr[2]\n    oSide[oSide &lt; LLim | oSide &gt; RLim] &lt;- c(x + r)[oSide &lt; LLim | \n        oSide &gt; RLim]\n    n &lt;- length(y.hat)\n    for (i in 1:n) {\n        lines(rep(x[i], 2), c(y[i], y.hat[i]), lty = 2, col = \"blue\")\n        if (showSquares) {\n            lines(rep(oSide[i], 2), c(y[i], y.hat[i]), lty = 3, \n                col = \"orange\")\n            lines(c(oSide[i], x[i]), rep(y.hat[i], 2), lty = 3, \n                col = \"orange\")\n            lines(c(oSide[i], x[i]), rep(y[i], 2), lty = 3, col = \"orange\")\n        }\n    }\n    SS &lt;- round(sum(r^2), 3)\n    cat(\"\\r                                \")\n    print(m1)\n    cat(\"Sum of Squares: \", SS)\n}\n\nNext run the next code chunk that for us resulted in Figure 27.3. You will have to pick to points on the plot that you think gives the best line.\n\n\nClick two points to make a line.\n                                \nCall:\nlm(formula = y ~ x, data = pts)\n\nCoefficients:\n(Intercept)            x  \n     5.0272       0.4199  \n\nSum of Squares:  19.121\n\n\n\n\n\n\n\n\nFigure 27.3: Plot of selected line and the associated residuals.\n\n\n\n\n\nOnce you’ve picked your two locations, the line you specified will be shown in black and the residuals in blue. Residuals are the difference between the observed values and the values predicted by the line:\n\\[ e_i = y_i - \\hat{y}_i \\]\nThe most common way to do linear regression is to select the line that minimizes the sum of squared residuals. To visualize the squared residuals, you can rerun the plot command and add the argument showSquares = TRUE.\n\nplot_ss(x = pf_expression_control, y = pf_score, data = hfi_sub, showSquares = TRUE)\n\nNote that the output from the plot_ss function provides you with the slope and intercept of your line as well as the sum of squares.\n\nExercise:\nUsing plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got?",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#the-linear-model",
    "href": "Linear-Regression-Case-Study.html#the-linear-model",
    "title": "27  Linear Regression Case Study",
    "section": "27.5 The linear model",
    "text": "27.5 The linear model\nIt is rather cumbersome to try to get the correct least squares line, i.e. the line that minimizes the sum of squared residuals, through trial and error. Instead, you can use the lm() function in R to fit the linear model (a.k.a. regression line).\n\nm1 &lt;- lm(pf_score ~ pf_expression_control, data = hfi_sub)\n\nThe first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of pf_score as a function of pf_expression_control. The second argument specifies that R should look in the hfi_sub data frame to find the two variables. This should be familiar to us since we have been doing this when we used the mosaic package.\nThe output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary() function.\n\nsummary(m1)\n\n\nCall:\nlm(formula = pf_score ~ pf_expression_control, data = hfi_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7559 -0.4512  0.1838  0.5369  1.2510 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            5.02721    0.23186  21.682  &lt; 2e-16 ***\npf_expression_control  0.41988    0.04312   9.736 1.26e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7288 on 36 degrees of freedom\nMultiple R-squared:  0.7248,    Adjusted R-squared:  0.7171 \nF-statistic:  94.8 on 1 and 36 DF,  p-value: 1.262e-11\n\n\nLet’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of pf_expression_control. With this table, we can write down the least squares regression line for the linear model:\n\\[\\hat{\\text{pf\\_score}} = 5.02721 + 0.41988 \\times \\text{pf\\_expression\\_control}\\]\nAt least these are the values we got on our machine using the seed provided. Yours may differ slightly. One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, \\(R^2\\). The \\(R^2\\) value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 72.48% of the variability in pf_score is explained by pr_expression_control.\n\nExercise:\nFit a new model that uses pf_expression_control to predict hf_score, or the total human freedom score. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between human freedom and the amount of political pressure on media content?\n\n\nm2&lt;-lm(hf_score ~ pf_expression_control, data = hfi_sub)\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = hf_score ~ pf_expression_control, data = hfi_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5151 -0.5776  0.2340  0.4622  1.0633 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            5.45660    0.21585  25.279  &lt; 2e-16 ***\npf_expression_control  0.30707    0.04015   7.649 4.72e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6785 on 36 degrees of freedom\nMultiple R-squared:  0.6191,    Adjusted R-squared:  0.6085 \nF-statistic:  58.5 on 1 and 36 DF,  p-value: 4.718e-09\n\n\n\\[\n  \\hat{\\text{hf\\_score}} = 5.45660 + 0.30707 \\times \\text{pf\\_expression\\_control}\n\\] As the political pressure increases by one unit, the average human freedom score increases by 0.307.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#prediction-and-prediction-errors",
    "href": "Linear-Regression-Case-Study.html#prediction-and-prediction-errors",
    "title": "27  Linear Regression Case Study",
    "section": "27.6 Prediction and prediction errors",
    "text": "27.6 Prediction and prediction errors\nLet’s create a scatterplot with the least squares line for m1, our first model, laid on top, Figure 27.4.\n\n\n\n\n\n\n\n\nFigure 27.4: Scatterplot of personal expression control and personal freedom score.\n\n\n\n\n\nHere, we are literally adding a layer on top of our plot. The stat_smooth() function creates the line by fitting a linear model, we could use geom_lm() as well. It can also show us the standard error se associated with our line, but we’ll suppress that for now.\nThis line can be used to predict \\(y\\) at any value of \\(x\\). When predictions are made for values of \\(x\\) that are beyond the range of the observed data, it is referred to as extrapolation and is not usually recommended. However, predictions made within the range of the data are more reliable. They’re also used to compute the residuals.\n\nExercise:\nIf someone saw the least squares regression line and not the actual data, how would they predict a country’s personal freedom score for one with a 6.75 rating for pf_expression_control? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction?\n\nTo predict, we will use the predict function, but we have to send the new data as a data frame.\n\npredict(m1,newdata=data.frame(pf_expression_control=6.75))\n\n       1 \n7.861402 \n\n\nWe thus predict a value of 7.86 for the average pf_score.\nThe observed value is 8.628272.\n\nhfi_sub %&gt;%\n  filter(pf_expression_control==6.75)\n\n# A tibble: 1 × 3\n# Groups:   pf_expression_control [1]\n  pf_expression_control pf_score hf_score\n                  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1                  6.75     8.63     8.25\n\n\nThe residual is:\n\n8.628272 - 7.861402\n\n[1] 0.76687\n\n\nWe underestimated the actual value.\nAnother way to do this is to use the broom package.\n\nlibrary(broom)\n\n\naugment(m1) %&gt;%\n   filter(pf_expression_control==6.75)\n\n# A tibble: 1 × 8\n  pf_score pf_expression_control .fitted .resid   .hat .sigma .cooksd .std.resid\n     &lt;dbl&gt;                 &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     8.63                  6.75    7.86  0.767 0.0421  0.727  0.0254       1.08",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#model-diagnostics",
    "href": "Linear-Regression-Case-Study.html#model-diagnostics",
    "title": "27  Linear Regression Case Study",
    "section": "27.7 Model diagnostics",
    "text": "27.7 Model diagnostics\nTo assess whether the linear model is reliable, we need to check for\n\nlinearity,\n\nindependence,\nnearly normal residuals, and\n\nconstant variability.\n\nLinearity: You already checked if the relationship between pf_score and pf_expression_control is linear using a scatterplot. We should also verify this condition with a plot of the residuals vs. fitted (predicted) values, Figure 27.5.\n\n\n\n\n\n\n\n\nFigure 27.5: Scatterplot of residuals and fitted values used to assess the assumptions of linearity and constant variance.\n\n\n\n\n\nNotice here that m1 can also serve as a data set because stored within it are the fitted values (\\(\\hat{y}\\)) and the residuals. Also note that we’re getting more sophisticated with the code in Figure 27.5. After creating the scatterplot on the first layer (first line of code), we overlay a horizontal dashed line at \\(y = 0\\) (to help us check whether residuals are distributed around 0), and we also rename the axis labels to be more informative and add a title.\n\nExercise:\nIs there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?\n\nThe width is constant and there is no trend in the data. The linearity assumption is not bad.\nIndependence: This is difficult to check with residuals and depends on how the data was collected.\nNearly normal residuals: To check this condition, we can look at a histogram, Figure 27.6.\n\n\n\n\n\n\n\n\nFigure 27.6: Histogram of residuals from linear regression model.\n\n\n\n\n\nor a normal probability plot of the residuals, Figure 27.7.\n\n\n\n\n\n\n\n\nFigure 27.7: The quantile-quantile residual plot used to assess the normality assumption.\n\n\n\n\n\nNote that the syntax for making a normal probability plot is a bit different than what you’re used to seeing: we set sample equal to the residuals instead of x, and we set a statistical method qq, which stands for “quantile-quantile”, another name commonly used for normal probability plots.\nIt is a little difficult at first to understand how the qq plot indicated that the distribution was skewed to the left. The points indicate the quantile from the sample, standardized to have mean zero and standard deviation one, plotted against the same quantiles from a standard normal. It the sample matched a standard normal the points would align on the 45-degree line. From the plot, we see that as the theoretical quantile get larger, further from zero, the sample do not. That is why the trajectory of the points in the upper right looks flat, below the 45-degree line. Thus the distribution is compressed on the right making it skewed to the left.\n\nExercise: Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met?\n\nNo, the sample is small but it appears the residual are skewed to the left.\nConstant variability:\n\nExercise: Based on the residuals vs. fitted plot, does the constant variability condition appear to be met?\n\nYes, the width of the plot seems constant.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Case-Study.html#brief-summary",
    "href": "Linear-Regression-Case-Study.html#brief-summary",
    "title": "27  Linear Regression Case Study",
    "section": "27.8 Brief summary",
    "text": "27.8 Brief summary\nThis case study introduced simple linear regression. We look at the criteria to find a best fit linear line between two variables. We also used R to fit the line. We examined the output from R and used it to explain and predict with our model. In the remainder of this block we will develop these ideas further and extend to multiple predictors and binary outcomes. This is a perfect introduction for Math 378.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Linear Regression Case Study</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Basics.html",
    "href": "Linear-Regression-Basics.html",
    "title": "28  Linear Regression Basics",
    "section": "",
    "text": "28.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear Regression Basics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Basics.html#objectives",
    "href": "Linear-Regression-Basics.html#objectives",
    "title": "28  Linear Regression Basics",
    "section": "",
    "text": "Differentiate between various statistical terminologies such as response, predictor, linear regression, simple linear regression, coefficients, residual, and extrapolation, and construct examples to demonstrate their proper use in context.\nEstimate the parameters of a simple linear regression model using a given sample of data.\nInterpret the coefficients of a simple linear regression model.\nCreate and evaluate scatterplots with regression lines.\nIdentify and assess the assumptions underlying linear regression models.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear Regression Basics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Basics.html#linear-regression-models",
    "href": "Linear-Regression-Basics.html#linear-regression-models",
    "title": "28  Linear Regression Basics",
    "section": "28.2 Linear regression models",
    "text": "28.2 Linear regression models\nThe rest of this block will serve as a brief introduction to linear models. In general, a model estimates the relationship between one variable (a response) and one or more other variables (predictors). Models typically serve two purposes: prediction and inference. A model allows us to predict the value of a response given particular values of predictors. Also, a model allows us to make inferences about the relationship between the response and the predictors.\nNot all models are used or built on the same principles. Some models are better for inference and others are better for prediction. Some models are better for qualitative responses and others are better for quantitative responses. Also, many models require making assumptions about the nature of the relationship between variables. If these assumptions are violated, the model loses usefulness. In a machine learning course, a wide array of models are discussed but most are used for the purpose of prediction.\nIn this block, we will focus on linear models and the use of linear regression to produce and evaluate the model. Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where graphs with straight lines are overlaid on scatterplots, much like we did in the last chapter. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.\nSuppose we were interested in exploring the relationship between one response variable (\\(Y\\)) and one predictor variable (\\(X\\)). We can postulate a linear relationship between the two:\n\\[\nY=\\beta_0+\\beta_1 X\n\\]\nA linear model can be expanded to include multiple predictor variables:\n\\[\nY=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p\n\\]\nThis model is often referred to as a linear regression model. (When there is only one predictor variable, we often refer to it as a simple linear regression model.) The \\(\\beta_j\\) terms are referred to as coefficients. Note that the coefficients are parameters and thus represented as Greek letters. We typically don’t know the true values of \\(\\beta_j\\), so we have to estimate them with samples from the population of \\(X\\) and \\(Y\\), our data. Estimating these parameters and using them for prediction and inference will be the majority of the work of this last block.\nWe consider the models above to be linear models because they are linear in the parameters. This means that the following model is also a linear model:\n\\[\nY=\\beta_0+\\beta_1 X^2\n\\]\nTechnically, we can write the parameters as a vector and the predictor or explanatory variables as a matrix. The response will then be an inner product of this vector and matrix and thus a linear combination.\nEven if we expect two random variables \\(X\\) and \\(Y\\) to share a linear relationship, we don’t expect it to be perfect. There will be some scatter around the estimated line. For example, consider the head length and total length of 104 brushtail possums from Australia. The data is in the file possum.csv in the data folder.\n\nExercise:\nRead in the data from data/possum.csv and look at the first few rows of data.\n\n\npossum &lt;- read_csv(\"data/possum.csv\")\n\n\nglimpse(possum)\n\nRows: 104\nColumns: 8\n$ site    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ pop     &lt;chr&gt; \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\",…\n$ sex     &lt;chr&gt; \"m\", \"f\", \"f\", \"f\", \"f\", \"f\", \"m\", \"f\", \"f\", \"f\", \"f\", \"f\", \"m…\n$ age     &lt;dbl&gt; 8, 6, 6, 6, 2, 1, 2, 6, 9, 6, 9, 5, 5, 3, 5, 4, 1, 2, 5, 4, 3,…\n$ head_l  &lt;dbl&gt; 94.1, 92.5, 94.0, 93.2, 91.5, 93.1, 95.3, 94.8, 93.4, 91.8, 93…\n$ skull_w &lt;dbl&gt; 60.4, 57.6, 60.0, 57.1, 56.3, 54.8, 58.2, 57.6, 56.3, 58.0, 57…\n$ total_l &lt;dbl&gt; 89.0, 91.5, 95.5, 92.0, 85.5, 90.5, 89.5, 91.0, 91.5, 89.5, 89…\n$ tail_l  &lt;dbl&gt; 36.0, 36.5, 39.0, 38.0, 36.0, 35.5, 36.0, 37.0, 37.0, 37.5, 39…\n\n\n\nhead(possum)\n\n# A tibble: 6 × 8\n   site pop   sex     age head_l skull_w total_l tail_l\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1 Vic   m         8   94.1    60.4    89     36  \n2     1 Vic   f         6   92.5    57.6    91.5   36.5\n3     1 Vic   f         6   94      60      95.5   39  \n4     1 Vic   f         6   93.2    57.1    92     38  \n5     1 Vic   f         2   91.5    56.3    85.5   36  \n6     1 Vic   f         1   93.1    54.8    90.5   35.5\n\n\nWe think the head and total length variables are linearly associated. Possums with an above average total length also tend to have above average head lengths. To visualize this data, we will use a scatterplot. We have used scatterplots multiple times in this book. Scatterplots are a graphical technique to present two numerical variables simultaneously. Such plots permit the relationship between the variables to be examined with ease. The following figure shows a scatterplot for the head length and total length of the possums. Each point represents a single possum from the data.\n\nExercise:\nCreate a scatterplot of head length and total length.\n\n\npossum %&gt;%\n  gf_point(head_l~total_l) %&gt;%\n  gf_labs(x=\"Total Length (cm)\",y=\"Head Length (mm)\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 28.1: A scatterplot of possum total length and head length.\n\n\n\n\n\nFrom Figure 28.1, we see that the relationship is not perfectly linear; however, it could be helpful to partially explain the connection between these variables with a straight line. Since some longer possums will have shorter heads and some shorter possums will have longer heads, there is no straight line that can go through all the data points. We expect some deviation from the linear fit. This deviation is represented by random variable \\(e\\) (this is not the Euler number), which we refer to as an error term or residual:\n\\[\nY=\\beta_0+\\beta_1X+e\n\\]\nFor our problem, \\(Y\\) is head length and \\(X\\) is total length.\nIn general we have:\n\\[ \\text{Data} = \\text{Fit} + \\text{Residual} \\]\nand what will change in our modeling process is how we specify the \\(\\text{Fit}\\).\nThe error term is assumed to follow a normal distribution with mean 0 and constant standard deviation \\(\\sigma\\). Note: the assumption of normality is only for inference using the \\(t\\) and \\(F\\) distributions and we can relax this assumption by using a bootstrap. Among other things, these assumptions imply that a linear model should only be used when the response variable is continuous in nature. There are other approaches for non-continuous response variables (for example logistic regression).\n\n28.2.1 Estimation\nWe want to describe the relationship between the head length and total length variables in the possum data set using a line. In this example, we will use the total length as the predictor variable, \\(x\\), to predict a possum’s head length, \\(y\\). Just as a side note, the choice of the predictor and response are not arbitrary. The response is typically what we want to predict or in the case of experiments is the causal result of the predictor(s).\nIn the possum data we have \\(n = 104\\) observations: \\((x_1,y_1), (x_2,y_2),...,(x_{104},y_{104})\\). Then for each observation the implied linear model is:\n\\[\ny_i=\\beta_0+\\beta x_i + e_i\n\\]\nWe could fit the linear relationship by eye like we did in the case study and obtain estimates of the slope and intercept but this is too ad hoc.1 So, given a set of data like the possum, how do we actually obtain estimates of \\(\\beta_0\\) and \\(\\beta_1\\)? What is the best fit line?\nWe begin by thinking about what we mean by “best”. Mathematically, we want a line that has small residuals. There are multiple methods, but the most common is the method of least squares. In this method, our goal is to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the squared vertical distance between the points and the resulting line, the residuals. See Figure 28.2 for a visual representation involving only four observations from made up data.\n\n\n\n\n\n\n\n\nFigure 28.2: An illustration of the least squares method.\n\n\n\n\n\nOur criterion for best is the estimates of slope and intercept that minimize the sum of the squared residuals:\n\\[\ne_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]\nThe following are three possible reasons to choose the least squares criterion over other criteria such as the sum of the absolute value of residuals:\n\nIt is the most commonly used method.\n\nComputing a line based on least squares was much easier by hand when computers were not available.\n\nIn many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy.\n\nThe first two reasons are largely for tradition and convenience; the last reason explains why least squares is typically helpful.2\nSo, we need to find \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the expression, observed minus expected:\n\\[\n\\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2\n\\]\nUsing calculus-based optimization yields the following estimates of \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}\n\\]\nNotice that this implies that the line will always go through the point \\(\\left(\\bar{x},\\bar{y} \\right)\\). As a reminder \\(\\bar{x}\\) is the sample mean of the explanatory variable and \\(\\bar{y}\\) is the sample mean of the response.\nAnd\n\\[\n\\hat{\\beta}_1 = {\\sum x_i y_i - n\\bar{x}\\bar{y} \\over{\\sum x_i^2 -n\\bar{x}^2}}\n\\]\nA more intuitive formula for the slope and one that links correlation to linear regression is:\n\\[\n\\hat{\\beta}_1 = \\frac{s_y}{s_x} R\n\\]\nwhere \\(R\\) is the correlation between the two variables, and \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable and response, respectively. Thus the slope is proportional to the correlation.\nYou may also be interested in estimating \\(\\sigma\\), the standard deviation of the error:\n\\[\n\\hat{\\sigma}=\\sqrt{{1\\over{n-2}} \\sum_{i=1}^n \\hat{e}_i^2}\n\\]\nwhere \\(\\hat{e}_i\\) is the observed \\(i\\)th residual (\\(\\hat{e}_i=y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i\\)). This estimate is based only on the assumption of constant variance.\n\n\n28.2.2 Possum example\nWe will let R do the heavy work of minimizing the sum of squares. The function is lm() as we learned in the case study. This function needs a formula and data for input. The formula notation should be easy for us since we have worked with formulas so much in the mosaic package.\nFirst create the model:\n\nposs_mod &lt;- lm(head_l~total_l,data=possum)\n\nThe output of the model object is minimal with just the estimated slope and intercept.\n\nposs_mod\n\n\nCall:\nlm(formula = head_l ~ total_l, data = possum)\n\nCoefficients:\n(Intercept)      total_l  \n    42.7098       0.5729  \n\n\nWe can get more information using the summary() function:\n\nsummary(poss_mod)\n\n\nCall:\nlm(formula = head_l ~ total_l, data = possum)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1877 -1.5340 -0.3345  1.2788  7.3968 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.70979    5.17281   8.257 5.66e-13 ***\ntotal_l      0.57290    0.05933   9.657 4.68e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.595 on 102 degrees of freedom\nMultiple R-squared:  0.4776,    Adjusted R-squared:  0.4725 \nF-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16\n\n\nThe model object, poss_mod, contains much more information. Using the names() function on the model objects, gives you a list of other quantities available, such as residuals.\n\nnames(poss_mod)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nFigure 28.3 is a plot the data points and least squares line together.\n\npossum %&gt;%\n  gf_point( head_l ~ total_l) %&gt;%\n  gf_lm(color=\"black\") %&gt;%\n  gf_labs(x=\"Total Length (cm)\",y=\"Head Length (mm)\") %&gt;%\n  gf_labs(title=\"Possum data including regression line\") %&gt;%\n  gf_theme(theme_classic()) \n\n\n\n\n\n\n\nFigure 28.3: A scatterplot of possum total length and head length including a regression line.\n\n\n\n\n\n\n\n28.2.3 Interpretation\nInterpreting parameters in a regression model is often one of the most important steps in the analysis. The intercept term, \\(\\beta_0\\), is usually uninteresting. It represents the average value of the response when the predictor is 0. Unless we center our predictor variable around 0, the actual value of the intercept is usually not important; it typically just gives the slope more flexibility to fit the data. The slope term, \\(\\beta_1\\) represents the average increase in the response variable per unit increase in the predictor variable. We keep using the word average in our discussion. With the assumption of a mean of 0 for the residuals, which least squares ensures with a line going through the point \\(\\left(\\bar{x},\\bar{y} \\right)\\), the output of the model is the expected or average response for the given input. Mathematically we have:\n\\[\nE(Y|X=x)=E(\\beta_0+\\beta_1x+e) = E(\\beta_0+\\beta_1x)+E(e)=\\beta_0+\\beta_1x\n\\]\nPredicting a value of the response variable simply becomes a matter of substituting the value of the predictor variable into the estimated regression equation. Again, it is important to note that for a given value of \\(x\\), the predicted response, \\(\\hat{y}\\) is what we expect the average value of \\(y\\) to be given that specific value of the predictor.\n\nExercise: The slope and intercept estimates for the possum data are 0.5729 and 42.7098. What do these numbers really mean, interpret them?\n\nInterpreting the slope parameter is helpful in almost any application. For each additional 1 cm of total length of a possum, we would expect the possum’s head to be 0.5729 mm longer on average. Note that a longer total length corresponds to longer head because the slope coefficient is positive. We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. That is, increasing a possum’s total length may not cause the possum’s head to be longer.\nThe estimated intercept \\(b_0=42.7\\) describes the average head length of a possum with zero total length! The meaning of the intercept is irrelevant to this application since a possum can not practically have a total length of 0.\nEarlier we noted a relationship between the slope estimate and the correlation coefficient estimate.\n\nExercise Find the slope from the correlation and standard deviations.\n\n\npossum %&gt;%\n  summarise(correlation=cor(head_l,total_l),sd_head=sd(head_l),\n            sd_total=sd(total_l),slope=correlation*sd_head/sd_total)\n\n# A tibble: 1 × 4\n  correlation sd_head sd_total slope\n        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1       0.691    3.57     4.31 0.573\n\n\n\n\n28.2.4 Extrapolation is dangerous\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February \\(6^{th}\\) it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.” Stephen Colbert April 6th, 20103\n\nLinear models can be used to approximate the relationship between two variables and are built on an observed random sample. These models have real limitations as linear regression is simply a modeling framework. The truth is almost always much more complex than our simple line. Extrapolation occurs when one tries to make a prediction of a response given a value of the predictor that is outside the range of values used to build the model. We only have information about the relationship between two variables in the region around our observed data. We do not know how the data outside of our limited window will behave. Be careful about extrapolating.\n\n\n28.2.5 Reading computer output\nWe stored the results of our linear regression model for the possum data in the object poss_mod but it provided only the bare minimum of information. We can get more information using the function summary().\n\nsummary(poss_mod)\n\n\nCall:\nlm(formula = head_l ~ total_l, data = possum)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1877 -1.5340 -0.3345  1.2788  7.3968 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.70979    5.17281   8.257 5.66e-13 ***\ntotal_l      0.57290    0.05933   9.657 4.68e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.595 on 102 degrees of freedom\nMultiple R-squared:  0.4776,    Adjusted R-squared:  0.4725 \nF-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16\n\n\nThe first line repeats the model formula. The second line is a descriptive summary of the residuals, plots of the residuals are more useful than this summary. We then have a table of the model fit. The first column of numbers provides estimates for \\(\\beta_0\\) and \\(\\beta_1\\), respectively. For the next columns, we’ll describe the meaning of the columns using the second row, which corresponds to information about the slope estimate. Again, the first column provides the point estimate for \\(\\beta_1\\). The second column is a standard error for this point estimate. The third column is a \\(t\\) test statistic for the null hypothesis that \\(\\beta_1 = 0\\): \\(T=9.657\\). The last column is the \\(p\\)-value for the \\(t\\) test statistic for the null hypothesis \\(\\beta_1=0\\) and a two-sided alternative hypothesis. We will get into more of these details in the next chapters.\nThe row with the residual standard error is an estimate of the unexplained variance. The next rows give a summary of the model fit and we will discuss in the next chapters.\nIn the tidyverse we may want to have the table above in a tibble. The broom package, which we have seen before, helps with this effort.\n\nlibrary(broom)\n\n\ntidy(poss_mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   42.7      5.17        8.26 5.66e-13\n2 total_l        0.573    0.0593      9.66 4.68e-16\n\n\n\nExercise:\nThe cars dataset (built-in to R) contains 50 observations of 2 variables. The data give the speed of cars (in mph) and the corresponding distance (in feet) that it took to stop. Attempt to answer the following questions.\n\n\nBuild a simple linear regression model fitting distance against speed.\n\n\ncars_mod &lt;- lm(dist~speed,data=cars)\nsummary(cars_mod)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nFigure 28.4 is a scatterplot of the cars data set.\n\ncars %&gt;%\n  gf_point(dist~speed) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(x=\"Speed (mph)\",y=\"Stopping distance (ft)\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 28.4: A scatterplot of speed and stopping distance.\n\n\n\n\n\nAs expected, it appears that for larger speeds, stopping distance is greater.\n\nReport and interpret the estimated model coefficients.\n\nThe estimated intercept term, \\(\\hat{\\beta}_0\\) is equal to -17.6. This estimate doesn’t have a helpful interpretation. Technically, it is the estimated average stopping distance for speed 0. However, “stopping distance” doesn’t make sense when a car has no speed. Also, a negative stopping distance doesn’t make sense. Furthermore, a speed of 0 is outside of the observed speeds in the data set, so even if a speed of 0 made sense, it is outside the scope of this data and thus an extrapolation.\nThe estimated slope term, \\(\\hat{\\beta}_1\\) is equal to 3.9. This means that for an increase of one mph, we expect stopping distance to increase by 3.9 feet, on average.\n\nReport the estimated common standard deviation of the residuals.\n\nThe estimated standard deviation of the error (residual), \\(\\hat{\\sigma}\\) is equal to 15.4.\n\n\n28.2.6 Assumptions\nAnytime we build a model, there are assumptions behind it that, if violated, could invalidate the conclusions of the model. In the description of simple linear regression, we briefly mentioned these assumptions. Next chapter, we will discuss how to validate these assumptions.\nFit. When we build a simple linear regression, we assume that the relationship between the response and the predictor is as we specify in the fit formula. This in simple linear regression is often just a linear relationship. Suppose two variables are non-linearly related, see Figure 28.5. While we could build a linear regression model between the two, the resulting model would not be very useful. If we built a model with the fit formulated as a quadratic, a similar plot of the residuals would look flat. We will discuss this more in a later chapter.\n\n\n\n\n\n\n\n\nFigure 28.5: An example of non-linear relationship between two variables fitted with a linear regression line.\n\n\n\n\n\nIndependent Observations. Another assumption is that all observations in a data set are independent of one another. A common way this assumption is violated is by using time as the predictor variable. For example, suppose we were interested in how an individual’s weight changes over time. While it may be tempting to plot this and fit a regression line through the data, the resulting model is inappropriate, as simple linear regression assumes that each observation is independent. Figure 28.6 shows correlated data fitted with a linear regression line.\n\n\n\n\n\n\n\n\nFigure 28.6: A scatterplot of correlated data fit using a linear regression model with the assumption of independence.\n\n\n\n\n\nConstant Error Variance. In simple linear regression, we assume that the residuals come from a normal distribution with mean 0 and constant standard deviation \\(\\sigma\\). Violation of this assumption is usually manifested as a “megaphone” pattern in the scatterplot. Specifically, as the value of the predictor increases, the variance in the response also increases, resulting in greater spread for larger values of the predictor.\nNormality of Errors. Again, we assume that the residuals are normally distributed. Normality of residuals is not easy to see graphically, so we have to use other diagnostics to check this assumption.\nThe last three assumptions are important not necessarily for estimating the relationship, but for inferring about the relationship. In future chapters, we will discuss how to use a model for prediction, and how to build a confidence/prediction interval around a prediction. Also, we will discuss inference about the coefficient estimates in a model. Violation of one of the last three assumptions will impact our ability to conduct inference about the population parameters.\n\n\n28.2.7 Residual plots\nOne purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. These can help to evaluate the assumptions.\nFigure 28.7 shows three scatterplots with linear models in the first row and residual plots in the second row.\n\n\n\n\n\n\n\n\nFigure 28.7: Residual plots and associated scatterplots.\n\n\n\n\n\nIn the first data set (first column), the residuals show no obvious patterns. The residuals appear to be scattered randomly around the dashed line that represents 0.\nThe second data set shows a pattern in the residuals. There is some curvature in the scatterplot, which is more obvious in the residual plot. We should not use a straight line to model these data. Instead, a more advanced technique should be used.\nIn the last plot the spread, variance of the data, seems to increase as the explanatory variable increases. We can see this clearly in the residual plot. To make inference using the \\(t\\) or \\(F\\) distribution would require a transformation to equalize the variance.\n\n\n28.2.8 Summary\nWe have introduced the ideas of linear regression in this chapter. There are many new terms as well as new R functions to learn. We will continue to use these ideas in the remainder of this block of material. Next we will learn about inference and prediction.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear Regression Basics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Basics.html#footnotes",
    "href": "Linear-Regression-Basics.html#footnotes",
    "title": "28  Linear Regression Basics",
    "section": "",
    "text": "If you want to do try this again use the plot_ss() from the last chapter.↩︎\nThere are applications where least absolute deviation may be more useful, and there are plenty of other criteria we might consider. However, this book only applies the least squares criterion. Math 378: Applied Statistical Learning (the follow-on course at USAFA) will look at other criteria such as a shrinkage method called the lasso.↩︎\nhttp://www.colbertnation.com/the-colbert-report-videos/269929/↩︎",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear Regression Basics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Inference.html",
    "href": "Linear-Regression-Inference.html",
    "title": "29  Linear Regression Inference",
    "section": "",
    "text": "29.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Linear Regression Inference</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Inference.html#objectives",
    "href": "Linear-Regression-Inference.html#objectives",
    "title": "29  Linear Regression Inference",
    "section": "",
    "text": "Apply statistical inference methods for \\(\\beta_0\\) and \\(\\beta_1\\), and evaluate the implications for the predictor-response relationship.\nWrite the estimated simple linear regression model and calculate and interpret the predicted response for a given value of the predictor.\nConstruct and interpret confidence and prediction intervals for the response variable.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Linear Regression Inference</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Inference.html#introduction",
    "href": "Linear-Regression-Inference.html#introduction",
    "title": "29  Linear Regression Inference",
    "section": "29.2 Introduction",
    "text": "29.2 Introduction\nIn this chapter we discuss uncertainty in the estimates of the slope and y-intercept for a regression line. This will allow us to perform inference and predictions. Just as we identified standard errors for point estimates in previous chapters, we first discuss standard errors for these new estimates. This chapter is a classical chapter in the sense that we will be using the normal distribution. We will assume that the errors are normally distributed with constant variance. Later in the book, we will relax these assumptions.\n\n29.2.1 Regression\nLast chapter, we introduced linear models using the simple linear regression model:\n\\[\nY=\\beta_0+\\beta_1X+e\n\\]\nwhere now we assume the error term follows a normal distribution with mean 0 and constant standard deviation \\(\\sigma\\). Using the method of least squares, which does not require the assumption of normality, we obtain estimates of \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\hat{\\beta}_1 = {\\sum x_i y_i - n\\bar{x}\\bar{y} \\over \\sum x_i^2 -n\\bar{x}^2}\n\\]\n\\[\n\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}\n\\]\nIf we assume a probability distribution for the errors, we could also find point estimates using maximum likelihood methods. This will not be discussed in this book.\nUsing these estimates, for a given value of the predictor, \\(x_*\\), we can obtain a prediction of the response variable. Here we are using the subscript \\(_*\\) to denote a new value for the explanatory variable. The resulting prediction, which we will denote \\(\\hat{Y}_*\\), is the average or expected value of the response given predictor value \\(x_*\\):\n\\[\n\\hat{Y}_*=\\hat{\\beta}_0+\\hat{\\beta}_1x_*\n\\]\nThe reason this model returns the expected value of the response at the given value of the predictor is because the error term has an expected value of zero. As a review of the properties of expectation as well as last chapter, we have:\n\\[\nE(Y|X=x)=E(\\beta_0+\\beta_1x+e)=Y=\\beta_0+\\beta_1x+E(e)=\\beta_0+\\beta_1x\n\\]\nbecause \\(\\beta_0\\), \\(\\beta_1\\), and \\(x\\) are constants.\nIt should be abundantly clear by now that \\(\\hat{Y}_*\\), \\(\\hat{\\beta}_0\\), and \\(\\hat{\\beta}_1\\) are estimators. Being estimators, they are dependent on our random sample, our data. If we collect a new random sample from the same population, we will get new estimates from these estimators. Thus, we can think of \\(\\hat{Y}_*\\), \\(\\hat{\\beta}_0\\), and \\(\\hat{\\beta}_1\\) as random variables. Like all random variables, they have distributions. We can use the distribution of an estimator to build confidence intervals and conduct hypothesis tests about the true values of the parameter it is intended to estimate. The estimators based on least squares are unbiased, their distributions are centered around the actual values of \\(Y\\), \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\n\n29.2.2 Review of assumptions\nWe will review the assumptions of the least squares model because they are important for inference. Refer to Figure 29.1, which plots the linear regression in the top row and the residuals in the second row. We generally assume the following:\n\nFit. The data should show a linear trend. If there is a nonlinear trend, a transformation of the explanatory variable or a more advanced regression method should be applied. When looking at the residual plot, if the trend is linear, we should see a spread of points that are flat. The left column of Figure 29.1 is an example of a nonlinear relationship. The top plot is the regression plot and we can see what looks like a quadratic relationship instead of a linear one. The residual plot, the plot in the lower left corner of Figure 29.1, also exhibits this non-linear trend.\n\nNearly normal residuals. Generally the residuals must be nearly normal to use a \\(t\\) or \\(F\\) for inference. When this assumption is found to be unreasonable, it is usually because of outliers or concerns about influential points. An example of non-normal residuals is shown in the second column of Figure 29.1. A qq plot is also useful as a diagnostic tool as we have seen. We can still use the bootstrap as an inference tool if the normality assumption is unreasonable.\n\nConstant variability. The variability of points around the least squares line remains roughly constant. An example of non-constant variability is shown in the third panel of Figure 29.1. The constant variability assumption is needed for the \\(t\\) and \\(F\\) distributions. It is not required for the bootstrap method.\n\nIndependent observations. Be cautious about applying regression to data collected sequentially in what is called a time series. Such data may have an underlying structure that should be considered in a model and analysis. An example of a time series where independence is violated is shown in the fourth panel of Figure 29.1. More advanced methods are required for time series data even including using a bootstrap.\n\nIn a later chapter we will explore more regression diagnostics.\n\n\n\n\n\n\n\n\nFigure 29.1: Plots of linear regression and residual to illustrate the assumptions of the model.\n\n\n\n\n\n\n\n29.2.3 Distribution of our estimators\nWith the assumption that the error term is normally distributed, we can find the distributions of our estimates, which turn out to be normal:\n\\[\n\\hat{\\beta}_0\\sim N\\left(\\beta_0, \\sigma\\sqrt{{1\\over n}+{\\bar{x}^2\\over \\sum (x_i-\\bar{x})^2}}\\right)\n\\]\n\\[\n\\hat{\\beta}_1\\sim N\\left(\\beta_1, {\\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}\\right)\n\\]\n\\[\n\\hat{Y}_* \\sim N\\left(\\beta_0+\\beta_1x_*, \\sigma\\sqrt{{1\\over n}+{(x_*-\\bar{x})^2\\over \\sum (x_i-\\bar{x})^2}}\\right)\n\\]\nNotice that all three of these are unbiased, the expected value is equal to the parameter being estimated. Looking at the variance of the slope estimate we can see that is a function of the underlying unexplained variance, \\(\\sigma^2\\) and the data. The denominator is increased by having a larger spread in the explanatory variable. The slope of the estimated line is more stable, less variable, if the independent variable has high variance. That is interesting. If you are designing an experiment, this gives you insight in how to select the range of values for your explanatory variable.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Linear Regression Inference</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Inference.html#inference",
    "href": "Linear-Regression-Inference.html#inference",
    "title": "29  Linear Regression Inference",
    "section": "29.3 Inference",
    "text": "29.3 Inference\nNow that we know how the coefficient estimates and the average predicted values behave, we can perform inference on their true values. Let’s take \\(\\hat{\\beta}_1\\) for demonstration:\n\\[\n\\hat{\\beta}_1\\sim N\\left(\\beta_1, {\\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}\\right)\n\\]\nThus,\n\\[\n{\\hat{\\beta}_1-\\beta_1 \\over {\\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\sim N\\left(0, 1\\right)\n\\]\nHowever, note that the expression on the left depends on error standard deviation, \\(\\sigma\\). In reality, we will not know this value and will have to estimate it with\n\\[\n\\hat{\\sigma}=\\sqrt{{1\\over n-2} \\sum_{i=1}^n \\hat{e}_i^2}\n\\]\nwhere \\(\\hat{e}_i\\) is the observed \\(i\\)th residual (\\(\\hat{e}_i=y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i\\)).\nAs we learned in the last block, if we replace population standard deviation (\\(\\sigma\\)) with an estimation, the resulting random variable no longer has the standard normal distribution. In fact, it can be shown that\n\\[\n{\\hat{\\beta}_1-\\beta_1 \\over {\\hat \\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\sim \\textsf{t}\\left(n-2\\right)\n\\]\nWe only have \\(n-2\\) degrees of freedom because in the estimation of \\(\\sigma^2\\) we had to estimate two parameters, \\(\\beta_0\\) and \\(\\beta_1\\).\nWe can use this information to build a \\((1-\\alpha)*100\\%\\) confidence interval for \\(\\beta_1\\). First, we recognize that\n\\[\n\\mbox{P}\\left(-t_{\\alpha/2,n-2} \\leq {\\hat{\\beta}_1-\\beta_1 \\over {\\hat \\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\leq t_{\\alpha/2,n-2} \\right) = 1-\\alpha\n\\]\nSolving the expression inside the probability statement for \\(\\beta_1\\) yields a confidence interval of\n\\[\n\\beta_1 \\in \\left(\\hat{\\beta_1} \\pm t_{\\alpha/2,n-2}{\\hat \\sigma \\over \\sqrt{\\sum(x_i-\\bar{x})^2}}\\right)\n\\]\nWe can also evaluate the null hypothesis \\(H_0: \\beta_1 =\\beta^*_1\\). If the true value of \\(\\beta_1\\) were \\(\\beta^*_1\\), then the estimated \\(\\hat{\\beta_1}\\) should be around that value. In fact, if \\(H_0\\) were true, the value\n\\[\n{\\hat{\\beta}_1-\\beta^*_1 \\over {\\hat \\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}}\n\\]\nhas the \\(\\textsf{t}\\) distribution with \\(n-2\\) degrees of freedom. Thus, once we collect a sample and obtain the observed \\(\\hat{\\beta_1}\\) and \\(\\hat \\sigma\\), we can calculate this quantity and determine whether it is far enough from zero to reject \\(H_0\\).\nSimilarly, we can use the distribution of \\(\\hat \\beta_0\\) to build a confidence interval or conduct a hypothesis test on \\(\\beta_0\\), but we usually don’t. This has to do with the interpretation of \\(\\beta_0\\).\n\n29.3.1 Starbucks\nThat was a great deal of mathematics and theory. Let’s put it to use on the example from Starbucks. In the file data/starbucks.csv we have nutritional facts for several Starbucks’ food items. We used this data in the homework for last chapter. We will use this data again to illustrate the ideas we have introduced in this section.\nRead in the data.\n\nstarbucks &lt;- read_csv(\"data/starbucks.csv\")  %&gt;%\n  mutate(type=factor(type))\n\n\nExercise:\nSummarize and explore the data.\n\nLet’s look at a summary of the data.\n\nglimpse(starbucks)\n\nRows: 77\nColumns: 7\n$ item     &lt;chr&gt; \"8-Grain Roll\", \"Apple Bran Muffin\", \"Apple Fritter\", \"Banana…\n$ calories &lt;dbl&gt; 350, 350, 420, 490, 130, 370, 460, 370, 310, 420, 380, 320, 3…\n$ fat      &lt;dbl&gt; 8, 9, 20, 19, 6, 14, 22, 14, 18, 25, 17, 12, 17, 21, 5, 18, 1…\n$ carb     &lt;dbl&gt; 67, 64, 59, 75, 17, 47, 61, 55, 32, 39, 51, 53, 34, 57, 52, 7…\n$ fiber    &lt;dbl&gt; 5, 7, 0, 4, 0, 5, 2, 0, 0, 0, 2, 3, 2, 2, 3, 3, 2, 3, 0, 2, 0…\n$ protein  &lt;dbl&gt; 10, 6, 5, 7, 0, 6, 7, 6, 5, 7, 4, 6, 5, 5, 12, 7, 8, 6, 0, 10…\n$ type     &lt;fct&gt; bakery, bakery, bakery, bakery, bakery, bakery, bakery, baker…\n\n\n\ninspect(starbucks)\n\n\ncategorical variables:  \n  name     class levels  n missing\n1 item character     77 77       0\n2 type    factor      7 77       0\n                                   distribution\n1 8-Grain Roll (1.3%) ...                      \n2 bakery (53.2%), petite (11.7%) ...           \n\nquantitative variables:  \n      name   class min  Q1 median  Q3 max       mean         sd  n missing\n1 calories numeric  80 300    350 420 500 338.831169 105.368701 77       0\n2      fat numeric   0   9     13  18  28  13.766234   7.095488 77       0\n3     carb numeric  16  31     45  59  80  44.870130  16.551634 77       0\n4    fiber numeric   0   0      2   4   7   2.220779   2.112764 77       0\n5  protein numeric   0   5      7  15  34   9.480519   8.079556 77       0\n\n\nLet’s predict calories from the carbohydrate content.\n\nExercise:\nCreate a scatterplot of calories and carbohydrate, carbs, content.\n\nFigure 29.2 is the scatterplot.\n\nstarbucks %&gt;%\n  gf_point(calories~carb) %&gt;%\n  gf_labs(x=\"Carbohydrates\",y=\"Calories\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 29.2: Scatterplot of calories and carbohydrate content in Starbucks’ products.\n\n\n\n\n\n\nExercise:\nUse R to fit a linear regression model by regressing calories on carb.\n\nThe results of fitting a linear least squares model is stored in the star_mod object.\n\nstar_mod &lt;- lm(formula = calories ~ carb, data = starbucks)\n\n\nsummary(star_mod)\n\n\nCall:\nlm(formula = calories ~ carb, data = starbucks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.962  -70.556   -0.636   54.908  179.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\ncarb          4.2971     0.5424   7.923 1.67e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 78.26 on 75 degrees of freedom\nMultiple R-squared:  0.4556,    Adjusted R-squared:  0.4484 \nF-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11\n\n\n\n29.3.1.1 Hypothesis test\nIn the second row of the Coefficients portion of the table we have our point estimate, standard error, test statistic, and \\(p\\)-value for the slope.\nThe hypotheses for this output is\n\\(H_0\\): \\(\\beta_1 = 0\\). The true linear model has slope zero. The carb content has no impact on the the calorie content.\n\\(H_A\\): \\(\\beta_1 \\neq 0\\). The true linear model has a slope different than zero. The higher the carb content, the greater the average calorie content or vice-versa.\nOur estimate of the slope is 4.297 with a standard error of 0.5424. Just for demonstration purposes, we will use R to calculate the test statistic and \\(p\\)-value as a series of steps. The test statistic under the null hypothesis is:\n\\[\n{\\hat{\\beta}_1-0 \\over {\\hat \\sigma \\over \\sqrt{ \\sum (x_i-\\bar{x})^2}}}\n\\]\nThe denominator is the standard error of the estimate. The estimate of the residual standard deviation is reported in the last line as 78.26. But it is just the square root of the sum of squared residuals divided by the degrees of freedom.\n\nsighat&lt;-sqrt(sum((star_mod$residuals)^2)/75)\nsighat\n\n[1] 78.25956\n\n\nThe standard error of the slope estimate is, and confirmed in the table:\n\nstd_er&lt;-sighat/sqrt(sum((starbucks$carb-mean(starbucks$carb))^2))\nstd_er\n\n[1] 0.5423626\n\n\nThe test statistic is\n\n(4.2971-0)/std_er\n\n[1] 7.922928\n\n\nAnd the \\(p\\)-value\n\n2*pt((4.2971-0)/std_er,73,lower.tail = FALSE)\n\n[1] 1.965319e-11\n\n\nThis is slightly different from the table value because of the precision of the computer and the small \\(p\\)-value.\nWe reject \\(H_0\\) in favor of \\(H_A\\) because the data provide strong evidence that the true slope parameter is greater than zero.\nThe computer software uses zero in the null hypothesis, if you wanted to test another value of the slope then you would have to do the calculations step by step like we did above.\nBy the way, this was not a tidy way to do the calculation. The broom package makes it easier to use tidy ideas on the regression model. We used these ideas in the last chapter.\nAs a reminder:\n\nlibrary(broom)\n\n\ntidy(star_mod) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   146.      25.9        5.63 2.93e- 7\n2 carb            4.30     0.542      7.92 1.67e-11\n\n\nAnd step by step:\n\ntidy(star_mod) %&gt;%\n  filter(term==\"carb\") %&gt;%\n  summarize(test_stat=(estimate-0)/std.error,p_value=2*pt(test_stat,df=73,lower.tail = FALSE))\n\n# A tibble: 1 × 2\n  test_stat  p_value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      7.92 1.97e-11\n\n\n\n\n29.3.1.2 Confidence interval\nWe could calculate the confidence interval from the point estimate, standard error, and critical value but we will let R do it for us.\n\nconfint(star_mod)\n\n                2.5 %     97.5 %\n(Intercept) 94.387896 197.652967\ncarb         3.216643   5.377526\n\n\nThis confidence interval does not contain the value 0. This suggests that a value of 0 is probably not feasible for \\(\\beta_1\\).\nIn the end, we would declare that carbohydrate and calorie content of Starbucks’ menu items are linearly correlated. However, we DID NOT prove causation. We simply showed that the two variables are correlated.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Linear Regression Inference</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Inference.html#inference-on-predictions",
    "href": "Linear-Regression-Inference.html#inference-on-predictions",
    "title": "29  Linear Regression Inference",
    "section": "29.4 Inference on Predictions",
    "text": "29.4 Inference on Predictions\nSimilarly, we can take advantage of the distribution of \\(\\hat Y_*\\) to build a confidence interval on \\(Y_*\\) (the average value of \\(Y\\) at some value \\(x_*\\)):\n\\[\nY_*\\in \\left(\\hat Y_* \\pm t_{\\alpha/2,n-2}\\hat \\sigma \\sqrt{{1\\over n}+{(x_*-\\bar{x})^2\\over \\sum (x_i-\\bar{x})^2}} \\right)\n\\]\nThere are a couple of things to point out about the above. First, note that the width of the confidence interval is dependent on how far \\(x_*\\) is from the average value of \\(x\\). The further we are from the center of the data, the wider the interval will be.\nSecond, note that this in an interval on \\(Y_*\\) the average value of \\(Y\\) at \\(x_*\\). If we want to build an interval for a single observation of \\(Y\\) (\\(Y_{new}\\)), we will need to build a prediction interval, which is considerably wider than a confidence interval on \\(Y_*\\):\n\\[\nY_{new}\\in \\left(\\hat Y_* \\pm t_{\\alpha/2,n-2}\\hat \\sigma \\sqrt{1+{1\\over n}+{(x_*-\\bar{x})^2\\over \\sum (x_i-\\bar{x})^2}} \\right)\n\\]\n\n29.4.1 Starbucks\nContinuing with the Starbucks example. In plotting the data, we can have R plot the confidence and prediction bands, Figure 29.3. We will observe the width of both of these intervals increase as we move away from the center of the data and also that prediction intervals are wider than the confidence interval.\n\nstarbucks %&gt;%\n  gf_point(calories~carb) %&gt;%\n  gf_labs(x=\"Carbohydrates\",y=\"Calories\") %&gt;%\n  gf_lm(stat=\"lm\",interval=\"confidence\") %&gt;%\n  gf_lm(stat=\"lm\",interval=\"prediction\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 29.3: Confidence and predictions bands for linear regression model of calories and carbs in Starbucks’ products.\n\n\n\n\n\nWe have not done diagnostics yet and it may be that using a linear regression model for this data may not be appropriate. But for the sake of learning we will continue. To find these confidence intervals we need a value for carb so let’s use 60 and 70.\nWe create a data frame with the new values of carb in it. Then we will use the predict function to find the confidence interval. Using the option interval set to confidence will return a confidence interval for the average calorie content for each value in the new data frame.\n\nnew_carb &lt;- data.frame(carb=c(60,70))\npredict(star_mod, newdata = new_carb, interval = 'confidence')\n\n       fit      lwr      upr\n1 403.8455 379.7027 427.9883\n2 446.8163 414.3687 479.2640\n\n\nOr using the broom package.\n\naugment(star_mod,newdata=new_carb,interval=\"confidence\")\n\n# A tibble: 2 × 4\n   carb .fitted .lower .upper\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    60    404.   380.   428.\n2    70    447.   414.   479.\n\n\nAs an example, we are 95% confident that the average calories in a Starbucks’ menu item with 60 grams of carbs is between 379.7 and 428.0.\n\nExercise: Give the 95% confidence interval of average calories for 70 grams of carbohydrates.\n\nWe are 95% confident that the average calories in a Starbucks’ menu item with 70 grams carbs is between 414.4 and 479.3.\nFor the prediction interval, we simply need to change the option in interval:\n\nnew_carb &lt;- data.frame(carb=c(60,70))\npredict(star_mod, newdata = new_carb, interval = 'prediction')\n\n       fit      lwr      upr\n1 403.8455 246.0862 561.6048\n2 446.8163 287.5744 606.0582\n\n\nWe are 95% confident the next Starbucks’ menu item that has 60 grams of carbs will have a calorie content between 246 and 561. Notice how prediction intervals are wider since they are intervals on individual observations and not an averages.\n\nExercise: Give the 90% prediction interval of average calories for 70 grams of carbohydrates.\n\nWe changed the confidence level. Since we are less confident, the interval will be narrower than the 95% prediction interval we just calculated.\n\npredict(star_mod, newdata = new_carb, level=0.9, interval = 'prediction')\n\n       fit      lwr      upr\n1 403.8455 271.9565 535.7345\n2 446.8163 313.6879 579.9448\n\n\nWe are 90% confident the next Starbucks’ menu item that has 70 grams of carbs will have a calorie content between 313.7 and 579.9.\n\n\n29.4.2 Summary\nThis chapter has introduced the process of inference for a simple linear regression model. We tested the slope estimate as well as generated confidence intervals for average and individual predicted values.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Linear Regression Inference</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html",
    "href": "Linear-Regression-Diagnostics.html",
    "title": "30  Regression Diagnostics",
    "section": "",
    "text": "30.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#objectives",
    "href": "Linear-Regression-Diagnostics.html#objectives",
    "title": "30  Regression Diagnostics",
    "section": "",
    "text": "Calculate and interpret the R-squared and F-statistic for a linear regression model. Evaluate these metrics to assess the model’s goodness-of-fit and overall significance.\nUse R to evaluate the assumptions underlying a linear regression model.\nIdentify, analyze, and explain the impact of outliers and leverage points in a linear regression model.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#introduction",
    "href": "Linear-Regression-Diagnostics.html#introduction",
    "title": "30  Regression Diagnostics",
    "section": "30.2 Introduction",
    "text": "30.2 Introduction\nOver the last two chapters, we have detailed simple linear regression. First, we described the model and its underlying assumptions. Next, we obtained parameter estimates using the method of least squares. Finally, we obtained the distributions of parameter estimates and used that information to conduct inference on parameters and predictions. Implementation was relatively straightforward; once we obtained the expressions of interest, we used R to find parameters estimates, interval estimates, etc. In this chapter we will explore more tools to assess the quality of our simple linear regression model. Some of these tools will generalize when we move to multiple predictors.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#assessing-our-model---understanding-the-output-from-lm",
    "href": "Linear-Regression-Diagnostics.html#assessing-our-model---understanding-the-output-from-lm",
    "title": "30  Regression Diagnostics",
    "section": "30.3 Assessing our model - understanding the output from lm",
    "text": "30.3 Assessing our model - understanding the output from lm\nThere is more that we can do with the output from the lm() function than just estimating parameters and predicting responses. There are metrics that allow us to assess the fit of our model. To explore some of these ideas let’s use the Starbucks example again.\nFirst load the data:\n\nstarbucks &lt;- read_csv(\"data/starbucks.csv\")\n\nNext build and summarize the model:\n\nstar_mod &lt;- lm(calories~carb,data=starbucks)\n\n\nsummary(star_mod)\n\n\nCall:\nlm(formula = calories ~ carb, data = starbucks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.962  -70.556   -0.636   54.908  179.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\ncarb          4.2971     0.5424   7.923 1.67e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 78.26 on 75 degrees of freedom\nMultiple R-squared:  0.4556,    Adjusted R-squared:  0.4484 \nF-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11\n\n\nYou may have noticed some other information that appeared in the summary of our model. We discussed the output in a previous chapter but let’s go a little more in depth.\n\n30.3.1 Residual Standard Error\nThe “residual standard error” is the estimate of \\(\\sigma\\), the unexplained variance in our response. In our example, this turned out to be 78.26. If the assumptions of normality and constant variance are valid, we would expect the majority, 68%, of the observed values at a given input to be within \\(\\pm 78.26\\) (one standard deviation) of the mean value. We would expect 95% of the observed values at a given input to be within \\(\\pm 156.52\\) (two standard deviations) of the mean value.\nIf we want to extract just this value from the model object, first recognize that summary(my.model) is a list with several components:\n\nnames(summary(star_mod))\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nAs expected, the sigma component shows the estimated value of \\(\\sigma\\).\n\nsummary(star_mod)$sigma\n\n[1] 78.25956\n\n\nAgain, this value is smaller the closer the points are to the regression fit. It is a measure of unexplained variance in the response variable.\n\n\n30.3.2 R-squared\nAnother quantity that appears is \\(R\\)-squared, also know as the coefficient of determination. \\(R\\)-squared is one measure of goodness of fit. Essentially \\(R\\)-squared is a ratio of variance (in the response) explained by the model to overall variance of the response. It helps to describe the decomposition of variance:\n\\[\n\\underbrace{\\sum_{i=1}^n (y_i-\\bar{y})^2}_{SS_{\\text{Total}}} = \\underbrace{\\sum_{i=1}^n (\\hat{y}_i-\\bar y)^2}_{SS_{\\text{Regression}}}+\\underbrace{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}_{SS_{\\text{Error}}}\n\\]\nIn other words, the overall variation in \\(y\\) can be separated into two parts: variation due to the linear relationship between \\(y\\) and the predictor variable(s), called \\(SS_\\text{Regression}\\), and residual variation (due to random scatter or perhaps a poorly chosen model), called \\(SS_\\text{Error}\\). Note: \\(SS_\\text{Error}\\) is used to estimate residual standard error in the previous section.1\n\\(R\\)-squared simply measures the ratio between \\(SS_\\text{Regression}\\) and \\(SS_\\text{Total}\\). A common definition of \\(R\\)-squared is the proportion of overall variation in the response that is explained by the linear model. \\(R\\)-squared can be between 0 and 1. Values of \\(R\\)-squared close to 1 indicate a tight fit (little scatter) around the estimated regression line. Value close to 0 indicate the opposite (large remaining scatter).\nWe can obtain \\(R\\)-squared “by hand” or by using the output of the lm() function:\n\nsummary(star_mod)$r.squared\n\n[1] 0.4556237\n\n\nFor simple linear regression, \\(R\\)-squared is related to correlation. We can compute the correlation using a formula, just as we did with the sample mean and standard deviation. However, this formula is rather complex,2 so we let R do the heavy lifting for us.\n\nstarbucks %&gt;%\n  summarize(correlation=cor(carb,calories),correlation_squared=correlation^2)\n\n# A tibble: 1 × 2\n  correlation correlation_squared\n        &lt;dbl&gt;               &lt;dbl&gt;\n1       0.675               0.456\n\n\nAs a review, Figure 30.1 below shows eight plots and their corresponding correlations. Only when the relationship is perfectly linear is the correlation either -1 or 1. If the relationship is strong and positive, the correlation will be near +1. If it is strong and negative, it will be near -1. If there is no apparent linear relationship between the variables, then the correlation will be near zero.\n\n\n\n\n\n\n\n\nFigure 30.1: Scatterplots demonstrating different correlations.\n\n\n\n\n\n\nExercise\nIf a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?3\n\nNote that one of the components of summary(lm()) function is adj.r.squared. This is a value of \\(R\\)-squared adjusted for number of predictors. This idea is covered more in depth in a machine learning course.\n\n\n30.3.3 F-Statistic\nAnother quantity that appears in the summary of the model is the \\(F\\)-statistic. This value evaluates the null hypothesis that all of the non-intercept coefficients are equal to 0. Rejecting this hypothesis implies that the model is useful in the sense that at least one of the predictors shares a significant linear relationship with the response.\n\\(H_0\\): \\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)\n\\(H_a\\): At least one coefficient not equal to 0.\nwhere \\(p\\) is the number of predictors in the model. Just like in ANOVA, this is a simultaneous test of all coefficients and does not inform us which one(s) are different from 0.\nThe \\(F\\)-statistic is given by\n\\[\n{n-p-1 \\over p}{\\sum (\\hat{y}_i-\\bar{y})^2\\over \\sum e_i^2}\n\\]\nUnder the null hypothesis, the \\(F\\)-statistic follows the \\(F\\) distribution with parameters \\(p\\) and \\(n-p-1\\).\nIn our example, the \\(F\\)-statistic is redundant since there is only one predictor. In fact, the \\(p\\)-value associated with the \\(F\\)-statistic is equal to the \\(p\\)-value associated with the estimate of \\(\\beta_1\\). However, when we move to cases with more predictor variables, we may be interested in the \\(F\\)-statistic.\n\nsummary(star_mod)$fstatistic\n\n   value    numdf    dendf \n62.77234  1.00000 75.00000",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#regression-diagnostics",
    "href": "Linear-Regression-Diagnostics.html#regression-diagnostics",
    "title": "30  Regression Diagnostics",
    "section": "30.4 Regression diagnostics",
    "text": "30.4 Regression diagnostics\nFinally, we can use the lm object to check the assumptions of the model. We have discussed the assumptions before but in this chapter we will use R to generate visual checks. There are also numeric diagnostic measures.\nThere are several potential problems with a regression model:\n\nAssumptions about the error structure. We assume:\n\n\n\nthe errors are normally distributed\n\nthe errors are independent\n\nthe errors have constant variance, homoskedastic\n\n\nAssumptions about the fit. We assume that fit of the model is correct. For a simple linear regression, this means that fit specified by the formula in lm is correct.\nProblems with outliers and leverage points. In this case a small number of points in the data could have an unusually large impact on the parameter estimates. These points may give a mistaken sense that our model has a great fit or conversely that there is not relationship between the variables.\nMissing predictors. We can potentially improve the fit and predictive performance of our model by including other predictors. We will spend one chapter on this topic, but machine learning courses devote more time to discussing how to build these more complex models. In the case of multivariate linear regression, many of the diagnostic tools discussed next will also be applicable.\n\n\n30.4.1 Residual plots\nThe assumptions about the error structure can be checked with residual plots. We have already done this, but let’s review again and provide a little more depth.\nApplying the plot() function to an “lm” object provides several graphs that allow us to visually evaluate a linear model’s assumptions. There are actually six plots (selected by the which option) available:\n\na plot of residuals against fitted values,\n\na Scale-Location plot of \\(\\sqrt(| \\text{residuals} |)\\) against fitted values,\n\na Normal Q-Q plot,\n\na plot of Cook’s distances versus row labels,\n\na plot of residuals against leverages,\n\nand a plot of Cook’s distances against leverage/(1-leverage).\n\nBy default, the first three and the fifth are provided by applying plot() to an “lm” object. To obtain all four at once, simply use plot(my.model) at the command line.\n\nplot(star_mod)\n\n\n\n\n\n\n\nFigure 30.2: Regression diagnostic plots.\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.3: Regression diagnostic plots.\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.4: Regression diagnostic plots.\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.5: Regression diagnostic plots.\n\n\n\n\n\nHowever, it’s best to walk through each of these four plots in our Starbucks example.\n\n\n30.4.2 Residuals vs Fitted\nBy providing a number in the which option, we can select the plot we want, Figure 30.6 is the first diagnostic plot.\n\nplot(star_mod,which = 1)\n\n\n\n\n\n\n\nFigure 30.6: A diagnostic residual plot.\n\n\n\n\n\nThis plot assesses linearity of the model and homoscedasticity (constant variance). The red line is a smoothed estimate of the fitted values versus the residuals. Ideally, the red line should coincide with the dashed horizontal line and the residuals should be centered around this dashed line. This would indicate that a linear fit is appropriate. Furthermore, the scatter around the dashed line should be relatively constant across the plot, homoscedasticity. In this case, it looks like there is some minor concern over linearity and non-constant error variance. We noted this earlier with the cluster of points in the lower left hand corner of the scatterplot.\nNote: the points that are labeled are points with a high residual value. They are extreme. We will discuss outliers and leverage points shortly.\n\n\n30.4.3 Normal Q-Q Plot\nAs it’s name suggests, this plot evaluates the normality of the residuals. We have seen and used this plot several times in this book. Remember if the number of data points is small, this plot has a greatly reduced effectiveness.\n\nplot(star_mod,which = 2)\n\n\n\n\n\n\n\nFigure 30.7: The quantile-quantile plot for checking normality.\n\n\n\n\n\nAlong the \\(y\\)-axis are the actual standardized residuals. Along the \\(x\\)-axis is where those points should be if the residuals were actually normally distributed. Ideally, the dots should fall along the diagonal dashed line. In Figure 30.7, it appears there is some skewness to the right or just longer tails than a normal distribution. We can tell this because for the smaller residuals, they don’t increase as they should to match a normal distribution, the points are above the line. This is concerning.\n\n\n30.4.4 Scale-Location Plot\nThe scale-location plot is a better indicator of non-constant error variance. It is a plot of fitted values versus square root of the absolute value of the standardized residuals. A standardized residual is the residual divided by its standard deviation\n\\[\ne^{'}_i=\\frac{e_i}{s}\n\\]\nThis plot illustrates the spread of the residuals over the entire range of the predictor. We are using fitted values because this will generalize well if we have more than one predictor.\n\nplot(star_mod,which=3)\n\n\n\n\n\n\n\nFigure 30.8: A scale-location diagnostic residual plot.\n\n\n\n\n\nA straight horizontal red line indicates constant error variance. In this case, Figure 30.8, there is some indication error variance is higher for lower carb counts.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#outliers-and-leverage",
    "href": "Linear-Regression-Diagnostics.html#outliers-and-leverage",
    "title": "30  Regression Diagnostics",
    "section": "30.5 Outliers and leverage",
    "text": "30.5 Outliers and leverage\nBefore discussing the last plot, we need to spend some time discussing outliers. Outliers in regression are observations that fall far from the “cloud” of points. These points are especially important because they can have a strong influence on the least squares line.\nIn regression, there are two types of outliers:\n- An outlier in the response variable is one that is not predicted well by the model. This could either be a problem with the data or the model. The residuals for this outlier will be large in absolute value.\n- An outlier in the explanatory variable. These are typically called leverage points because they can have a undue impact on the parameter estimates. With multiple predictors, we can have a leverage point when we have an unusual combination of the predictors.\nAn outlier is a influential point if it drastically alters the regression output. For example by causing large changes in the estimated slope or hypothesis \\(p\\)-values, if it is omitted.\n\nExercise:\nThere are six plots shown in Figure 30.9 along with the least squares line and residual plots. For each scatterplot and residual plot pair, identify any obvious outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points.\n\n\n\n\n\n\n\n\n\nFigure 30.9: Examples of outliers and leverage points.\n\n\n\n\n\n\nThere is one outlier far from the other points, though it only appears to slightly influence the line. This is an outlier in the response and will have a large residual in magnitude.\n\nThere is one outlier on the right, though it is quite close to the least squares line, which suggests it wasn’t very influential although it is a leverage point.\n\nThere is one point far away from the cloud, and this leverage point appears to pull the least squares line up on the right; examine how the line around the primary cloud doesn’t appear to fit very well. This point has a high influence on the estimated slope.\n\nThere is a primary cloud and then a small secondary cloud of four outliers. The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. There might be an interesting explanation for the dual clouds, which is something that could be investigated.\n\nThere is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line. This point is an outlier in both the response and predictor. It is a highly influential point.\n\nThere is one outlier in both the response and predictor, thus a leverage point, far from the cloud, however, it falls quite close to the least squares line and does not appear to be very influential.\n\nExamining the residual plots in Figure 30.9, you will probably find that there is some trend in the main clouds of (3) and (4). In these cases, the outliers influenced the slope of the least squares lines. In (5), data with no clear trend were assigned a line with a large trend simply due to one outlier!\n\nLeverage\nPoints that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage.\n\nPoints that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line – as in cases (3), (4), and (5) – then we call it an influential point. Usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line. Leverage can be calculated from what is called the hat matrix, the actual mathematics is beyond the scope of this book.\nA point can be an outlier but not a leverage point as we have already discussed. It is tempting to remove outliers from your data set. Don’t do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings – the ``outliers’’ – they would soon go bankrupt by making poorly thought-out investments.\n\n30.5.1 Residuals vs Leverage Plot\nThe residuals vs leverage plot is a good way to identify influential observations. Sometimes, influential observations are representative of the population, but they could also indicate an error in recording data, or an otherwise unrepresentative outlier. It could be worth looking into these cases.\n\nplot(star_mod,5)\n\n\n\n\n\n\n\nFigure 30.10: Diagnostic plots for Starbucks regression model.\n\n\n\n\n\nFigure 30.10 helps us to find influential cases, those leverage points that impact the estimated slope. Unlike the other plots, patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook’s distance. In our particular plot, a dotted line for Cook’s distance was outside the bounds of the plot and thus did not come into play. When cases are outside of the Cook’s distance (meaning they have high Cook’s distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases. In this example, there are no points that tend to have undue influence.\n\n\n30.5.2 What If Our Assumptions Are Violated\nIf the assumptions of the model are violated and/or we have influential points, a linear regression model with normality assumptions is not appropriate. Sometimes it is appropriate to transform the data (either response or predictor), so that the assumptions are met on the transformed data. Other times, it is appropriate to explore other models. There are entire courses on regression where blocks of material are devoted to diagnostics and transformations to reduce the impact of violations of assumptions. We will not go into these methods in this book. Instead, when confronted with clear violated assumptions, we will use resampling as a possible solution. We will learn about this in the next chapter because it does not assume normality in the residuals. This is a limited solution, but as this is an introductory text, this is an excellent first step.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Linear-Regression-Diagnostics.html#footnotes",
    "href": "Linear-Regression-Diagnostics.html#footnotes",
    "title": "30  Regression Diagnostics",
    "section": "",
    "text": "\\(\\hat{\\sigma}=\\sqrt\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\text{degrees of freedom}}\\)↩︎\nFormally, we can compute the correlation for observations \\((x_1, y_1)\\), \\((x_2, y_2)\\), …, \\((x_n, y_n)\\) using the formula \\[\nR = \\frac{1}{n-1}\\sum_{i=1}^{n} \\frac{x_i-\\bar{x}}{s_x}\\frac{y_i-\\bar{y}}{s_y}\n\\] where \\(\\bar{x}\\), \\(\\bar{y}\\), \\(s_x\\), and \\(s_y\\) are the sample means and standard deviations for each variable.↩︎\nAbout \\(R^2 = (-0.97)^2 = 0.94\\) or 94% of the variation is explained by the linear model.↩︎",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html",
    "href": "Simulation-Based-Linear-Regression.html",
    "title": "31  Simulation-Based Linear Regression",
    "section": "",
    "text": "31.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#objectives",
    "href": "Simulation-Based-Linear-Regression.html#objectives",
    "title": "31  Simulation-Based Linear Regression",
    "section": "",
    "text": "Apply the bootstrap to generate and interpret confidence intervals and estimates of standard error for parameter estimates in a linear regression model.\nApply the bootstrap to generate and interpret confidence intervals for predicted values from a linear regression model.\nGenerate bootstrap samples using two methods: sampling rows of the data and sampling residuals. Justify why you might prefer one method over the other.\nGenerate and interpret regression coefficients in a linear regression model with categorical explanatory variables.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#introduction",
    "href": "Simulation-Based-Linear-Regression.html#introduction",
    "title": "31  Simulation-Based Linear Regression",
    "section": "31.2 Introduction",
    "text": "31.2 Introduction\nIn the last couple of chapters we examined how to perform inference for a simple linear regression model assuming the errors were independent normally distributed random variables. We examined diagnostic tools to check assumptions and look for outliers. In this chapter we will use the bootstrap to create confidence and prediction intervals.\nThere are at least two ways we can consider creating a bootstrap distribution for a linear model. We can easily fit a linear model to a resampled data set. But in some situations this may have undesirable features. Influential observations, for example, will appear duplicated in some resamples and be missing entirely from other resamples.\nAnother option is to use “residual resampling”. In residual resampling, the new data set has all of the predictor values from the original data set and a new response is created by adding to the fitted function a resampled residual.\nIn summary, suppose we have \\(n\\) observations, each with \\(Y\\) and some number of \\(X\\)’s, with each observation stored as a row in a data set. The two basic procedures when bootstrapping regression are:\na. bootstrap observations, and\nb. bootstrap residuals.\nThe latter is a special case of a more general rule: sample \\(Y\\) from its estimated conditional distribution given \\(X\\).\nIn bootstrapping observations, we sample with replacement from the rows of the data; each \\(Y\\) comes with the corresponding \\(X\\)’s. In any bootstrap sample some observations may be repeated multiple times, and others not included. This is the same idea we used before when we used the bootstrap for hypothesis testing.\nIn bootstrapping residuals, we fit the regression model, compute predicted values \\(\\hat{Y}_i\\) and residuals \\(e_i = Y_i - \\hat{Y}_i\\), then create a bootstrap sample using the same \\(X\\) values as in the original data, but with new \\(Y\\) values obtained using the prediction plus a random residual, \\(Y^{*}_i = \\hat{Y}_i + e^{*}_i\\), where the residuals \\(e^{*}_i\\) are sampled randomly with replacement from the original residuals. We still have the chance of selecting a large residual from an outlier, but if it paired with an \\(x\\) value near \\(\\bar{x}\\), it will have little leverage.\nBootstrapping residuals corresponds to a designed experiment, where the \\(x\\) values are fixed and only \\(Y\\) is random. If we bootstrap observations, then essentially both \\(X\\) and \\(Y\\) are sampled. By the principle of sampling the way the data were drawn, the second method implies that both \\(Y\\) and \\(X\\) are random.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#confidence-intervals-for-parameters",
    "href": "Simulation-Based-Linear-Regression.html#confidence-intervals-for-parameters",
    "title": "31  Simulation-Based Linear Regression",
    "section": "31.3 Confidence intervals for parameters",
    "text": "31.3 Confidence intervals for parameters\nTo build a confidence interval for the slope parameter, we will resample the data or residuals and generate a new regression model. This process does not assume normality of the residuals. We will use functions from the mosaic package to complete this work. However, know that tidymodels and purrr are more sophisticated tools for doing this work.\n\n31.3.1 Resampling\nTo make this ideas more salient, let’s use the Starbucks example again.\nFirst read the data into R:\n\nstarbucks &lt;- read_csv(\"data/starbucks.csv\")\n\nBuild the model:\n\nstar_mod &lt;- lm(calories~carb,data=starbucks)\n\nLet’s see the output of the model:\n\nsummary(star_mod)\n\n\nCall:\nlm(formula = calories ~ carb, data = starbucks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.962  -70.556   -0.636   54.908  179.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\ncarb          4.2971     0.5424   7.923 1.67e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 78.26 on 75 degrees of freedom\nMultiple R-squared:  0.4556,    Adjusted R-squared:  0.4484 \nF-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11\n\n\nIn preparation for resampling, let’s see how do() treats a linear model object.\n\nset.seed(401)\nobs&lt;-do(1)*star_mod\nobs\n\n  Intercept     carb    sigma r.squared        F numdf dendf .row .index\n1  146.0204 4.297084 78.25956 0.4556237 62.77234     1    75    1      1\n\n\nNice. To resample the data we use do() with resample(). This will sample the rows, what we were referring to above as the first method.\n\ndo(2)*lm(calories~carb,data=resample(starbucks))\n\n  Intercept     carb    sigma r.squared        F numdf dendf .row .index\n1  145.6345 4.089065 73.32243 0.4980692 74.42299     1    75    1      1\n2  160.0193 3.828742 66.81016 0.4298148 56.53621     1    75    1      2\n\n\nPerfect, we are ready to scale up.\n\nset.seed(532)\nresults &lt;- do(1000)*lm(calories~carb,data=resample(starbucks))\n\nNow let’s look at the first 6 rows of results.\n\nhead(results)\n\n  Intercept     carb    sigma r.squared        F numdf dendf .row .index\n1  154.7670 4.176327 78.94717 0.4127581 52.71568     1    75    1      1\n2  166.8589 3.807697 72.09482 0.4032196 50.67437     1    75    1      2\n3  105.3658 4.899956 77.62517 0.5310212 84.92195     1    75    1      3\n4  227.4138 2.805156 79.97902 0.2467094 24.56317     1    75    1      4\n5  194.9190 3.457191 83.74624 0.2670279 27.32313     1    75    1      5\n6  183.1159 3.549460 73.90153 0.3931691 48.59292     1    75    1      6\n\n\nIf we plot all the slopes, the red lines in Figure 31.1, we get a sense of the variability in the estimated slope and intercept. This also gives us an idea of the width of the confidence interval on the estimated mean response. We plotted the confidence interval in a gray shade and we can see it matches the red shaded region of the bootstrap slopes. We can see that the confidence interval will be wider at the extreme values of the predictor.\n\nggplot(starbucks, aes(x=carb, y=calories)) +\n  geom_abline(data = results,\n              aes(slope =  carb, intercept = Intercept), \n              alpha = 0.01,color=\"red\") +\n  geom_point() +\n  theme_classic() +\n  labs(x=\"Carbohydrates (g)\",y=\"Calories\",title=\"Bootstrap Slopes\",subtitle =\"1000 Slopes\") +\n  geom_lm(interval=\"confidence\")\n\n\n\n\n\n\n\nFigure 31.1: Plot of slopes from resampled regression.\n\n\n\n\n\nWith all this data in results, we can generate confidence intervals for the slope, \\(R\\)-squared (\\(R^2\\)), and the \\(F\\) statistic. Figure 31.2 is a histogram of slope values from resampling.\n\nresults %&gt;%\n  gf_histogram(~carb,fill=\"cyan\",color = \"black\") %&gt;%\n  gf_vline(xintercept = obs$carb,color=\"red\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"Carbohydrate regression slope.\",y=\"\")\n\n\n\n\n\n\n\nFigure 31.2: Histogram of slopes from resampled regression.\n\n\n\n\n\nThe confidence interval is found using cdata().\n\ncdata(~carb,data=results,p=0.95)\n\n        lower    upper central.p\n2.5% 3.166546 5.377743      0.95\n\n\nWe are 95% confident that the true slope is between 3.17 and 5.37. As a reminder, using the normality assumption we had a 95% confidence interval of \\((3.21,5.38)\\):\n\nconfint(star_mod)\n\n                2.5 %     97.5 %\n(Intercept) 94.387896 197.652967\ncarb         3.216643   5.377526\n\n\nThe bootstrap confidence interval for \\(R^2\\) is:\n\ncdata(~r.squared,data=results)\n\n         lower     upper central.p\n2.5% 0.2837033 0.6234751      0.95\n\n\nAnd the bootstrap sampling distribution of \\(R^2\\) is displayed in Figure 31.3.\n\nresults %&gt;%\n  gf_histogram(~r.squared,fill=\"cyan\",color=\"black\") %&gt;%\n  gf_vline(xintercept = obs$r.squared,color=\"red\") %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(y=\"\",x=expression(R^2))\n\n\n\n\n\n\n\nFigure 31.3: A histogram of the \\(R^2\\) values from resampled regression.\n\n\n\n\n\nThis is nice work. So powerful.\nLet’s see how we could accomplish this same work using the infer package.\n\nlibrary(infer)\n\nTo check that we can use this package, let’s find the slope estimate.\n\nslope_estimate &lt;- starbucks %&gt;%\n  specify(calories ~ carb) %&gt;%\n  calculate(stat=\"slope\")\n\n\nslope_estimate\n\nResponse: calories (numeric)\nExplanatory: carb (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  4.30\n\n\nGood, let’s get the bootstrap sampling distribution of the regression slope.\n\nresults2 &lt;- starbucks %&gt;%\n  specify(calories~carb) %&gt;%\n  generate(reps=1000,type=\"bootstrap\") %&gt;%\n  calculate(stat=\"slope\")\nhead(results2)\n\nResponse: calories (numeric)\nExplanatory: carb (numeric)\n# A tibble: 6 × 2\n  replicate  stat\n      &lt;int&gt; &lt;dbl&gt;\n1         1  3.75\n2         2  5.43\n3         3  3.76\n4         4  4.69\n5         5  4.38\n6         6  3.63\n\n\nNext the confidence interval.\n\nslope_ci&lt;-results2 %&gt;%\n  get_confidence_interval(level=0.95)\nslope_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     3.14     5.26\n\n\nThis matches the work we have already done. Finally, let’s visualize the results, Figure 31.4.\n\nresults2 %&gt;%\n  visualize() +\n  shade_confidence_interval(slope_ci,color=\"blue\",fill=\"lightblue\") +\n  geom_vline(xintercept = slope_estimate$stat,color=\"black\",size=2) +\n  labs(x=\"Estimated Slope\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 31.4: Sampling distribution of the slope using resampling. (Black line is estimate slope from original data and blue lines are the confidence bounds.)\n\n\n\n\n\n\n\n31.3.2 Resample residuals\nWe could also resample the residuals instead of the data. This makes a stronger assumption that the linear model is appropriate. However, it guarantees that every \\(X\\) value is in the resampled data frame. In the lm function, we send the model instead of the data to resample the residuals. Since R is an object oriented programming language, in sending a model object to the resample() function, the code automatically resamples from the residuals.\n\nresults_resid &lt;- do(1000)*lm( calories~carb, data = resample(star_mod)) # resampled residuals\nhead(results_resid)\n\n  Intercept     carb    sigma r.squared        F numdf dendf .row .index\n1  151.9999 4.356740 73.07024 0.4967052 74.01804     1    75    1      1\n2  101.6226 5.280410 82.24346 0.5336627 85.82779     1    75    1      2\n3  152.4453 4.346918 82.64249 0.4344055 57.60383     1    75    1      3\n4  159.1311 3.846912 84.42784 0.3656236 43.22634     1    75    1      4\n5  167.9957 3.981328 67.50240 0.4912807 72.42905     1    75    1      5\n6  198.5458 3.239953 86.11143 0.2821237 29.47482     1    75    1      6\n\n\nNext a plot of the bootstrap sampling distribution, Figure 31.5.\n\nresults_resid %&gt;%\n  gf_histogram(~carb,fill=\"cyan\",color=\"black\") %&gt;%\n  gf_vline(xintercept = obs$carb,color=\"red\") %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x=\"Estimated slope of carbs\",y=\"\")\n\n\n\n\n\n\n\nFigure 31.5: Histogram of estimated regression slope using resampling from residuals.\n\n\n\n\n\nAnd finally the confidence interval for the slope.\n\ncdata(~carb,data=results_resid)\n\n       lower    upper central.p\n2.5% 3.24622 5.323031      0.95\n\n\nSimilar to the previous bootstrap confidence interval just a little narrower.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#confidence-intervals-for-prediction",
    "href": "Simulation-Based-Linear-Regression.html#confidence-intervals-for-prediction",
    "title": "31  Simulation-Based Linear Regression",
    "section": "31.4 Confidence intervals for prediction",
    "text": "31.4 Confidence intervals for prediction\nWe now want to generate a confidence interval for the average calories from 60 grams of carbohydrates.\nUsing the normal assumption, we had\n\npredict(star_mod,newdata = data.frame(carb=60),interval=\"confidence\")\n\n       fit      lwr      upr\n1 403.8455 379.7027 427.9883\n\n\nWe have all the bootstrap slope and intercept estimates in the results object. We can use tidyverse functions to find the confidence interval by predicting the response for each of these slope and intercept estimate.\n\nhead(results)\n\n  Intercept     carb    sigma r.squared        F numdf dendf .row .index\n1  154.7670 4.176327 78.94717 0.4127581 52.71568     1    75    1      1\n2  166.8589 3.807697 72.09482 0.4032196 50.67437     1    75    1      2\n3  105.3658 4.899956 77.62517 0.5310212 84.92195     1    75    1      3\n4  227.4138 2.805156 79.97902 0.2467094 24.56317     1    75    1      4\n5  194.9190 3.457191 83.74624 0.2670279 27.32313     1    75    1      5\n6  183.1159 3.549460 73.90153 0.3931691 48.59292     1    75    1      6\n\n\n\nresults %&gt;%\n  mutate(pred=Intercept+carb*60) %&gt;%\n  cdata(~pred,data=.)\n\n        lower    upper central.p\n2.5% 385.2706 423.6689      0.95\n\n\nThis is similar to the interval we found last chapter. We are 95% confident that the average calorie content for a menu item with 60 grams of carbohydrates is between 380.8 and 425.7.\n\n31.4.1 Prediction interval\nThe prediction interval is more difficult to perform with a bootstrap. We would have to account for the variability of the slope but also the residual variability since this is an individual observation. We can’t just add the residual to the predicted value. Remember the variance of a sum of independent variables is the sum of the variances. But here we have standard deviations and we can’t just add them.\nLet’s look at what would happen if we try. First as a reminder, the prediction interval at 60 grams of carb using the assumption of normally distributed errors from last chapter is:\n\npredict(star_mod,newdata = data.frame(carb=60),interval=\"prediction\")\n\n       fit      lwr      upr\n1 403.8455 246.0862 561.6048\n\n\nIf we are generating a bootstrap of size 1000, we will resample from the residuals 1000 times.\n\nresults %&gt;%\n  mutate(pred=Intercept+carb*60) %&gt;% \n  cbind(resid=sample(star_mod$residuals,size=1000,replace = TRUE)) %&gt;%\n  mutate(pred_ind=pred+resid) %&gt;%\n  cdata(~pred_ind,data=.)\n\n        lower    upper central.p\n2.5% 277.4886 577.0957      0.95\n\n\nThis prediction interval appears to be biased. Thus generating a prediction interval is beyond the scope of this book.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#categorical-predictor",
    "href": "Simulation-Based-Linear-Regression.html#categorical-predictor",
    "title": "31  Simulation-Based Linear Regression",
    "section": "31.5 Categorical predictor",
    "text": "31.5 Categorical predictor\nWe want to finish up simple linear regression by discussing a categorical predictor. It somewhat changes the interpretation of the regression model.\nThus far, we have only discussed regression in the context of a quantitative, continuous, response AND a quantitative, continuous, predictor. We can build linear models with categorical predictor variables as well.\nIn the case of a binary covariate, nothing about the linear model changes. The two levels of the binary covariate are typically coded as 1 and 0, and the model is built, evaluated and interpreted in an analogous fashion as before. The difference between the continuous predictor and categorical is that there are only two values the predictor can take and the regression model will simply predict the average value of the response within each value of the predictor.\nIn the case of a categorical covariate with \\(k\\) levels, where \\(k&gt;2\\), we need to include \\(k-1\\) dummy variables in the model. Each of these dummy variables takes the value 0 or 1. For example, if a covariate has \\(k=3\\) categories or levels (say A, B or C), we create two dummy variables, \\(X_1\\) and \\(X_2\\), each of which can only take values 1 or 0. We arbitrarily state that if \\(X_1=1\\), it represents the covariate has the value A. Likewise if \\(X_2=1\\), then we state that the covariate takes the value B. If both \\(X_1=0\\) and \\(X_2=0\\), this is known as the reference category, and in this case the covariate takes the value C. The arrangement of the levels of the categorical covariate are arbitrary and can be adjusted by the user. This coding of the covariate into dummy variables is called contrasts and again is typically taught in a more advanced course on linear models.\nIn the case \\(k=3\\), the linear model is \\(Y=\\beta_0 + \\beta_1X_1 + \\beta_2X_2+e\\).\nWhen the covariate takes the value A, \\(\\mbox{E}(Y|X=A)=\\beta_0 + \\beta_1\\).\nWhen the covariate takes the value B, \\(\\mbox{E}(Y|X=B)=\\beta_0 + \\beta_2\\).\nWhen the covariate takes the value C, \\(\\mbox{E}(Y|X=C)=\\beta_0\\).\nBased on this, think about how you would interpret the coefficients \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\).\n\n31.5.1 Lending Club\nLet’s do an example with some data.\nThe Lending Club data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. Of course, not all loans are created equal. Someone who is essentially a sure bet to pay back a loan will have an easier time getting a loan with a low interest rate than someone who appears to be riskier. And for people who are very risky? They may not even get a loan offer, or they may not have accepted the loan offer due to a high interest rate. It is important to keep that last part in mind, since this data set only represents loans actually made, i.e. do not mistake this data for loan applications! The data set is loans.csv from the data folder.\n\nloans &lt;- read_csv(\"data/loans.csv\")\n\nLet’s look at the size of the data:\n\ndim(loans)\n\n[1] 10000    55\n\n\nThis is a big data set. For educational purposes, we will sample 100 points from the original data. We only need the variables interest_rate and homeownership. First let’s break down the homeownership variable.\n\ntally(~homeownership,data=loans,format=\"proportion\")\n\nhomeownership\nMORTGAGE      OWN     RENT \n  0.4789   0.1353   0.3858 \n\n\nWe want to sample the data so that each level of home ownership has the same proportion as the original, a stratified sample.\n\nset.seed(905)\nloans100 &lt;- loans %&gt;%\n  select(interest_rate,homeownership) %&gt;%\n  mutate(homeownership=factor(homeownership)) %&gt;%\n  group_by(homeownership) %&gt;%\n  slice_sample(prop=0.01) %&gt;%\n  ungroup()\n\n\ndim(loans100)\n\n[1] 98  2\n\n\nNot quite a 100 observations, but we preserved the proportion of homeownership.\n\ntally(~homeownership,data=loans100,format=\"proportion\")\n\nhomeownership\n MORTGAGE       OWN      RENT \n0.4795918 0.1326531 0.3877551 \n\n\nLet’s look at the data with a boxplot, Figure 31.6.\n\nloans100 %&gt;%\n  gf_boxplot(interest_rate~homeownership) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(title=\"Lending Club\",x=\"Homeownership\",y=\"Interest Rate\")\n\n\n\n\n\n\n\nFigure 31.6: Boxplot of loan interest rates from the Lending Club based on homeownership status.\n\n\n\n\n\nIt appears that there is some evidence that home ownership impacts the interest rate. We can build a linear model to explore whether this difference in significant. We can use the lm() function in R, but in order to include a categorical predictor, we need to make sure that variable is stored as a “factor” type. If it is not, we’ll need to convert it.\n\nstr(loans100)\n\ntibble [98 × 2] (S3: tbl_df/tbl/data.frame)\n $ interest_rate: num [1:98] 19.03 9.44 6.07 7.96 10.9 ...\n $ homeownership: Factor w/ 3 levels \"MORTGAGE\",\"OWN\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nNow we can build the model:\n\nloan_mod&lt;-lm(interest_rate ~ homeownership,data=loans100)\nsummary(loan_mod)\n\n\nCall:\nlm(formula = interest_rate ~ homeownership, data = loans100)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5442 -3.1472  0.1628  2.1228 12.9658 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        10.4972     0.5889  17.825  &lt; 2e-16 ***\nhomeownershipOWN    2.6135     1.2652   2.066  0.04158 *  \nhomeownershipRENT   2.3570     0.8808   2.676  0.00878 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.037 on 95 degrees of freedom\nMultiple R-squared:  0.08517,   Adjusted R-squared:  0.06591 \nF-statistic: 4.422 on 2 and 95 DF,  p-value: 0.01458\n\n\nNote that by default, R set the MORTGAGE level as the reference category. This is because it is first value when sorted alphabetically. You can control this by changing the order of the factor levels. The package forcats helps with this effort.\nHow would we interpret this output? Since MORTGAGE is the reference category, the intercept is effectively the estimated, average, interest rate for home owners with a mortgage.\n\nloans100 %&gt;%\n  filter(homeownership == \"MORTGAGE\") %&gt;%\n  summarise(average=mean(interest_rate))\n\n# A tibble: 1 × 1\n  average\n    &lt;dbl&gt;\n1    10.5\n\n\nThe other terms represent the expected difference in average interest rates for the ownership types.\n\nloans100 %&gt;%\n  group_by(homeownership) %&gt;%\n  summarise(average=mean(interest_rate),std_dev=sd(interest_rate))\n\n# A tibble: 3 × 3\n  homeownership average std_dev\n  &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 MORTGAGE         10.5    3.44\n2 OWN              13.1    2.89\n3 RENT             12.9    4.94\n\n\nSpecifically, on average, loan interest rates for home owners who own their home is 2.61 percentage points higher than those with a mortgage. Those who rent is 2.36 percent higher on average. The highest interest rate is for those who own their own home. This seems odd but it is interesting.\n\nExercise:\nUsing the coefficient from the regression model, how do we find the difference in average interest rates between home owners and renters?\n\nThe first coefficient \\[\\beta_\\text{homeownershipOWN} = \\mu_\\text{OWN} - \\mu_\\text{MORTGAGE}\\]\nand \\[\\beta_\\text{homeownershipRENT} = \\mu_\\text{RENT} - \\mu_\\text{MORTGAGE}.\\] Thus \\[\\mu_\\text{OWN} -\\mu_\\text{RENT} = \\beta_\\text{homeownershipOWN} - \\beta_\\text{homeownershipRENT},\\] the difference in coefficients.\nThe model is not fitting a line to the data but just estimating average with the coefficients representing difference from the reference level.\nThe Std.Error, t value, and Pr(&gt;|t|) values can be used to conduct inference about the respective estimates. Both \\(p\\)-values are significant. This is similar to the ANOVA analysis we conducted last block except that hypothesis test was simultaneously testing all coefficients and here we are testing them pairwise.\n\n\n31.5.2 Bootstrap\nFrom the boxplots, the biggest difference in means is between home owners and those with a mortgage. However, in the regression output there is no \\(p\\)-value to test the difference between owners and renters. An easy solution would be to change the reference level but what if you had many levels? How would you know which ones to test? In the next section we will look at multiple comparisons but before then we can use the bootstrap to help us.\nLet’s bootstrap the regression.\n\nset.seed(532)\nresults &lt;- do(1000)*lm(interest_rate ~ homeownership,data=resample(loans100))\n\n\nhead(results)\n\n  Intercept homeownershipOWN homeownershipRENT    sigma  r.squared        F\n1   9.98300         2.088250          2.110000 3.832758 0.07223701 3.698421\n2  11.04875         3.868250          1.449750 3.421090 0.11065485 5.910085\n3  10.52865         3.200096          2.065557 3.958047 0.08231134 4.260474\n4  11.10000         2.572000          2.163000 4.752496 0.05494338 2.761539\n5  10.52939         2.459703          1.214033 4.157461 0.03970813 1.964128\n6  10.08280         4.100533          2.531745 3.650730 0.16435823 9.342539\n  numdf dendf .row .index\n1     2    95    1      1\n2     2    95    1      2\n3     2    95    1      3\n4     2    95    1      4\n5     2    95    1      5\n6     2    95    1      6\n\n\nWe of course can generate a confidence interval on either of the coefficients in the results object.\n\nobs&lt;-do(1)*loan_mod\nobs\n\n  Intercept homeownershipOWN homeownershipRENT    sigma  r.squared        F\n1  10.49723         2.613535          2.356976 4.037396 0.08516582 4.421978\n  numdf dendf .row .index\n1     2    95    1      1\n\n\nFigure 31.7 is a histogram of the estimated coefficient for those that own their home.\n\nresults %&gt;%\n  gf_histogram(~homeownershipOWN,fill=\"cyan\",color=\"black\") %&gt;%\n  gf_vline(xintercept = obs$homeownershipOWN,color=\"red\") %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(y=\"\",x=\"Homeownership (Own).\")\n\n\n\n\n\n\n\nFigure 31.7: Distribution of estimated regression coefficent for homeownership.\n\n\n\n\n\n\ncdata(~homeownershipOWN,data=results)\n\n         lower    upper central.p\n2.5% 0.7674164 4.489259      0.95\n\n\nWhich is similar to the results assuming normality.\n\nconfint(loan_mod)\n\n                      2.5 %    97.5 %\n(Intercept)       9.3280904 11.666378\nhomeownershipOWN  0.1018118  5.125259\nhomeownershipRENT 0.6083964  4.105557\n\n\nHowever, we want a confidence interval for the difference between home owners and renters.\n\nresults %&gt;%\n  mutate(own_rent=homeownershipOWN - homeownershipRENT) %&gt;%\n  cdata(~own_rent,data=.)\n\n         lower    upper central.p\n2.5% -1.943183 2.536296      0.95\n\n\nDone! From this interval we can infer that home owners that own their home and those that rent do not have significantly different interest rates.\n\n\n31.5.3 ANOVA Table\nAs a reminder, we could also report the results of loans analysis using an analysis of variance, or ANOVA, table.\n\nanova(loan_mod)\n\nAnalysis of Variance Table\n\nResponse: interest_rate\n              Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nhomeownership  2  144.16  72.081   4.422 0.01458 *\nResiduals     95 1548.55  16.301                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis table lays out how the variation between observations is broken down. This is a simultaneous test of equality of the three means. Using the \\(F\\)-statistic, we would reject the null hypothesis of no differences in mean response across levels of the categorical variable. Notice it is the same \\(p\\)-value reported for the \\(F\\) distribution in the regression summary.\n\n\n31.5.4 Pairwise Comparisons\nThe ANOVA table above (along with the summary of the linear model output before that) merely tells you whether any difference exists in the mean response across the levels of the categorical predictor. It does not tell you where that difference lies. In the case of using regression we can compare MORTGAGE to the other two levels but can’t conduct a hypothesis of OWN vs RENT. In order to make all pairwise comparisons, we need another tool. A common one is the Tukey method. Essentially, the Tukey method conducts three hypothesis tests (each under the null of no difference in mean) but corrects the \\(p\\)-values based on the understanding that we are conducting three simultaneous hypothesis tests with the same set of data and we don’t want to inflate the Type 1 error.\nWe can obtain these pairwise comparisons using the TukeyHSD() function in R. The “HSD” stands for “Honest Significant Differences”. This function requires an anova object, which is obtained by using the aov() function:\n\nTukeyHSD(aov(interest_rate~homeownership, data=loans100))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = interest_rate ~ homeownership, data = loans100)\n\n$homeownership\n                    diff        lwr      upr     p adj\nOWN-MORTGAGE   2.6135352 -0.3988868 5.625957 0.1025289\nRENT-MORTGAGE  2.3569765  0.2598263 4.454127 0.0236346\nRENT-OWN      -0.2565587 -3.3453062 2.832189 0.9786730\n\n\nAccording to this output, only the average interest rate for those with a mortgage is different from renters.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Simulation-Based-Linear-Regression.html#assumptions",
    "href": "Simulation-Based-Linear-Regression.html#assumptions",
    "title": "31  Simulation-Based Linear Regression",
    "section": "31.6 Assumptions",
    "text": "31.6 Assumptions\nKeep in mind that ANOVA is a special case of a simple linear model. Therefore, all of the assumptions remain the same except for the linearity. The order of the levels is irrelevant and thus a line does not need to go through the three levels. In order to evaluate these assumptions, we would need to obtain the appropriate diagnostic plots.\n\nplot(loan_mod,2)\n\n\n\n\n\n\n\nFigure 31.8: Q-Q normality plot.\n\n\n\n\n\nFigure 31.8 shows that normality is suspect but we have a large sample size and thus we did not get much of a difference in results from the bootstrap which does not assume normality.\n\nplot(loan_mod,3)\n\n\n\n\n\n\n\nFigure 31.9: Scale-location residual diagnostic plot.\n\n\n\n\n\nThe assumption of equal variance is also suspect, Figure 31.9. The variance for the homeowners might be less than that for the other two.\n\nplot(loan_mod,5)\n\n\n\n\n\n\n\nFigure 31.10: Residual plot for outliers and leverage points.\n\n\n\n\n\nWe have three points that might be outliers but they are not too extreme, Figure 31.10. In general, nothing in this plot is concerning to us.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation-Based Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html",
    "href": "Multiple-Linear-Regression.html",
    "title": "32  Multiple Linear Regression",
    "section": "",
    "text": "32.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html#objectives",
    "href": "Multiple-Linear-Regression.html#objectives",
    "title": "32  Multiple Linear Regression",
    "section": "",
    "text": "Generate and interpret the coefficients of a multiple linear regression model. Asses the assumptions underlying multiple linear regression models.\nWrite the estimates multiple linear regression model and calculate and interpret the predicted response for given values of the predictors.\nGenerate and interpret confidence intervals for parameter estimates in a multiple linear regression model.\nGenerate and interpret confidence and prediction intervals for predicted values in a multiple linear regression model.\nExplain the concepts of adjusted R-squared and multicollinearity in the context of multiple linear regression.\nDevelop and interpret multiple linear regression models that include higher-order terms, such as polynomial terms or interaction effects.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html#introduction-to-multiple-regression",
    "href": "Multiple-Linear-Regression.html#introduction-to-multiple-regression",
    "title": "32  Multiple Linear Regression",
    "section": "32.2 Introduction to multiple regression",
    "text": "32.2 Introduction to multiple regression\nThe principles of simple linear regression lay the foundation for more sophisticated regression methods used in a wide range of challenging settings. In our last two chapters, we will explore multiple regression, which introduces the possibility of more than one predictor.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html#multiple-regression",
    "href": "Multiple-Linear-Regression.html#multiple-regression",
    "title": "32  Multiple Linear Regression",
    "section": "32.3 Multiple regression",
    "text": "32.3 Multiple regression\nMultiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted \\(x_1\\), \\(x_2\\), \\(x_3\\), …). The method is motivated by scenarios where many variables may be simultaneously connected to an output.\nTo explore and explain these ideas, we will consider Ebay auctions of a video game called Mario Kart for the Nintendo Wii. The outcome variable of interest is the total price of an auction, which is the highest bid plus the shipping cost. We will try to determine how total price is related to each characteristic in an auction while simultaneously controlling for other variables. For instance, with all other characteristics held constant, are longer auctions associated with higher or lower prices? And, on average, how much more do buyers tend to pay for additional Wii wheels (plastic steering wheels that attach to the Wii controller) in auctions? Multiple regression will help us answer these and other questions.\nThe data set is in the file mariokart.csv in the data folder. This data set includes results from 141 auctions.1 Ten observations from this data set are shown in the R code below. Note that we force the first column to be interpreted as a character string since it is the identification code for each sale and has no numeric meaning. Just as in the case of simple linear regression, multiple regression also allows for categorical variables with many levels. Although we do have this type of variable in this data set, we will leave the discussion of these types of variables in multiple regression for advanced regression or machine learning courses.\n\nmariokart &lt;-read_csv(\"data/mariokart.csv\", col_types = list(col_character()))\nhead(mariokart,n=10)\n\n# A tibble: 10 × 12\n   id        duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 15037742…        3     20 new       0.99    4        51.6 standa…        1580\n 2 26048337…        7     13 used      0.99    3.99     37.0 firstC…         365\n 3 32043234…        3     16 new       0.99    3.5      45.5 firstC…         998\n 4 28040522…        3     18 new       0.99    0        44   standa…           7\n 5 17039222…        1     20 new       0.01    0        71   media           820\n 6 36019515…        3     19 new       0.99    4        45   standa…      270144\n 7 12047772…        1     13 used      0.01    0        37.0 standa…        7284\n 8 30035550…        1     15 new       1       2.99     54.0 upsGro…        4858\n 9 20039206…        3     29 used      0.99    4        47   priori…          27\n10 33036416…        7      8 used     20.0     4        50   firstC…         201\n# ℹ 3 more variables: stock_photo &lt;chr&gt;, wheels &lt;dbl&gt;, title &lt;chr&gt;\n\n\nWe are only interested in total_pr, cond, stock_photo, duration, and wheels. These variables are described in the following list:\n\ntotal_pr: final auction price plus shipping costs, in US dollars\n\ncond: a two-level categorical factor variable\n\nstock_photo: a two-level categorical factor variable\n\nduration: the length of the auction, in days, taking values from 1 to 10\n\nwheels: the number of Wii wheels included with the auction (a Wii wheel is a plastic racing wheel that holds the Wii controller and is an optional but helpful accessory for playing Mario Kart)\n\n\n32.3.1 A single-variable model for the Mario Kart data\nLet’s fit a linear regression model with the game’s condition as a predictor of auction price. Before we start let’s change cond and stock_photo into factors.\n\nmariokart &lt;- mariokart %&gt;%\n  mutate(cond=factor(cond),stock_photo=factor(stock_photo))\n\nNext let’s summarize the data.\n\ninspect(mariokart)\n\n\ncategorical variables:  \n         name     class levels   n missing\n1          id character    143 143       0\n2        cond    factor      2 143       0\n3     ship_sp character      8 143       0\n4 stock_photo    factor      2 143       0\n5       title character     80 142       1\n                                   distribution\n1 110439174663 (0.7%) ...                      \n2 used (58.7%), new (41.3%)                    \n3 standard (23.1%), upsGround (21.7%) ...      \n4 yes (73.4%), no (26.6%)                      \n5  (%) ...                                     \n\nquantitative variables:  \n         name   class   min      Q1 median      Q3       max         mean\n1    duration numeric  1.00   1.000    3.0    7.00     10.00     3.769231\n2      n_bids numeric  1.00  10.000   14.0   17.00     29.00    13.538462\n3    start_pr numeric  0.01   0.990    1.0   10.00     69.95     8.777203\n4     ship_pr numeric  0.00   0.000    3.0    4.00     25.51     3.143706\n5    total_pr numeric 28.98  41.175   46.5   53.99    326.51    49.880490\n6 seller_rate numeric  0.00 109.000  820.0 4858.00 270144.00 15898.419580\n7      wheels numeric  0.00   0.000    1.0    2.00      4.00     1.146853\n            sd   n missing\n1 2.585693e+00 143       0\n2 5.878786e+00 143       0\n3 1.506745e+01 143       0\n4 3.213179e+00 143       0\n5 2.568856e+01 143       0\n6 5.184032e+04 143       0\n7 8.471829e-01 143       0\n\n\nFinally, let’s plot the data.\n\nmariokart %&gt;% \n  gf_boxplot(total_pr~cond) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(title=\"Ebay Auction Prices\",x=\"Condition\", y=\"Total Price\")\n\n\n\n\n\n\n\nFigure 32.1: Total price of Mario Kart on Ebay for each condition.\n\n\n\n\n\nWe have several outliers that may impact our analysis, Figure 32.1.\nNow let’s build the model.\n\nmario_mod &lt;- lm(total_pr~cond,data=mariokart)\n\n\nsummary(mario_mod)\n\n\nCall:\nlm(formula = total_pr ~ cond, data = mariokart)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.168  -7.771  -3.148   1.857 279.362 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   53.771      3.329  16.153   &lt;2e-16 ***\ncondused      -6.623      4.343  -1.525     0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.57 on 141 degrees of freedom\nMultiple R-squared:  0.01622,   Adjusted R-squared:  0.009244 \nF-statistic: 2.325 on 1 and 141 DF,  p-value: 0.1296\n\n\nThe model may be written as\n\\[\n\\hat{\\text{totalprice}} = 53.771 - 6.623 \\times \\text{condused}\n\\]\nA scatterplot for price versus game condition is shown in Figure 32.2. Since the predictor is binary, the scatterplot is not appropriate but we will look at it for reference.\n\nmariokart %&gt;%\n  gf_point(total_pr~cond) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(title=\"Ebay Auction Prices\",x=\"Condition\", y=\"Total Price\")\n\n\n\n\n\n\n\nFigure 32.2: Scatterplot of total price of Mario Kart on Ebay versus condition.\n\n\n\n\n\nThe largest outlier probably is significantly impacting the relationship in the model. If we find the mean and median for the two groups, we will see this.\n\nmariokart %&gt;%\n  group_by(cond) %&gt;%\n  summarize(xbar=mean(total_pr), stand_dev=sd(total_pr),xmedian=median(total_pr))\n\n# A tibble: 2 × 4\n  cond   xbar stand_dev xmedian\n  &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 new    53.8      7.44    54.0\n2 used   47.1     32.7     42.8\n\n\nIt appears that used items have a right skewed distribution where their average is higher because of at least one of the outliers.\nThere are at least two outliers in the plot. Let’s gather more information about them.\n\nmariokart %&gt;%\n  filter(total_pr &gt; 100)\n\n# A tibble: 2 × 12\n  id         duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 110439174…        7     22 used      1       25.5     327. parcel          115\n2 130335427…        3     27 used      6.95     4       118. parcel           41\n# ℹ 3 more variables: stock_photo &lt;fct&gt;, wheels &lt;dbl&gt;, title &lt;chr&gt;\n\n\nIf you look at the variable title there were additional items in the sale for these two observations. Let’s remove those two outliers and run the model again. Note that the reason we are removing them is not because they are annoying us and messing up our model. It is because we don’t think they are representative of the population of interest. Figure 32.3 is a boxplot of the data with the outliers dropped.\n\nmariokart_new &lt;- mariokart %&gt;%\n  filter(total_pr &lt;= 100) %&gt;% \n  select(total_pr,cond,stock_photo,duration,wheels)\n\n\nsummary(mariokart_new)\n\n    total_pr       cond    stock_photo    duration          wheels     \n Min.   :28.98   new :59   no : 36     Min.   : 1.000   Min.   :0.000  \n 1st Qu.:41.00   used:82   yes:105     1st Qu.: 1.000   1st Qu.:0.000  \n Median :46.03                         Median : 3.000   Median :1.000  \n Mean   :47.43                         Mean   : 3.752   Mean   :1.149  \n 3rd Qu.:53.99                         3rd Qu.: 7.000   3rd Qu.:2.000  \n Max.   :75.00                         Max.   :10.000   Max.   :4.000  \n\n\n\nmariokart_new %&gt;% \n  gf_boxplot(total_pr~cond) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(title=\"Ebay Auction Prices\",subtitle=\"Outliers removed\",x=\"Condition\", y=\"Total Price\")\n\n\n\n\n\n\n\nFigure 32.3: Boxplot of total price and condition with outliers removed.\n\n\n\n\n\n\nmario_mod2 &lt;- lm(total_pr~cond,data=mariokart_new)\n\n\nsummary(mario_mod2)\n\n\nCall:\nlm(formula = total_pr ~ cond, data = mariokart_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8911  -5.8311   0.1289   4.1289  22.1489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  53.7707     0.9596  56.034  &lt; 2e-16 ***\ncondused    -10.8996     1.2583  -8.662 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.371 on 139 degrees of freedom\nMultiple R-squared:  0.3506,    Adjusted R-squared:  0.3459 \nF-statistic: 75.03 on 1 and 139 DF,  p-value: 1.056e-14\n\n\nNotice how much the residual standard error has decreased and likewise the \\(R\\)-squared has increased.\nThe model may be written as:\n\\[\n\\hat{total price} = 53.771 - 10.90 \\times condused\n\\]\nNow we see that the average price for a used items is $10.90 less than the average of new items.\n\nExercise:\nDoes the linear model seem reasonable? Which assumptions should you check?\n\nThe model does seem reasonable in the sense that the assumptions on the errors is plausible. The residuals indicate some skewness to the right which may be driven predominantly by the skewness in the new items, Figure 32.4.\n\nplot(mario_mod2,2)\n\n\n\n\n\n\n\nFigure 32.4: Check of normality using quantile-quantile plot.\n\n\n\n\n\nThe normality assumption is somewhat suspect but we have more than 100 data points so the short tails of the distribution are not a concern. The shape of this curve indicates a positive skew.\n\nplot(mario_mod2,3)\n\n\n\n\n\n\n\nFigure 32.5: Residual plot to assess equal variance assumption.\n\n\n\n\n\nFrom Figure 32.5, equal variance seems reasonable.\n\nplot(mario_mod2,5)\n\n\n\n\n\n\n\nFigure 32.6: Residual plot for checking leverage points.\n\n\n\n\n\nNo high leverage points, Figure 32.6.\nNo need to check linearity, we only have two different values for the explanatory variable.\n\nExample: Interpretation\nInterpret the coefficient for the game’s condition in the model. Is this coefficient significantly different from 0?\n\nNote that cond is a two-level categorical variable and the reference level is new. So - 10.90 means that the model predicts an extra $10.90 on average for those games that are new versus those that are used. Examining the regression output, we can see that the \\(p\\)-value for cond is very close to zero, indicating there is strong evidence that the coefficient is different from zero when using this simple one-variable model.\n\n\n32.3.2 Including and assessing many variables in a model\nSometimes there are underlying structures or relationships between predictor variables. For instance, new games sold on Ebay tend to come with more Wii wheels, which may have led to higher prices for those auctions. We would like to fit a model that includes all potentially important variables simultaneously. This would help us evaluate the relationship between a predictor variable and the outcome while controlling for the potential influence of other variables. This is the strategy used in multiple regression. While we remain cautious about making any causal interpretations using multiple regression, such models are a common first step in providing evidence of a causal connection.\nWe want to construct a model that accounts for not only the game condition, but simultaneously accounts for three other variables: stock_photo, duration, and wheels. This model can be represented as:\n\\[\n\\widehat{\\text{totalprice}}\n    = \\beta_0 + \\beta_1 \\times \\text{cond} + \\beta_2 \\times \\text{stockphoto}\n    + \\beta_3 \\times  \\text{duration} +\n        \\beta_4 \\times  \\text{wheels}\n\\]\nor:\n\\[\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 \\tag{32.1}\\]\nIn Equation 32.1, \\(y\\) represents the total price, \\(x_1\\) indicates whether the game is new, \\(x_2\\) indicates whether a stock photo was used, \\(x_3\\) is the duration of the auction, and \\(x_4\\) is the number of Wii wheels included with the game. Just as with the single predictor case, a multiple regression model may be missing important components or it might not precisely represent the relationship between the outcome and the available explanatory variables. While no model is perfect, we wish to explore the possibility that this model may fit the data reasonably well.\nWe estimate the parameters \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_4\\) in the same way as we did in the case of a single predictor. We select \\(b_0\\), \\(b_1\\), …, \\(b_4\\) that minimize the sum of the squared residuals:\n\\[\n\\text{SSE} = e_1^2 + e_2^2 + \\dots + e_{141}^2\n    = \\sum_{i=1}^{141} e_i^2\n     = \\sum_{i=1}^{141} \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nIn our problem, there are 141 residuals, one for each observation. We use a computer to minimize the sum and compute point estimates.\n\nmario_mod_multi &lt;- lm(total_pr~., data=mariokart_new)\n\nThe formula total_pr~. uses a dot. This means we want to use all the predictors. We could have also used the following code:\n\nmario_mod_multi &lt;- lm(total_pr~cond+stock_photo+duration+wheels, data=mariokart_new)\n\nRecall, the + symbol does not mean to literally add the predictors together. It is not a mathematical operation but a formula operation that means to include the predictor.\nYou can view a summary of the model using the summmary() function.\n\nsummary(mario_mod_multi)\n\n\nCall:\nlm(formula = total_pr ~ ., data = mariokart_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3788  -2.9854  -0.9654   2.6915  14.0346 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    41.34153    1.71167  24.153  &lt; 2e-16 ***\ncondused       -5.13056    1.05112  -4.881 2.91e-06 ***\nstock_photoyes  1.08031    1.05682   1.022    0.308    \nduration       -0.02681    0.19041  -0.141    0.888    \nwheels          7.28518    0.55469  13.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.901 on 136 degrees of freedom\nMultiple R-squared:  0.719, Adjusted R-squared:  0.7108 \nF-statistic: 87.01 on 4 and 136 DF,  p-value: &lt; 2.2e-16\n\n\nWhich we can summarize in a tibble using the broom package.\n\n\n\n\nTable 32.1: Multiple regression coefficients.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n41.3415318\n1.7116684\n24.1527693\n0.0000000\n\n\ncondused\n-5.1305641\n1.0511238\n-4.8810276\n0.0000029\n\n\nstock_photoyes\n1.0803108\n1.0568238\n1.0222241\n0.3084897\n\n\nduration\n-0.0268075\n0.1904122\n-0.1407868\n0.8882467\n\n\nwheels\n7.2851779\n0.5546928\n13.1337172\n0.0000000\n\n\n\n\n\n\n\n\nUsing this output, Table 32.1, we identify the point estimates \\(b_i\\) of each \\(\\beta_i\\), just as we did in the one-predictor case.\n\nMultiple regression model\nA multiple regression model is a linear model with many predictors.\n\nIn general, we write the model as\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k %+ \\epsilon\n\\]\nwhen there are \\(k\\) predictors. We often estimate the \\(\\beta_i\\) parameters using a computer.\n\nExercise: Write out the the multiple regression model using the point estimates from regression output. How many predictors are there in this model?2\n\n\nExercise:\nWhat does \\(\\beta_4\\), the coefficient of variable \\(x_4\\) (Wii wheels), represent? What is the point estimate of \\(\\beta_4\\)?3\n\n\nExercise:\nCompute the residual of the first observation in the dataframe using the regression equation.\n\n\nmario_mod_multi$residuals[1]\n\n       1 \n1.923402 \n\n\nThe broom package has a function augment() that will calculate the predicted and residuals.\n\nlibrary(broom)\n\n\naugment(mario_mod_multi) %&gt;%\n  head(1)\n\n# A tibble: 1 × 11\n  total_pr cond  stock_photo duration wheels .fitted .resid   .hat .sigma\n     &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     51.6 new   yes                3      1    49.6   1.92 0.0215   4.92\n# ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\n\\(e_i = y_i - \\hat{y_i} = 51.55 - 49.62 = 1.93\\)\n\nExample:\nWe estimated a coefficient for cond as \\(b_1 = - 10.90\\) with a standard error of \\(SE_{b_1} = 1.26\\) when using simple linear regression. Why might there be a difference between that estimate and the one in the multiple regression setting?\n\nIf we examined the data carefully, we would see that some predictors are correlated. For instance, when we estimated the connection of the outcome total_pr and predictor cond using simple linear regression, we were unable to control for other variables like the number of Wii wheels included in the auction. That model was biased by the confounding variable wheels. When we use both variables, this particular underlying and unintentional bias is reduced or eliminated (though bias from other confounding variables may still remain).\nThe previous example describes a common issue in multiple regression: correlation among predictor variables. We say the two predictor variables are collinear (pronounced as co-linear) when they are correlated, and this collinearity complicates model estimation. While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being collinear.\n\nExercise:\nThe estimated value of the intercept is 41.34, and one might be tempted to make some interpretation of this coefficient, such as, it is the model’s predicted price when each of the variables take a value of zero: the game is new, the primary image is not a stock photo, the auction duration is zero days, and there are no wheels included. Is there any value gained by making this interpretation?4\n\n\n\n32.3.3 Inference\nFrom the printout of the model summary, we can see that both the stock_photo and duration variables are not significantly different from zero. Thus we may want to drop them from the model. In a machine learning course, you explore different ways to determine the best model.\nLikewise, we could generate confidence intervals for the coefficients:\n\nconfint(mario_mod_multi)\n\n                    2.5 %     97.5 %\n(Intercept)    37.9566036 44.7264601\ncondused       -7.2092253 -3.0519030\nstock_photoyes -1.0096225  3.1702442\nduration       -0.4033592  0.3497442\nwheels          6.1882392  8.3821165\n\n\nThis confirms that the stock_photo and duration may not have an impact on total price.\n\n\n32.3.4 Adjusted \\(R^2\\) as a better estimate of explained variance\nWe first used \\(R^2\\) in simple linear regression to determine the amount of variability, we used sum of squares and not mean squared errors, in the response that was explained by the model:\n\\[\nR^2 = 1 - \\frac{\\text{sum of squares of residuals}}{\\text{sum of squares of the outcome}}\n\\]\nThis equation remains valid in the multiple regression framework, but a small enhancement can often be even more informative.\n\nExercise: The variance of the residuals for the model is \\(4.901^2\\), and the variance of the total price in all the auctions is 83.06. Estimate the \\(R^2\\) for this model.5\n\nTo get the \\(R^2\\) we need the sum of squares and not variance, so we multiply by the appropriate degrees of freedom.\n\n1-(24.0198*136)/(83.05864*140)\n\n[1] 0.7190717\n\n\n\nsummary(mario_mod_multi)$r.squared\n\n[1] 0.7190261\n\n\nThis strategy for estimating \\(R^2\\) is acceptable when there is just a single variable. However, it becomes less helpful when there are many variables. The regular \\(R^2\\) is actually a biased estimate of the amount of variability explained by the model. To get a better estimate, we use the adjusted \\(R^2\\).\n\nAdjusted \\(\\mathbf{R^2}\\) as a tool for model assessment:\nThe adjusted \\(\\mathbf{R^2}\\) is computed as: \\[\nR_{adj}^{2} = 1-\\frac{\\text{sum of squares of residuals} / (n-k-1)}{\\text{sum of squares of the outcome} / (n-1)}\n\\] where \\(n\\) is the number of cases used to fit the model and \\(k\\) is the number of predictor variables in the model.\n\nBecause \\(k\\) is never negative, the adjusted \\(R^2\\) will be smaller – often times just a little smaller – than the unadjusted \\(R^2\\). The reasoning behind the adjusted \\(R^2\\) lies in the degrees of freedom associated with each variance. 6\n\nExercise:\nSuppose you added another predictor to the model, but the variance of the errors didn’t go down. What would happen to the \\(R^2\\)? What would happen to the adjusted \\(R^2\\)?7\n\nAgain, in a machine learning course, you will spend more time on how to select models. Using internal metrics of performance such as \\(p\\)-values or adjusted \\(R\\) squared are one way but using external measures of predictive performance such as cross validation or hold out sets will be introduced.\n\n\n32.3.5 Reduced model\nNow let’s drop duration from the model and compare to our previous model:\n\nmario_mod_multi2 &lt;- lm(total_pr~cond+stock_photo+wheels, data=mariokart_new)\n\nAnd the summary:\n\nsummary(mario_mod_multi2)\n\n\nCall:\nlm(formula = total_pr ~ cond + stock_photo + wheels, data = mariokart_new)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.454  -2.959  -0.949   2.712  14.061 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     41.2245     1.4911  27.648  &lt; 2e-16 ***\ncondused        -5.1763     0.9961  -5.196 7.21e-07 ***\nstock_photoyes   1.1177     1.0192   1.097    0.275    \nwheels           7.2984     0.5448  13.397  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.884 on 137 degrees of freedom\nMultiple R-squared:  0.719, Adjusted R-squared:  0.7128 \nF-statistic: 116.8 on 3 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nAs a reminder, the previous model summary is:\n\nsummary(mario_mod_multi)\n\n\nCall:\nlm(formula = total_pr ~ ., data = mariokart_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3788  -2.9854  -0.9654   2.6915  14.0346 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    41.34153    1.71167  24.153  &lt; 2e-16 ***\ncondused       -5.13056    1.05112  -4.881 2.91e-06 ***\nstock_photoyes  1.08031    1.05682   1.022    0.308    \nduration       -0.02681    0.19041  -0.141    0.888    \nwheels          7.28518    0.55469  13.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.901 on 136 degrees of freedom\nMultiple R-squared:  0.719, Adjusted R-squared:  0.7108 \nF-statistic: 87.01 on 4 and 136 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that the adjusted \\(R^2\\) improved by dropping duration. Finally, let’s drop stock_photo.\n\nmario_mod_multi3 &lt;- lm(total_pr~cond+wheels, data=mariokart_new)\n\n\nsummary(mario_mod_multi3)\n\n\nCall:\nlm(formula = total_pr ~ cond + wheels, data = mariokart_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  42.3698     1.0651  39.780  &lt; 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\nwheels        7.2328     0.5419  13.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: &lt; 2.2e-16\n\n\nThough the adjusted \\(R^2\\) dropped a little, it is only in the fourth decimal place and thus essentially the same value. We therefore will go with this model.\n\n\n32.3.6 Confidence and prediction intervals\nLet’s suppose we want to predict the average total price for a Mario Kart sale with 2 wheels and in new condition. We can again use the predict() function.\n\npredict(mario_mod_multi3,newdata=data.frame(cond=\"new\",wheels=2),interval = \"confidence\")\n\n       fit      lwr      upr\n1 56.83544 55.49789 58.17299\n\n\nWe are 95% confident that the average price of a Mario Kart sale for a new item with 2 wheels will be between 55.50 and 58.17.\n\nExercise: Find and interpret the prediction interval for a new Mario Kart with 2 wheels.\n\n\npredict(mario_mod_multi3,newdata=data.frame(cond=\"new\",wheels=2),interval = \"prediction\")\n\n       fit      lwr      upr\n1 56.83544 47.07941 66.59147\n\n\nWe are 95% confident that the price of a Mario Kart sale for a new item with 2 wheels will be between 47.07 and 66.59.\n\n\n32.3.7 Diagnostics\nThe diagnostics for the model are similar to what we did in a previous chapter. Nothing in these plots gives us concern; however, there is one leverage point.\n\nplot(mario_mod_multi3)\n\n\n\n\nDiagnostic residual plots for multiple regression model.\n\n\n\n\n\n\n\nDiagnostic residual plots for multiple regression model.\n\n\n\n\n\n\n\nDiagnostic residual plots for multiple regression model.\n\n\n\n\n\n\n\nDiagnostic residual plots for multiple regression model.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html#interaction-and-higher-order-terms",
    "href": "Multiple-Linear-Regression.html#interaction-and-higher-order-terms",
    "title": "32  Multiple Linear Regression",
    "section": "32.4 Interaction and Higher Order Terms",
    "text": "32.4 Interaction and Higher Order Terms\nAs a final short topic we want to explore feature engineering. Thus far we have not done any transformation to the predictors in the data set except maybe making categorical variables into factors. In data analysis competitions, such as Kaggle, feature engineering is often one of the most important steps. In a machine learning course, you will look at different tools but in this book we will look at simple transformations such as higher order terms and interactions.\nTo make this section more relevant, we are going to switch to a different data set. Load the library ISLR.\n\nlibrary(ISLR)\n\nThe data set of interest is Credit. Use the help menu to read about the variables. This is a simulated data set of credit card debt.\n\nglimpse(Credit)\n\nRows: 400\nColumns: 12\n$ ID        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;int&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;int&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;int&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;int&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;int&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Gender    &lt;fct&gt;  Male, Female,  Male, Female,  Male,  Male, Female,  Male, F…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Ethnicity &lt;fct&gt; Caucasian, Asian, Asian, Asian, Caucasian, Caucasian, Africa…\n$ Balance   &lt;int&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…\n\n\nNotice that ID is being treated as an integer. We could change it to a character since it is a label, but for our work in this chapter we will not bother.\nSuppose we suspected that there is a relationship between Balance, the response, and the predictors Income and Student. Note: we actually are using this model for educational purposes and did not go through a model selection process.\nThe first model simply has these predictors in the model.\n\ncredit_mod1&lt;-lm(Balance~Income+Student,data=Credit)\n\n\nsummary(credit_mod1)\n\n\nCall:\nlm(formula = Balance ~ Income + Student, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-762.37 -331.38  -45.04  323.60  818.28 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 211.1430    32.4572   6.505 2.34e-10 ***\nIncome        5.9843     0.5566  10.751  &lt; 2e-16 ***\nStudentYes  382.6705    65.3108   5.859 9.78e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 391.8 on 397 degrees of freedom\nMultiple R-squared:  0.2775,    Adjusted R-squared:  0.2738 \nF-statistic: 76.22 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s plot the data and the regression line. The impact of putting in the categorical variable Student is to just shift the intercept. The slope remains the same, Figure 32.7.\n\naugment(credit_mod1) %&gt;%\n  gf_point(Balance~Income,color=~Student) %&gt;%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod1), Student == \"Yes\"),color=~Student)%&gt;%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod1), Student == \"No\"),color=~Student) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 32.7: Scatterplot of credit card balance for income and student status.\n\n\n\n\n\n\nExercise:\nWrite the equation for the regression model.\n\n\\[\n\\mbox{E}(Balance)=\\beta_0 + \\beta_1*\\text{Income}+ \\beta_2*\\text{(Student=Yes)}\n\\]\nor\n\\[\n\\mbox{E}(Balance)=211.14 + 5.98*\\text{Income}+ 382.67*\\text{(Student=Yes)}\n\\]\nIf the observation is a student, then the intercept is increased by 382.67.\nIn this next case, we would want to include an interaction term in the model: an interaction term allows the slope to change as well. To include an interaction term when building a model in R, we use *.\n\ncredit_mod2&lt;-lm(Balance~Income*Student,data=Credit)\n\n\nsummary(credit_mod2)\n\n\nCall:\nlm(formula = Balance ~ Income * Student, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-773.39 -325.70  -41.13  321.65  814.04 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       200.6232    33.6984   5.953 5.79e-09 ***\nIncome              6.2182     0.5921  10.502  &lt; 2e-16 ***\nStudentYes        476.6758   104.3512   4.568 6.59e-06 ***\nIncome:StudentYes  -1.9992     1.7313  -1.155    0.249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 391.6 on 396 degrees of freedom\nMultiple R-squared:  0.2799,    Adjusted R-squared:  0.2744 \nF-statistic:  51.3 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\naugment(credit_mod2) %&gt;%\n  gf_point(Balance~Income,color=~Student) %&gt;%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod2), Student == \"Yes\"),color=~Student)%&gt;%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod2), Student == \"No\"),color=~Student) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 32.8: Scatterplot of credit card balance for income and student status with an interaction term.\n\n\n\n\n\nNow we have a different slope and intercept for each case of the Student variable, Figure 32.8. Thus there is a synergy or interaction between these variables. The student status changes the impact of Income on Balance. If you are a student, then for every increase in income of 1 the balance increase by 4.219 on average. If you are not a student, every increase in income of 1 increases the average balance by 6.2182.\nFurthermore, if you suspect that perhaps a curved relationship exists between two variables, we could include a higher order term. As an example, let’s add a quadratic term for Income to our model (without the interaction). To do this in R, we need to wrap the higher order term in I(). If we include a higher order term, we usually want to include the lower order terms as well; a better approach is to make the decision on what to include using predictive performance.\n\ncredit_mod3&lt;-lm(Balance~Income+I(Income^2),data=Credit)\n\n\nsummary(credit_mod3)\n\n\nCall:\nlm(formula = Balance ~ Income + I(Income^2), data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-782.88 -361.40  -54.98  316.26 1104.39 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 285.3973    54.1720   5.268 2.26e-07 ***\nIncome        4.3972     1.9078   2.305   0.0217 *  \nI(Income^2)   0.0109     0.0120   0.908   0.3642    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 408 on 397 degrees of freedom\nMultiple R-squared:  0.2166,    Adjusted R-squared:  0.2127 \nF-statistic: 54.88 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\n\naugment(credit_mod3) %&gt;%\n  gf_point(Balance~Income) %&gt;%\n  gf_line(.fitted~Income) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 32.9: Scatterplot of credit card balance for income with a quadratic fit.\n\n\n\n\n\nThere is not much of a quadratic relationship, Figure 32.9.\n\n32.4.1 Summary\nIn this chapter we have extended the linear regression model by allowing multiple predictors. This allows us to account for confounding variables and make more sophisticated models. The interpretation and evaluation of the model changes.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Multiple-Linear-Regression.html#footnotes",
    "href": "Multiple-Linear-Regression.html#footnotes",
    "title": "32  Multiple Linear Regression",
    "section": "",
    "text": "Diez DM, Barr CD, and etinkaya-Rundel M. 2012. openintro: OpenIntro data sets and supplemental functions. http://cran.r-project.org/web/packages/openintro↩︎\n\\(\\hat{y} = 41.34 + - 5.13x_1 + 1.08x_2 - 0.03x_3 + 7.29x_4\\), and there are \\(k=4\\) predictor variables.↩︎\nIt is the average difference in auction price for each additional Wii wheel included when holding the other variables constant. The point estimate is \\(b_4 = 7.29\\).↩︎\nThree of the variables (cond, stock_photo, and wheels) do take value 0, but the auction duration is always one or more days. If the auction is not up for any days, then no one can bid on it! That means the total auction price would always be zero for such an auction; the interpretation of the intercept in this setting is not insightful.↩︎\n\\(R^2 = 1 - \\frac{24.0198}{83.06} = 0.7108\\).↩︎\nIn multiple regression, the degrees of freedom associated with the variance of the estimate of the residuals is \\(n-k-1\\), not \\(n-1\\). For instance, if we were to make predictions for new data using our current model, we would find that the unadjusted \\(R^2\\) is an overly optimistic estimate of the reduction in variance in the response, and using the degrees of freedom in the adjusted \\(R^2\\) formula helps correct this bias.↩︎\nThe unadjusted \\(R^2\\) would stay the same and the adjusted \\(R^2\\) would go down. Note that unadjusted \\(R^2\\) never decreases by adding another predictor, it can only stay the same or increase. The adjusted \\(R^2\\) increases only if the addition of a predictor reduces the variance of the error larger than add one to \\(k\\) in denominator.↩︎",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html",
    "href": "Logistic-Regression.html",
    "title": "33  Logistic Regression",
    "section": "",
    "text": "33.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#objectives",
    "href": "Logistic-Regression.html#objectives",
    "title": "33  Logistic Regression",
    "section": "",
    "text": "Apply logistic regression using R to analyze binary outcome data. Interpret the regression output, and perform model selection.\nWrite the estimated logistic regression, and calculate and interpret the predicted outputs for given values of the predictors.\nCalculate and interpret confidence intervals for parameter estimates and predicted probabilities in a logistic regression model.\nGenerate and analyze a confusion matrix to evaluate the performance of a logistic regression model.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#logistic-regression-introduction",
    "href": "Logistic-Regression.html#logistic-regression-introduction",
    "title": "33  Logistic Regression",
    "section": "33.2 Logistic regression introduction",
    "text": "33.2 Logistic regression introduction\nIn this chapter we introduce logistic regression as a tool for building models when there is a categorical response variable with two levels. Logistic regression is a type of generalized linear model (GLM) for response variables where the assumptions of normally distributed errors is not appropriate. We are prepping you for advanced statistical models and machine learning, where we will explore predictive models of many different types of response variables including ones that don’t assume an underlying functional relationship between inputs and outputs. So cool!\nGLMs can be thought of as a two-stage modeling approach. We first model the response variable using a probability distribution, such as the binomial or Poisson distribution. Second, we model the parameter of the distribution using a collection of predictors and a special form of multiple regression.\nTo explore and explain these ideas, we will again use the Ebay auctions of a video game called Mario Kart for the Nintendo Wii. Remember, the data set is in the file mariokart.csv and includes results from 141 auctions.1\nIn this chapter, we want the outcome variable of interest to be game condition, cond. In Chapter 32 we used the total price of an auction as the response. We are moving from a quantitative response to a binary qualitative variable. If we were only interested in determining if an association exists between the variables cond and total_pr, we could use linear regression with total_pr as the response. However, in this problem we want to predict game condition. We will start by reviewing some of the previous models and then introduce logistic regression. We will finish with a multiple logistic regression model, more than one predictor.\n\n33.2.1 Mario Kart data\nRead the data and summarize.\n\nmariokart &lt;- read_csv(\"data/mariokart.csv\", col_types = list(col_character()))\nhead(mariokart, n = 10)\n\n# A tibble: 10 × 12\n   id        duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 15037742…        3     20 new       0.99    4        51.6 standa…        1580\n 2 26048337…        7     13 used      0.99    3.99     37.0 firstC…         365\n 3 32043234…        3     16 new       0.99    3.5      45.5 firstC…         998\n 4 28040522…        3     18 new       0.99    0        44   standa…           7\n 5 17039222…        1     20 new       0.01    0        71   media           820\n 6 36019515…        3     19 new       0.99    4        45   standa…      270144\n 7 12047772…        1     13 used      0.01    0        37.0 standa…        7284\n 8 30035550…        1     15 new       1       2.99     54.0 upsGro…        4858\n 9 20039206…        3     29 used      0.99    4        47   priori…          27\n10 33036416…        7      8 used     20.0     4        50   firstC…         201\n# ℹ 3 more variables: stock_photo &lt;chr&gt;, wheels &lt;dbl&gt;, title &lt;chr&gt;\n\n\n\ninspect(mariokart)\n\n\ncategorical variables:  \n         name     class levels   n missing\n1          id character    143 143       0\n2        cond character      2 143       0\n3     ship_sp character      8 143       0\n4 stock_photo character      2 143       0\n5       title character     80 142       1\n                                   distribution\n1 110439174663 (0.7%) ...                      \n2 used (58.7%), new (41.3%)                    \n3 standard (23.1%), upsGround (21.7%) ...      \n4 yes (73.4%), no (26.6%)                      \n5  (%) ...                                     \n\nquantitative variables:  \n         name   class   min      Q1 median      Q3       max         mean\n1    duration numeric  1.00   1.000    3.0    7.00     10.00     3.769231\n2      n_bids numeric  1.00  10.000   14.0   17.00     29.00    13.538462\n3    start_pr numeric  0.01   0.990    1.0   10.00     69.95     8.777203\n4     ship_pr numeric  0.00   0.000    3.0    4.00     25.51     3.143706\n5    total_pr numeric 28.98  41.175   46.5   53.99    326.51    49.880490\n6 seller_rate numeric  0.00 109.000  820.0 4858.00 270144.00 15898.419580\n7      wheels numeric  0.00   0.000    1.0    2.00      4.00     1.146853\n            sd   n missing\n1 2.585693e+00 143       0\n2 5.878786e+00 143       0\n3 1.506745e+01 143       0\n4 3.213179e+00 143       0\n5 2.568856e+01 143       0\n6 5.184032e+04 143       0\n7 8.471829e-01 143       0\n\n\nWe are again only interested in total_pr, cond, stock_photo, duration, and wheels. These variables are described in the following list:\n\ntotal_pr: final auction price plus shipping costs, in US dollars\n\ncond: a two-level categorical factor variable\n\nstock_photo: a two-level categorical factor variable\n\nduration: the length of the auction, in days, taking values from 1 to 10\n\nwheels: the number of Wii wheels included with the auction (a Wii wheel is a plastic racing wheel that holds the Wii controller and is an optional but helpful accessory for playing Mario Kart)\n\nRemember that we removed a couple of outlier sales that included multiple items. Before we start let’s clean up the data again to include removing those outliers.\n\nmariokart &lt;- mariokart %&gt;%\n  filter(total_pr &lt;= 100) %&gt;% \n  mutate(cond = factor(cond),\n         stock_photo = factor(stock_photo)) %&gt;% \n  select(cond, stock_photo, total_pr, duration, wheels)\n\nNext let’s summarize the data.\n\ninspect(mariokart)\n\n\ncategorical variables:  \n         name  class levels   n missing\n1        cond factor      2 141       0\n2 stock_photo factor      2 141       0\n                                   distribution\n1 used (58.2%), new (41.8%)                    \n2 yes (74.5%), no (25.5%)                      \n\nquantitative variables:  \n      name   class   min Q1 median    Q3 max      mean        sd   n missing\n1 total_pr numeric 28.98 41  46.03 53.99  75 47.431915 9.1136514 141       0\n2 duration numeric  1.00  1   3.00  7.00  10  3.751773 2.5888663 141       0\n3   wheels numeric  0.00  0   1.00  2.00   4  1.148936 0.8446146 141       0\n\n\n\n\n33.2.2 Analyzing contingency table\nAs a review and introduction to logistic regression, let’s analyze the relationship between game condition and stock photo.\n\ntally(cond ~ stock_photo, data = mariokart, margins = TRUE, \n      format = \"proportion\")\n\n       stock_photo\ncond           no       yes\n  new   0.1111111 0.5238095\n  used  0.8888889 0.4761905\n  Total 1.0000000 1.0000000\n\n\nWe could analyze this by comparing the proportion of new condition games for each stock photo value using both randomization, empirical \\(p\\)-values, and the central limit theorem. We will just use an exact permutation test, Fisher Exact Test, which just uses the hypergeometric distribution.\n\nfisher.test(tally(~cond + stock_photo, data = mariokart))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tally(~cond + stock_photo, data = mariokart)\np-value = 9.875e-06\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.02766882 0.35763723\nsample estimates:\nodds ratio \n 0.1152058 \n\n\nClearly, these variables are not independent of each other. This model does not gives us much more information so let’s move to logistic regression.\n\n\n33.2.3 Modeling the probability of an event\nThe outcome variable for a GLM is denoted by \\(Y_i\\), where the index \\(i\\) is used to represent observation \\(i\\). In the Mario Kart application, \\(Y_i\\) will be used to represent whether the game condition \\(i\\) is new (\\(Y_i=1\\)) or used (\\(Y_i=0\\)).\nThe predictor variables are represented as follows: \\(x_{1,i}\\) is the value of variable 1 for observation \\(i\\), \\(x_{2,i}\\) is the value of variable 2 for observation \\(i\\), and so on.\nLogistic regression is a generalized linear model where the outcome is a two-level categorical variable. The outcome, \\(Y_i\\), takes the value 1 (in our application, this represents a game in new condition but we could easily switch and make the outcome of interest a used game) with probability \\(p_i\\) and the value 0 with probability \\(1-p_i\\). It is the probability \\(p_i\\) that we model in relation to the predictor variables.\nThe logistic regression model relates the probability a game is new (\\(p_i\\)) to values of the predictors \\(x_{1,i}\\), \\(x_{2,i}\\), …, \\(x_{k,i}\\) through a framework much like that of multiple regression:\n\\[\n\\text{transformation}(p_{i}) = \\beta_0 + \\beta_1x_{1,i} + \\beta_2 x_{2,i} + \\cdots \\beta_k x_{k,i}\n\\]\nWe want to choose a transformation that makes practical and mathematical sense. For example, we want a transformation that makes the range of possibilities on the left hand side of the above equation equal to the range of possibilities for the right hand side. If there was no transformation for this equation, the left hand side could only take values between 0 and 1, but the right hand side could take values outside of this range. A common transformation for \\(p_i\\) is the logit transformation, which may be written as\n\\[\n\\text{logit}(p_i) = \\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n\\]\nBelow, we expand the equation using the logit transformation of \\(p_i\\):\n\\[\n\\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n    = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_k x_{k,i}\n\\]\nSolving for \\(p_i\\) we get the logistic function:\n\\[\np_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_k x_{k,i})}}\n\\]\nThe logistic function is shown in Figure 33.1.\n\n\n\n\n\n\n\n\nFigure 33.1: Logistic function with some example points plotted.\n\n\n\n\n\nNotice the output of the logistic function restricts the values between 0 and 1. The curve is fairly flat on the edges with a sharp rise in the center. There are other functions that achieve this same result. However, for reasons beyond the scope of this book, the logit function has desirable mathematical properties that relate to making sure all the common GLMs fall within the exponential family of distributions. This topic is at the graduate school level and not needed for our studies.\nIn our Mario Kart example, there are 4 predictor variables, so \\(k = 4\\). This nonlinear model isn’t very intuitive, but it still has some resemblance to multiple regression, and we can fit this model using software. In fact, once we look at results from software, it will start to feel like we’re back in multiple regression, even if the interpretation of the coefficients is more complex.\n\n\n33.2.4 First model - intercept only\nHere we create a model with just an intercept.\nIn R we use the glm() function to fit a logistic regression model. It has the same formula format as lm but also requires a family argument. Since our response is binary, we use binomial. If we wanted to use glm() for linear regression assuming normally distributed residual, the family argument would be gaussian. This implies that multiple linear regression with the assumption of normally distributed errors is a special case of a generalized linear model. In R, the response is a 0/1 variable, we can control the outcome of interest, the 1, by using a logical argument in the formula.\nFirst to understand the output of logistic regression, let’s just run a model with an intercept term. Notice in the code chunk that the left hand side of the formula has a logical argument, this gives a 0/1 output with 1 being the value we want to predict.\n\nmario_mod1 &lt;- glm(cond == \"new\" ~ 1, data = mariokart, family = \"binomial\")\n\nLet’s get regression output using the summary() function.\n\nsummary(mario_mod1)\n\n\nCall:\nglm(formula = cond == \"new\" ~ 1, family = \"binomial\", data = mariokart)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.3292     0.1707  -1.928   0.0538 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 191.7  on 140  degrees of freedom\nResidual deviance: 191.7  on 140  degrees of freedom\nAIC: 193.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis looks similar to the regression output we saw in previous chapters. However, the model has a different, nonlinear, form. Remember, Equation 33.1 is the general form of the model.\n\\[  \\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n    = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_k x_{k,i} \\tag{33.1}\\]\nThus using the output of R, Equation 33.2 is the estimated model.\n\\[\\log\\left( \\frac{p_i}{1-p_i} \\right) = -0.329 \\tag{33.2}\\]\nSolving Equation 33.2 for \\(p_i\\): \\(\\frac{e^{-0.329}}{1 + e^{-0.329}} = 0.418\\). This is the estimated probability of the game condition being new. This point is plotted in Figure 33.1. We can also check this result using a summary table.\n\ntally(~cond, data = mariokart, format = \"proportion\")\n\ncond\n      new      used \n0.4184397 0.5815603 \n\n\n\n\n33.2.5 Second model - stock_photo\nNow that we are starting to understand the logistic regression model. Let’s add a predictor variable, stock_photo. Again, we have many methods to determine if a relationship between two categorical variables exists, logistic regression is another method.\n\nmario_mod2 &lt;- glm(cond == \"new\" ~ stock_photo, data = mariokart,\n                 family = \"binomial\")\n\n\nsummary(mario_mod2)\n\n\nCall:\nglm(formula = cond == \"new\" ~ stock_photo, family = \"binomial\", \n    data = mariokart)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.0794     0.5303  -3.921 8.81e-05 ***\nstock_photoyes   2.1748     0.5652   3.848 0.000119 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 191.70  on 140  degrees of freedom\nResidual deviance: 170.44  on 139  degrees of freedom\nAIC: 174.44\n\nNumber of Fisher Scoring iterations: 4\n\n\nExamining the \\(p\\)-value associated with the coefficient for stock_photo, we can see that it is significant. Thus we reject the null hypothesis that the coefficient is zero. There is a relationship between cond and stock_photo, as we found with the Fisher’s test.\nWe can use the broom package to summarize the output and generate model fits.\n\ntidy(mario_mod2)\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       -2.08     0.530     -3.92 0.0000881\n2 stock_photoyes     2.17     0.565      3.85 0.000119 \n\n\nLet’s convert these coefficients to estimated probabilities using the augment() function. We need to specify the output as the response, this returns a probability, or else we will get the logit of the probability, the link value.\n\naugment(mario_mod2,\n        newdata = tibble(stock_photo = c(\"yes\", \"no\")),\n        type.predict = \"response\")\n\n# A tibble: 2 × 2\n  stock_photo .fitted\n  &lt;chr&gt;         &lt;dbl&gt;\n1 yes           0.524\n2 no            0.111\n\n\nThese are the conditional probability of a new condition based on status of stock_photo. We can see this using the tally() function.\n\ntally(cond ~ stock_photo, data = mariokart, margins = TRUE, \n      format = \"proportion\")\n\n       stock_photo\ncond           no       yes\n  new   0.1111111 0.5238095\n  used  0.8888889 0.4761905\n  Total 1.0000000 1.0000000\n\n\nOr from the model coefficients.\n\nexp(-2.079442) / (1 + exp(-2.079442))\n\n[1] 0.1111111\n\nexp(-2.079442 + 2.174752) / (1 + exp(-2.079442 + 2.174752))\n\n[1] 0.5238095\n\n\n\nExercise: Fit a logistic regression model with cond as used and stock_photo as a predictor.\n\nWe repeat the code from above.\n\nmario_mod3 &lt;- glm(cond == \"used\" ~ stock_photo, data = mariokart,\n                 family = \"binomial\")\n\n\ntidy(mario_mod3)\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        2.08     0.530      3.92 0.0000881\n2 stock_photoyes    -2.17     0.565     -3.85 0.000119 \n\n\nAgain, let’s convert these coefficients to estimated probabilities using the augment() function.\n\naugment(mario_mod3,\n        newdata = tibble(stock_photo = c(\"yes\", \"no\")),\n        type.predict = \"response\")\n\n# A tibble: 2 × 2\n  stock_photo .fitted\n  &lt;chr&gt;         &lt;dbl&gt;\n1 yes           0.476\n2 no            0.889\n\n\nThis matches the output from the tally() function we observed above.\nNotice that it was not important whether we select new or used condition as the desired outcome. In either case, the logistic regression model returns the conditional probability given the value of the predictor.\n\n\n33.2.6 Interpreting the coefficients\nAt this point it seems that we created a great deal of work just to get the same results that we had from other methods. However, the logistic regression model allows us to add other predictors and it also gives us standard errors for the parameter estimates.\nLet’s first discuss the interpretation of coefficients. As a reminder, the fitted coefficients are reported from the model summary.\n\ntidy(mario_mod2)\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       -2.08     0.530     -3.92 0.0000881\n2 stock_photoyes     2.17     0.565      3.85 0.000119 \n\n\nThe variable stock_photo takes on the values 0 and 1, the value 1 of indicates the sale had a stock photo. The logistic regression model we are fitting is Equation 33.3.\n\\[\\log_{e}\\left( \\frac{p_{new}}{1-p_{new}} \\right)\n    = \\beta_0 + \\beta_1 \\mbox{stock photo} \\tag{33.3}\\]\nIf the photo is not a stock photo then the model is Equation 33.4. The left-hand side is the natural logarithm of the odds, where odds are the ratio of the probability of success divided by the probability of failure.\n\\[\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n    = \\beta_0 + \\beta_1   \n\\]\n\\[\\log_{e}\\left( \\frac{p_{\\mbox{new|no stock photo}}}{1-p_{\\mbox{new|no stock photo}}} \\right)= \\beta_0 \\tag{33.4}\\]\nIf we have a stock photo, the variable stock_photo is 1. Then Equation 33.5 is the resulting model.\n\\[ \\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n    = \\beta_0 + \\beta_1 \\tag{33.5}\\]\nThus the difference of these gives an interpretation of the \\(\\beta_1\\) coefficient, it is the log odds ratio as is shown in the derivation that follows.\n\\[\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n-\n\\log_{e}\\left( \\frac{p_{\\mbox{new|no stock photo}}}{1-p_{\\mbox{new|no stock photo}}} \\right) = \\beta_1\n\\]\n\\[\n\\log_{e}\\left(\\frac{\\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}}}{\\frac{p_{\\mbox{new|no stock photo}}}{1-p_{\\mbox{new|no stock photo}}}} \\right)\n= \\beta_1\n\\]\nFor our problem, the log odds more than double if the photo is a stock photo. It is easier to interpret odds ratios, so often analysts use \\(e^{\\beta_1}\\) as the odds ratio. Again, for our problem, the odds of a new condition game increase by a factor of 8.8 if a stock photo is used. Note that an odds ratio is not a relative risk. Relative risk is the ratio of the probability of a new game with stock photo to the probability of a new game without a stock photo. Be careful in your interpretation.\n\\[\n\\text{Relative Risk} = \\left(\\frac{p_{\\mbox{new|stock photo}}}{p_{\\mbox{new|no stock photo}}} \\right)\n\\]\n\n\n33.2.7 Comparing models\nJust as is the case for linear regression, we can compare nested models. When we examine the output of model there is a line with the residual deviance. This model is not fit using least squares but using maximum likelihood. Deviance is 2 times the negative of the log likelihood. We negate the log likelihood so that maximizing the log likelihood is equivalent to minimizing the negation. This allows the same thought process of minimizing deviance as we had for minimizing residual sum of squares. The multiplication by 2 is because an asymptotic argument shows that 2 times the negative log likelihood is approximately distributed as a Chi-square random variable.\n\nsummary(mario_mod2)\n\n\nCall:\nglm(formula = cond == \"new\" ~ stock_photo, family = \"binomial\", \n    data = mariokart)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.0794     0.5303  -3.921 8.81e-05 ***\nstock_photoyes   2.1748     0.5652   3.848 0.000119 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 191.70  on 140  degrees of freedom\nResidual deviance: 170.44  on 139  degrees of freedom\nAIC: 174.44\n\nNumber of Fisher Scoring iterations: 4\n\n\nSimilar to linear regression, we can use the anova() function to compare nested models.\n\nanova(mario_mod1, mario_mod2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: cond == \"new\" ~ 1\nModel 2: cond == \"new\" ~ stock_photo\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n1       140     191.70                         \n2       139     170.44  1    21.26 4.01e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAdding, stock_photo is a statistically significant result. The \\(p\\)-value is different from the summary() function, because it assumes the coefficient follows a normal distribution. Different assumptions, but the same conclusion.\nThe use of \\(p\\)-value to pick a best model uses statistical assumptions to select the features. Another approach is to use a predictive measure. In machine learning contexts, we use many different predictive performance measures for model selection but many are based on a confusion matrix.\nA confusion matrix generates a 2 by 2 matrix of predicted outcomes versus actual outcomes. For logistic regression, the output is a probability of success. To convert this to 0/1 outcome we pick a threshold. It is common to use 0.5 as the threshold. Probabilities above 0.5 are considered a success, in the context of our problem a new game. Let’s generate the confusion matrix.\n\naugment(mario_mod2, type.predict = \"response\") %&gt;%\n  rename(actual= starts_with('cond')) %&gt;%\n  transmute(result = as.integer(.fitted &gt; 0.5),\n            actual = as.integer(actual)) %&gt;%\n  table()\n\n      actual\nresult  0  1\n     0 32  4\n     1 50 55\n\n\nOne single number summary metric is accuracy. In this case the model was correct on \\(32 + 55\\) out of the 141 cases, or 61.7% are correct.\nThis looks like the same table we get comparing cond to stock_photo. This is the case because of the binary nature of the predictor. We only have two probability values in our prediction.\n\ntally(~cond + stock_photo, data = mariokart)\n\n      stock_photo\ncond   no yes\n  new   4  55\n  used 32  50\n\n\nIf we change the threshold we get a different accuracy. In a machine learning course, we learn about other metrics such as area under the ROC curve. Back to our problem, let’s add another variable to see if we can improve the model.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#multiple-logistic-regression",
    "href": "Logistic-Regression.html#multiple-logistic-regression",
    "title": "33  Logistic Regression",
    "section": "33.3 Multiple logistic regression",
    "text": "33.3 Multiple logistic regression\nLet’s add total_pr to the model. This model is something that we could not have done in the previous models we learned about.\n\nmario_mod4 &lt;- glm(cond == \"new\" ~ stock_photo + total_pr,\n                  data = mariokart, family = \"binomial\")\n\nNotice that we use the same formula syntax as we had done with linear regression.\n\nsummary(mario_mod4)\n\n\nCall:\nglm(formula = cond == \"new\" ~ stock_photo + total_pr, family = \"binomial\", \n    data = mariokart)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -11.31951    1.88333  -6.010 1.85e-09 ***\nstock_photoyes   2.11633    0.68551   3.087  0.00202 ** \ntotal_pr         0.19348    0.03562   5.431 5.60e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 191.70  on 140  degrees of freedom\nResidual deviance: 119.21  on 138  degrees of freedom\nAIC: 125.21\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the summary, both stock_photo and total_pr are statistically significant.\n\nExercise:\nInterpret the coefficient associated with the predictor total_pr.\n\nFor one dollar increase in total price of the auction, the odds ratio increases by \\(exp(\\beta_2)\\), 1.21, for a given condition of the stock photo variable.\nThis is similar to an interpretation we had for multiple linear regression. We had to specify that the other predictors are held constant and then we increased the variable of interest by one unit.\nBesides using individual predictor \\(p\\)-values to assess the model, can also use a confusion matrix.\n\naugment(mario_mod4, type.predict = \"response\") %&gt;%\n  rename(actual = starts_with('cond')) %&gt;%\n  transmute(result = as.integer(.fitted &gt; 0.5),\n            actual = as.integer(actual)) %&gt;%\n  table()\n\n      actual\nresult  0  1\n     0 71 16\n     1 11 43\n\n\nFor our new model, the accuracy improved to \\(71 + 43\\) out of the 141 cases, or 80.9.7%. Without a measure of variability, we don’t know if this is significant improvement or just the variability in the modeling procedure. On the surface, it appears to be an improvement.\nAs we experiment to improve the model, let’s use a quadratic term in our model.\n\nmario_mod5 &lt;- glm(cond == \"new\" ~ stock_photo + poly(total_pr, 2),\n                  data = mariokart, family = \"binomial\")\n\nUsing the individual \\(p\\)-values, it appears that a quadratic term is significant but it is marginal.\n\nsummary(mario_mod5)\n\n\nCall:\nglm(formula = cond == \"new\" ~ stock_photo + poly(total_pr, 2), \n    family = \"binomial\", data = mariokart)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -2.4407     0.6347  -3.845  0.00012 ***\nstock_photoyes       2.0411     0.6494   3.143  0.00167 ** \npoly(total_pr, 2)1  23.7534     4.5697   5.198 2.01e-07 ***\npoly(total_pr, 2)2  -9.9724     4.1999  -2.374  0.01758 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 191.70  on 140  degrees of freedom\nResidual deviance: 114.05  on 137  degrees of freedom\nAIC: 122.05\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe get a similar result if we use the anova() function.\n\nanova(mario_mod4, mario_mod5, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: cond == \"new\" ~ stock_photo + total_pr\nModel 2: cond == \"new\" ~ stock_photo + poly(total_pr, 2)\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1       138     119.21                       \n2       137     114.05  1   5.1687    0.023 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFinally, the confusion matrix results in a slight improvement in accuracy to 82.3%.\n\naugment(mario_mod5, type.predict = \"response\") %&gt;%\n  rename(actual = starts_with('cond')) %&gt;%\n  transmute(result = as.integer(.fitted &gt; 0.5),\n            actual = as.integer(actual)) %&gt;%\n  table()\n\n      actual\nresult  0  1\n     0 69 12\n     1 13 47\n\n\nAlmost any classifier will have some error. In the model above, we have decided that it is okay to allow up to 9%, 13 out of 141, of the games for sale to be classified as new when they are really used. If we wanted to make it a little harder to classify an item as new, we could use a cutoff, threshold, of 0.75. This would have two effects. Because it raises the standard for what can be classified as new, it reduces the number of used games that are classified as new. However, it will also fail to correctly classify an increased fraction of new games as new, see the code below. No matter the complexity and the confidence we might have in our model, these practical considerations are absolutely crucial to making a helpful classification model. Without them, we could actually do more harm than good by using our statistical model. This tradeoff is similar to the one we found between Type 1 and Type 2 errors. Notice that the accuracy has also dropped slightly.\n\naugment(mario_mod5, type.predict = \"response\") %&gt;%\n  rename(actual = starts_with('cond')) %&gt;%\n  transmute(result = as.integer(.fitted &gt; 0.75),\n            actual = as.integer(actual)) %&gt;%\n  table()\n\n      actual\nresult  0  1\n     0 78 22\n     1  4 37\n\n\nIn a machine learning course, we learn about better methods to assess predictive accuracy as well as more sophisticated methods to transform and adapt our predictor variables.\n\nExercise Find the probability that an auctioned game is new if the total price is 50 and it uses a stock photo.\n\nIt is not clear how to use the coefficients in the regression output since R is performing a transformation on total_pr variable. Let’s approach this in two ways. First we will use the augment() function to do the hard work.\n\naugment(mario_mod5,\n        newdata = tibble(stock_photo = \"yes\", total_pr = 50),\n        type.predict = \"response\")\n\n# A tibble: 1 × 3\n  stock_photo total_pr .fitted\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 yes               50   0.693\n\n\nWe predict that the probability of the game being new if it uses a stock photo and the total price is 50 is 69.3%.\nIf we want to recreate the calculation, we need to use a raw polynomial.\n\nmario_mod6 &lt;- glm(cond == \"new\" ~ stock_photo + total_pr + I(total_pr^2),\n                  data = mariokart, family = \"binomial\")\ntidy(mario_mod6)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -30.7       9.08        -3.38 0.000732\n2 stock_photoyes   2.04      0.649        3.14 0.00167 \n3 total_pr         0.969     0.343        2.83 0.00470 \n4 I(total_pr^2)   -0.00760   0.00320     -2.37 0.0176  \n\n\nWe can calculate the link as a linear combination, an inner product of coefficients and values.\n\\[\n-30.67 + 2.04 + 0.969 * 50 -0.007*50^2 = 0.814\n\\]\n\ntidy(mario_mod6) %&gt;%\n  select(estimate) %&gt;% \n  pull() %*% c(1, 1, 50, 50^2)\n\n          [,1]\n[1,] 0.8140013\n\n\nUsing the inverse transform of the logit function, we find the probability of the game being new given the predictor values.\n\\[\n\\frac{\\ e^{.814}\\ }{\\ 1\\ +\\ e^{.814}\\ } = 0.693\n\\]\n\nexp(.814) / (1 + exp(.814))\n\n[1] 0.6929612\n\n\n\n33.3.1 Diagnostics for logistic regression\nThe assumptions for logistic regression and the diagnostic tools are similar to what we found for linear regression. However, with the binary nature of the outcome, we often need large data sets to check. We will not devote much time to performing diagnostics for logistic regression because we are interested in using it as a predictive model. The assumptions are:\n\nEach predictor \\(x_i\\) is linearly related to logit\\((p_i)\\) if all other predictors are held constant. This is similar to our linear fit diagnostic in linear multiple regression.\n\nEach outcome \\(Y_i\\) is independent of the other outcomes.\n\nThere are no influential data points.\n\nMulticollinearity is minimal.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#confidence-intervals",
    "href": "Logistic-Regression.html#confidence-intervals",
    "title": "33  Logistic Regression",
    "section": "33.4 Confidence intervals",
    "text": "33.4 Confidence intervals\nIn this section we will generate confidence intervals. This section is experimental since we are not sure how do() from the mosaic package will work with the glm() function, but let’s experiment.\n\n33.4.1 Confidence intervals for a parameter\nFirst, let’s use the R built-in function confint() to find the confidence interval for the simple logistic regression model coefficients.\n\nconfint(mario_mod4)\n\nWaiting for profiling to be done...\n\n\n                     2.5 %     97.5 %\n(Intercept)    -15.4048022 -7.9648042\nstock_photoyes   0.8888216  3.6268545\ntotal_pr         0.1297024  0.2705395\n\n\nThese are not symmetric around the estimate because the method is using a profile-likelihood method. We can get symmetric intervals based on the central limit theorem using the function confint.default().\n\nconfint.default(mario_mod4)\n\n                     2.5 %     97.5 %\n(Intercept)    -15.0107641 -7.6282654\nstock_photoyes   0.7727450  3.4599054\ntotal_pr         0.1236583  0.2632982\n\n\nThese results are close. We recommend using the profile-likelihood method.\nNow, let’s work with the do() function to determine if we can get similar results.\n\ndo(1)*mario_mod4\n\n  Intercept stock_photoyes  total_pr .row .index\n1 -11.31951       2.116325 0.1934783    1      1\n\n\n\ntidy(mario_mod4)\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic       p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     -11.3      1.88       -6.01 0.00000000185\n2 stock_photoyes    2.12     0.686       3.09 0.00202      \n3 total_pr          0.193    0.0356      5.43 0.0000000560 \n\n\nIt looks like do() is performing as expected. Let’s now perform one resample to see what happens.\n\nset.seed(23)\ndo(1)*glm(cond == \"new\" ~ stock_photo + total_pr,\n          data = resample(mariokart), family = \"binomial\")\n\n  Intercept stock_photoyes  total_pr .row .index\n1 -14.06559       4.945683 0.1940279    1      1\n\n\nAgain, it looks like what we expect. Now let’s bootstrap the coefficients and summarize the results.\n\nset.seed(5011)\nresults &lt;- do(1000)*glm(cond == \"new\" ~ stock_photo + total_pr, \n                        data = resample(mariokart), family = \"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nhead(results)\n\n  Intercept stock_photoyes  total_pr .row .index\n1 -11.22155       1.665492 0.1986654    1      1\n2 -13.25708       1.889510 0.2371109    1      2\n3 -11.54544       2.871460 0.1867757    1      3\n4 -19.25785       5.816050 0.2829247    1      4\n5 -10.86631       3.255767 0.1672335    1      5\n6 -13.62425       1.842765 0.2533934    1      6\n\n\nNow we will plot the bootstrap sampling distribution on the parameter associated with total_pr.\n\nresults %&gt;%\n  gf_histogram(~total_pr, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(title = \"Bootstrap sampling distribtuion\",\n          x = \"total price paramater estimate\")\n\n\n\n\n\n\n\n\nThe printout from the logistic regression model assumes normality for the sampling distribution of the total_pr coefficient, but it appears to be positively skewed, skewed to the right. The 95% confidence interval found using cdata().\n\ncdata(~total_pr, data = results)\n\n         lower     upper central.p\n2.5% 0.1388783 0.3082659      0.95\n\n\nThis result is closer to the result from profile-likelihood. Since the interval does not include the value of zero, we can be 95% confident that it is not zero. This is close to what we found using the R function confint().\n\n\n33.4.2 Confidence intervals for probability of success\nWe can use the results from the bootstrap to get a confidence interval on probability of success. We will calculate a confidence for a game with a stock photo and total price of $50. As a reminder, the probability of the game being new is 0.69.\n\naugment(mario_mod5,\n        newdata = tibble(stock_photo = \"yes\", total_pr = 50),\n        type.predict = \"response\")\n\n# A tibble: 1 × 3\n  stock_photo total_pr .fitted\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 yes               50   0.693\n\n\nThe key is to use the coefficient from each resampled data set to calculate a probability of success.\n\nhead(results)\n\n  Intercept stock_photoyes  total_pr .row .index\n1 -11.22155       1.665492 0.1986654    1      1\n2 -13.25708       1.889510 0.2371109    1      2\n3 -11.54544       2.871460 0.1867757    1      3\n4 -19.25785       5.816050 0.2829247    1      4\n5 -10.86631       3.255767 0.1672335    1      5\n6 -13.62425       1.842765 0.2533934    1      6\n\n\n\nresults_pred &lt;- results %&gt;% \n  mutate(pred = 1 / (1 + exp(-1*(Intercept + stock_photoyes + 50*total_pr))))\n\n\ncdata(~pred, data = results_pred)\n\n       lower     upper central.p\n2.5% 0.50388 0.7445598      0.95\n\n\nWe are 95% confident that expected probability a game with a stock photo and a total price of $50 is between 50.4% and 74.4%.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#summary",
    "href": "Logistic-Regression.html#summary",
    "title": "33  Logistic Regression",
    "section": "33.5 Summary",
    "text": "33.5 Summary\nIn this chapter, we learned how to extend linear models to outcomes that are binary. We built and interpreted models. We also used resampling to find confidence intervals.",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic-Regression.html#footnotes",
    "href": "Logistic-Regression.html#footnotes",
    "title": "33  Logistic Regression",
    "section": "",
    "text": "Diez DM, Barr CD, and etinkaya-Rundel M. 2012. openintro: OpenIntro data sets and supplemental functions. http://cran.r-project.org/web/packages/openintro↩︎",
    "crumbs": [
      "Statistical Modeling - Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Probability and Statistics",
    "section": "",
    "text": "Preface\nThis book is based on the notes we created for our students as part of a one semester course on probability and statistics. We developed these notes from three primary resources. The most important is the OpenIntro Introductory Statistics with Randomization and Simulation (ISRS) (Diez, Barr, and Çetinkaya-Rundel 2014) book. In parts, we have used their notes and homework problems. However, in most cases we have altered their work to fit our needs. The second most important book for our work is Introduction to Probability and Statistics Using R (Kerns 2010). Finally, we have used some examples, code, and ideas from the first edition of Prium’s book, Foundations and Applications of Statistics: An Introduction Using R (R. J. Pruim 2011).\nIn a 2024 reorganization of our inference block, we revised our inference case study and added a chapter on sampling distributions. The materials for the case study utilized the OpenIntro ISRS (Diez, Barr, and Çetinkaya-Rundel 2014) and Introduction to Modern Statistics (2e) (Çetinkaya-Rundel and Hardin 2024). We have altered their work to fit our needs, primarily by piecing together information from their inference block. Additionally, the new sampling distributions chapter borrows heavily from the OpenIntro Statistics (Diez, Çetinkaya-Rundel, and Barr 2019) and from the sampling distributions lessons by Skew the Script (Skew the Script 2024). We have used their materials with minor modifications to transform the lesson activities into a book chapter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Computational Probability and Statistics",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nWe designed this book for the study of statistics that maximizes computational ideas while minimizing algebraic symbol manipulation. Although we do discuss traditional small-sample, normal-based inference and some of the classical probability distributions, we rely heavily on ideas such as simulation, permutations, and the bootstrap. This means that students with a background in differential and integral calculus will be successful with this book.\nThis book makes extensive using of the R programming language. In particular we focus both on the tidyverse and mosaic packages. We include a significant amount of code in our notes and frequently demonstrate multiple ways of completing a task. We have used this book for junior and sophomore college students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure-and-how-to-use-it",
    "href": "index.html#book-structure-and-how-to-use-it",
    "title": "Computational Probability and Statistics",
    "section": "Book structure and how to use it",
    "text": "Book structure and how to use it\nThis book is divided into four parts. Each part begins with a case study that introduces many of the main ideas of each part. Each chapter is designed to be a standalone 50 minute lesson. Within each chapter, we give exercises that can be worked in class and we provide learning objectives.\nThis book assumes students have access to R. Finally, we keep the number of homework problems to a reasonable level and assign all problems.\nThe four parts of the book are:\n\nDescriptive Statistical Modeling: This part introduces the student to data collection methods, summary statistics, visual summaries, and exploratory data analysis.\nProbability Modeling: We discuss the foundational ideas of probability, counting methods, and common distributions. We use both calculus and simulation to find moments and probabilities. We introduce basic ideas of multivariate probability. We include method of moments and maximum likelihood estimators.\nInferential Statistical Modeling: We discuss many of the basic inference ideas found in a traditional introductory statistics class but we add ideas of bootstrap and permutation methods.\nPredictive Statistical Modeling: The final part introduces prediction methods, mainly in the form of linear regression. This part also includes inference for regression.\n\nThe learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for:\n\nDeveloping probabilistic models.\n\nDeveloping statistical models for description, inference, and prediction.\n\nAdvancing practical and theoretical analytic experience and skills.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Computational Probability and Statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo take this course, students are expected to have completed calculus up through and including integral calculus. We do have multivariate ideas in the course, but they are easily taught and don’t require previous exposure to calculus III (multivariable calculus). We don’t assume the students have any programming experience and, thus, we include a great deal of code. We have historically supplemented the course with Data Camp courses. We have also used Posit Cloud to help students get started in R without the burden of loading and maintaining software.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Computational Probability and Statistics",
    "section": "Packages",
    "text": "Packages\nThese notes make use of the following packages in R: knitr (Xie 2024), rmarkdown (Allaire et al. 2024), mosaic (R. Pruim, Kaplan, and Horton 2024), tidyverse (Wickham 2023), ISLR (James et al. 2021), vcd (Meyer et al. 2023), ggplot2 (Wickham et al. 2024), MASS (Ripley 2024), openintro (Çetinkaya-Rundel et al. 2024), broom (Robinson, Hayes, and Couch 2024), infer (Bray et al. 2024), kableExtra (Zhu 2024), and DT (Xie, Cheng, and Tan 2024).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Probability and Statistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe have been lucky to have numerous open sources to help facilitate this work. Thank you to those who helped to provide edits including Jessica Hauschild, Justin Graham, Kris Pruitt, Matt Davis, and Skyler Royse.\n\n\n\n\n\n\n\n\n\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#file-creation-information",
    "href": "index.html#file-creation-information",
    "title": "Computational Probability and Statistics",
    "section": "File Creation Information",
    "text": "File Creation Information\n\nFile creation date: 2024-08-06\nR version 4.4.1 (2024-06-14)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Computational Probability and Statistics",
    "section": "References",
    "text": "References\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nBray, Andrew, Chester Ismay, Evgeni Chasnovski, Simon Couch, Ben Baumer, and Mine Cetinkaya-Rundel. 2024. Infer: Tidy Statistical Inference. https://github.com/tidymodels/infer.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. Openintro: Datasets and Supplemental Functions from OpenIntro Textbooks and Labs. http://openintrostat.github.io/openintro/.\n\n\nÇetinkaya-Rundel, Mine, and Johanna Hardin. 2024. Introduction to Modern Statistics. 2nd ed. OpenIntro. https://openintro-ims.netlify.app/.\n\n\nDiez, David, Christopher Barr, and Mine Çetinkaya-Rundel. 2014. Introductory Statistics with Randomization and Simulation. 1st ed. Openintro. https://www.openintro.org/book/isrs/.\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Christopher D Barr. 2019. OpenIntro Statistics. 4th ed. OpenIntro. https://www.openintro.org/book/os/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://www.statlearning.com.\n\n\nKerns, Jay. 2010. Introductory to Probability and Statistics with r. 1st ed. http://ipsur.r-forge.r-project.org/book/download/IPSUR.pdf.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. Vcd: Visualizing Categorical Data.\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.\n\n\nPruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2024. Mosaic: Project MOSAIC Statistics and Mathematics Teaching Utilities. https://github.com/ProjectMOSAIC/mosaic.\n\n\nRipley, Brian. 2024. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2024. Broom: Convert Statistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nSkew the Script. 2024. “Sampling Distributions.” https://skewthescript.org/ap-stats-curriculum/part-6.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Joe Cheng, and Xianying Tan. 2024. DT: A Wrapper of the JavaScript Library DataTables. https://github.com/rstudio/DT.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with Kable and Pipe Syntax. http://haozhu233.github.io/kableExtra/.",
    "crumbs": [
      "Preface"
    ]
  }
]