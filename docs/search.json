[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Probability and Statistics",
    "section": "",
    "text": "Preface\nThis book is based on the notes we created for our students as part of a one semester course on probability and statistics. We developed these notes from three primary resources. The most important is the Openintro Introductory Statistics with Randomization and Simulation (Diez, Barr, and Çetinkaya-Rundel 2014) book. In parts, we have used their notes and homework problems. However, in most cases we have altered their work to fit our needs. The second most important book for our work is Introduction to Probability and Statistics Using R (Kerns 2010). Finally, we have used some examples, code, and ideas from the first edition of Prium’s book, Foundations and Applications of Statistics: An Introduction Using R (R. J. Pruim 2011).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Computational Probability and Statistics",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nWe designed this book for the study of statistics that maximizes computational ideas while minimizing algebraic symbol manipulation. Although we do discuss traditional small-sample, normal-based inference and some of the classical probability distributions, we rely heavily on ideas such as simulation, permutations, and the bootstrap. This means that students with a background in differential and integral calculus will be successful with this book.\nThis book makes extensive using of the R programming language. In particular we focus both on the tidyverse and mosaic packages. We include a significant amount of code in our notes and frequently demonstrate multiple ways of completing a task. We have used this book for junior and sophomore college students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure-and-how-to-use-it",
    "href": "index.html#book-structure-and-how-to-use-it",
    "title": "Computational Probability and Statistics",
    "section": "Book structure and how to use it",
    "text": "Book structure and how to use it\nThis book is divided into four parts. Each part begins with a case study that introduces many of the main ideas of each part. Each chapter is designed to be a standalone 50 minute lesson. Within each chapter, we give exercises that can be worked in class and we provide learning objectives.\nThis book assumes students have access to R. Finally, we keep the number of homework problems to a reasonable level and assign all problems.\nThe four parts of the book are:\n\nDescriptive Statistical Modeling: This part introduces the student to data collection methods, summary statistics, visual summaries, and exploratory data analysis.\nProbability Modeling: We discuss the foundational ideas of probability, counting methods, and common distributions. We use both calculus and simulation to find moments and probabilities. We introduce basic ideas of multivariate probability. We include method of moments and maximum likelihood estimators.\nInferential Statistical Modeling: We discuss many of the basic inference ideas found in a traditional introductory statistics class but we add ideas of bootstrap and permutation methods.\nPredictive Statistical Modeling: The final part introduces prediction methods, mainly in the form of linear regression. This part also includes inference for regression.\n\nThe learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for:\n\nDeveloping probabilistic models.\n\nDeveloping statistical models for description, inference, and prediction.\n\nAdvancing practical and theoretical analytic experience and skills.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Computational Probability and Statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo take this course, students are expected to have completed calculus up through and including integral calculus. We do have multivariate ideas in the course, but they are easily taught and don’t require calculus III. We don’t assume the students have any programming experience and, thus, we include a great deal of code. We have historically supplemented the course with Data Camp courses. We have also used RStudio Cloud to help students get started in R without the burden of loading and maintaining software.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Computational Probability and Statistics",
    "section": "Packages",
    "text": "Packages\nThese notes make use of the following packages in R: knitr (Xie 2024), rmarkdown (Allaire et al. 2024), mosaic (R. Pruim, Kaplan, and Horton 2024), tidyverse (Wickham 2023), ISLR (James et al. 2021), vcd (Meyer et al. 2023), ggplot2 (Wickham et al. 2024), MASS (Ripley 2024), openintro (Çetinkaya-Rundel et al. 2022), broom (Robinson, Hayes, and Couch 2023), infer (R-infer?), kableExtra (Zhu 2024), and DT (Xie, Cheng, and Tan 2024).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#solutions-manual",
    "href": "index.html#solutions-manual",
    "title": "Computational Probability and Statistics",
    "section": "Solutions Manual",
    "text": "Solutions Manual\nThe accompanying solutions manual is available here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Probability and Statistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe have been lucky to have numerous open sources to help facilitate this work. Thank you to those who helped to correct mistakes to include Skyler Royse.\n\n\n\n\n\n\n\n\n\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#file-creation-information",
    "href": "index.html#file-creation-information",
    "title": "Computational Probability and Statistics",
    "section": "File Creation Information",
    "text": "File Creation Information\n\nFile creation date: 2024-06-18\nR version 4.3.3 (2024-02-29)\n\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2022. Openintro: Data Sets and Supplemental Functions from OpenIntro Textbooks and Labs. http://openintrostat.github.io/openintro/.\n\n\nDiez, David, Christopher Barr, and Mine Çetinkaya-Rundel. 2014. Introductory Statistics with Randomization and Simulation. 1st ed. Openintro. https://www.openintro.org/book/isrs/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://www.statlearning.com.\n\n\nKerns, Jay. 2010. Introductory to Probability and Statistics with r. 1st ed. http://ipsur.r-forge.r-project.org/book/download/IPSUR.pdf.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. Vcd: Visualizing Categorical Data.\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.\n\n\nPruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2024. Mosaic: Project MOSAIC Statistics and Mathematics Teaching Utilities. https://github.com/ProjectMOSAIC/mosaic.\n\n\nRipley, Brian. 2024. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. Broom: Convert Statistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Joe Cheng, and Xianying Tan. 2024. DT: A Wrapper of the JavaScript Library DataTables. https://github.com/rstudio/DT.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with Kable and Pipe Syntax. http://haozhu233.github.io/kableExtra/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-Objectives.html",
    "href": "00-Objectives.html",
    "title": "Objectives",
    "section": "",
    "text": "Descriptive Statistical Modeling",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#descriptive-statistical-modeling",
    "href": "00-Objectives.html#descriptive-statistical-modeling",
    "title": "Objectives",
    "section": "",
    "text": "1 - Data Case Study\n\nUse R for basic analysis and visualization.\nCompile a pdf file report from a RMD or qmd file in R.\n\n\n\n2 - Data Basics\n\nDefine and use properly in context all new terminology, to include: case, variables, data frame, associated variables, independent, and discrete and continuous variables.\nIdentify and define the different types of variables.\nGiven a study description, describe the research question.\nIn R, create a scatterplot and determine the association of two numerical variables from the plot.\n\n\n\n3 - Overview of Data Collection Principles\n\nDefine and use properly in context all new terminology, to include: population, sample, anecdotal evidence, bias, simple random sample, systematic sample, non-response bias, representative sample, convenience sample, explanatory variable, response variable, observational study, cohort, experiment, randomized experiment, and placebo.\nFrom a description of a research project, be able to describe the population of interest, the generalizability of the study, the explanatory and response variables, whether it is observational or experimental, and determine the type of sample.\nIn the context of a problem, explain how to conduct a sample for the different types of sampling procedures.\n\n\n\n4 - Studies\n\nDefine and use properly in context all new terminology, to include: confounding variable, prospective study, retrospective study, simple random sampling, stratified sampling, strata, cluster sampling, multistage sampling, experiment, randomized experiment, control, replicate, blocking, treatment group, control group, blinded study, placebo, placebo effect, and double-blind.\nGiven a study description, be able to describe the study using correct terminology.\nGiven a scenario, describe flaws in reasoning and propose study and sampling designs.\n\n\n\n5 - Numerical Data\n\nDefine and use properly in context all new terminology, to include: scatterplot, dot plot, mean, distribution, point estimate, weighted mean, histogram, data density, right skewed, left skewed, symmetric, mode, unimodal, bimodal, multimodal, variance, standard deviation, box plot, median, interquartile range, first quartile, third quartile, whiskers, outlier, robust estimate, transformation.\nIn R, generate summary statistics for a numerical variable, including breaking down summary statistics by groups.\nIn R, generate appropriate graphical summaries of numerical variables.\nInterpret and explain output both graphically and numerically.\n\n\n\n6 - Categorical Data\n\nDefine and use properly in context all new terminology, to include: factor, contingency table, marginal counts, joint counts, frequency table, relative frequency table, bar plot, conditioning, segmented bar plot, mosaic plot, pie chart, side-by-side box plot, density plot.\nIn R, generate tables for categorical variable(s).\nIn R, generate appropriate graphical summaries of categorical and numerical variables.\nInterpret and explain output both graphically and numerically.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#probability-modeling",
    "href": "00-Objectives.html#probability-modeling",
    "title": "Objectives",
    "section": "Probability Modeling",
    "text": "Probability Modeling\n\n7 - Probability Case Study\n\nUse R to simulate a probabilistic model.\nUse basic counting methods.\n\n\n\n8 - Probability Rules\n\nDefine and use properly in context all new terminology related to probability, including: sample space, outcome, event, subset, intersection, union, complement, probability, mutually exclusive, exhaustive, independent, multiplication rule, permutation, combination.\nApply basic probability and counting rules to find probabilities.\nDescribe the basic axioms of probability.\nUse R to calculate and simulate probabilities of events.\n\n\n\n9 - Conditional Probability\n\nDefine conditional probability and distinguish it from joint probability.\nFind a conditional probability using its definition.\nUsing conditional probability, determine whether two events are independent.\nApply Bayes’ Rule mathematically and via simulation.\n\n\n\n10 - Random Variables\n\nDefine and use properly in context all new terminology, to include: random variable, discrete random variable, continuous random variable, mixed random variable, distribution function, probability mass function, cumulative distribution function, moment, expectation, mean, variance.\nGiven a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events.\nSimulate random variables for a discrete distribution.\nFind the moments of a discrete random variable.\nFind the expected value of a linear transformation of a random variable.\n\n\n\n11 - Continuous Random Variables\n\nDefine and properly use in context all new terminology, to include: probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables.\nGiven a continuous random variable, find probabilities using the pdf and/or the cdf.\nFind the mean and variance of a continuous random variable.\n\n\n\n12 - Named Discrete Distributions\n\nRecognize and set up for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments.\nUse R to calculate probabilities and quantiles involving random variables with common discrete distributions.\n\n\n\n13 - Named Continuous Distributions\n\nRecognize when to use common continuous distributions (Uniform, Exponential, Gamma, Normal, Weibull, and Beta), identify parameters, and find moments.\nUse R to calculate probabilities and quantiles involving random variables with common continuous distributions.\nUnderstand the relationship between the Poisson process and the Poisson & Exponential distributions.\nKnow when to apply and then use the memory-less property.\n\n\n\n14 - Multivariate Distributions\n\nDefine (and distinguish between) the terms joint probability mass/density function, marginal pmf/pdf, and conditional pmf/pdf.\nGiven a joint pmf/pdf, obtain the marginal and conditional pmfs/pdfs.\nUse joint, marginal and conditional pmfs/pdfs to obtain probabilities.\n\n\n\n15 - Multivariate Expectation\n\nGiven a joint pmf/pdf, obtain means and variances of random variables and functions of random variables.\nDefine the terms covariance and correlation, and given a joint pmf/pdf, obtain the covariance and correlation between two random variables.\nGiven a joint pmf/pdf, determine whether random variables are independent of one another.\nFind conditional expectations.\n\n\n\n16 - Transformations\n\nGiven a discrete random variable, determine the distribution of a transformation of that random variable.\nGiven a continuous random variable, use the cdf method to determine the distribution of a transformation of that random variable.\nUse simulation methods to find the distribution of a transform of single or multivariate random variables.\n\n\n\n17 - Estimation Methods\n\nObtain a method of moments estimate of a parameter or set of parameters.\nGiven a random sample from a distribution, obtain the likelihood function.\nObtain a maximum likelihood estimate of a parameter or set of parameters.\nDetermine if an estimator is unbiased.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#inferential-statistical-modeling",
    "href": "00-Objectives.html#inferential-statistical-modeling",
    "title": "Objectives",
    "section": "Inferential Statistical Modeling",
    "text": "Inferential Statistical Modeling\n\n18 - Hypothesis Testing Case Study\n\nDefine and use properly in context all new terminology, to include: point estimate, null hypothesis, alternative hypothesis, hypothesis test, randomization, permutation test, test statistic, and \\(p\\)-value.\nConduct a hypothesis test using a randomization test, to include all 4 steps.\n\n\n\n19 - Hypothesis Testing with Simulation\n\nKnow and properly use the terminology of a hypothesis test, to include: null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type I error, type II error, false positive, false negative, null distribution, and sampling distribution.\nConduct all four steps of a hypothesis test using randomization.\nDiscuss and explain the ideas of decision errors, one-sided versus two-sided tests, and the choice of a significance level.\n\n\n\n20 - Hypothesis Testing with Known Distributions\n\nKnow and properly use the terminology of a hypothesis test, to include: permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, and power.\nConduct all four steps of a hypothesis test using probability models.\n\n\n\n21 - Hypothesis Testing with the Central Limit Theorem\n\nExplain the central limit theorem and when it can be used for inference.\nConduct hypothesis tests of a single mean and proportion using the CLT and R.\nExplain how the \\(t\\) distribution relates to the normal distribution, where it is used, and how changing parameters impacts the shape of the distribution.\n\n\n\n22 - Additional Hypothesis Tests\n\nConduct and interpret a goodness of fit test using both Pearson’s chi-squared and randomization to evaluate the independence between two categorical variables.\nExplain how the chi-squared distribution relates to the normal distribution, where it is used, and how changing parameters impacts the shape of the distribution.\nConduct and interpret a hypothesis test for equality of two means and equality of two variances using both permutation and the CLT.\nConduct and interpret a hypothesis test for paired data.\nKnow and check the assumptions for Pearson’s chi-square and two-sample \\(t\\) tests.\n\n\n\n23 - Analysis of Variance\n\nConduct and interpret a hypothesis test for equality of two or more means using both permutation and the \\(F\\) distribution.\nKnow and check the assumptions for ANOVA.\n\n\n\n24 - Confidence Intervals\n\nUsing asymptotic methods based on the normal distribution, construct and interpret a confidence interval for an unknown parameter.\nDescribe the relationships between confidence intervals, confidence level, and sample size.\nDescribe the relationships between confidence intervals and hypothesis testing.\nCalculate confidence intervals for proportions using three different approaches in R: explicit calculation, binom.test(), and prop_test().\n\n\n\n25 - Bootstrap\n\nUse the bootstrap to estimate the standard error of a sample statistic.\nUsing bootstrap methods, obtain and interpret a confidence interval for an unknown parameter, based on a random sample.\nDescribe the advantages, disadvantages, and assumptions behind bootstrapping for confidence intervals.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "00-Objectives.html#predictive-statistical-modeling",
    "href": "00-Objectives.html#predictive-statistical-modeling",
    "title": "Objectives",
    "section": "Predictive Statistical Modeling",
    "text": "Predictive Statistical Modeling\n\n26 - Linear Regression Case Study\n\nUsing R, generate a linear regression model and use it to produce a prediction model.\nUsing plots, check the assumptions of a linear regression model.\n\n\n\n27 - Linear Regression Basics\n\nObtain parameter estimates of a simple linear regression model, given a sample of data.\nInterpret the coefficients of a simple linear regression.\nCreate a scatterplot with a regression line.\nExplain and check the assumptions of linear regression.\nUse and be able to explain all new terminology, to include: response, predictor, linear regression, simple linear regression, coefficients, residual, extrapolation.\n\n\n\n28 - Linear Regression Inference\n\nGiven a simple linear regression model, conduct inference on the coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nGiven a simple linear regression model, calculate the predicted response for a given value of the predictor.\nBuild and interpret confidence and prediction intervals for values of the response variable.\n\n\n\n29 - Linear Regression Diagnostics\n\nObtain and interpret \\(R\\)-squared and the \\(F\\)-statistic.\nUse R to evaluate the assumptions of a linear model.\nIdentify and explain outliers and leverage points.\n\n\n\n30 - Simulated-Based Linear Regression\n\nUsing the bootstrap, generate confidence intervals and estimates of standard error for parameter estimates from a linear regression model.\nGenerate and interpret bootstrap confidence intervals for predicted values.\nGenerate bootstrap samples from sampling rows of the data and from sampling residuals, and explain why you might prefer one method over the other.\nInterpret regression coefficients for a linear model with a categorical explanatory variable.\n\n\n\n31 - Multiple Linear Regression\n\nCreate and interpret a model with multiple predictors and check assumptions.\nGenerate and interpret confidence intervals for estimates.\nExplain adjusted \\(R^2\\) and multi-collinearity.\nInterpret regression coefficients for a linear model with multiple predictors.\nBuild and interpret models with higher order terms.\n\n\n\n32 - Logistic Regression\n\nUsing R, conduct logistic regression, interpret the output, and perform model selection.\nWrite the logistic regression model and predict outputs for given inputs.\nFind confidence intervals for parameter estimates and predictions.\nCreate and interpret a confusion matrix.",
    "crumbs": [
      "Objectives"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html",
    "href": "01-Data-Case-Study.html",
    "title": "1  Data Case Study",
    "section": "",
    "text": "1.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#objectives",
    "href": "01-Data-Case-Study.html#objectives",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Use R for basic analysis and visualization.\nCompile a pdf file report from a RMD or qmd file in R.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "href": "01-Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "title": "1  Data Case Study",
    "section": "1.2 Introduction to descriptive statistical modeling",
    "text": "1.2 Introduction to descriptive statistical modeling\nIn this first block of material, we will focus on data types, collection methods, summaries, and visualizations. We also intend to introduce computing via the R package. Programming in R requires some focus early in this book and we will supplement with some online courses. There is relatively little mathematics in this first block.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#the-data-analytic-process",
    "href": "01-Data-Case-Study.html#the-data-analytic-process",
    "title": "1  Data Case Study",
    "section": "1.3 The data analytic process",
    "text": "1.3 The data analytic process\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. It is helpful to put statistics in the context of a general process of investigation:\n\nIdentify a question or problem.\nCollect relevant data on the topic.\nExplore and understand the data.\nAnalyze the data.\nForm a conclusion.\nMake decisions based on the conclusion.\n\nThis is typical of an explanatory process because it starts with a research question and proceeds. However, sometimes an analysis is exploratory in nature. There is data but not necessarily a research question. The purpose of the analysis is to find interesting features in the data and sometimes generate hypotheses. In this book, we focus on the explanatory aspects of analysis.\nStatistics as a subject focuses on making stages 2-5 objective, rigorous, and efficient. That is, statistics has three primary components:\n\nHow best can we collect data?\n\nHow should it be analyzed?\n\nAnd what can we infer from the analysis?\n\nThe topics scientists investigate are as diverse as the questions they ask. However, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference. This chapter provides a glimpse into these and other themes we will encounter throughout the rest of the book.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#case-study",
    "href": "01-Data-Case-Study.html#case-study",
    "title": "1  Data Case Study",
    "section": "1.4 Case study",
    "text": "1.4 Case study\nIn this chapter, we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke. 1 2 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer:\n\n1.4.1 Research question\n\nDoes the use of stents reduce the risk of stroke?\n\n\n\n1.4.2 Collect the relevant data\nThe researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups:\nTreatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification.\nControl group. Patients in the control group received the same medical management as the treatment group but did not receive stents.\nResearchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group.\nThis is an experiment and not an observational study. We will learn more about these ideas in this block.\nResearchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment.\n\n\n1.4.3 Import data\nWe begin our first use of R.\nIf you need to install a package, most likely it will be on CRAN, the Comprehensive R Archive Network. Before a package can be used, it must be installed on the computer (once per computer or account) and loaded into a session (once per R session). When you exit R, the package stays installed on the computer but will not be reloaded when R is started again.\nIn summary, R has packages that can be downloaded and installed from online repositories such as CRAN. When you install a package, which only needs to be done once per computer or account, in R all it is doing is placing the source code in a library folder designated during the installation of R. Packages are typically collections of functions and variables that are specific to a certain task or subject matter.\nFor example, to install the mosaic package, enter:\ninstall.packages(\"mosaic\") # fetch package from CRAN\nIn RStudio, there is a Packages tab that makes it easy to add and maintain packages.\nTo use a package in a session, we must load it. This makes it available to the current session only. When you start R again, you will have to load packages again. The command library() with the package name supplied as the argument is all that is needed. For this session, we will load tidyverse and mosaic. Note: the box below is executing the R commands, this is known as reproducible research since you can see the code and then you can run or modify as you need.\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nNext read in the data into the working environment.\n\n# This code reads the `stent_study.csv` file into the `stent_study` object.\nstent_study &lt;- read_csv(\"data/stent_study.csv\")\n\nNote on commenting code: It is good practice to comment code. Here are some of the best practices for commenting computer code:\nComments should explain why code is written the way it is, rather than explaining what the code does. This means that you should explain the intent of the code, not just the steps that it takes to achieve that intent.\nComments should be brief and to the point. There is no need to write long, rambling comments. Just write enough to explain what the code is doing and why.\nComments should be clear and concise. Use plain language that is easy to understand. Avoid jargon and technical terms that the reader may not be familiar with.\nComments should be consistent with the style of the code. If the code is written in a formal style, then the comments should also be formal. If the code is written in a more informal style, then the comments should be informal.\nComments should be up-to-date. If you make changes to the code, then you should also update the comments to reflect those changes.\nIn additional, consider the following practices in writing your code:\nUsing a consistent comment style. This will make it easier for other people to read and understand your code.\nUsing meaningful names for variables and functions. This will help to reduce the need for comments.\nUse indentation and whitespace to make your code easier to read. This will also help to reduce the need for comments.\nDocument your code. This means writing a separate document that explains the purpose of the code, how to use it, and any known limitations.\nBy following these best practices, you can write code that is easy to understand and maintain. This will make your code more reusable and will help to prevent errors.\nNow back to our code. Let’s break this code down. We are reading from a .csv file and assigning the results into an object called stent_study. The assignment arrow &lt;- means we assign what is on the right to what is on the left. The R function we use in this case is read_csv(). When using R functions, you should ask yourself:\n\nWhat do I want R to do?\nWhat information must I provide for R to do this?\n\nWe want R to read in a .csv file. We can get help on this function by typing ?read_csv or help(read_csv) at the prompt. The only required input to read_csv() is the file location. We have our data stored in a folder called “data” under the working directory. We can determine the working directory by typing getwd() at the prompt.\n\ngetwd()\n\nSimilarly, if we wish to change the working directory, we can do so by using the setwd() function:\n\nsetwd('C:/Users/Brianna.Hitt/Documents/ProbStat/Another Folder')\n\nIn R if you use the view(), you will see the data in what looks like a standard spreadsheet.\n\nView(stent_study)\n\n\n\n1.4.4 Explore data\nBefore we attempt to answer the research question, let’s look at the data. We want R to print out the first 10 rows of the data. The appropriate function is head() and it needs the data object. By default, R will output the first 6 rows. By using the n = argument, we can specify how many rows we want to view.\n\nhead(stent_study, n = 10)\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nWe also want to “inspect” the data. The function is inspect() and R needs the data object stent_study.\n\ninspect(stent_study)\n\n\ncategorical variables:  \n        name     class levels   n missing\n1      group character      2 451       0\n2  outcome30 character      2 451       0\n3 outcome365 character      2 451       0\n                                   distribution\n1 control (50.3%), trmt (49.7%)                \n2 no_event (89.8%), stroke (10.2%)             \n3 no_event (83.8%), stroke (16.2%)             \n\n\nTo keep things simple, we will only look at the outcome30 variable in this case study. We will summarize the data in a table. Later in the book, we will learn to do this using the tidy package; for now we use the mosaic package. This package makes use of the modeling formula that you will use extensively later in this book. The modeling formula is also used in Math 378.\nWe want to summarize the data by making a table. From mosaic, we use the tally() function. Before using this function, we have to understand the basic formula notation that mosaic uses. The basic format is:\ngoal(y ~ x, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x as “y tilde x” and interpret it in the equivalent forms: “y broken down by x”; “y modeled by x”; “y explained by x”; “y depends on x”; or “y accounted for by x.” For graphics, it’s reasonable to read the formula as “y vs. x”, which is exactly the convention used for coordinate axes.\nFor this exercise, we want to apply tally() to the variables group and outcome30. In this case it does not matter which we call y and x; however, it is more natural to think of outcome30 as a dependent variable.\n\ntally(outcome30 ~ group, data = stent_study, margins = TRUE)\n\n          group\noutcome30  control trmt\n  no_event     214  191\n  stroke        13   33\n  Total        227  224\n\n\nThe margins option totals the columns.\nOf the 224 patients in the treatment group, 33 had a stroke by the end of the first month. Using these two numbers, we can use R to compute the proportion of patients in the treatment group who had a stroke by the end of their first month.\n\n33 / (33 + 191)\n\n[1] 0.1473214\n\n\n\nExercise:\nWhat proportion of the control group had a stroke in the first 30 days of the study? And why is this proportion different from the proportion reported by inspect()?\n\nLet’s have R calculate proportions for us. Use ? or help() to look at the help menu for tally(). Note that one of the option arguments of the tally() function is format =. Setting this equal to proportion will output the proportions instead of the counts.\n\ntally(outcome30 ~ group, data = stent_study, format = 'proportion', margins = TRUE)\n\n          group\noutcome30     control       trmt\n  no_event 0.94273128 0.85267857\n  stroke   0.05726872 0.14732143\n  Total    1.00000000 1.00000000\n\n\nWe can compute summary statistics from the table. A summary statistic is a single number summarizing a large amount of data.3 For instance, the primary results of the study after 1 month could be described by two summary statistics: the proportion of people who had a stroke in the treatment group and the proportion of people who had a stroke in the control group.\n\nProportion who had a stroke in the treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\)\nProportion who had a stroke in the control group: \\(13/227 = 0.06 = 6\\%\\)\n\n\n\n1.4.5 Visualize the data\nIt is often important to visualize the data. The table is a type of visualization, but in this section we will introduce a graphical method called bar charts.\nWe will use the ggformula package to visualize the data. It is a wrapper to the ggplot2 package which is becoming the industry standard for generating professional graphics. However, the interface for ggplot2 can be difficult to learn and we will ease into it by using ggformula, which makes use of the formula notation introduced above. The ggformula package was loaded when we loaded mosaic.4\nTo generate a basic graphic, we need to ask ourselves what information we are trying to see, what particular type of graph is best, what corresponding R function to use, and what information that R function needs in order to build a plot. For categorical data, we want a bar chart and the R function gf_bar() needs the data object and the variable(s) of interest.\nHere is our first attempt. In Figure 1.1, we leave the y portion of our formula blank. Doing this implies that we simply want to view the number/count of outcome30 by type. We will see the two levels of outcome30 on the x-axis and counts on the y-axis.\n(ref:ggfbold) Using ggformula to create a bar chart.\n\ngf_bar(~outcome30, data = stent_study)\n\n\n\n\n\n\n\nFigure 1.1: Using ggformula to create a bar chart.\n\n\n\n\n\n\nExercise:\nExplain Figure 1.1.\n\nThis plot graphically shows us the total number of “stroke” and the total number of “no_event”. However, this is not what we want. We want to compare the 30-day outcomes for both treatment groups. So, we need to break the data into different groups based on treatment type. In the formula notation, we now update it to the form:\ngoal(y ~ x|z, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x|z as “y tilde x by z” and interpret it in the equivalent forms: “y modeled by x for each z”; “y explained by x within each z”; or “y accounted for by x within z.” For graphics, it’s reasonable to read the formula as “y vs. x for each z”. Figure Figure 1.2 shows the results.\n\ngf_bar(~outcome30|group, data = stent_study) \n\n\n\n\n\n\n\nFigure 1.2: Bar charts conditioned on the group variable.\n\n\n\n\n\n\n1.4.5.1 More advanced graphics\nAs a prelude for things to come, the above graphic needs work. The labels don’t help and there is no title. We could add color. Does it make more sense to use proportions? Here is the code and results for a better graph, see Figure Figure 1.3. Don’t worry if this seems a bit advanced, but feel free to examine each new component of this code.\n\n# This code creates a graph showing the impact of stents on stroke.\n# The `gf_props()` function creates a bar graph showing the number of events\n# for each experimental group. The `fill` argument specifies the fill color\n# for each group. The `position = 'fill'` argument specifies that the bars\n# should be filled to the top.\n\n# The `gf_labs()` function adds the title, subtitle, x-axis label, and y-axis\n# label to the graph.\n\n# The `gf_theme()` function applies a black-and-white theme to the graph.\n\nstent_study %&gt;%\ngf_props(~group, fill = ~outcome30, position = 'fill') %&gt;%\n  gf_labs(title = \"Impact of Stents of Stroke\",\n          subtitle = 'Experiment with 451 Patients',\n          x = \"Experimental Group\",\n          y = \"Number of Events\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 1.3: Better graph.\n\n\n\n\n\nNotice that we used the pipe operator, %&gt;%. This operator allows us to string functions together in a manner that makes it easier to read the code. In the above code, we are sending the data object stent_study into the function gf_props() to use as data, so we don’t need the data = argument. In math, this is a composition of functions. Instead of f(g(x)) we could use a pipe f(g(x)) = g(x) %&gt;% f().\n\n\n\n1.4.6 Conclusion\nThese two summary statistics (the proportions of people who had a stroke) are useful in looking for differences in the groups, and we are in for a surprise: an additional 9% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a real difference due to the treatment?\nThis second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 9% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance?\nThis is a preview of step 4, analyze the data, and step 5, form a conclusion, of the analysis cycle. While we haven’t yet covered statistical tools to fully address these steps, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients.\nBe careful: do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#homework-problems",
    "href": "01-Data-Case-Study.html#homework-problems",
    "title": "1  Data Case Study",
    "section": "1.5 Homework Problems",
    "text": "1.5 Homework Problems\nCreate an Rmd file 01 Data Case Study Application.Rmd in R (it may be provided), and start by inserting your name in the header. The code blocks below can be copied and pasted, and then you can complete the code and answer the questions. When you are done, knit the Rmd into an html or pdf file by clicking the Knit button in RStudio and selecting either “Knit to HTML” or “Knit to PDF”.\nTo create an R code chunk, type CTRL+ALT+I or click the “Insert a new code chunk” button (a green C with a + icon) and use the drop down menu to select R. Anything between the dashes is interpreted as R code.\nFor more on RMarkdown, see the following video: https://www.youtube.com/watch?v=DNS7i2m4sB0. This video only demonstrates how to knit to an html, but we can also knit to a pdf since it is set up for us. You can also take the first chapter of the Data Camp course, Reporting with R Markdown, to learn more.\n\nStent study continued. Complete a similar analysis for the stent data, but this time use the one year outcome. In particular,\n\n\n\nRead the data into your working directory.\n\n\nstent_study &lt;- read_csv(___)\n\nComplete the steps below. The start of code is provided below. You will need to add {r} to the start of each code chunk or insert your own code chunks to use the code.\n\n\ni. Use `inspect` on the data.  \n\ninspect(___)\n\nii. Create a table of `outcome365` and `group`. Comment on the results.  \n\ntally(outcome365 ~ ___, data = stent_study, format = ___, margins = TRUE)\n\niii. Create a barchart of the data.  \n\nstent_study %&gt;%\n  gf_props(~___, fill = ~___, position = 'fill') %&gt;%\n  gf_labs(title = ___,\n          subtitle = ___,\n          x = ___,\n          y = ___)\n\nMigraine and acupuncture. A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. The 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. The 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). Then 24 hours after patients received acupuncture, they were asked if they were pain free.5\nThe data is in the file migraine_study.csv in the data folder. Complete the following work:\n\n\n\nRead the data into an object called migraine_study.\n\n\nmigraine_study &lt;- read_csv(\"data/___\")\n\nhead(migraine_study)\n\nCreate a table of the data.\n\n\ntally(___)\n\nReport the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture.\nRepeat for the control group.\nAt first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning.\nDo the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance?\n\n\n\nCompile, knit, this report into an html and a pdf. In order to knit the report into a pdf, you may need to install the knitr and tinytex packages in R.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#solutions-manual",
    "href": "01-Data-Case-Study.html#solutions-manual",
    "title": "1  Data Case Study",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#footnotes",
    "href": "01-Data-Case-Study.html#footnotes",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003.↩︎\nNY Times article reporting on the study: http://www.nytimes.com/2011/09/08/health/research/08stent.html↩︎\nFormally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others.↩︎\nhttps://cran.r-project.org/web/packages/ggformula/vignettes/ggformula-blog.html↩︎\nG. Allais et al. “Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints”. http://www.ncbi.nlm.nih.gov/pubmed/21533739 In: Neurological Sci. 32.1 (2011), pp. 173–175.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html",
    "href": "02-Data-Basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#objectives",
    "href": "02-Data-Basics.html#objectives",
    "title": "2  Data Basics",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: case, variables, data frame, associated variables, independent, and discrete and continuous variables.\nIdentify and define the different types of variables.\nGiven a study description, describe the research question.\nIn R, create a scatterplot and determine the association of two numerical variables from the plot.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#data-basics",
    "href": "02-Data-Basics.html#data-basics",
    "title": "2  Data Basics",
    "section": "2.2 Data basics",
    "text": "2.2 Data basics\nEffective presentation and description of data is a first step in most analyses. This chapter introduces one structure for organizing data, as well as some terminology that will be used throughout this book.\n\n2.2.1 Observations, variables, and data matrices\nFor reference we will be using a data set concerning 50 emails received in 2012. These observations will be referred to as the email50 data set, and they are a random sample from a larger data set. This data is in the openintro package so let’s install and then load this package.\n\ninstall.packages(\"openintro\")\nlibrary(openintro)\n\nTable 2.1 shows 4 rows of the email50 data set and we have elected to only list 5 variables for ease of observation.\nEach row in the table represents a single email or case.1 The columns represent variables, which represent characteristics for each of the cases (emails). For example, the first row represents email 1, which is not spam, contains 21,705 characters, 551 line breaks, is written in HTML format, and contains only small numbers.\n\n\n\n\nTable 2.1: First 5 rows of email data frame\n\n\n\n\n\n\nspam\nnum_char\nline_breaks\nformat\nnumber\n\n\n\n\n0\n21.705\n551\n1\nsmall\n\n\n0\n7.011\n183\n1\nbig\n\n\n1\n0.631\n28\n0\nnone\n\n\n0\n15.829\n242\n1\nsmall\n\n\n\n\n\n\n\n\nLet’s look at the first 10 rows of data from email50 using R. Remember to ask the two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want the first 10 rows so we use head() and R needs the data object and the number of rows. The data object is called email50 and is accessible once the openintro package is loaded.\n\nhead(email50, n = 10)\n\n# A tibble: 10 × 21\n   spam  to_multiple from     cc sent_email time                image attach\n   &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 0     0           1         0 1          2012-01-04 13:19:16     0      0\n 2 0     0           1         0 0          2012-02-16 20:10:06     0      0\n 3 1     0           1         4 0          2012-01-04 15:36:23     0      2\n 4 0     0           1         0 0          2012-01-04 17:49:52     0      0\n 5 0     0           1         0 0          2012-01-27 09:34:45     0      0\n 6 0     0           1         0 0          2012-01-17 17:31:57     0      0\n 7 0     0           1         0 0          2012-03-18 04:18:55     0      0\n 8 0     0           1         0 1          2012-03-31 13:58:56     0      0\n 9 0     0           1         1 1          2012-01-11 01:57:54     0      0\n10 0     0           1         0 0          2012-01-07 19:29:16     0      0\n# ℹ 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;,\n#   password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;,\n#   re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;,\n#   number &lt;fct&gt;\n\n\nIn practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and the units of measurement. Descriptions of all variables in the email50 data set are given in its documentation which can be accessed in R by using the ? command:\n?email50\n(Note that not all data sets will have associated documentation; the authors of openintro package included this documentation with the email50 data set contained in the package.)\nThe data in email50 represent a data matrix, or in R terminology a data frame or tibble 2, which is a common way to organize data. Each row of a data matrix corresponds to a unique case, and each column corresponds to a variable. This is called tidy data.3 The data frame for the stroke study introduced in the previous chapter had patients as the cases and there were three variables recorded for each patient. If we are thinking of patients as the unit of observation, then this data is tidy.\n\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nIf we think of an outcome as a unit of observation, then it is not tidy since the two outcome columns are variable values (month or year). The tidy data for this case would be:\n\n\n# A tibble: 10 × 4\n   patient_id group   time  result  \n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   \n 1          1 control month no_event\n 2          1 control year  no_event\n 3          2 trmt    month no_event\n 4          2 trmt    year  no_event\n 5          3 control month no_event\n 6          3 control year  no_event\n 7          4 trmt    month no_event\n 8          4 trmt    year  no_event\n 9          5 trmt    month no_event\n10          5 trmt    year  no_event\n\n\nThere are three interrelated rules which make a data set tidy:\n\nEach variable must have its own column.\n\nEach observation must have its own row.\n\nEach value must have its own cell.\n\nWhy ensure that your data is tidy? There are two main advantages:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. This will be more clear as we progress in our studies. Since most built-in R functions work with vectors of values, it makes transforming tidy data feel particularly natural.\n\nData frames are a convenient way to record and store data. If another individual or case is added to the data set, an additional row can be easily added. Similarly, another column can be added for a new variable.\n\nExercise:\nWe consider a publicly available data set that summarizes information about the 3,142 counties in the United States, and we create a data set called county_subset data set. This data set will include information about each county: its name, the state where it resides, its population in 2000 and 2010, per capita federal spending, poverty rate, and four additional characteristics. We create this data object in the code following this description. The parent data set is part of the usdata library and is called county_complete. The variables are summarized in the help menu built into the usdata package4. How might these data be organized in a data matrix? 5\n\nUsing R we will create our data object. First we load the library usdata.\n\nlibrary(usdata)\n\nWe only want a subset of the columns and we will use the select verb in dplyr to select and rename columns. We also create a new variable which is federal spending per capita using the mutate function.\n\ncounty_subset &lt;- county_complete %&gt;% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %&gt;%\n  mutate(fed_spend = fed_spend / pop2010)\n\nUsing R, we will display seven rows of the county_subset data frame.\n\nhead(county_subset, n = 7)\n\n            name   state pop2000 pop2010 fed_spend poverty homeownership\n1 Autauga County Alabama   43671   54571  6.068095    10.6          77.5\n2 Baldwin County Alabama  140415  182265  6.139862    12.2          76.7\n3 Barbour County Alabama   29038   27457  8.752158    25.0          68.0\n4    Bibb County Alabama   20826   22915  7.122016    12.6          82.9\n5  Blount County Alabama   51024   57322  5.130910    13.4          82.0\n6 Bullock County Alabama   11714   10914  9.973062    25.3          76.9\n7  Butler County Alabama   21399   20947  9.311835    25.0          69.0\n  multi_unit income med_income\n1        7.2  24568      53255\n2       22.6  26469      50147\n3       11.1  15875      33219\n4        6.6  19918      41770\n5        3.7  21070      45549\n6        9.9  20289      31602\n7       13.7  16916      30659\n\n\n\n\n2.2.2 Types of variables\nExamine the fed_spend, pop2010, and state variables in the county data set. Each of these variables is inherently different from the others, yet many of them share certain characteristics.\nFirst consider fed_spend. It is said to be a numerical variable (sometimes called a quantitative variable) since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical; even though area codes are made up of numerical digits, their average, sum, and difference have no clear meaning.\nThe pop2010 variable is also numerical; it is sensible to add, subtract, or take averages with those values, although it seems to be a little different than fed_spend. This variable of the population count can only be a whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). For this reason, the population variable is said to be discrete since it can only take specific numerical values. On the other hand, the federal spending variable is said to be continuous because it can take on any value in some interval. Now technically, there are no truly continuous numerical variables since all measurements are finite up to some level of accuracy or measurement precision (e.g., we typically measure federal spending in dollars and cents). However, in this book, we will treat both types of numerical variables the same, that is as continuous variables for statistical modeling. The only place this will be different in this book is in probability models, which we will see in the probability modeling block.\nThe variable state can take up to 51 values, after accounting for Washington, DC, and are summarized as: Alabama, Alaska, …, and Wyoming. Because the responses themselves are categories, state is a categorical variable (sometimes also called a qualitative variable), and the possible values are called the variable’s levels.\n\n\n\n\n\n\n\n\nFigure 2.1: Taxonomy of Variables.\n\n\n\n\n\nFinally, consider a hypothetical variable on education, which describes the highest level of education completed and takes on one of the values noHS, HS, College or Graduate_school. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable. A categorical variable with levels that do not have a natural ordering is called a nominal variable. To simplify analyses, any ordinal variables in this book will be treated as nominal categorical variables. In R, categorical variables can be treated in different ways; one of the key differences is that we can leave them as character values (character strings, or text) or as factors. A factor is essentially a categorical variable with defined levels. When R handles factors, it is only concerned about the levels of the factors. We will learn more about this as we progress.\nFigure 2.1 captures this classification of variables we have described.\n\nExercise:\nData were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical.6\n\n\nExercise:\nConsider the variables group and outcome30 from the stent study in the case study chapter. Are these numerical or categorical variables? 7\n\n\n\n2.2.3 Relationships between variables\nMany analyses are motivated by a researcher looking for a relationship between two or more variables. This is the heart of statistical modeling. A social scientist may like to answer some of the following questions:\n\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\n\nIf homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average?\n\nTo answer these questions, data must be collected, such as the county_complete data set. Examining summary statistics could provide insights for each of the two questions about counties. Graphs can be used to visually summarize data and are useful for answering such questions as well.\nScatterplots are one type of graph used to study the relationship between two numerical variables. Figure 2.2 compares the variables fed_spend and poverty. Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 1088 in the county_subset data set: Owsley County, Kentucky, which had a poverty rate of 41.5% and federal spending of $21.50 per capita. The dense cloud in the scatterplot suggests a relationship between the two variables: counties with a high poverty rate also tend to have slightly more federal spending. We might brainstorm as to why this relationship exists and investigate each idea to determine which is the most reasonable explanation.\n\n\n\n\n\n\n\n\nFigure 2.2: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted.\n\n\n\n\n\n\nExercise:\nExamine the variables in the email50 data set. Create two research questions about the relationships between these variables that are of interest to you.8\n\nThe fed_spend and poverty variables are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa.\n\nExample:\nThe relationship between the homeownership rate and the percent of units in multi-unit structures (e.g. apartments, condos) is visualized using a scatterplot in Figure 2.3. Are these variables associated?\n\nIt appears that the larger the fraction of units in multi-unit structures, the lower the homeownership rate. Since there is some relationship between the variables, they are associated.\n\n\n\n\n\n\n\n\nFigure 2.3: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties.\n\n\n\n\n\nBecause there is a downward trend in Figure 2.3 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. A positive association (upward trend) is shown in the relationship between the poverty and fed_spend variables represented in Figure 2.2, where counties with higher poverty rates tend to receive more federal spending per capita.\nIf two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two.\n\nA pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.\n\n\n\n2.2.4 Creating a scatterplot\nIn this section, we will create a simple scatterplot and then ask you to create one on your own. First, we will recreate the scatterplot seen in Figure 2.2. This figure uses the county_subset data set.\nHere are two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want R to create a scatterplot and to do this it needs, at a minimum, the data object, what we want on the \\(x\\)-axis, and what we want on the \\(y\\)-axis. More information on ggformula can be found here.\n\ncounty_subset %&gt;%\n  gf_point(fed_spend ~ poverty)\n\n\n\n\n\n\n\nFigure 2.4: Scatterplot with ggformula.\n\n\n\n\n\nFigure 2.4 is bad. There are poor axis labels, no title, dense clustering of points, and the \\(y\\)-axis is being driven by a couple of extreme points. We will need to clear this up. Again, try to read the code and use help() or ? to determine the purpose of each command in Figure 2.5.\n\ncounty_subset %&gt;%\n  filter(fed_spend &lt; 32) %&gt;%\n  gf_point(fed_spend ~ poverty,\n           xlab = \"Poverty Rate (Percent)\", \n           ylab = \"Federal Spending Per Capita\",\n           title = \"A scatterplot showing fed_spend against poverty\", \n           cex = 1, alpha = 0.2) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 2.5: Better example of a scatterplot.\n\n\n\n\n\n\nExercise:\nCreate the scatterplot in Figure 2.3.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#homework-problems",
    "href": "02-Data-Basics.html#homework-problems",
    "title": "2  Data Basics",
    "section": "2.3 Homework Problems",
    "text": "2.3 Homework Problems\n\nIdentify study components. Identify (i) the cases, (ii) the variables and their types, and (iii) the main research question in the studies described below.\n\n\n\nResearchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study, air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggests that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.9\nThe Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: patients who practiced the Buteyko method and those who did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.10\n\n\n\nIn the openintro package is a data set called ames, containing information on individual residential properties sold in Ames, IA between 2006 and 2010. Create a scatterplot for the above ground living area square feet versus sale price in US dollars. Describe the relationship between these two variables. Note: you may have to load the library and data set.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#solutions-manual",
    "href": "02-Data-Basics.html#solutions-manual",
    "title": "2  Data Basics",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#footnotes",
    "href": "02-Data-Basics.html#footnotes",
    "title": "2  Data Basics",
    "section": "",
    "text": "A case is also sometimes called a unit of observation or an observational unit.↩︎\nA tibble is a data frame with attributes for such things as better display and printing.↩︎\nTidy data is data in which each row corresponds to a unique case and each column represents a single variable. For more information on tidy data, see the Simply Statistics blog and the R for Data Science book by Hadley Wickham and Garrett Grolemund.↩︎\nThese data were collected from the US Census website.↩︎\nEach county may be viewed as a case, and there are ten pieces of information recorded for each case. A table with 3,142 rows and 10 columns could hold these data, where each row represents a county and each column represents a particular piece of information.↩︎\nThe number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical.↩︎\nThere are only two possible values for each variable, and in both cases they describe categories. Thus, each is a categorical variable.↩︎\nTwo sample questions: (1) Intuition suggests that if there are many line breaks in an email then there would also tend to be many characters: does this hold true? (2) Is there a connection between whether an email format is plain text (versus HTML) and whether it is a spam message?↩︎\nB. Ritz et al. “Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993”. In: Epidemiology 11.5 (2000), pp. 502–511.↩︎\nJ. McGowan. “Health Education: Does the Buteyko Institute Method make a difference?” In: Thorax 58 (2003).↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html",
    "href": "03-Overview-of-Data-Collection-Principles.html",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "3.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#objectives",
    "href": "03-Overview-of-Data-Collection-Principles.html#objectives",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: population, sample, anecdotal evidence, bias, simple random sample, systematic sample, non-response bias, representative sample, convenience sample, explanatory variable, response variable, observational study, cohort, experiment, randomized experiment, and placebo.\nFrom a description of a research project, be able to describe the population of interest, the generalizability of the study, the explanatory and response variables, whether it is observational or experimental, and determine the type of sample.\nIn the context of a problem, explain how to conduct a sample for the different types of sampling procedures.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "href": "03-Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "title": "3  Overview of Data Collection Principles",
    "section": "3.2 Overview of data collection principles",
    "text": "3.2 Overview of data collection principles\nThe first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals.\n\n3.2.1 Populations and samples\nConsider the following three research questions:\n\nWhat is the average mercury content in swordfish in the Atlantic Ocean?\n\nOver the last 5 years, what is the average time to complete a degree for Duke undergraduate students?\n\nDoes a new drug reduce the number of deaths in patients with severe heart disease?\n\nEach research question refers to a target population, the entire collection of individuals about which we want information. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question.\n\nExercise:\nFor the second and third questions above, identify the target population and what represents an individual case.1\n\n\n\n3.2.2 Anecdotal evidence\nConsider the following possible responses to the three research questions:\n\nA man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high.\nI met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges.\nMy friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work.\n\nEach conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence.\n\n\n\n\n\nIn February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, It’s one storm, in one region, of one country.\n\n\n\n\n\nAnecdotal evidence: Be careful of data collected haphazardly. Such evidence may be true and verifiable, but it may only represent extraordinary cases.\n\nAnecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population.\n\n\n3.2.3 Sampling from a population\nWe might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. This is illustrated in Figure 3.1 .\n\n\n\n\n\n\n\n\nFigure 3.1: In this graphic, five graduates are randomly selected from the population to be included in the sample.\n\n\n\n\n\nWhy pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario.\n\nExample:\nSuppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates? 2\n\n\n\n\n\n\n\n\n\nFigure 3.2: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often.\n\n\n\n\n\nIf someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces sampling bias (see Figure 3.2), where some individuals in the population are more likely to be sampled than others. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample.\nSometimes a simple random sample is difficult to implement and an alternative method is helpful. One such substitute is a systematic sample, where one case is sampled after letting a fixed number of others, say 10 other cases, pass by. Since this approach uses a mechanism that is not easily subject to personal biases, it often yields a reasonably representative sample. This book will focus on simple random samples since the use of systematic samples is uncommon and requires additional considerations of the context.\nThe act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative3 of the entire population, the survey might suffer from non-response bias4.\n\n\n\n\n\n\n\n\nFigure 3.3: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem.\n\n\n\n\n\nAnother common pitfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample, see Figure 3.3. For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents.\n\nExercise:\nWe can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?5\n\n\n\n3.2.4 Explanatory and response variables\nConsider the following question for the county data set:\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\nIf we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.6 If there are many variables, it may be possible to consider a number of them as explanatory variables.\n\nExplanatory and response variables\nTo identify the explanatory variable in a pair of variables, identify which of the two variables is suspected as explaining or causing changes in the other. In data sets with more than two variables, it is possible to have multiple explanatory variables. The response variable is the outcome or result of interest.\n\n\nCaution: Association does not imply causation. Labeling variables as explanatory and response does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other. We also use this language to help in our use of R and the formula notation.\n\nIn some cases, there is no explanatory or response variable. Consider the following question:\nIf homeownership in a particular county is lower than the national average, will the percent of multi-unit structures in that county likely be above or below the national average?\nIt is difficult to decide which of these variables should be considered the explanatory and response variable; i.e. the direction is ambiguous, so no explanatory or response labels are suggested here.\n\n\n3.2.5 Introducing observational studies and experiments\nThere are two primary types of data collection: observational studies and experiments.\nResearchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort7 of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but by themselves, they cannot show a causal connection.\nWhen researchers want to investigate the possibility of a causal connection, they conduct an experiment, a study in which the explanatory variables are assigned rather than observed. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a treatment group, and we are comparing at least two treatments, the experiment is called a randomized comparative experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. The case study at the beginning of the book is another example of an experiment, though that study did not employ a placebo. Math 359 is a course on the design and analysis of experimental data, DOE, at USAFA. In the Air Force, these types of experiments are an important part of test and evaluation. Many Air Force analysts are expert practitioners of DOE. In this book though, we will minimize our discussion of DOE.\n\nAssociation \\(\\neq\\) Causation\nAgain, association does not imply causation. In a data analysis, association does not imply causation, and causation can only be inferred from a randomized experiment. Although, a hot field is the analysis of causal relationships in observational data. This is important because consider cigarette smoking, how do we know it causes lung cancer? We only have observational data and clearly cannot do an experiment. We think analysts will be charged in the near future with using causal reasoning on observational data.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#homework-problems",
    "href": "03-Overview-of-Data-Collection-Principles.html#homework-problems",
    "title": "3  Overview of Data Collection Principles",
    "section": "3.3 Homework Problems",
    "text": "3.3 Homework Problems\n\nGeneralizability and causality. Identify the population of interest and the sample in the studies described below. These are the same studies from the previous chapter. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships.\n\n\n\nResearchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study, air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggests that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.8\nThe Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: patients who practiced the Buteyko method and those who did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.9\n\n\n\nGPA and study time. A survey was conducted on 193 undergraduates who took an introductory statistics course at a private US university in 2012. This survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables.\n\n\n\n\n\n\n\n\n\n\n\nWhat is the explanatory variable and what is the response variable?\nDescribe the relationship between the two variables. Make sure to discuss unusual observations, if any.\nIs this an experiment or an observational study?\nCan we conclude that studying longer hours leads to higher GPAs?\n\n\n\nIncome and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelor’s degree in 3,143 counties in the US in 2010.\n\n\n\n\n\n\n\n\n\n\n\nWhat are the explanatory and response variables?\nDescribe the relationship between the two variables. Make sure to discuss unusual observations, if any.\nCan we conclude that having a bachelor’s degree increases one’s income?",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#solutions-manual",
    "href": "03-Overview-of-Data-Collection-Principles.html#solutions-manual",
    "title": "3  Overview of Data Collection Principles",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#footnotes",
    "href": "03-Overview-of-Data-Collection-Principles.html#footnotes",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "2) Notice that the second question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. 3) A person with severe heart disease represents a case. The population includes all people with severe heart disease.↩︎\nPerhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern.↩︎\nA representative sample accurately reflects the characteristics of the population.↩︎\nNon-response bias is bias that can be introduced when subjects elect not to participate in a study. Often, the individuals that do participate are systematically different from the individuals who do not.↩︎\nAnswers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind.↩︎\nSometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so be careful and consider the context when using or reading these words.↩︎\nA cohort is a group of individuals who are similar in some way.↩︎\nB. Ritz et al. “Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993”. In: Epidemiology 11.5 (2000), pp. 502–511.↩︎\nJ. McGowan. “Health Education: Does the Buteyko Institute Method make a difference?” In: Thorax 58 (2003).↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "04-Studies.html",
    "href": "04-Studies.html",
    "title": "4  Studies",
    "section": "",
    "text": "4.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "04-Studies.html#objectives",
    "href": "04-Studies.html#objectives",
    "title": "4  Studies",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: confounding variable, prospective study, retrospective study, simple random sampling, stratified sampling, strata, cluster sampling, multistage sampling, experiment, randomized experiment, control, replicate, blocking, blocks, treatment group, control group, blinded study, placebo, placebo effect, and double-blind.\nGiven a study description, be able to describe the study using correct terminology.\nGiven a scenario, describe flaws in reasoning and propose study and sampling designs.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "04-Studies.html#observation-studies-sampling-strategies-and-experiments",
    "href": "04-Studies.html#observation-studies-sampling-strategies-and-experiments",
    "title": "4  Studies",
    "section": "4.2 Observation studies, sampling strategies, and experiments",
    "text": "4.2 Observation studies, sampling strategies, and experiments\n\n4.2.1 Observational studies\nGenerally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers.\nMaking causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations.\n\nExercise:\nSuppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?1\n\nSome previous research2 tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation.\n\n\n\n\n\n\n\n\nFigure 4.1: un exposure is a confounding variable because it is related to both response and explanatory variables.\n\n\n\n\n\nSun exposure is what is called a confounding variable,3 which is a variable that is correlated with both the explanatory and response variables, see Figure 4.1. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured.\nLet’s look at an example of confounding visually. Using the SAT data from the mosaic package let’s look at expenditure per pupil versus SAT scores. Figure 4.2 is a plot of the data.\n\nExercise:\nWhat conclusion do you reach from the plot in Figure 4.2?4\n\n\n\n\n\n\n\n\n\nFigure 4.2: Average SAT score versus expenditure per pupil; reminder: each observation represents an individual state.\n\n\n\n\n\nThe implication that spending less might give better results is not justified. Expenditures are confounded with the proportion of students who take the exam, and scores are higher in states where fewer students take the exam.\nIt is interesting to look at the original plot if we place the states into two groups depending on whether more or fewer than 40% of students take the SAT. Figure 4.3 is a plot of the data broken down into the 2 groups.\n\n\n\n\n\n\n\n\nFigure 4.3: Average SAT score versus expenditure per pupil; broken down by level of participation.\n\n\n\n\n\nOnce we account for the fraction of students taking the SAT, the relationship between expenditures and SAT scores changes.\nIn the same way, the county data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions.\n\nExercise:\nFigure 4.4 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in the Figure 4.4.5\n\n\n\n\n\n\n\n\n\nFigure 4.4: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties.\n\n\n\n\n\nObservational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses Health Study, started in 1976 and expanded in 1989.6 This prospective study recruits registered nurses and then collects data from them using questionnaires.\nRetrospective studies collect data after events have taken place; e.g. researchers may review past events in medical records. Some data sets, such as county, may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retail sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population).\n\n\n4.2.2 Three sampling methods\nAlmost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. Figure 4.5, Figure 4.6, and Figure 4.7 provide a graphical representation of these techniques.\n\n\n\n\n\n\n\n\nFigure 4.5: Examples of simple random sampling. In this figure, simple random sampling was used to randomly select the 18 cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: In this figure, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: In this figure, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected.\n\n\n\n\n\nSimple random sampling is probably the most intuitive form of random sampling, in which each individual in the population has an equal chance of being chosen. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that season’s 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included or not.\nStratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (we’re looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players.\nStratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling.\n\nExample:\nWhy would it be good for cases within each stratum to be very similar?7\n\nIn cluster sampling, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this book can be extended to handle such data.\n\nExample:\nSuppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed?8\n\nAnother technique called multistage sampling is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling.\n\n\n4.2.3 Experiments\nStudies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables.\n\n4.2.3.1 Principles of experimental design\nRandomized experiments are generally built on four principles.\n\nControlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill.\nRandomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study.\nReplication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. You replicate to the level of variability you want to estimate. For example, in flight test, we can run the same flight conditions again to get a replicate; however, if the same plane and pilot are being used, the replicate is not getting the pilot-to-pilot or the plane-to-plane variability.\nBlocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block, or group, to the treatments. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 4.8. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients.\n\n\n\n\n\n\n\n\n\nFigure 4.8: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories.\n\n\n\n\n\nIt is important to incorporate the first three experimental design principles into any study, and this chapter describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this chapter may be extended to analyze data collected using blocking. Math 359 is an entire course at USAFA devoted to the design and analysis of experiments.\n\n\n4.2.3.2 Reducing bias in human experiments\nRandomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.9 In particular, researchers wanted to know if the drug reduced deaths in patients.\nThese researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers10 were randomly placed into two study groups. One group, the treatment group, received the experimental treatment of interest (the new drug to treat heart attack patients). The other group, called the control group, did not receive any drug treatment. The comparison between the treatment and control groups allows researchers to determine whether the treatment really has an effect.\nPut yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify.\nResearchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect.\nThe patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.11\n\nExercise:\nLook back to the stent study in the first chapter where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?12",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "04-Studies.html#homework-problems",
    "href": "04-Studies.html#homework-problems",
    "title": "4  Studies",
    "section": "4.3 Homework Problems",
    "text": "4.3 Homework Problems\n\nPropose a sampling strategy. A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each with 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the student’s overall satisfaction with the course.\n\n\n\nWhat type of study is this?\nSuggest a sampling strategy for carrying out this study.\n\n\n\nFlawed reasoning. Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they want to be able to make such strong conclusions.\n\n\n\nStudents at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school.\nA survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers.\n\n\n\nSampling strategies. A statistics student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. Note: Sampling methods from both Chapter 3 (Overview of Data Collection Principles) and Chapter 4 (Studies) may be used for this problem.\n\n\n\nHe randomly samples 40 students from the study’s population, gives them the survey, asks them to fill it out and bring it back the next day.\nHe gives out the survey only to his friends, and makes sure each one of them fills out the survey.\nHe posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey.\nHe stands outside the QRC and asks every third person that walks out the door to fill out the survey.\n\n\n\nVitamin supplements. In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at the onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms.\n\n\n\nIs this an experiment or an observational study? Why?\nWhat are the explanatory and response variables in this study?\nWere the patients blinded to their treatment?\nWas this study double-blind?\nParticipants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning.\n\n\n\nExercise and mental health. A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results.\n\n\n\nWhat type of study is this?\nWhat are the treatment and control groups in this study?\nDoes this study make use of blocking? If so, what is the blocking variable?\nDoes this study make use of blinding?\nComment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large.\nSuppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal?",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "04-Studies.html#solutions-manual",
    "href": "04-Studies.html#solutions-manual",
    "title": "4  Studies",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "04-Studies.html#footnotes",
    "href": "04-Studies.html#footnotes",
    "title": "4  Studies",
    "section": "",
    "text": "No. See the paragraph following the exercise for an explanation.↩︎\nhttp://www.sciencedirect.com/science/article/pii/S0140673698121682\nhttp://archderm.ama-assn.org/cgi/content/abstract/122/5/537\nStudy with a similar scenario to that described here:\nhttp://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full↩︎\nAlso called a lurking variable, confounding factor, or a confounder.↩︎\nIt appears that average SAT score declines as expenditures per student increases.↩︎\nAnswers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents.↩︎\nhttp://www.channing.harvard.edu/nhs/↩︎\nWe might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population.↩︎\nA simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information.↩︎\nAnturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.↩︎\nHuman subjects are often called patients, volunteers, or study participants.↩︎\nThere are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the study’s patients and do not tell the blinded health care professionals who is receiving which treatment.↩︎\nThe researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Studies</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html",
    "href": "05-Numerical-Data.html",
    "title": "5  Numerical Data",
    "section": "",
    "text": "5.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html#objectives",
    "href": "05-Numerical-Data.html#objectives",
    "title": "5  Numerical Data",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: scatterplot, dot plot, mean, distribution, point estimate, weighted mean, histogram, data density, right skewed, left skewed, symmetric, mode, unimodal, bimodal, multimodal, variance, standard deviation, box plot, median, interquartile range, first quartile, third quartile, whiskers, outlier, robust estimate, transformation.\nIn R, generate summary statistics for a numerical variable, including breaking down summary statistics by groups.\nIn R, generate appropriate graphical summaries of numerical variables.\nInterpret and explain output both graphically and numerically.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html#numerical-data",
    "href": "05-Numerical-Data.html#numerical-data",
    "title": "5  Numerical Data",
    "section": "5.2 Numerical Data",
    "text": "5.2 Numerical Data\nThis chapter introduces techniques for exploring and summarizing numerical variables. The email50 and mlb data sets from the openintro package and a subset of county_complete from the usdata package provide rich opportunities for examples. Recall that outcomes of numerical variables are numbers on which it is reasonable to perform basic arithmetic operations. For example, the pop2010 variable, which represents the population of counties in 2010, is numerical since we can sensibly discuss the difference or ratio of the populations in two counties. On the other hand, area codes and zip codes are not numerical.\n\n5.2.1 Scatterplots for paired data\nA scatterplot provides a case-by-case view of data for two numerical variables. In Figure 5.1, we again present a scatterplot used to examine how federal spending and poverty are related in the county data set.\n\n\n\n\n\n\n\n\nFigure 5.1: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted.\n\n\n\n\n\nAnother scatterplot is shown in Figure 5.2, comparing the number of line_breaks and number of characters, num_char, in emails for the email50 data set. In any scatterplot, each point represents a single case. Since there are 50 cases in email50, there are 50 points in Figure 5.2.\n\n\n\n\n\n\n\n\nFigure 5.2: A scatterplot of line_breaks versus num_char for the email50 data.\n\n\n\n\n\nTo put the number of characters in perspective, this paragraph in the text has 357 characters. Looking at Figure 5.2, it seems that some emails are incredibly long! Upon further investigation, we would actually find that most of the long emails use the HTML format, which means most of the characters in those emails are used to format the email rather than provide text.\n\nExercise:\nWhat do scatterplots reveal about the data, and how might they be useful?1\n\n\nExample:\nConsider a new data set of 54 cars with two variables: vehicle price and weight.2 A scatterplot of vehicle price versus weight is shown in Figure 5.3. What can be said about the relationship between these variables?\n\n\n\n\n\n\n\n\n\nFigure 5.3: A scatterplot of price versus weight for 54 cars.\n\n\n\n\n\nThe relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we’ve seen which show relationships that are very linear.\n\nExercise:\nDescribe two variables that would have a horseshoe-shaped association in a scatterplot.3\n\n\n\n5.2.2 Dot plots and the mean\nSometimes two variables are one too many: only one variable may be of interest. In these cases, a dot plot provides the most basic of displays. A dot plot is a one-variable scatterplot; an example using the number of characters from 50 emails is shown in Figure 5.4.\n\n\n\n\n\n\n\n\nFigure 5.4: A dot plot of num_char for the email50 data set.\n\n\n\n\n\nThe mean, sometimes called the average, is a common way to measure the center of a distribution4 of data. To find the mean number of characters in the 50 emails, we add up all the character counts and divide by the number of emails. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.\n\\[\\bar{x} = \\frac{21.7 + 7.0 + \\cdots + 15.8}{50} = 11.6\\]\nThe sample mean is often labeled \\(\\bar{x}\\). There is a bar over the letter, and the letter \\(x\\) is being used as a generic placeholder for the variable of interest, num_char.\n\nMean\nThe sample mean of a numerical variable is the sum of all of the observations divided by the number of observations, Equation 1.\n\n\\[\\begin{equation}\n  \\bar{x} = \\frac{x_1+x_2+\\cdots+x_n}{n}\n  \\tag{1}\n\\end{equation}\\]\nwhere \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\n\nExercise:\nExamine the two equations above. What does \\(x_1\\) correspond to? And \\(x_2\\)? Can you infer a general meaning to what \\(x_i\\) might represent?5\n\n\nExercise:\nWhat was \\(n\\) in this sample of emails?6\n\nThe email50 data set is a sample from a larger population of emails that were received in January and March. We could compute a mean for this population in the same way as the sample mean. However, there is a difference in notation: the population mean has a special label: \\(\\mu\\). The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g. \\(\\mu_x\\).\n\nExample: The average number of characters across all emails can be estimated using the sample data. Based on the sample of 50 emails, what would be a reasonable estimate of \\(\\mu_x\\), the mean number of characters in all emails in the email data set? (Recall that email50 is a sample from email.)\n\nThe sample mean, 11.6, may provide a reasonable estimate of \\(\\mu_x\\). While this number will not be perfect, it provides a point estimate, a single plausible value, of the population mean. Later in the text, we will develop tools to characterize the accuracy of point estimates, and we will find that point estimates based on larger samples tend to be more accurate than those based on smaller samples.\n\nExample:\nWe might like to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes from the 3,143 counties in the county data set. What would be a better approach?\n\nThe county data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the county data, we would find that the per capita income for the US is $27,348.43. Had we computed the simple mean of per capita income across counties, the result would have been just $22,504.70!\nThis previous example used what is called a weighted mean7, which will be a key topic in the probability section. As a look ahead, the probability mass function gives the population proportions of each county’s mean value, and thus, to find the population mean \\(\\mu\\), we will use a weighted mean.\n\n\n5.2.3 Histograms and shape\nDot plots show the exact value of each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, think of the value as belonging to a bin. For example, in the email50 data set, we create a table of counts for the number of cases with character counts between 0 and 5,000, then the number of cases between 5,000 and 10,000, and so on. Observations that fall on the boundary of a bin (e.g. 5,000) are allocated to the lower bin. This tabulation is shown below.\n\n\n\n  (0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] \n     19      12       6       2       3       5       0       0       2       0 \n(50,55] (55,60] (60,65] \n      0       0       1 \n\n\nThese binned counts are plotted as bars in Figure 5.5 in what is called a histogram8.\n\n\n\n\n\n\n\n\nFigure 5.5: A histogram of num_char. This distribution is very strongly skewed to the right.\n\n\n\n\n\nHistograms provide a view of the data density. Higher bars represent where the data are relatively more dense. For instance, there are many more emails between 0 and 10,000 characters than emails between 10,000 and 20,000 characters in the data set. The bars make it easy to see how the density of the data changes relative to the number of characters.\nHistograms are especially convenient for describing the shape of the data distribution. Figure 5.5 shows that most emails have a relatively small number of characters, while fewer emails have a very large number of characters. When data trail off to the right in this way and have a longer right tail, the shape is said to be right skewed.9\nData sets with the reverse characteristic – a long, thin tail to the left – are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric.\n\nLong tails to identify skew\nWhen data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed.\n\n\nExercise:\nTake a look at the dot plot above, Figure 5.4. Can you see the skew in the data? Is it easier to see the skew in this histogram or the dot plots?10\n\n\nExercise:\nBesides the mean, what can you see in the dot plot that you cannot see in the histogram?11\n\n\n5.2.3.1 Making our own histogram\nLet’s take some time to make a simple histogram. We will use the ggformula package, which is a wrapper for the ggplot2 package.\nHere are two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want R to make a histogram. In ggformula, the plots have the form gf_plottype so we will use the gf_histogram(). To find options and more information about the function, type:\n?gf_histogram\nTo start, we just have to give the formulas and data to R.\n\ngf_histogram(~num_char, data = email50, color = \"black\", fill = \"cyan\")\n\n\n\n\n\n\n\n\n\nExercise:\nLook at the help menu for gf_histogram and change the x-axis label, change the bin width to 5, and have the left bin start at 0.\n\nHere is the code for the exercise:\nemail50 %&gt;%\n   gf_histogram(~num_char, binwidth = 5,boundary = 0,\n   xlab = \"The Number of Characters (in thousands)\", \n   color = \"black\", fill = \"cyan\") %&gt;%\n   gf_theme(theme_classic())\nIn addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution.12 There is only one prominent peak in the histogram of num_char.\nFigure 5.6 shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since the separation between the two peaks is relatively small, and it only differs from its neighboring bins by a few observations.\n\n\n\n\n\n\n\n\nFigure 5.6: Histograms that demonstrate unimodal, bimodal, and multimodal data.\n\n\n\n\n\n\nExercise:\nHeight measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you anticipate in this height data set?13\n\n\nLooking for modes\nLooking for modes isn’t about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in these notes. The important part of this examination is to better understand your data and how it might be structured.\n\n\n\n\n5.2.4 Variance and standard deviation\nThe mean is used to describe the center of a data set, but the variability in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though the formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to conceptually understand; it roughly describes how far away the typical observation is from the mean. Equation 2 is the equation for sample variance. We will demonstrate it with data so that the notation is easier to understand.\n\\[\\begin{align}\ns_{}^2 &= \\sum_{i = 1}^{n} \\frac{(x_i - \\bar{x})^2}{n - 1} \\\\\n    &= \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}\n  \\tag{2}\n\\end{align}\\]\nwhere \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nWe call the distance of an observation from its mean the deviation. Below are the deviations for the \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), and \\(50^{th}\\) observations of the num_char variable. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.\n\\[\n\\begin{aligned}\nx_1^{}-\\bar{x} &= 21.7 - 11.6 = 10.1 \\hspace{5mm}\\text{ } \\\\\nx_2^{}-\\bar{x} &= 7.0 - 11.6 = -4.6 \\\\\nx_3^{}-\\bar{x} &= 0.6 - 11.6 = -11.0 \\\\\n            &\\ \\vdots \\\\\nx_{50}^{}-\\bar{x} &= 15.8 - 11.6 = 4.2\n\\end{aligned}\n\\]\nIf we square these deviations and then take an average, the result is equal to the sample variance, denoted by \\(s_{}^2\\):\n\\[\n\\begin{aligned}\ns_{}^2 &= \\frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \\cdots + 4.2_{}^2}{50-1} \\\\\n    &= \\frac{102.01 + 21.16 + 121.00 + \\cdots + 17.64}{49} \\\\\n    &= 172.44\n\\end{aligned}\n\\]\nWe divide by \\(n - 1\\), rather than dividing by \\(n\\), when computing the variance; you need not worry about this mathematical nuance yet. Notice that squaring the deviations does two things. First, it makes large values much larger, seen by comparing \\(10.1^2\\), \\((-4.6)^2\\), \\((-11.0)^2\\), and \\(4.2^2\\). Second, it gets rid of any negative signs.\nThe sample standard deviation, \\(s\\), is the square root of the variance:\n\\[s = \\sqrt{172.44} = 13.13\\]\nThe sample standard deviation of the number of characters in an email is 13.13 thousand. A subscript of \\(_x\\) may be added to the variance and standard deviation, i.e. \\(s_x^2\\) and \\(s_x^{}\\), as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1^{}\\), \\(x_2^{}\\), …, \\(x_n^{}\\). The \\(_{x}\\) subscript is usually omitted when it is clear which data the variance or standard deviation is referencing.\n\nVariance and standard deviation\nThe variance is roughly the average squared distance from the mean. The standard deviation is the square root of the variance and describes how close the data are to the mean.\n\nFormulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.14 However, like the mean, the population values have special symbols: \\(\\sigma_{}^2\\) for the variance and \\(\\sigma\\) for the standard deviation. The symbol \\(\\sigma\\) is the Greek letter sigma.\n\nTip: standard deviation describes variability\nFocus on the conceptual meaning of the standard deviation as a descriptor of variability rather than the formulas. Usually 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, as we have seen, these percentages are not strict rules.\n\n\n\n\n\n\n\n\n\nFigure 5.7: The first of three very different population distributions with the same mean, 0, and standard deviation, 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: The second plot with mean 0 and standard deviation 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: The final plot with mean 0 and standard deviation 1.\n\n\n\n\n\n\nExercise:\nEarlier, the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using the three figures, Figures 5.7, 5.8, 5.9 as examples, explain why such a description is important.15\n\n\nExample:\nDescribe the distribution of the num_char variable using the histogram in Figure 5.5. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context: the number of characters in emails. Also note any especially unusual cases/observations.16\n\nIn practice, the variance and standard deviation are sometimes used as a means to an end, where the end is being able to accurately estimate the uncertainty associated with a sample statistic. For example, later in the book we will use the variance and standard deviation to assess how close the sample mean is to the population mean.\n\n\n5.2.5 Box plots, quartiles, and the median\nA box plot summarizes a data set using five statistics, while also plotting unusual observations. Figure 5.10 provides an annotated vertical dot plot alongside a box plot of the num_char variable from the email50 data set.\n\n\n\n\n\n\n\n\nFigure 5.10: A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6,890), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively.\n\n\n\n\n\nThe first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. Figure 5.10 shows 50% of the data falling below the median (red dashes) and the other 50% falling above the median (blue open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the \\(50^{th}\\) percentile: \\((\\text{6,768} + \\text{7,012}) / 2 = \\text{6,890}\\). When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed).\n\nMedian: the number in the middle\nIf the data are ordered from smallest to largest, the median is the observation in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average.\n\nThe second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The total length of the box, shown vertically in Figure 5.10, is called the interquartile range (IQR, for short). It, like the standard deviation, is a measure of variability in the data. The more variable the data, the larger the standard deviation and IQR. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e. 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile), and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively.\n\nInterquartile range (IQR)\nThe IQR is the length of the box in a box plot. It is computed as \\[ IQR = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) are the \\(25^{th}\\) and \\(75^{th}\\) percentiles, respectively.\n\n\nExercise:\nWhat percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?17\n\nExtending out from the box, the whiskers attempt to capture the data outside of the box, however, their reach is never allowed to be more than \\(1.5\\times IQR\\).18 They capture everything within this reach. In Figure 5.10, the upper whisker does not extend to the last three points, which are beyond \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whisker’s limit is not shown in the figure because the plot does not extend down to \\(Q_1 - 1.5\\times IQR\\). In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data.\nAny observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points – instead of just extending the whiskers to the minimum and maximum observed values – is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called outliers. In this case, it would be reasonable to classify the emails with character counts of 41,623, 42,793, and 64,401 as outliers since they are numerically distant from most of the data.\n\nOutliers are extreme\nAn outlier is an observation that is extreme, relative to the rest of the data.\n\n\nWhy it is important to look for outliers\nExamination of data for possible outliers serves many useful purposes, including:\n1. Identifying strong skew in the distribution.\n2. Identifying data collection or entry errors. For instance, we re-examined the email purported to have 64,401 characters to ensure this value was accurate.\n3. Providing insight into interesting properties of the data.\n\n\nExercise:\nThe observation with value 64,401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?19\n\n\nExercise:\nUsing Figure 5.10, estimate the following values for num_char in the email50 data set:\n(a) \\(Q_1\\),\n(b) \\(Q_3\\), and\n(c) IQR.20\n\nOf course, R can calculate these summary statistics for us. First, we will do these calculations individually and then in one function call. Remember to ask yourself what you want R to do and what it needs to do this.\n\nmean(~num_char, data = email50)\n\n[1] 11.59822\n\nsd(~num_char, data = email50)\n\n[1] 13.12526\n\nquantile(~num_char, data = email50)\n\n      0%      25%      50%      75%     100% \n 0.05700  2.53550  6.88950 15.41075 64.40100 \n\niqr(~num_char, data = email50)\n\n[1] 12.87525\n\n\n\nfavstats(~num_char, data = email50)\n\n   min     Q1 median       Q3    max     mean       sd  n missing\n 0.057 2.5355 6.8895 15.41075 64.401 11.59822 13.12526 50       0\n\n\n\n\n5.2.6 Robust statistics\nHow are the sample statistics of the num_char data set affected by the observation with value 64,401? What would we see if this email wasn’t present in the data set? What would happen to these summary statistics if the observation at 64,401 had been even larger, say 150,000? These scenarios are plotted alongside the original data in Figure 5.11, and sample statistics are computed in R.\nFirst, we create a new data frame containing the three scenarios: 1) the original data, 2) the data with the extreme observation dropped, and 3) the data with the extreme observation increased.\n\n# code to create the `robust` data frame\np1 &lt;- email50$num_char\np2 &lt;- p1[-which.max(p1)]\np3 &lt;- p1\np3[which.max(p1)] &lt;- 150\n\nrobust &lt;- data.frame(value = c(p1, p2, p3),\n                     group = c(rep(\"Original\", 50),\n                             rep(\"Dropped\", 49), rep(\"Increased\", 50)))\nhead(robust)\n\n   value    group\n1 21.705 Original\n2  7.011 Original\n3  0.631 Original\n4  2.454 Original\n5 41.623 Original\n6  0.057 Original\n\n\nNow, we create a side-by-side boxplots for each scenario.\n\ngf_boxplot(value ~ group, data = robust, xlab = \"Data Group\",\n           ylab = \"Number of Characters (in thousands)\") %&gt;%\n   gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 5.11: Box plots of the original character count data and two modified data sets, one where the outlier at 64,401 is dropped and one where its value is increased.\n\n\n\n\n\nWe can also use favstats() to calculate summary statistics of value by group, using the robust data frame created above.\n\nfavstats(value ~ group, data = robust)\n\n      group   min     Q1 median       Q3     max     mean       sd  n missing\n1   Dropped 0.057 2.4540 6.7680 14.15600  42.793 10.52061 10.79768 49       0\n2 Increased 0.057 2.5355 6.8895 15.41075 150.000 13.31020 22.43436 50       0\n3  Original 0.057 2.5355 6.8895 15.41075  64.401 11.59822 13.12526 50       0\n\n\nNotice by using the formula notation, we were able to calculate the summary statistics within each group.\n\nExercise:\n(a) Which is affected more by extreme observations, the mean or median? The data summary may be helpful.21\n(b) Which is affected more by extreme observations, the standard deviation or IQR?22\n\nThe median and IQR are called robust statistics because extreme observations have little effect on their values. The mean and standard deviation are affected much more by changes in extreme observations.\n\nExample:\nThe median and IQR do not change much under the three scenarios above. Why might this be the case?23\n\n\nExercise:\nThe distribution of vehicle prices tends to be right skewed, with a few luxury and sports cars lingering out into the right tail. If you were searching for a new car and cared about price, should you be more interested in the mean or median price of vehicles sold, assuming you are in the market for a regular car?24\n\n\n\n5.2.7 Transforming data\nWhen data are very strongly skewed, we sometimes transform them so they are easier to model. Consider the histogram of Major League Baseball players’ salaries from 2010, which is shown in Figure 5.12.\n\n\n\n\n\n\n\n\nFigure 5.12: Histogram of MLB player salaries for 2010, in millions of dollars.\n\n\n\n\n\n\nExample:\nThe histogram of MLB player salaries is somewhat useful because we can see that the data are extremely skewed and centered (as gauged by the median) at about $1 million. What about this plot is not useful?25\n\nThere are some standard transformations that are often applied when much of the data cluster near zero (relative to the larger values in the data set) and all observations are positive. A transformation is a rescaling of the data using a function. For instance, a plot of the natural logarithm26 of player salaries results in a new histogram in Figure 5.13. Transformed data are sometimes easier to work with when applying statistical models because the transformed data are much less skewed and outliers are usually less extreme.\n\n\n\n\n\n\n\n\nFigure 5.13: Histogram of the log-transformed MLB player salaries for 2010.\n\n\n\n\n\nTransformations can also be applied to one or both variables in a scatterplot. A scatterplot of the original line_breaks and num_char variables is shown in Figure 5.2 above. We can see a positive association between the variables and that many observations are clustered near zero. Later in this text, we might want to use a straight line to model the data. However, we’ll find that the data in their current state cannot be modeled very well. Figure 5.14 shows a scatterplot where both line_breaks and num_char have been transformed using a natural log (log base \\(e\\)) transformation. While there is a positive association in each plot, the transformed data show a steadier trend, which is easier to model than the original (un-transformed) data.\n\n\n\n\n\n\n\n\nFigure 5.14: A scatterplot of line_breaks versus num_char for the email50 data, where both variables have been log-transformed.\n\n\n\n\n\nTransformations other than the logarithm can be useful, too. For instance, the square root (\\(\\sqrt{\\text{original observation}}\\)) and inverse \\(\\left(\\frac{1}{\\text{original observation}}\\right)\\) are used commonly by statisticians. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html#homework-problems",
    "href": "05-Numerical-Data.html#homework-problems",
    "title": "5  Numerical Data",
    "section": "5.3 Homework Problems",
    "text": "5.3 Homework Problems\nCreate an Rmd file for the work including headers, file creation data, and explanation of your work. Make sure your plots have a title and the axes are labeled.\n\nMammals exploratory. Data were collected on 39 species of mammals distributed over 13 taxonomic orders. The data is in the mammals data set in the openintro package.\n\n\n\nUsing the documentation for the mammals data set, report the units for the variable brain_wt.\nUsing inspect(), how many variables are numeric?\nWhat type of variable is danger?\nCreate a histogram of total_sleep and describe the distribution.\nCreate a boxplot of life_span and describe the distribution.\nReport the mean and median life span of a mammal.\nCalculate the summary statistics for life_span broken down by danger. What is the standard deviation of life span in danger outcome 5?\n\n\n\nMammal life spans. Continue using the mammals data set.\n\n\n\nCreate side-by-side boxplots for life_span broken down by exposure. Note: you will have to change exposure to a factor(). Report on any findings.\nWhat happened to the median and third quartile in exposure group 4?\nUsing the same variables, create faceted histograms. What are the shortcomings of this plot?\nCreate a new variable exposed that is a factor with level Low if exposure is 1 or 2 and High otherwise.\nRepeat part c) with the new exposed variable. Explain what you see in the plot.\n\n\n\nMammal life spans continued\n\n\n\nCreate a scatterplot of life span versus length of gestation.\nWhat type of association is apparent between life span and length of gestation?\nWhat type of association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span?\nCreate the new scatterplot suggested in part c).\nAre life span and length of gestation independent? Explain your reasoning.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html#solutions-manual",
    "href": "05-Numerical-Data.html#solutions-manual",
    "title": "5  Numerical Data",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "05-Numerical-Data.html#footnotes",
    "href": "05-Numerical-Data.html#footnotes",
    "title": "5  Numerical Data",
    "section": "",
    "text": "Answers may vary. Scatterplots are helpful in quickly spotting associations between variables, whether those associations represent simple or more complex relationships.↩︎\nSubset of data from http://www.amstat.org/publications/jse/v1n1/datasets.lock.html↩︎\nConsider the case where your vertical axis represents something “good” and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description since water becomes toxic when consumed in excessive quantities.↩︎\nThe distribution of a variable is essentially the collection of all values of the variable in the data set. It tells us what values the variable takes on and how often. In the email50 data set, we used a dotplot to view the distribution of num_char.↩︎\n\\(x_1\\) corresponds to the number of characters in the first email in the sample (21.7, in thousands), \\(x_2\\) to the number of characters in the second email (7.0, in thousands), and \\(x_i\\) corresponds to the number of characters in the \\(i^{th}\\) email in the data set.↩︎\nThe sample size, \\(n = 50\\).↩︎\nA weighted mean is an average in which some observations contribute more “weight” than others. In the county data set, we “weighted” the income for each county by dividing income by the county population.↩︎\nA histogram displays the distribution of a quantitative variable. It shows binned counts, the number of observations in a bin, or range of values.↩︎\nOther ways to describe data that are skewed to the right: skewed to the right, skewed to the high end, or skewed to the positive end.↩︎\nThe skew is visible in both plots, though the dot plot is the least useful.↩︎\nCharacter counts for individual emails.↩︎\nAnother definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have no observations with the same value in a data set, which makes this other definition useless for many real data sets.↩︎\nThere might be two height groups visible in the data set: one for the students and one for the adults. That is, the data are probably bimodal. But it could be multimodal because within each group we may be able to see a difference in males and females.↩︎\nThe only difference is that the population variance has a division by \\(n\\) instead of \\(n - 1\\).↩︎\nStarting with Figure @ref(fig:hist53-fig), the three figures show three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution.↩︎\nThe distribution of email character counts is unimodal and very strongly skewed to the high end (right skewed). Many of the counts fall near the mean at 11,600, and most fall within one standard deviation (13,130) of the mean. There is one exceptionally long email with about 65,000 characters.↩︎\nSince \\(Q_1\\) and \\(Q_3\\) capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between \\(Q_1\\) and the median, and another 25% fall between the median and \\(Q_3\\).↩︎\nWhile the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.↩︎\nThat occasionally there may be very long emails.↩︎\nThese visual estimates will vary a little from one person to the next: \\(Q_1\\) ~ 3,000, \\(Q_3\\) ~ 15,000, IQR = \\(Q_3 - Q_1\\) ~ 12,000. (The true values: $Q_1 = $ 2,536, $Q_3 = $ 15,411, IQR = 12,875.)↩︎\nThe mean is affected more.↩︎\nThe standard deviation is affected more.↩︎\nThe median and IQR are only sensitive to numbers near \\(Q_1\\), the median, and \\(Q_3\\). Since values in these regions are relatively stable – there aren’t large jumps between observations – the median and IQR estimates are also quite stable.↩︎\nBuyers of a regular car should be more concerned about the median price. High-end car sales can drastically inflate the mean price while the median will be more robust to the influence of those sales.↩︎\nMost of the data are collected into one bin in the histogram and the data are so strongly skewed that many details in the data are obscured.↩︎\nStatisticians often write the natural logarithm as \\(\\log\\). You might be more familiar with it being written as \\(\\ln\\).↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Numerical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html",
    "href": "06-Categorical-Data.html",
    "title": "6  Categorical Data",
    "section": "",
    "text": "6.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html#objectives",
    "href": "06-Categorical-Data.html#objectives",
    "title": "6  Categorical Data",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: factor, contingency table, marginal counts, joint counts, frequency table, relative frequency table, bar plot, conditioning, segmented bar plot, mosaic plot, pie chart, side-by-side box plot, density plot.\nIn R, generate tables for categorical variable(s).\nIn R, generate appropriate graphical summaries of categorical and numerical variables.\nInterpret and explain output both graphically and numerically.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html#categorical-data",
    "href": "06-Categorical-Data.html#categorical-data",
    "title": "6  Categorical Data",
    "section": "6.2 Categorical data",
    "text": "6.2 Categorical data\nLike numerical data, categorical data can also be organized and analyzed. This section introduces tables and other basic tools for use with categorical data. Remember at the beginning of this block of material, our case study had categorical data so we have already seen some of the ideas in this chapter.\nThe email50 data set represents a sample from a larger email data set called email. This larger data set contains information on 3,921 emails. In this section, we will use the email data set to examine whether the presence of numbers, small or large, in an email provides any useful information in classifying email as spam or not spam.\n\n6.2.1 Contingency tables and bar plots\nIn the email data set, we have two variables, spam and number, that we want to summarize. Let’s use inspect() to get information and insight about the two variables. We can also type ?email or help(email) to learn more about the data. First, load the openintro library.\n\nlibrary(openintro)\n\n\nemail %&gt;%\n  select(spam, number) %&gt;%\n  inspect()\n\n\ncategorical variables:  \n    name  class levels    n missing\n1 number factor      3 3921       0\n                                   distribution\n1 small (72.1%), none (14%) ...                \n\nquantitative variables:  \n  name   class min Q1 median Q3 max       mean        sd    n missing\n1 spam numeric   0  0      0  0   1 0.09359857 0.2913066 3921       0\n\n\nNotice the use of the pipe operator and how it adds to the ease of reading the code. The select() function allows us to narrow down the columns/variables to the two of interest. Then inspect() gives us information about those variables. We read from top line; we start with the data set email, input it into select() and select variables from it, and then use inspect() to summarize the variables.\nAs indicated above, number is a categorical variable (a factor) that describes whether an email contains no numbers, only small numbers (values under 1 million), or at least one big number (a value of 1 million or more). The variable spam is a numeric variable, where 1 indicates the email is spam and 0 indicates the email is not spam. To treat spam as categorical, we will want to change it to a factor, but first we will build a table that summarizes data for the two variables (Table 6.1). This table is called a contingency table1. Each value in the table represents the number of times a particular combination of variable outcomes occurred.\n\n\n\n\nTable 6.1: A contingency table for the `email` data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpam\n\n\nNumber\n\n\n\n\n\nnone\nsmall\nbig\nTotal\n\n\n\n\n0\n400\n2659\n495\n3554\n\n\n1\n149\n168\n50\n367\n\n\nTotal\n549\n2827\n545\n3921\n\n\n\n\n\n\n\n\n\n\n\nBelow is the R code to generate the contingency table.\n\ntally(~spam + number, data = email, margins = TRUE)\n\n       number\nspam    none small  big Total\n  0      400  2659  495  3554\n  1      149   168   50   367\n  Total  549  2827  545  3921\n\n\nThe value 149 corresponds to the number of emails in the data set that are spam and had no numbers listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g. \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. The row and column totals are known as marginal2 counts (hence, margins = TRUE) and the values in the table are known as joint3 counts.\nLet’s turn spam into a factor and update the email data object. We will use mutate() to do this.\n\nemail &lt;- email %&gt;%\n  mutate(spam = factor(email$spam, levels = c(1, 0), \n                       labels = c(\"spam\", \"not spam\")))\n\nNow, let’s check the data again.\n\nemail %&gt;%\n  select(spam, number) %&gt;%\n  inspect()\n\n\ncategorical variables:  \n    name  class levels    n missing\n1   spam factor      2 3921       0\n2 number factor      3 3921       0\n                                   distribution\n1 not spam (90.6%), spam (9.4%)                \n2 small (72.1%), none (14%) ...                \n\n\nLet’s generate the contingency table again.\n\ntally(~spam + number, data = email, margins = TRUE)\n\n          number\nspam       none small  big Total\n  spam      149   168   50   367\n  not spam  400  2659  495  3554\n  Total     549  2827  545  3921\n\n\nA table for a single variable is called a frequency table. The table below is a frequency table for the number variable.\n\ntally(~number, data = email)\n\nnumber\n none small   big \n  549  2827   545 \n\n\nIf we replaced the counts with percentages or proportions, the table would be called a relative frequency table.\n\ntally(~number, data = email, format = 'proportion')\n\nnumber\n     none     small       big \n0.1400153 0.7209895 0.1389952 \n\n\n\nround(tally(~number, data = email, format = 'percent'), 2)\n\nnumber\n none small   big \n 14.0  72.1  13.9 \n\n\nA bar plot is a common way to display a single categorical variable. Figure 6.1 shows a bar plot for the number variable.\n\nemail %&gt;%\n  gf_bar(~number) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 6.1: Bar chart of the number variable.\n\n\n\n\n\nNext, the counts are converted into proportions (e.g., \\(549 / 3921 = 0.140\\) for none) in Figure 6.2.\n\nemail %&gt;%\n  gf_props(~number) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Proportion\")\n\n\n\n\n\n\n\nFigure 6.2: Bar chart of the number variable as a proportion.\n\n\n\n\n\nAgain, let’s clean up the plot into a style that we could use in a report.\n\nemail %&gt;%\n  gf_props(~number, \n           title = \"The proportions of emails with a number in it\",\n           subtitle = \"From 2012\", xlab = \"Type of number in the email\",\n           ylab = \"Proportion of emails\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\n\n\n\n6.2.2 Column proportions\nThe table below shows the column proportions. The column proportions are computed as the counts divided by their column totals. The value 149 at the intersection of spam and none is replaced by \\(149 / 549 = 0.271\\), i.e., 149 divided by its column total, 549. So what does 0.271 represent? It corresponds to the proportion of emails in the sample with no numbers that are spam. That is, the proportion of emails that are spam, out of all the emails with no numbers. We are conditioning, restricting, on emails with no number. This rate of spam is much higher than emails with only small numbers (5.9%) or big numbers (9.2%). Because these spam rates vary between the three levels of number (none, small, big), this provides evidence that the spam and number variables are associated.\n\ntally(spam ~ number, data = email, margins = TRUE, format = 'proportion')\n\n          number\nspam             none      small        big\n  spam     0.27140255 0.05942695 0.09174312\n  not spam 0.72859745 0.94057305 0.90825688\n  Total    1.00000000 1.00000000 1.00000000\n\n\nThe tally() function will always condition on the variable on the right-hand side of the tilde, ~, when calculating proportions. Thus, tally() only generates column or overall proportions. It cannot generate row proportions. The more general table() function of R will allow either column or row proportions.\n\nExercise:\nCreate a table of column proportions where the variable spam is the column variable.\n\n\ntally(number ~ spam, data = email, margins = TRUE, format = 'proportion')\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\n\nExercise:\nIn the table you just created, what does 0.748 represent?4\n\n\nExercise: Create a table of proportions, where spam is the column variable and the values shown represent the proportion of the entire sample in each category.\n\n\ntally(~ number + spam, data = email, margins = TRUE, format = \"proportion\")\n\n       spam\nnumber        spam   not spam      Total\n  none  0.03800051 0.10201479 0.14001530\n  small 0.04284621 0.67814333 0.72098954\n  big   0.01275185 0.12624331 0.13899515\n  Total 0.09359857 0.90640143 1.00000000\n\n\n\nExample:\nData scientists use statistics to filter spam from incoming email messages. By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. One of those characteristics is whether the email contains no numbers, small numbers, or big numbers. Another characteristic is whether or not an email has any HTML content (given by the format variable). A contingency table for the spam and format variables is needed.\n1. Make format into a categorical factor variable. The levels should be “text” and “HTML”.5\n2. Create a contingency table from the email data set with format in the columns and spam in the rows.\n\n\nemail &lt;- email %&gt;% \n  mutate(format = factor(email$format, levels = c(1, 0), \n                         labels = c(\"HTML\", \"text\")))\n\nIn deciding which variable to use as a column, the data scientist would be interested in how the proportion of spam changes within each email format. This corresponds to column proportions based on format: the proportion of spam in plain text emails and the proportion of spam in HTML emails.\n\ntally(spam ~ format, data = email, margins = TRUE, format = \"proportion\")\n\n          format\nspam             HTML       text\n  spam     0.05796038 0.17489540\n  not spam 0.94203962 0.82510460\n  Total    1.00000000 1.00000000\n\n\nIn generating the column proportions, we can see that a higher fraction of plain text emails are spam (\\(209 / 1195 = 17.5\\%\\)) compared to HTML emails (\\(158 / 2726 = 5.8\\%\\)). This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. Yet, when we carefully combine this information with many other characteristics, such as number and other variables, we stand a reasonable chance of being able to classify an email as spam or not spam.\nIn constructing a table, we need to think about which variable we want in the column and which in the row. The formula notation in some ways makes us think about the response and predictor variables, with the response variable (left-hand side) displayed in the rows and the predictor variable (right-hand side) displayed in the columns. However, in some cases, it is not clear which variable should be in the column and row and the analyst must decide what is being communicated with the table. Before settling on one form for a table, it is important to consider the audience and the message they are to receive from the table.\n\nExercise:\nCreate two tables with number and spam: one where number is in the columns, and one where spam is in the columns. Which table would be more useful to someone hoping to identify spam emails based on the type of numbers in the email?6\n\n\ntally(spam ~ number, data = email, format = 'proportion', margin = TRUE)\n\n          number\nspam             none      small        big\n  spam     0.27140255 0.05942695 0.09174312\n  not spam 0.72859745 0.94057305 0.90825688\n  Total    1.00000000 1.00000000 1.00000000\n\n\n\ntally(number ~ spam, data = email, format = 'proportion', margin = TRUE)\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\n\n\n6.2.3 Segmented bar and mosaic plots\nContingency tables using column proportions are especially useful for examining how two categorical variables are related. Segmented bar and mosaic plots provide a way to visualize the information in these tables.\nA segmented bar plot is a graphical display of contingency table information. For example, a segmented bar plot representing the table with number in the columns is shown in Figure 6.3, where we have first created a bar plot using the number variable and then separated each group by the levels of spam using the fill argument.\n\nemail %&gt;%\n  gf_bar(~number, fill = ~spam) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 6.3: Segmented bar plot for numbers found in emails, where the counts have been further broken down by spam.\n\n\n\n\n\nThe column proportions of the table have been translated into a standardized segmented bar plot in Figure 6.4, which is a helpful visualization of the fraction of spam emails within each level of number.\n\nemail %&gt;%\n  gf_props(~number, fill = ~spam, position = 'fill') %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Size of Number\", y = \"Proportion\")\n\n\n\n\n\n\n\nFigure 6.4: Standardized version of Figure 6.3.\n\n\n\n\n\n\nExample:\nExamine both of the segmented bar plots. Which is more useful?7\n\nSince the proportion of spam changes across the groups in Figure 6.4, we can conclude the variables are dependent, which is something we were also able to discern using table proportions. Because both the none and big groups have relatively few observations compared to the small group, the association is more difficult to see in Figure 6.3.\nIn other cases, a segmented bar plot that is not standardized will be more useful in communicating important information. Before settling on a particular segmented bar plot, create standardized and non-standardized forms and decide which is more effective at communicating features of the data.\nA mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. It seems strange, but mosaic plots are not part of the mosaic package. We must load another set of packages called vcd and vcdExtra. Mosaic plots help to visualize the pattern of associations among variables in two-way and larger tables. Mosaic plots are controversial because they rely on the perception of area; human vision is not good at distinguishing areas.\nWe introduce mosaic plots as another way to visualize contingency tables. Figure 6.5 shows a one-variable mosaic plot for the number variable. Each row represents a level of number, and the row heights correspond to the proportion of emails of each number type. For instance, there are fewer emails with no numbers than emails with only small numbers, so the none outcome row is shorter in height. In general, mosaic plots use box areas to represent the number of observations. Since there is only one variable, the widths are all constant. Thus area is simply related to row height making this visual easy to read.\n\nlibrary(vcd)\n\n\nmosaic(~number, data = email)\n\n\n\n\n\n\n\nFigure 6.5: Mosaic plot where emails are grouped by the number variable.\n\n\n\n\n\nThis one-variable mosaic plot can be further divided into pieces as in Figure 6.6 using the spam variable. The first variable in the formula is used to determine row height. That is, each row is split proportionally according to the fraction of emails in each number category. These heights are similar to those in Figure 6.5. Next, each row is split horizontally according to the proportion of emails that were spam in that number group. For example, the second row, representing emails with only small numbers, was divided into emails that were spam (left) and not spam (right). The area of the rectangles represents the overall proportions in the table, where each cell count is divided by the total count. First, we will generate the table and then represent it as a mosaic plot.\n\ntally(~number + spam, data = email, format = 'proportion')\n\n       spam\nnumber        spam   not spam\n  none  0.03800051 0.10201479\n  small 0.04284621 0.67814333\n  big   0.01275185 0.12624331\n\n\n\nmosaic(~number + spam, data = email)\n\n\n\n\n\n\n\nFigure 6.6: Mosaic plot with number as the first (row) variable.\n\n\n\n\n\nThese plots are hard to use in a visual comparison of area. For example, is the area for small number spam emails different from none number spam emails? The rectangles have different shapes but from the table we can tell the areas are very similar.\nAn important use of the mosaic plot is to determine if an association between variables may be present. The bottom row of the first column represents spam emails that had big numbers, and the bottom row of the second column represents regular emails that had big numbers. We can again use this plot to see that the spam and number variables are associated since some rows are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized version of the segmented bar plot.\nIn a similar way, a mosaic plot representing column proportions where spam is in the column could be constructed.\n\nmosaic(~spam + number, data = email)\n\n\n\n\n\n\n\nFigure 6.7: Mosaic plot with spam as the first (row) variable.\n\n\n\n\n\nTo completely understand the mosaic plot as shown in Figure 6.7, let’s first find the proportions of spam.\n\ntally(~spam, data = email, format = \"proportion\")\n\nspam\n      spam   not spam \n0.09359857 0.90640143 \n\n\nSo, the row heights will be split 90-10. Next, let’s find the proportions of number within each value of spam. In the spam row, none will be 41%, small will be 46%, and big will be 13%. In the not spam row, none will be 11%, small will be 75%, and big will be 14%.\n\ntally(number ~ spam, data = email, margins = TRUE, format = \"proportion\")\n\n       spam\nnumber       spam  not spam\n  none  0.4059946 0.1125492\n  small 0.4577657 0.7481711\n  big   0.1362398 0.1392797\n  Total 1.0000000 1.0000000\n\n\nHowever, because it is more insightful for this application to consider the fraction of spam in each category of the number variable, we prefer Figure 6.6.\n\n\n6.2.4 The only pie chart you will see in this book, hopefully\nWhile pie charts are well known, they are typically not as useful as other charts in a data analysis. A pie chart is shown in Figure 6.8. It is generally more difficult to compare group sizes in a pie chart than in a bar plot, especially when categories have nearly identical counts or proportions. Just as human vision is bad at distinguishing areas, human vision is also bad at distinguishing angles. In the case of the none and big categories, the difference is so slight you may be unable to distinguish any difference in group sizes.\n\npie(table(email$number), col = COL[c(3, 1, 2)], radius = 0.75)\n\n\n\n\n\n\n\nFigure 6.8: A pie chart for number in the email data set.\n\n\n\n\n\nPie charts are popular in the Air Force due to the ease of generating them in Excel and PowerPoint. However, the values for each slice are often printed on top of the chart making the chart irrelevant. We recommend a minimal use of pie charts in your work.\n\n\n6.2.5 Comparing numerical data across groups\nSome of the more interesting investigations can be done by examining numerical data across groups. This is the case where one variable is categorical and the other is numerical. The methods required here aren’t really new. All that is required is to make a numerical plot for each group. Here, two convenient methods are introduced: side-by-side box plots and density plots.\nWe will again take a look at the subset of the county_complete data set. Let’s compare the median household income for counties that gained population from 2000 to 2010 versus counties that had no gain. While we might like to make a causal connection here, remember that these are observational data, so such an interpretation would be unjustified.\nThis section will give us a chance to perform some data wrangling. We will be using the tidyverse verbs in the process. Data wrangling is an important part of analysis work and typically makes up a significant portion of the analysis work.\nHere is the code to generate the data we need.\n\nlibrary(usdata)\n\n\ncounty_tidy &lt;- county_complete %&gt;% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %&gt;%\n  mutate(fed_spend = fed_spend / pop2010)\n\nFirst, as a reminder, let’s look at the data.\nWhat do we want R to do?\nWe want to select the variables pop2000, pop2010, and med_income.\nWhat does R need in order to do this?\nIt needs the data object, and the desired variable names.\nWe will use the select() and inspect() functions.\n\ncounty_tidy %&gt;%\n  select(pop2000, pop2010, med_income) %&gt;%\n  inspect()\n\n\nquantitative variables:  \n        name   class   min       Q1 median    Q3     max     mean        sd\n1    pop2000 numeric    67 11223.50  24621 61775 9519338 89649.99 292547.67\n2    pop2010 numeric    82 11114.50  25872 66780 9818605 98262.04 312946.70\n3 med_income numeric 19351 36956.25  42450 49144  115574 44274.12  11547.49\n     n missing\n1 3139       3\n2 3142       0\n3 3142       0\n\n\nNotice that three counties are missing population values for the year 2000, reported as NA. Let’s remove them and find which counties increased in population by creating a new variable.\n\ncc_reduced &lt;- county_tidy %&gt;%\n  drop_na(pop2000) %&gt;%\n  select(pop2000, pop2010, med_income) %&gt;%\n  mutate(pop_gain = sign(pop2010-pop2000))\n\n\ntally(~pop_gain, data = cc_reduced)\n\npop_gain\n  -1    0    1 \n1097    1 2041 \n\n\nThere were 2,041 counties where the population increased from 2000 to 2010, and there were 1,098 counties with no gain. Only 1 county had a net of zero, and 1,0987 had a loss. Let’s just look at the counties with a gain or loss in a side-by-side boxplot. Again, we will use filter() to select the two groups and then make the variable pop_gain into a categorical variable. It’s time for more data wrangling.\n\ncc_reduced &lt;- cc_reduced %&gt;%\n  filter(pop_gain != 0) %&gt;%\n  mutate(pop_gain = factor(pop_gain, levels = c(-1, 1), \n                           labels = c(\"Loss\", \"Gain\")))\n\n\ninspect(cc_reduced)\n\n\ncategorical variables:  \n      name  class levels    n missing\n1 pop_gain factor      2 3138       0\n                                   distribution\n1 Gain (65%), Loss (35%)                       \n\nquantitative variables:  \n        name   class   min       Q1  median      Q3     max     mean        sd\n1    pop2000 numeric    67 11217.25 24608.0 61783.5 9519338 89669.37 292592.28\n2    pop2010 numeric    82 11127.00 25872.0 66972.0 9818605 98359.23 313133.28\n3 med_income numeric 19351 36950.00 42443.5 49120.0  115574 44253.24  11528.95\n     n missing\n1 3138       0\n2 3138       0\n3 3138       0\n\n\nThe side-by-side box plot is a traditional tool for comparing across groups. An example is shown in Figure 6.9 where there are two box plots, one for each group, drawn on the same scale.\n\ncc_reduced %&gt;%\n  gf_boxplot(med_income ~ pop_gain,\n             subtitle = \"The income data were collected between 2006 and 2010.\",\n             xlab = \"Population change from 2000 to 2010\",\n             ylab = \"Median Household Income\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 6.9: Side-by-side box plot for median household income, where the counties are split by whether there was a population gain or loss from 2000 to 2010.\n\n\n\n\n\nAnother useful plotting method uses density plots to compare numerical data across groups. A histogram bins data but is highly dependent on the number and boundary of the bins. A density plot also estimates the distribution of a numerical variable but does this by estimating the density of data points in a small window around each data point. The overall curve is the sum of this small density estimate. A density plot can be thought of as a smooth version of the histogram. Several options go into a density estimate, such as the width of the window and type of smoothing function. These ideas are beyond the scope here and we will just use the default options. Figure 6.10 is a plot of the two density curves.\n\ncc_reduced %&gt;%\n  gf_dens(~med_income, color = ~pop_gain, lwd = 1) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Median household income\", y = \"Density\", col = \"Population \\nChange\")\n\n\n\n\n\n\n\nFigure 6.10: Density plots of median household income for counties with population gain versus population loss.\n\n\n\n\n\n\nExercise:\nUse the box plots and density plots to compare the incomes for counties across the two groups. What do you notice about the approximate center of each group? What do you notice about the variability between groups? Is the shape relatively consistent between groups? How many prominent modes are there for each group?8\n\n\nExercise:\nWhat components of Figures 6.9, 6.10 do you find most useful?9",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html#homework-problems",
    "href": "06-Categorical-Data.html#homework-problems",
    "title": "6  Categorical Data",
    "section": "6.3 Homework Problems",
    "text": "6.3 Homework Problems\nCreate an Rmd file for the work including headers, file creation data, and explanation of your work. Make sure your plots have a title and the axes are labeled.\n\nViews on immigration. 910 randomly sampled, registered voters from Tampa, FL were asked if they thought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for US citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US citizenship, or (iii) lose their jobs and have to leave the country.\nThe data is in the openintro package in the immigration data set.\n\n\n\nHow many levels of political are there?\nCreate a table using tally(). Note: a table showing overall proportions or percents may be most helpful for parts c) through e).\nWhat percent of these Tampa, FL voters identify themselves as conservatives?\nWhat percent of these Tampa, FL voters are in favor of the citizenship option?\nWhat percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citizenship option?\nWhat percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the citizenship option? What percent of moderates and liberal share this view?\nCreate a stacked bar chart to reflect your work in part f).\nUsing your plot, do political ideology and views on immigration appear to be independent? Explain your reasoning.\n\n\n\nViews on the DREAM Act. The same survey from Exercise 1 also asked respondents if they support the DREAM Act, a proposed law which would provide a path to citizenship for people brought illegally to the US as children.\nThe data is in the openintro package in the dream data object.\n\n\n\nCreate a mosaic plot of political view versus stance on the DREAM Act.\nBased on the mosaic plot, are views on the DREAM Act and political ideology independent?\n\n\n\nHeart transplants. The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable transplant indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called survived was used to indicate whether or not the patient was alive at the end of the study.\nThe data is in the openintro package and is called heart_transplant.\n\n\n\nCreate a mosaic plot of treatment versus survival status.\nBased on the mosaic plot, is survival independent of whether or not the patient got a transplant? Explain your reasoning.\nCreate side-by-side boxplots of survival time for the control and treatment groups.\nWhat do the box plots suggest about the efficacy (effectiveness) of transplants?",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html#solutions-manual",
    "href": "06-Categorical-Data.html#solutions-manual",
    "title": "6  Categorical Data",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "06-Categorical-Data.html#footnotes",
    "href": "06-Categorical-Data.html#footnotes",
    "title": "6  Categorical Data",
    "section": "",
    "text": "A contingency table is a two-way table that shows the distribution of one variable in rows and a second variable in columns.↩︎\nMarginal counts are counts based on only one of the variables in a contingency table. For example, there are 367 spam emails in the table.↩︎\nJoint counts are counts based on both variables in a contingency table. For example, there are 149 emails that are spam and contain no numbers.↩︎\nThis is the proportion of not spam emails that had a small number in it.↩︎\nFrom the help menu on the data, HTML is coded as a 1.↩︎\nThe table with number in the columns will probably be most useful. This table makes it easier to see that emails with small numbers are spam about 5.9% of the time (relatively rare). In contrast, we see that about 27.1% of emails with no numbers are spam, and 9.2% of emails with big numbers are spam.↩︎\nFigure 6.3 contains more information, but Figure 6.4 presents the information more clearly. This second plot makes it clear that emails with no number have a relatively high rate of spam email – about 27%! On the other hand, less than 10% of emails with small or big numbers are spam.↩︎\nAnswers may vary a little. The counties with population gains tend to have higher income (median of about $45,000) versus counties without a gain (median of about $40,000). The variability is also slightly larger for the population gain group. This is evident in the IQR, which is about 50% bigger in the gain group. Both distributions show slight to moderate right skew and are unimodal. There is a secondary small bump at about $60,000 for the no gain group, visible in the density plot, that seems out of place. (Looking into the data set, we would find that 8 of these 15 counties are in Alaska and Texas.) The box plots indicate there are many observations far above the median in each group, though we should anticipate that many observations will fall beyond the whiskers when using such a large data set.↩︎\nThe side-by-side box plots are especially useful for comparing centers and spreads, while the density plots are more useful for seeing distribution shape, skew, and groups of anomalies.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Categorical Data</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html",
    "href": "07-Probability-Case-Study.html",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#objectives",
    "href": "07-Probability-Case-Study.html#objectives",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "Use R to simulate a probabilistic model.\nUse basic counting methods.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#introduction-to-probability-models",
    "href": "07-Probability-Case-Study.html#introduction-to-probability-models",
    "title": "7  Probability Case Study",
    "section": "7.2 Introduction to probability models",
    "text": "7.2 Introduction to probability models\nIn this second block of material we will focus on probability models. We will take two approaches, one is mathematical and the other is computational. In some cases we can use both methods on a problem and in others only the computational approach is feasible. The mathematical approach to probability modeling allows us insight into the problem and the ability to understand the process. Simulation has a much greater ability to generalize but can be time intensive to run and often requires the writing of custom functions.\nThis case study is extensive and may seem overwhelming, but do not worry. We will discuss these ideas again in the many chapters we have coming up this block.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#probability-models",
    "href": "07-Probability-Case-Study.html#probability-models",
    "title": "7  Probability Case Study",
    "section": "7.3 Probability models",
    "text": "7.3 Probability models\nProbability models are an important tool for data analysts. They are used to explain variation in outcomes that cannot be explained by other variables. We will use these ideas in the Statistical Modeling Block to help us make decisions about our statistical models.\nOften probability models are used to answer a question of the form “What is the chance that …..?” This means that we typically have an experiment or trial where multiple outcomes are possible and we only have an idea of the frequency of those outcomes. We use this frequency as a measure of the probability of a particular outcome.\nFor this block we will focus just on probability models. To apply a probability model we will need to\n\nSelect the experiment and its possible outcomes.\nHave probability values for the outcomes which may include parameters that determine the probabilities.\nUnderstand the assumptions behind the model.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#case-study",
    "href": "07-Probability-Case-Study.html#case-study",
    "title": "7  Probability Case Study",
    "section": "7.4 Case study",
    "text": "7.4 Case study\nThere is a famous example of a probability question that we will attack in this case study. The question we want to answer is “In a room of \\(n\\) people what is the chance that at least two people have the same birthday?”\n\nExercise:\nThe typical classroom at USAFA has 18 students in it. What do you think the chance that at least two students have the same birthday?1\n\n\n7.4.1 Break down the question\nThe first action we should take is to understand what is being asked.\n\nWhat is the experiment or trial?\nWhat does it mean to have the same birthday?\nWhat about leap years?\nWhat about the frequency of births? Are some days less likely than others?\n\n\nExercise:\nDiscuss these questions and others that you think are relevant.2\n\nThe best first step is to make a simple model, often these are the only ones that will have a mathematical solution. For our problem this means we answer the above questions.\n\nWe have a room of 18 people and we look at their birthdays. We either have two or more birthdays matching or not; thus there are two outcomes.\nWe don’t care about the year, only the day and month. Thus two people born on May 16th are a match.\nWe will ignore leap years.\nWe will assume that a person has equal probability of being born on any of the 365 days of the year.\nAt least two means we could have multiple matches on the same day or several different days where multiple people have matching birthdays.\n\n\n\n7.4.2 Simulate (computational)\nNow that we have an idea about the structure of the problem, we next need to think about how we would simulate a single classroom. We have 18 students in the classroom and they all could have any of the 365 days of the year as a birthday. What we need to do is sample birthdays for each of the 18 students. But how do we code the days of the year?\nAn easy solution is to just label the days from 1 to 365. The function seq() does this for us.\n\ndays &lt;- seq(1,365)\n\nNext we need to pick one of the days using the sample function. Note that we set the seed to get repeatable results, this is not required.\n\nset.seed(2022)\nsample(days,1)\n\n[1] 228\n\n\nThe first person was born on the 228th day of the year.\nSince R works on vectors, we don’t have to write a loop to select 18 days, we just have sample() do it for us.\n\nclass &lt;- sample(days,size=18,replace = TRUE)\nclass\n\n [1] 206 311 331 196 262 191 206 123 233 270 248   7 349 112   1 307 288 354\n\n\nWhat do we want R to do? Sample from the numbers 1 to 365 with replacement, which means a number can be picked more than once.\nNotice in our sample we have at least one match, although it is difficult to look at this list and see the match. Let’s sort them to make it easier for us to see.\n\nsort(class)\n\n [1]   1   7 112 123 191 196 206 206 233 248 262 270 288 307 311 331 349 354\n\n\nThe next step is to find a way in R for the code to detect that there is a match.\n\nExercise:\nWhat idea(s) can we use to determine if a match exists?\n\nWe could sort the data and look at differences in sequential values and then check if the set of differences contains a zero. This seems to be computationally expensive. Instead we will use the function unique() which gives a vector of unique values in an object. The function length() gives the number of elements in the vector.\n\nlength(unique(class))\n\n[1] 17\n\n\nSince we only have 17 unique values in a vector of size 18, we have a match. Now let’s put this all together to generate another classroom of size 18.\n\nlength(unique(sample(days,size=18,replace = TRUE)))\n\n[1] 16\n\n\nThe next problem that needs to be solved is how to repeat the classrooms and keep track of those that have a match. There are several functions we could use to include replicate() but we will use do() from the mosaic package because it returns a data frame so we can use tidyverse verbs to wrangle the data.\nThe do() function allows us to repeat an operation many times. The following template\ndo(n) * {stuff to do}              # pseudo-code\nwhere {stuff to do} is typically a single R command, but may be something more complicated.\nLoad the libraries.\n\nlibrary(mosaic)\nlibrary(tidyverse)\n\n\ndo(5)*length(unique(sample(days,size=18,replace = TRUE)))\n\n  length\n1     18\n2     17\n3     17\n4     17\n5     18\n\n\nLet’s repeat for a larger number of simulated classroom, remember you should be asking yourself:\nWhat do I want R to do?\nWhat does R need to do this?\n\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;%\n  mutate(match=if_else(length==18,0,1)) %&gt;%\n  summarise(prob=mean(match))\n\n  prob\n1 0.36\n\n\nThis is within 2 decimal places of the mathematical solution we develop shortly.\nHow many classrooms do we need to simulate to get an accurate estimate of the probability of a match? That is a statistical modeling question and it depends on how much variability we can accept. We will discuss these ideas later in the book. For now, you can run the code multiple times and see how the estimate varies. If computational power is cheap, you can increase the number of simulations.\n\n(do(10000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;%\n  mutate(match=if_else(length==18,0,1)) %&gt;%\n  summarise(prob=mean(match))\n\n    prob\n1 0.3442\n\n\n\n\n7.4.3 Plotting\nBy the way, the method we have used to create the data allows us to summarize the number of unique birthdays using a table or bar chart. Let’s do that now. Note that since the first argument in tally() is not data then the pipe operator will not work without some extra effort. We must tell R that the data is the previous argument in the pipeline and thus use the symbol . to denote this.\n\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;%\n  tally(~length,data=.)\n\nlength\n 14  15  16  17  18 \n  1   7  52 253 687 \n\n\nFigure 7.1 is a plot of the number of unique birthdays in our sample.\n\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;%\n  gf_bar(~length) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"Number of unique birthdays\",y=\"Count\")\n\n\n\n\n\n\n\nFigure 7.1: Bar chart of the number of unique birthdays in the sample.\n\n\n\n\n\n\nExercise:\nWhat does it mean if the length of unique birthdays is 16, in terms of matches?3\n\n\n\n7.4.4 Mathematical solution\nTo solve this problem mathematically, we will step through the logic one step at a time. One of the key ideas that we will see many times is the idea of the multiplication rule. This idea is the foundation for permutation and combinations which are counting methods frequently used in probability calculations.\nThe first step that we take is to understand the idea of 2 or more people with the same birthday. With 18 people, there are a great deal of possibilities for 2 or more birthdays. We could have exactly 2 people with the same birthday. We could have 18 people with the same birthday, We could have 3 people with the same birthday and another 2 people with the same birthday but different from the other 3. Accounting for all these possibilities is too large a counting process. Instead, we will take the approach of finding the probability of no one having a matching birthday. Then the probability of at least 2 people having a matching birthday is 1 minus the probability that no one has a matching birthday. This is known as a complementary probability. A simpler example is to think about rolling a single die. The probability of rolling a 6 is equivalent to 1 minus the probability of not rolling a 6.\nWe first need to think about all the different ways we could get 18 birthdays. This is going to be our denominator in the probability calculation. First let’s just look at 2 people. The first person could have 365 different days for their birthday. The second person could also have 365 different birthdays. So for each birthday of the first person there could be 365 birthdays for the second. Thus for 2 people there are \\(365^2\\) possible sets of birthdays. This is an example of the multiplication rule. For 18 people there are \\(365^{18}\\) sets of birthdays. That is a large number. Again, this will be our denominator in calculating the probability.\nThe numerator is the number of sets of birthdays with no matches. Again, let’s consider 2 people. The first person can have a birthday on any day of the year, so 365 possibilities. Since we don’t want a match, the second person can only have 364 possibilities for a birthday. Thus we have \\(365 \\times 364\\) possibilities for two people to have different birthdays.\n\nExercise:\nWhat is the number of possibilities for 18 people so that no one has the same birthday.\n\nThe answer for 18 people is \\(365 \\times 364 \\times 363 ... \\times 349 \\times 348\\). This looks like a truncated factorial. Remember a factorial, written as \\(n!\\) with an explanation point, is the product of successive positive integers. As an example \\(3!\\) is \\(3 \\times 2 \\times 1\\) or 6. We could write the multiplication for the numerator as \\[\\frac{365!}{(365-n)!}\\] As we will learn, the multiplication rule for the numerator is known as a permutation.\nWe are ready to put it all together. For 18 people, the probability of 2 or more people with the same birthday is 1 minus the probability that no one has the same birthday, which is\n\\[1 - \\frac{\\frac{365!}{(365-18)!}}{365^{18}}\\] or\n\\[1 - \\frac{\\frac{365!}{347!}}{365^{18}}\\]\nIn R there is a function called factorial() but factorials get large fast and we will overflow the memory. Try factorial(365) in R to see what happens.\n\nfactorial(365)\n\n[1] Inf\n\n\nIt is returning infinity because the number is too large for the buffer. As is often the case we will have when using a computational method, we must be clever about our approach. Instead of using factorials we can make use of Rs ability to work on vectors. If we provide R with a vector of values, the prod() will perform a product of all the elements.\n\n365*364\n\n[1] 132860\n\n\n\nprod(365:364)\n\n[1] 132860\n\n\n\n1- prod(365:348)/(365^18)\n\n[1] 0.3469114\n\n\n\n\n7.4.5 General solution\nWe now have the mathematics to understand the problem. We can easily generalize this to any number of people. To do this, we have to write a function in R. As with everything in R, we save a function as an object. The general format for creating a function is\n\nmy_function &lt;- function(parameters){\n  code for function\n}\n\nFor this problem we will call the function birthday_prob(). The only parameter we need is the number of people in the room, n. Let’s write this function.\n\nbirthday_prob &lt;- function(n=20){\n  1- prod(365:(365-(n-1)))/(365^n)\n}\n\nNotice we assigned the function to the name birthday_prob, we told R to expect one argument to the function, which we are calling n, and then we provide R with the code to find the probability. We set a default value for n in case one is not provided to prevent an error when the function is run. We will learn more about writing functions throughout this book and in the follow-on USAFA course, Math 378: Applied Statistical Modeling.\nTest the code with a know answer.\n\nbirthday_prob(18)\n\n[1] 0.3469114\n\n\nNow we can determine the probability for any size room. You may have heard that it only takes about 23 people in a room to have a 50% probability of at least 2 people matching birthdays.\n\nbirthday_prob(23)\n\n[1] 0.5072972\n\n\nLet’s create a plot of the probability versus number of people in the room. To do this, we need to apply the function to a vector of values. The function sapply() will work or we can also use Vectorize() to alter our existing function. We choose the latter option.\nFirst notice what happens if we input a vector into our function.\n\nbirthday_prob(1:20)\n\nWarning in 365:(365 - (n - 1)): numerical expression has 20 elements: only the\nfirst used\n\n\n [1] 0.0000000 0.9972603 0.9999925 1.0000000 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n\n\nIt only uses the first value. There are several ways to solve this problem. We can use the map() function in the purrr package. This idea of mapping a function to a vector is important in data science. It is used in scenarios where there is a lot of data. In this case the idea of map-reduce is used to make the analysis amenable to parallel computing.\n\nmap_dbl(1:20,birthday_prob)\n\n [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n[13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n[19] 0.379118526 0.411438384\n\n\nWe could also just vectorize the function.\n\nbirthday_prob &lt;- Vectorize(birthday_prob)\n\nNow notice what happens.\n\nbirthday_prob(1:20)\n\n [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n[13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n[19] 0.379118526 0.411438384\n\n\nWe are good to go. Let’s create our line plot, Figure 7.2.\n\ngf_line(birthday_prob(1:100)~ seq(1,100),\n        xlab=\"Number of People\",\n        ylab=\"Probability of Match\",\n        title=\"Probability of at least 2 people with matching birthdays\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.2: The probability of at least 2 people having mathcing birthdays.\n\n\n\n\n\nIs this what you expected the curve to look like? We, the authors, did not expect this. It has a sigmodial shape with a large increase in the middle range and flatten in the tails.\n\n\n7.4.6 Data science approach\nThe final approach we will take is one based on data, a data science approach. In the mosaicData package is a data set called Births that contains the number of births in the US from 1969 to 1988. This data will allow us to estimate the number of births on any day of the year. This allows us to eliminate the reliance on the assumption that each day is equally likely. Let’s first inspect() the data object.\n\ninspect(Births)\n\n\ncategorical variables:  \n  name   class levels    n missing\n1 wday ordered      7 7305       0\n                                   distribution\n1 Wed (14.3%), Thu (14.3%), Fri (14.3%) ...    \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 date  Date 1969-01-01 1988-12-31   1 days   1 days 7305       0\n\nquantitative variables:  \n          name   class  min   Q1 median    Q3   max        mean          sd\n1       births integer 6675 8792   9622 10510 12851 9648.940178 1127.315229\n2         year integer 1969 1974   1979  1984  1988 1978.501027    5.766735\n3        month integer    1    4      7    10    12    6.522930    3.448939\n4  day_of_year integer    1   93    184   275   366  183.753593  105.621885\n5 day_of_month integer    1    8     16    23    31   15.729637    8.800694\n6  day_of_week integer    1    2      4     6     7    4.000274    1.999795\n     n missing\n1 7305       0\n2 7305       0\n3 7305       0\n4 7305       0\n5 7305       0\n6 7305       0\n\n\nIt could be argued that we could randomly pick one year and use it. Let’s see what happens if we just used 1969. Figure 7.3 is a scatter plot of the number of births in 1969 for each day of the year.\n\nBirths %&gt;%\n  filter(year == 1969) %&gt;%\n  gf_point(births~day_of_year) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"Day of the Year\",y=\"Number of Births\")\n\n\n\n\n\n\n\nFigure 7.3: The number of births for each day of the year in 1969.\n\n\n\n\n\n\nExercise:\nWhat patterns do you see in Figure 7.3? What might explain them?\n\nThere are definitely bands appearing in the data which could be the day of the week; there are less birthdays on the weekend. There is also seasonality with more birthdays in the summer and fall. There is also probably an impact from holidays.\nQuickly, let’s look at the impact of day of the week by using color for day of the week. Figure 7.4 makes it clear that the weekends have less number of births as compared to the work week.\n\nBirths %&gt;%\n  filter(year == 1969) %&gt;%\n  gf_point(births~day_of_year,color=~factor(day_of_week)) %&gt;%\n  gf_labs(x=\"Day of the Year\",col=\"Day of Week\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.4: The number of births for each day of the year in 1969 broken down by day of the week.\n\n\n\n\n\nBy only using one year, this data might give poor results since holidays will fall on certain days of the week and the weekends will also be impacted. Note that we also still have the problem of leap years.\n\nBirths %&gt;%\n  group_by(year) %&gt;%\n  summarise(n=n())\n\n# A tibble: 20 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1969   365\n 2  1970   365\n 3  1971   365\n 4  1972   366\n 5  1973   365\n 6  1974   365\n 7  1975   365\n 8  1976   366\n 9  1977   365\n10  1978   365\n11  1979   365\n12  1980   366\n13  1981   365\n14  1982   365\n15  1983   365\n16  1984   366\n17  1985   365\n18  1986   365\n19  1987   365\n20  1988   366\n\n\nThe years 1972, 1976, 1980, 1984, and 1988 are all leap years. At this point, to make the analysis easier, we will drop those years.\n\nBirths %&gt;%\n  filter(!(year %in% c(1972,1976,1980,1984,1988))) %&gt;%\n  group_by(year) %&gt;%\n  summarise(n=n())\n\n# A tibble: 15 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1969   365\n 2  1970   365\n 3  1971   365\n 4  1973   365\n 5  1974   365\n 6  1975   365\n 7  1977   365\n 8  1978   365\n 9  1979   365\n10  1981   365\n11  1982   365\n12  1983   365\n13  1985   365\n14  1986   365\n15  1987   365\n\n\nNotice in filter() we used the %in% argument. This is a logical argument checking if year is one of the values. The ! at the front negates this in a sense requiring year not to be one of those values.`\nWe are almost ready to simulate. We need to get the count of births on each day of the year for the non-leap years.\n\nbirth_data &lt;- Births %&gt;%\n  filter(!(year %in% c(1972,1976,1980,1984,1988))) %&gt;%\n  group_by(day_of_year) %&gt;%\n  summarise(n=sum(births)) \n\n\nhead(birth_data)\n\n# A tibble: 6 × 2\n  day_of_year      n\n        &lt;int&gt;  &lt;int&gt;\n1           1 120635\n2           2 129042\n3           3 135901\n4           4 136298\n5           5 137319\n6           6 140044\n\n\nLet’s look at a plot of the number of births versus day of the year. We combined years in Figure 7.5.\n\nbirth_data %&gt;%\n  gf_point(n~day_of_year,\n          xlab=\"Day of the year\",\n          ylab=\"Number of births\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 7.5: Number of births by day of the year for all years.\n\n\n\n\n\nThis curve has the seasonal cycling we would expect. The smaller scale cycling is unexpected. Maybe because we are dropping the leap years, we are getting some days appearing in our time interval more frequently on weekends. We leave it to you to investigate this phenomenon.\nWe use these counts as weights in a sampling process. Days with more births will have a higher probability of being selected. Days such as Christmas and Christmas Eve have a lower probability of being selected. Let’s save the weights in an object to use in the sample() function.\n\nbirth_data_weights &lt;- birth_data %&gt;%\n  select(n) %&gt;%\n  pull()\n\nThe pull() function pulls the vectors of values out of the data frame format into a vector format which the sample() needs.\nNow let’s simulate the problem. The probability of a match should change slightly, maybe go down slightly?, but not much since most of the days have about the same probability or number of occurrences.\n\nset.seed(20)\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE,prob=birth_data_weights)))) %&gt;%\n  mutate(match=if_else(length==18,0,1)) %&gt;%\n  summarise(prob=mean(match))\n\n   prob\n1 0.352\n\n\nWe could not solve this problem of varying frequency of birth days using mathematics, at least as far as we know.\nCool stuff, let’s get to learning more about probability models in the next chapters.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#homework-problems",
    "href": "07-Probability-Case-Study.html#homework-problems",
    "title": "7  Probability Case Study",
    "section": "7.5 Homework Problems",
    "text": "7.5 Homework Problems\n\nExactly 2 people with the same birthday - Simulation. Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation.\n\n\n\nCreate a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report.\n\nSimulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23.\n\nPlot the frequency of occurrences as a bar chart.\n\nEstimate the probability of exactly two people having the same birthday.\n\n\n\nExactly 2 people with the same birthday - Mathematical. Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first.\n\n\n\nFind a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday.\n\nGeneralize your solution to any number n people in the room and create a function.\n\nVectorize the function.\n\nPlot the probability of exactly 2 people having the same birthday versus number of people in the room.\n\nComment on the shape of the curve and explain it.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#solutions-manual",
    "href": "07-Probability-Case-Study.html#solutions-manual",
    "title": "7  Probability Case Study",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "07-Probability-Case-Study.html#footnotes",
    "href": "07-Probability-Case-Study.html#footnotes",
    "title": "7  Probability Case Study",
    "section": "",
    "text": "The answer is around 34.7%, how close were you?↩︎\nAnother question may be What does it mean at least two people have matching birthdays?↩︎\nIt is possible that 3 people all have the same birthday or two sets of 2 people have the same birthday but different from the other pair.↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Case Study</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html",
    "href": "08-Probability-Rules.html",
    "title": "8  Probability Basics",
    "section": "",
    "text": "8.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#objectives",
    "href": "08-Probability-Rules.html#objectives",
    "title": "8  Probability Basics",
    "section": "",
    "text": "Define and use properly in context all new terminology related to probability, including: sample space, outcome, event, subset, intersection, union, complement, probability, mutually exclusive, exhaustive, independent, multiplication rule, permutation, combination.\nApply basic probability and counting rules to find probabilities.\nDescribe the basic axioms of probability.\nUse R to calculate and simulate probabilities of events.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#probability-vs-statistics",
    "href": "08-Probability-Rules.html#probability-vs-statistics",
    "title": "8  Probability Basics",
    "section": "8.2 Probability vs Statistics",
    "text": "8.2 Probability vs Statistics\nAs a review, remember this book is divided into four general blocks: data collection/summary, probability models, inference and statistical modeling/prediction. This second block, probability, is the study of stochastic (random) processes and their properties. Specifically, we will explore random experiments. As its name suggests, a random experiment is an experiment whose outcome is not predictable with exact certainty. In the statistical models we develop in the last two blocks of this book, we will use other variables to explain the variance of the outcome of interest. Any remaining variance is modeled with probability models.\nEven though an outcome is determined by chance, this does not mean that we know nothing about the random experiment. Our favorite simple example is that of a coin flip. If we flip a coin, the possible outcomes are heads and tails. We don’t know for sure what outcome will occur, but this doesn’t mean we don’t know anything about the experiment. If we assume the coin is fair, we know that each outcome is equally likely. Also, we know that if we flip the coin 100 times (independently), we are likely, the highest frequency event, to see around 50 heads, and very unlikely to see 10 heads or fewer.\nIt is important to distinguish probability from inference and modeling. In probability, we consider a known random experiment, including knowing the parameters, and answer questions about what we expect to see from this random experiment. In statistics (inference and modeling), we consider data (the results of a mysterious random experiment) and infer about the underlying process. For example, suppose we have a coin and we are unsure whether this coin is fair or unfair, the parameter is unknown. We flipped it 20 times and it landed on heads 14 times. Inferential statistics will help us answer questions about the underlying process (could this coin be unfair?).\n\n\n\n\n\nA graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we don’t know the underlying process, and must infer based on representative samples.\n\n\n\n\nThis block (10 chapters or so) is devoted to the study of random experiments. First, we will explore simple experiments, counting rule problems, and conditional probability. Next, we will introduce the concept of a random variable and the properties of random variables. Following this, we will cover common distributions of discrete and continuous random variables. We will end the block on multivariate probability (joint distributions and covariance).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#basic-probability-terms",
    "href": "08-Probability-Rules.html#basic-probability-terms",
    "title": "8  Probability Basics",
    "section": "8.3 Basic probability terms",
    "text": "8.3 Basic probability terms\nWe will start our work with some definitions and examples.\n\n8.3.1 Sample space\nSuppose we have a random experiment. The sample space of this experiment, \\(S\\), is the set of all possible results of that experiment. For example, in the case of a coin flip, we could write \\(S=\\{H,T\\}\\). Each element of the sample space is considered an outcome. An event is a set of outcomes, it is a subset of the sample space.\n\nExample:\nLet’s let R flip a coin for us and record the number of heads and tails. We will have R flip the coin twice. What is the sample space, what is an example of an outcome, and what is an example of an event.\n\nWe will load the mosaic package as it has a function rflip() that will simulate flipping a coin.\n\nlibrary(mosaic)\n\n\nset.seed(18)\nrflip(2)\n\n\nFlipping 2 coins [ Prob(Heads) = 0.5 ] ...\n\nH H\n\nNumber of Heads: 2 [Proportion Heads: 1]\n\n\nThe sample space is \\(S=\\{HH, TH, HT, TT\\}\\), an example of an outcome is \\(HH\\) which we see in the output from R, and finally an example of an event is the number of heads, which in this case takes on the values 0, 1, and 2. Another example of an event is “At least one heads”. In this case the event would be \\(\\{HH,TH, HT\\}\\). Also notice that \\(TH\\) is different from \\(HT\\) as an outcome; this is because those are different outcomes from flipping a coin twice.\n\nExample of Event:\nSuppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]\n\nEach vehicle represents a possible outcome of the experiment. Let \\(A\\) be the event that a blue vehicle is selected. This event contains the outcomes blue sedan and blue SUV.\n\n\n8.3.2 Union and intersection\nSuppose we have two events \\(A\\) and \\(B\\).\n\n\\(A\\) is considered a subset of \\(B\\) if all of the outcomes of \\(A\\) are also contained in \\(B\\). This is denoted as \\(A \\subset B\\).\nThe intersection of \\(A\\) and \\(B\\) is all of the outcomes contained in both \\(A\\) and \\(B\\). This is denoted as \\(A \\cap B\\).\nThe union of \\(A\\) and \\(B\\) is all of the outcomes contained in either \\(A\\) or \\(B\\), or both. This is denoted as \\(A \\cup B\\).\nThe complement of \\(A\\) is all of the outcomes not contained in \\(A\\). This is denoted as \\(A^C\\) or \\(A'\\).\n\nNote: Here we are treating events as sets and the above definitions are basic set operations.\nIt is sometimes helpful when reading probability notation to think of Union as an or and Intersection as an and.\n\nExample:\nConsider our rental car example above. Let \\(A\\) be the event that a blue vehicle is selected, let \\(B\\) be the event that a black vehicle is selected, and let \\(C\\) be the event that an SUV is selected.\n\nFirst, let’s list all of the outcomes of each event. \\(A = \\{\\mbox{blue sedan},\\mbox{blue SUV}\\}\\), \\(B=\\{\\mbox{black SUV}\\}\\), and \\(C= \\{\\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\).\nSince all outcomes in \\(B\\) are contained in \\(C\\), we know that \\(B\\) is a subset of \\(C\\), or \\(B\\subset C\\). Also, since \\(A\\) and \\(B\\) have no outcomes in common, \\(A \\cap B = \\emptyset\\). Note that \\(\\emptyset = \\{ \\}\\) is the empty set and contains no elements. Further, \\(A \\cup C = \\{\\mbox{blue sedan}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#probability",
    "href": "08-Probability-Rules.html#probability",
    "title": "8  Probability Basics",
    "section": "8.4 Probability",
    "text": "8.4 Probability\nProbability is a number assigned to an event or outcome that describes how likely it is to occur. A probability model assigns a probability to each element of the sample space. What makes a probability model is not just the values assigned to each element but the idea this model contains all the information about the outcomes and there are no other explanatory variables involved.\nA probability model can be thought of as a function that maps outcomes, or events, to a real number in the interval \\([0,1]\\).\nThere are some basic axioms of probability you should know, although this list is not complete. Let \\(S\\) be the sample space of a random experiment and let \\(A\\) be an event where \\(A\\subset S\\).\n\n\\(\\mbox{P}(A) \\geq 0\\).\n\\(\\mbox{P}(S) = 1\\).\n\nThese two axioms essentially say that probability must be positive, and the probability of all outcomes must sum to 1.\n\n8.4.1 Probability properties\nLet \\(A\\) and \\(B\\) be events in a random experiment. Most of these can be proven fairly easily.\n\n\\(\\mbox{P}(\\emptyset)=0\\)\n\\(\\mbox{P}(A')=1-\\mbox{P}(A)\\) We used this in the case study.\nIf \\(A\\subset B\\), then \\(\\mbox{P}(A)\\leq \\mbox{P}(B)\\).\n\\(\\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)\\). This property can be generalized to more than two events. The intersection is subtracted because outcomes in both events \\(A\\) and \\(B\\) get counted twice in the first sum.\nLaw of Total Probability: Let \\(B_1, B_2,...,B_n\\) be mutually exclusive, this means disjoint or no outcomes in common, and exhaustive, this means the union of all the events labeled with a \\(B\\) is the sample space. Then\n\n\\[\n\\mbox{P}(A)=\\mbox{P}(A\\cap B_1)+\\mbox{P}(A\\cap B_2)+...+\\mbox{P}(A\\cap B_n)\n\\]\nA specific application of this law appears in Bayes’ Rule (more to follow). It says that \\(\\mbox{P}(A)=\\mbox{P}(A \\cap B)+\\mbox{P}(A \\cap B')\\). Essentially, it points out that \\(A\\) can be partitioned into two parts: 1) everything in \\(A\\) and \\(B\\) and 2) everything in \\(A\\) and not in \\(B\\).\n\nExample:\nConsider rolling a six sided die. Let event \\(A\\) be the number showing is less than 5. Let event \\(B\\) be the number is even. Then\n\n\\[\\mbox{P}(A)=\\mbox{P}(A \\cap B) + \\mbox{P}(A \\cap B')\\]\n\\[\n\\mbox{P}(&lt; 5)=\\mbox{P}(&lt;5 \\cap Even)+\\mbox{P}(&lt;5 \\cap Odd)\n\\]\n\nDeMorgan’s Laws: \\[\n\\mbox{P}((A \\cup B)')=\\mbox{P}(A' \\cap B')\n\\] \\[\n\\mbox{P}((A \\cap B)')=\\mbox{P}(A' \\cup B')\n\\]\n\n\n\n8.4.2 Equally likely scenarios\nIn some random experiments, outcomes can be defined such that each individual outcome is equally likely. In this case, probability becomes a counting problem. Let \\(A\\) be an event in an experiment where each outcome is equally likely. \\[\n\\mbox{P}(A)=\\frac{\\mbox{# of outcomes in A}}{\\mbox{# of outcomes in S}}\n\\]\n\nExample:\nSuppose a family has three children, with each child being either a boy (B) or girl (G). Assume that the likelihood of boys and girls are equal and independent, this is the idea that the probability of the gender of the second child does not change based on the gender of the first child. The sample space can be written as: \\[\nS=\\{\\mbox{BBB},\\mbox{BBG},\\mbox{BGB},\\mbox{BGG},\\mbox{GBB},\\mbox{GBG},\\mbox{GGB},\\mbox{GGG}\\}\n\\] What is the probability that the family has exactly 2 girls?\n\nThis only happens in three ways: BGG, GBG, and GGB. Thus, the probability of exactly 2 girls is 3/8 or 0.375.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#counting-rules",
    "href": "08-Probability-Rules.html#counting-rules",
    "title": "8  Probability Basics",
    "section": "8.5 Counting rules",
    "text": "8.5 Counting rules\nThere are three types of counting problems we will consider. In each case, the multiplication rule is being used and all that changes is whether an element is allowed to be reused, replacement, and whether the order of selection matters. This latter question is difficult. Each case will be demonstrated with an example.\n\n8.5.1 Multiplication rule 1: Order matters, sample with replacement\nThe multiplication rule is at the center of each of the three methods. In this first case we are using the idea that order matters and items can be reused. Let’s use an example to help.\n\nExample:\nA license plate consists of three numeric digits (0-9) followed by three single letters (A-Z). How many possible license plates exist?\n\nWe can divide this problem into two sections. In the numeric section, we are selecting 3 objects from 10, with replacement. This means that a number can be used more than once. Order clearly matters because a license plate starting with “432” is distinct from a license plate starting with “234”. There are \\(10^3 = 1000\\) ways to select the first three digits; 10 for the first, 10 for the second, and 10 for the third. Why do you multiply and not add?1\nIn the alphabet section, we are selecting 3 objects from 26, where order matters. Thus, there are \\(26^3=17576\\) ways to select the last three letters of the plate. Combined, there are \\(10^3 \\times 26^3 = 17576000\\) ways to select license plates. Visually, \\[\n\\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} = 17,576,000\n\\]\nNext we are going to use this new counting method to find a probability.\n\nExercise:\nWhat is the probability a license plate starts with the number “8” or “0” and ends with the letter “B”?\n\nIn order to find this probability, we simply need to determine the number of ways to select a license plate starting with “8” or “0” and ending with the letter “B”. We can visually represent this event: \\[\n\\underbrace{\\underline{\\quad 2 \\quad }}_\\text{8 or 0} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 1 \\quad }}_\\text{B} = 135,200\n\\]\nDividing this number by the total number of possible license plates yields the probability of this event occurring.\n\ndenom&lt;-10*10*10*26*26*26\nnum&lt;-2*10*10*26*26*1\nnum/denom\n\n[1] 0.007692308\n\n\nThe probability of obtaining a license plate starting with “8” or “0” and ending with “B” is 0.0077. Simulating this would be difficult because we would need special functions to check the first number and last letter. This gets into text mining an important subject in data science but unfortunately we don’t have much time in this book for the topic.\n\n\n8.5.2 Multiplication rule 2 (Permutation): Order Matters, Sampling Without Replacement\nConsider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment depends on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(n(n-1)(n-2)...(n-k+1)\\). This is known as a permutation and is sometimes written as \\[\n{}_nP_{k} = \\frac{n!}{(n-k)!}\n\\]\nRecall that \\(n!\\) is read as \\(n\\) factorial and represents the number of ways to arrange \\(n\\) objects.\n\nExample:\nTwenty-five friends participate in a Halloween costume party. Three prizes are given during the party: most creative costume, scariest costume, and funniest costume. No one can win more than one prize. How many possible ways can the prizes by distributed?\n\nThere are \\(k=3\\) prizes to be assigned to \\(n=25\\) people. Once someone is selected for a prize, they are removed from the pool of eligibles. In other words, we are sampling without replacement. Also, order matters. For example, if Tom, Mike, and Jane, win most creative, scariest and funniest costume, respectively, this is a different outcome than if Mike won creative, Jane won scariest and Tom won funniest. Thus, the number of ways the prizes can be distributed is given by \\({}_{25}P_3 = \\frac{25!}{22!} = 13,800\\). A more visually pleasing way to express this would be: \\[\n\\underbrace{\\underline{\\quad 25 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{funniest} = 13,800\n\\]\nNotice that it is sometime difficult to determine if order matters or not in a problem, but in this example the name of the prize was a hint that indeed order matters.\nLet’s use the idea of a permutation to calculate a probability.\n\nExercise:\nAssume that all 25 participants are equally likely to win any one of the three prizes. What is the probability that Tom doesn’t win any of them?\n\nJust like in the previous probability calculation, we simply need to count the number of ways Tom doesn’t win any prize. In other words, we need to count the number of ways that prizes are distributed without Tom. So, remove Tom from the group of 25 eligible participants. The number of ways Tom doesn’t get a prize is \\({}_{24}P_3 = \\frac{24!}{21!}=12,144\\). Again visually: \\[\n\\underbrace{\\underline{\\quad 24 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 22 \\quad }}_\\text{funniest} = 12,144\n\\]\nThe probability Tom doesn’t get a prize is simply the second number divided by the first:\n\ndenom&lt;-factorial(25)/factorial(25-3)\n# Or, denom&lt;-25*24*23\nnum&lt;-24*23*22\nnum/denom\n\n[1] 0.88\n\n\n\n\n8.5.3 Multiplication rule 3 (Combination): Order Does Not Matter, Sampling Without Replacement\nConsider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment does not depend on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(\\frac{n!} {(n-k)!k!}\\). This is known as a combination and is written as: \\[\n\\binom{n}{k} = \\frac{n!}{(n-k)!k!}\n\\]\nThis is read as “\\(n\\) choose \\(k\\)”. Take a moment to compare combinations to permutations, discussed in Rule 2. The difference between these two rules is that in a combination, order no longer matters. A combination is equivalent to a permutation divided by \\(k!\\), the number of ways to arrange the \\(k\\) objects selected.\n\nExample:\nSuppose we draw 5 cards out of a standard deck (52 cards, no jokers). How many possible 5 card hands are there?\n\nIn this example, order does not matter. I don’t care if I receive 3 jacks then 2 queens or 2 queens then 3 jacks. Either way, it’s the same collection of 5 cards. Also, we are drawing without replacement. Once a card is selected, it cannot be selected again. Thus, the number of ways to select 5 cards is given by: \\[\n\\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960\n\\]\n\nExample:\nWhen drawing 5 cards, what is the probability of drawing a “flush” (5 cards of the same suit)?\n\nLet’s determine how many ways to draw a flush. There are four suits (clubs, hearts, diamonds and spades). Each suit has 13 cards. We would like to pick 5 of those 13 cards and 0 of the remaining 39. Let’s consider just one of those suits (clubs): \\[\n\\mbox{P}(\\mbox{5 clubs})=\\frac{\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]\nThe second part of the numerator (\\(\\binom{39}{0}\\)) isn’t necessary, since it simply represents the number of ways to select 0 objects from a group (1 way), but it helps clearly lay out the events. This brings up the point of what \\(0!\\) equals. By definition it is 1. This allows us to use \\(0!\\) in our work.\nNow, we expand this to all four suits by multiplying by 4, or \\(\\binom{4}{1}\\) since we are selecting 1 suit out of the 4: \\[\n\\mbox{P}(\\mbox{flush})=\\frac{\\binom{4}{1}\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]\n\nnum&lt;-4*choose(13,5)*1\ndenom&lt;-choose(52,5)\nnum/denom\n\n[1] 0.001980792\n\n\nThere is a probability of 0.0020 of drawing a flush in a draw of 5 cards from a standard deck of cards.\n\nExercise:\nWhen drawing 5 cards, what is the probability of drawing a “full house” (3 cards of the same rank and the other 2 of the same rank)?\n\nThis problem uses several ideas from this chapter. We need to pick the rank of the three of a kind. Then pick 3 cards from the 4 possible. Next we pick the rank of the pair from the remaining 12 ranks. Finally pick 2 cards of that rank from the 4 possible.\n\\[\n\\mbox{P}(\\mbox{full house})=\\frac{\\binom{13}{1}\\binom{4}{3}\\binom{12}{1}\\binom{4}{2}}{\\binom{52}{5}}\n\\]\n\nnum&lt;-choose(13,1)*choose(4,3)*choose(12,1)*choose(4,2)\ndenom&lt;-choose(52,5)\nnum/denom\n\n[1] 0.001440576\n\n\nWhy not use \\(\\binom{13}{2}\\) instead of \\(\\binom{13}{1}\\binom{12}{1}\\)?2\nWe have just determined that a full house has a lower probability of occurring than a flush. This is why in gambling, a flush is valued less than a full house.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#homework-problems",
    "href": "08-Probability-Rules.html#homework-problems",
    "title": "8  Probability Basics",
    "section": "8.6 Homework Problems",
    "text": "8.6 Homework Problems\n\nLet \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), and \\(\\mbox{P}(C)=0.4\\). Also, we know that \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Find the following:\n\n\n\n\\(\\mbox{P}(A\\cup B)\\)\n\n\\(\\mbox{P}(A\\cup B \\cup C)\\)\n\n\\(\\mbox{P}(B'\\cap C')\\)\n\n\\(\\mbox{P}(A\\cup (B\\cap C))\\)\n\n\\(\\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)')\\)\n\n\n\nConsider the example of the family in the reading. What is the probability that the family has at least one boy?\nThe Birthday Problem Revisited.\n\n\n\nSuppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely.\n\nIn R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 300. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%?\n\n\n\n\nConsider the license plate example from the reading.\n\n\n\nWhat is the probability that a license plate contains exactly one “B”?\n\nWhat is the probability that a license plate contains at least one “B”?\n\n\n\nConsider the party example in the reading.\n\n\n\nSuppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies?\n\nWhat is the probability that zombies win “most creative” and “funniest” but not “scariest”?\n\n\n\nConsider the cards example from the reading.\n\n\n\nHow many ways can we obtain a “two pairs” (2 of one number, 2 of another, and the final different)?\n\nWhat is the probability of drawing a “four of a kind” (four cards of the same value)?\n\n\n\nAdvanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house?",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#solutions-manual",
    "href": "08-Probability-Rules.html#solutions-manual",
    "title": "8  Probability Basics",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "08-Probability-Rules.html#footnotes",
    "href": "08-Probability-Rules.html#footnotes",
    "title": "8  Probability Basics",
    "section": "",
    "text": "Multiplication is repeated adding so in a sense we are adding. However in a more serious tone, for this problem for every first number there are 10 possibilities for the second number and for every second number there are 10 possibilities for the third numbers. This is multiplication.↩︎\nBecause this implies the order selection of the ranks does not matter. In other words, this assumes that for example 3 Kings and 2 fours is the same full house as 3 fours and 2 Kings. This is not true so we break the rank selection about essentially making it a permutation.↩︎",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html",
    "href": "09-Conditional-Probability.html",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "9.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#objectives",
    "href": "09-Conditional-Probability.html#objectives",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "Define conditional probability and distinguish it from joint probability.\n\nFind a conditional probability using its definition.\n\nUsing conditional probability, determine whether two events are independent.\n\nApply Bayes’ Rule mathematically and via simulation.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#conditional-probability",
    "href": "09-Conditional-Probability.html#conditional-probability",
    "title": "9  Conditional Probability",
    "section": "9.2 Conditional Probability",
    "text": "9.2 Conditional Probability\nSo far, we’ve covered the basic axioms of probability, the properties of events (set theory) and counting rules. Another important concept, perhaps one of the most important, is conditional probability. Often, we know a certain event or sequence of events has occurred and we are interested in the probability of another event.\n\nExample:\nSuppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]\n\n\nWhat is the probability that a blue vehicle is selected, given a sedan was selected?\n\nSince we know that a sedan was selected, our sample space has been reduced to just “red sedan” and “blue sedan”. The probability of selecting a blue vehicle out of this sample space is simply 1/2.\nIn set notation, let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. We are looking for \\(\\mbox{P}(A \\mbox{ given } B)\\), which is also written as \\(\\mbox{P}(A|B)\\). By definition, \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)}\n\\]\nIt is important to distinguish between the event \\(A|B\\) and \\(A \\cap B\\). This is a common misunderstanding about probability. \\(A \\cap B\\) is the event that an outcome was selected at random from the total sample space, and that outcome was contained in both \\(A\\) and \\(B\\). On the other hand, \\(A|B\\) assumes the \\(B\\) has occurred, and an outcome was drawn from the remaining sample space, and that outcome was contained in \\(A\\).\nAnother common misunderstanding involves the direction of conditional probability. Specifically, \\(A|B\\) is NOT the same event as \\(B|A\\). For example, consider a medical test for a disease. The probability that someone tests positive given they had the disease is different than the probability that someone has the disease given they tested positive. We will explore this example further in our Bayes’ Rule section.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#independence",
    "href": "09-Conditional-Probability.html#independence",
    "title": "9  Conditional Probability",
    "section": "9.3 Independence",
    "text": "9.3 Independence\nTwo events, \\(A\\) and \\(B\\), are said to be independent if the probability of one occurring does not change whether or not the other has occurred. We looked at this last chapter but now we have another way of looking at it using conditional probabilities. For example, let’s say the probability that a randomly selected student has seen the latest superhero movie is 0.55. What if we randomly select a student and we see that he/she is wearing a black backpack? Does that probability change? Likely not, since movie attendance is probably not related to choice of backpack color. These two events are independent.\nMathematically, \\(A\\) and \\(B\\) are considered independent if and only if \\[\n\\mbox{P}(A|B)=\\mbox{P}(A)\n\\]\nResult: \\(A\\) and \\(B\\) are independent if and only if \\[\n\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\n\\]\nThis follows from the definition of conditional probability and from above: \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A\\cap B)}{\\mbox{P}(B)}=\\mbox{P}(A)\n\\]\nThus, \\(\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\\).\n\nExample: Consider the example above. Recall events \\(A\\) and \\(B\\). Let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. Are \\(A\\) and \\(B\\) independent?\n\nNo. First, recall that \\(\\mbox{P}(A|B)=0.5\\). The probability of selecting a blue vehicle (\\(\\mbox{P}(A)\\)) is \\(2/7\\) (the number of blue vehicles in our sample space divided by 7, the total number vehicles in \\(S\\)). This value is different from 0.5; thus, \\(A\\) and \\(B\\) are not independent.\nWe could also use the result above to determine whether \\(A\\) and \\(B\\) are independent. Note that \\(\\mbox{P}(A)= 2/7\\). Also, we know that \\(\\mbox{P}(B)=2/7\\). So, \\(\\mbox{P}(A)\\mbox{P}(B)=4/49\\). But, \\(\\mbox{P}(A\\cap B) = 1/7\\), since there is just one blue sedan in the sample space. \\(4/49\\) is not equal to \\(1/7\\); thus, \\(A\\) and \\(B\\) are not independent.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#bayes-rule",
    "href": "09-Conditional-Probability.html#bayes-rule",
    "title": "9  Conditional Probability",
    "section": "9.4 Bayes’ Rule",
    "text": "9.4 Bayes’ Rule\nAs mentioned in the introduction to this section, \\(\\mbox{P}(A|B)\\) is not the same quantity as \\(\\mbox{P}(B|A)\\). However, if we are given information about \\(A|B\\) and \\(B\\), we can use Bayes’ Rule to find \\(\\mbox{P}(B|A)\\). Let \\(B_1, B_2, ..., B_n\\) be mutually exclusive and exhaustive events and let \\(\\mbox{P}(A)&gt;0\\). Then, \\[\n\\mbox{P}(B_k|A)=\\frac{\\mbox{P}(A|B_k)\\mbox{P}(B_k)}{\\sum_{i=1}^n \\mbox{P}(A|B_i)\\mbox{P}(B_i)}\n\\]\nLet’s use an example to dig into where this comes from.\n\nExample:\nSuppose a doctor has developed a blood test for a certain rare disease (only one out of every 10,000 people have this disease). After careful and extensive evaluation of this blood test, the doctor determined the test’s sensitivity and specificity.\n\nSensitivity is the probability of detecting the disease for those who actually have it. Note that this is a conditional probability.\nSpecificity is the probability of correctly identifying “no disease” for those who do not have it. Again, another conditional probability.\nSee Figure 9.1 for a visual representation of these terms and others related to what is termed a confusion matrix.\n\n\n\n\n\n\n\n\nFigure 9.1: A table of true results and test results for a hypothetical disease. The terminology is included in the table. These ideas are important when evaluating machine learning classification models.\n\n\n\n\n\nIn fact, this test had a sensitivity of 100% and a specificity of 99.9%. Now suppose a patient walks in, the doctor administers the blood test, and it returns positive. What is the probability that that patient actually has the disease?\nThis is a classic example of how probability could be misunderstood. Upon reading this question, you might guess that the answer to our question is quite high. After all, this is a nearly perfect test. After exploring the problem more in depth, we find a different result.\n\n9.4.1 Approach using whole numbers\nWithout going directly to the formulaic expression above, let’s consider a collection of 100,000 randomly selected people. What do we know?\n\nBased on the prevalence of this disease (one out of every 10,000 people have this disease), we know that 10 of them should have the disease.\nThis test is perfectly sensitive. Thus, of the 10 people that have the disease, all of them test positive.\nThis test has a specificity of 99.9%. Of the 99,990 that don’t have the disease, \\(0.999*99990\\approx 99890\\) will test negative. The remaining 100 will test positive.\n\nThus, of our 100,000 randomly selected people, 110 will test positive. Of these 110, only 10 actually have the disease. Thus, the probability that someone has the disease given they’ve tested positive is actually around \\(10/110 = 0.0909\\).\n\n\n9.4.2 Mathematical approach\nNow let’s put this in context of Bayes’ Rule as stated above. First, let’s define some events. Let \\(D\\) be the event that someone has the disease. Thus, \\(D'\\) would be the event that someone does not have the disease. Similarly, let \\(T\\) be the event that someone has tested positive. What do we already know? \\[\n\\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D')=0.9999\n\\] \\[\n\\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T'|D)=0\n\\] \\[\n\\mbox{P}(T'|D')=0.999 \\hspace{1cm} \\mbox{P}(T|D') = 0.001\n\\]\nWe are looking for \\(\\mbox{P}(D|T)\\), the probability that someone has the disease, given he/she has tested positive. By the definition of conditional probability, \\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)}\n\\]\nThe numerator can be rewritten, again utilizing the definition of conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\).\nThe denominator can be rewritten using the Law of Total Probability (discussed [here][Probability properties]) and then the definition of conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D') = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')\\). So, putting it all together, \\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')}\n\\]\nNow we have stated our problem in the context of quantities we know: \\[\n\\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909\n\\]\nNote that in the original statement of Bayes’ Rule, we considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). In this example, we only have two: \\(D\\) and \\(D'\\).\n\n\n9.4.3 Simulation\nTo do the simulation, we can think of it as flipping a coin. First let’s assume we are pulling 1,000,000 people from the population. The probability that any one person has the disease is 0.0001. We will use rflip() to get the 1,000,000 people and designate as no disease or disease.\n\nset.seed(43)\nresults &lt;- rflip(1000000,0.0001,summarize = TRUE)\nresults\n\n      n heads  tails  prob\n1 1e+06   100 999900 1e-04\n\n\nIn this case 100 people had the disease. Now let’s find the positive test results. Of the 100 with the disease, all will test positive. Of those without disease, there is a 0.001 probability of testing positive.\n\nrflip(as.numeric(results['tails']),prob=.001,summarize = TRUE)\n\n       n heads  tails  prob\n1 999900   959 998941 0.001\n\n\nNow 959 tested positive. Thus the probability of having the disease given a positive test result is approximately:\n\n100/(100+959)\n\n[1] 0.09442871",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#homework-problems",
    "href": "09-Conditional-Probability.html#homework-problems",
    "title": "9  Conditional Probability",
    "section": "9.5 Homework Problems",
    "text": "9.5 Homework Problems\n\nConsider: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\).\n\n\n\nAre \\(A\\) and \\(B\\) independent?\n\nAre \\(B\\) and \\(C\\) independent?\n\n\n\nSuppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another).\n\n\n\nWhat is the probability that the second flip results in heads?\n\nWhat is the probability that the second flip results in heads, given the first also resulted in heads?\n\nWhat is the probability both flips result in heads?\n\nWhat is the probability exactly one coin flip results in heads?\n\nNow assume I flip the coin five times. What is the probability the result is 5 heads?\n\nWhat is the probability the result is exactly 2 heads (out of 5 flips)?\n\n\n\nSuppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants.\nYou are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected.\n\n\n\nUse Bayes formula to solve this problem.\n\nUse simulation to solve this problem.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#solutions-manual",
    "href": "09-Conditional-Probability.html#solutions-manual",
    "title": "9  Conditional Probability",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html",
    "href": "10-Discrete-Random-Variables.html",
    "title": "10  Random Variables",
    "section": "",
    "text": "10.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html#objectives",
    "href": "10-Discrete-Random-Variables.html#objectives",
    "title": "10  Random Variables",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: random variable, discrete random variable, continuous random variable, mixed random variable, distribution function, probability mass function, cumulative distribution function, moment, expectation, mean, variance.\nGiven a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events.\nSimulate random variables for a discrete distribution.\nFind the moments of a discrete random variable.\nFind the expected value of a linear transformation of a random variable.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html#random-variables",
    "href": "10-Discrete-Random-Variables.html#random-variables",
    "title": "10  Random Variables",
    "section": "10.2 Random variables",
    "text": "10.2 Random variables\nWe have already discussed random experiments. We have also discussed \\(S\\), the sample space for an experiment. A random variable essentially maps the events in the sample space to the real number line. For a formal definition: A random variable \\(X\\) is a function \\(X: S\\rightarrow \\mathbb{R}\\) that assigns exactly one number to each outcome in an experiment.\n\nExample:\nSuppose you flip a coin three times. The sample space, \\(S\\), of this experiment is \\[\nS=\\{\\mbox{HHH}, \\mbox{HHT}, \\mbox{HTH}, \\mbox{HTT}, \\mbox{THH}, \\mbox{THT}, \\mbox{TTH}, \\mbox{TTT}\\}\n\\]\n\nLet the random variable \\(X\\) be the number of heads in three coin flips. Whenever introduced to a new random variable, you should take a moment to think about what possible values can \\(X\\) take? When tossing a coin 3 times, we can get no heads, one head, two heads or three heads. The random variable \\(X\\) assigns each outcome in our experiment to one of these values. Visually: \\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]\nThe sample space of \\(X\\), sometimes referred to as the support, is the list of numerical values that \\(X\\) can take. \\[\nS_X=\\{0,1,2,3\\}\n\\]\nBecause the sample space of \\(X\\) is a countable list of numbers, we consider \\(X\\) to be a discrete random variable (more on that later).\n\n10.2.1 How does this help?\nSticking with our example, we can now frame a problem of interest in the context of our random variable \\(X\\). For example, suppose we wanted to know the probability of at least two heads. Without our random variable, we have to write this as: \\[\n\\mbox{P}(\\mbox{at least two heads})= \\mbox{P}(\\{\\mbox{HHH},\\mbox{HHT},\\mbox{HTH},\\mbox{THH}\\})\n\\]\nIn the context of our random variable, this simply becomes \\(\\mbox{P}(X\\geq 2)\\). It may not seem important in a case like this, but imagine if we were flipping a coin 50 times and wanted to know the probability of obtaining at least 30 heads. It would be unfeasible to write out all possible ways to obtain at least 30 heads. It is much easier to write \\(\\mbox{P}(X\\geq 30)\\) and explore the distribution of \\(X\\).\nEssentially, a random variable often helps us reduce a complex random experiment to a simple variable that is easy to characterize.\n\n\n10.2.2 Discrete vs Continuous\nA discrete random variable has a sample space that consists of a countable set of values. \\(X\\) in our example above is a discrete random variable. Note that “countable” does not necessarily mean “finite”. For example, a random variable with a Poisson distribution (a topic for a later chapter) has a sample space of \\(\\{0,1,2,...\\}\\). This sample space is unbounded, but it is considered countably infinite, and thus the random variable would be considered discrete.\nA continuous random variable has a sample space that is a continuous interval. For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. \\(Y\\) is a continuous random variable because a person could measure 68.1 inches, 68.2 inches, or perhaps any value in between. Note that when we measure height, our precision is limited by our measuring device, so we are technically “discretizing” height. However, even in these cases, we typically consider height to be a continuous random variable.\nA mixed random variable is exactly what it sounds like. It has a sample space that is both discrete and continuous. How could such a thing occur? Consider an experiment where a person rolls a standard six-sided die. If it lands on anything other than one, the result of the die roll is recorded. If it lands on one, the person spins a wheel, and the angle in degrees of the resulting spin, divided by 360, is recorded. If our random variable \\(Z\\) is the number that is recorded in this experiment, the sample space of \\(Z\\) is \\([0,1] \\cup \\{2,3,4,5,6\\}\\). We will not be spending much time on mixed random variables. However they do occur in practice, consider the job of analyzing bomb error data. If the bomb hits within a certain radius, the error is 0. Otherwise it is measured in a radial direction. This data is mixed.\n\n\n10.2.3 Discrete distribution functions\nOnce we have defined a random variable, we need a way to describe its behavior and we will use probabilities for this purpose.\nDistribution functions describe the behavior of random variables. We can use these functions to determine the probability that a random variable takes a value or a range of values. For discrete random variables, there are two distribution functions of interest: the probability mass function (pmf) and the cumulative distribution function (cdf).\n\n\n10.2.4 Probability mass function\nLet \\(X\\) be a discrete random variable. The probability mass function (pmf) of \\(X\\), given by \\(f_X(x)\\), is a function that assigns probability to each possible outcome of \\(X\\). \\[\nf_X(x)=\\mbox{P}(X=x)\n\\]\nNote that the pmf is a function. Functions have input and output. The input of a pmf is any real number. The output of a pmf is the probability that the random variable takes the inputted value. The pmf must follow the axioms of probability described in the Probability Rules chapter. Primarily,\n\nFor all \\(x \\in \\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\).\n\\(\\sum_x f_X(x) = 1\\), where the \\(x\\) in the index of the sum simply denotes that we are summing across the entire domain or support of \\(X\\).\n\n\nExample:\nRecall our example again. You flip a coin three times and let \\(X\\) be the number of heads in those three coin flips. We know that \\(X\\) can only take values 0, 1, 2 or 3. But at what probability does it take these three values? In that example, we had listed out the possible outcomes of the experiment and denoted what value of \\(X\\) corresponds to each outcome. \\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]\n\nEach of these eight outcomes is equally likely (each with a probability of \\(\\frac{1}{8}\\)). Thus, building the pmf of \\(X\\) becomes a matter of counting the number of outcomes associated with each possible value of \\(X\\): \\[\nf_X(x)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{1}{8}, & x=0 \\\\\n\\frac{3}{8}, & x=1 \\\\\n\\frac{3}{8}, & x=2 \\\\\n\\frac{1}{8}, & x=3 \\\\\n0, & \\mbox{otherwise} \\end{array} \\right .\n\\]\nNote that this function specifies the probability that \\(X\\) takes any of the four values in the sample space (0, 1, 2, and 3). Also, it specifies that the probability that \\(X\\) takes any other value is 0.\nGraphically, the pmf is not terribly interesting. The pmf is 0 at all values of \\(X\\) except for 0, 1, 2 and 3, Figure 10.1 .\n\n\n\n\n\n\n\n\nFigure 10.1: Probability Mass Function of \\(X\\) from Coin Flip Example\n\n\n\n\n\n\nExample:\nWe can use a pmf to answer questions about an experiment. For example, consider the same context. What is the probability that we flip at least one heads? We can write this in the context of \\(X\\): \\[\n\\mbox{P}(\\mbox{at least one heads})=\\mbox{P}(X\\geq 1)=\\mbox{P}(X=1)+\\mbox{P}(X=2)+\\mbox{P}(X=3)\n\\] \\[=\\frac{3}{8} + \\frac{3}{8}+\\frac{1}{8}=\\frac{7}{8}\n\\]\n\nAlternatively, we can recognize that \\(\\mbox{P}(X\\geq 1)=1-\\mbox{P}(X=0)=1-\\frac{1}{8}=\\frac{7}{8}\\).\n\n\n10.2.5 Cumulative distribution function\nLet \\(X\\) be a discrete random variable. The cumulative distribution function (cdf) of \\(X\\), given by \\(F_X(x)\\), is a function that assigns to each value of \\(X\\) the probability that \\(X\\) takes that value or lower: \\[\nF_X(x)=\\mbox{P}(X\\leq x)\n\\]\nAgain, note that the cdf is a function with an input and output. The input of a cdf is any real number. The output of a cdf is the probability that the random variable takes the inputted value or less.\nIf we know the pmf, we can obtain the cdf: \\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\sum_{y\\leq x} f_X(y)\n\\]\nLike the pmf, the cdf must be between 0 and 1. Also, since the pmf is always non-negative, the cdf must be non-decreasing.\n\nExample:\nObtain and plot the cdf of \\(X\\) of the previous example. \\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, & x &lt;0 \\\\\n\\frac{1}{8}, & 0\\leq x &lt; 1 \\\\\n\\frac{4}{8}, & 1\\leq x &lt; 2 \\\\\n\\frac{7}{8}, & 2\\leq x &lt; 3 \\\\\n1, & x\\geq 3 \\end{array}\\right .\n\\]\n\nVisually, the cdf of a discrete random variable has a stairstep appearance. In this example, the cdf takes a value 0 up until \\(X=0\\), at which point the cdf increases to 1/8. It stays at this value until \\(X=1\\), and so on. At and beyond \\(X=3\\), the cdf is equal to 1, Figure 10.2 .\n\n\n\n\n\n\n\n\nFigure 10.2: Cumulative Distribution Function of \\(X\\) from Coin Flip Example\n\n\n\n\n\n\n\n10.2.6 Simulating random variables\nWe can simulate values from a random variable using the cdf, we will use a similar idea for continuous random variables. Since the range of the cdf is in the interval \\([0,1]\\) we will generate a random number in that same interval and then use the inverse function to find the value of the random variable. The pseudo code is:\n1) Generate a random number, \\(U\\).\n2) Find the index \\(k\\) such that \\(\\sum_{j=1}^{k-1}f_X(x_{j}) \\leq U &lt; \\sum_{j=1}^{k}f_X(x_{j})\\) or \\(F_x(k-1) \\leq U &lt; F_{x}(k)\\).\n\nExample:\nSimulate a random variable for the number of heads in flipping a coin three times.\n\nFirst we will create the pmf.\n\npmf &lt;- c(1/8,3/8,3/8,1/8)\nvalues &lt;- c(0,1,2,3)\npmf\n\n[1] 0.125 0.375 0.375 0.125\n\n\nWe get the cdf from the cumulative sum.\n\ncdf &lt;- cumsum(pmf)\ncdf\n\n[1] 0.125 0.500 0.875 1.000\n\n\nNext, we will generate a random number between 0 and 1.\n\nset.seed(1153)\nran_num &lt;- runif(1)\nran_num\n\n[1] 0.7381891\n\n\nFinally, we will find the value of the random variable. We will do each step separately first so you can understand the code.\n\nran_num &lt; cdf\n\n[1] FALSE FALSE  TRUE  TRUE\n\n\n\nwhich(ran_num &lt; cdf)\n\n[1] 3 4\n\n\n\nwhich(ran_num &lt; cdf)[1]\n\n[1] 3\n\n\n\nvalues[which(ran_num &lt; cdf)[1]]\n\n[1] 2\n\n\nLet’s make this a function.\n\nsimple_rv &lt;- function(values,cdf){\nran_num &lt;- runif(1)\nreturn(values[which(ran_num &lt; cdf)[1]])\n}\n\nNow let’s generate 10000 values from this random variable.\n\nresults &lt;- do(10000)*simple_rv(values,cdf)\ninspect(results)\n\n\nquantitative variables:  \n       name   class min Q1 median Q3 max   mean       sd     n missing\n1 simple_rv numeric   0  1      2  2   3 1.5048 0.860727 10000       0\n\n\n\ntally(~simple_rv,data=results,format=\"proportion\")\n\nsimple_rv\n     0      1      2      3 \n0.1207 0.3785 0.3761 0.1247 \n\n\nNot a bad approximation.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html#moments",
    "href": "10-Discrete-Random-Variables.html#moments",
    "title": "10  Random Variables",
    "section": "10.3 Moments",
    "text": "10.3 Moments\nDistribution functions are excellent characterizations of random variables. The pmf and cdf will tell you exactly how often the random variables takes particular values. However, distribution functions are often a lot of information. Sometimes, we may want to describe a random variable \\(X\\) with a single value or small set of values. For example, we may want to know the average or some measure of center of \\(X\\). We also may want to know a measure of spread of \\(X\\). Moments are values that summarize random variables with single numbers. Since we are dealing with the population, these moments are population values and not summary statistics as we used in the first block of material.\n\n10.3.1 Expectation\nAt this point, we should define the term expectation. Let \\(g(X)\\) be some function of a discrete random variable \\(X\\). The expected value of \\(g(X)\\) is given by: \\[\n\\mbox{E}(g(X))=\\sum_x g(x) \\cdot f_X(x)\n\\]\n\n\n10.3.2 Mean\nThe most common moments used to describe random variables are mean and variance. The mean (often referred to as the expected value of \\(X\\)), is simply the average value of a random variable. It is denoted as \\(\\mu_X\\) or \\(\\mbox{E}(X)\\). In the discrete case, the mean is found by: \\[\n\\mu_X=\\mbox{E}(X)=\\sum_x x \\cdot f_X(x)\n\\]\nThe mean is also known as the first moment of \\(X\\) around the origin. It is a weighted sum with the weight being the probability. If each outcome were equally likely, the expected value would just be the average of the values of the random variable since each weight is the reciprocal of the number of values.\n\nExample:\nFind the expected value (or mean) of \\(X\\): the number of heads in three flips of a fair coin. \\[\n\\mbox{E}(X)=\\sum_x x\\cdot f_X(x) = 0*\\frac{1}{8} + 1*\\frac{3}{8} + 2*\\frac{3}{8} + 3*\\frac{1}{8}=1.5\n\\]\n\nWe are using \\(\\mu\\) because it is a population parameter.\nFrom our simulation above, we can find the mean as an estimate of the expected value. This is really a statistic since our simulation is data from the population and thus will have variance from sample to sample.\n\nmean(~simple_rv,data=results)\n\n[1] 1.5048\n\n\n\n\n10.3.3 Variance\nVariance is a measure of spread of a random variable. The variance of \\(X\\) is denoted as \\(\\sigma^2_X\\) or \\(\\mbox{Var}(X)\\). It is equivalent to the average squared deviation from the mean: \\[\n\\sigma^2_X=\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\n\\]\nIn the discrete case, this can be evaluated by: \\[\n\\mbox{E}[(X-\\mu_X)^2]=\\sum_x (x-\\mu_X)^2f_X(x)\n\\]\nVariance is also known as the second moment of \\(X\\) around the mean.\nThe square root of \\(\\mbox{Var}(X)\\) is denoted as \\(\\sigma_X\\), the standard deviation of \\(X\\). The standard deviation is often reported because it is measured in the same units as \\(X\\), while the variance is measured in squared units and is thus harder to interpret.\n\nExample:\nFind the variance of \\(X\\): the number of heads in three flips of a fair coin.\n\n\\[\n\\mbox{Var}(X)=\\sum_x (x-\\mu_X)^2 \\cdot f_X(x)\n\\]\n\\[\n= (0-1.5)^2 \\times \\frac{1}{8} + (1-1.5)^2 \\times \\frac{3}{8}+(2-1.5)^2 \\times \\frac{3}{8} + (3-1.5)^2\\times \\frac{1}{8}\n\\] In R this is:\n\n(0-1.5)^2*1/8 + (1-1.5)^2*3/8 + (2-1.5)^2*3/8 + (3-1.5)^2*1/8\n\n[1] 0.75\n\n\nThe variance of \\(X\\) is 0.75.\nWe can find the variance of the simulation but R uses the sample variance and this is the population variance. So we need to multiply by \\(\\frac{n-1}{n}\\)\n\nvar(~simple_rv,data=results)*(10000-1)/10000\n\n[1] 0.740777\n\n\n\n\n10.3.4 Mean and variance of Linear Transformations\nLemma: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Then: \\[\n\\mbox{E}(aX+b)=a\\mbox{E}(X)+b\n\\] and \\[\n\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\n\\]\nThe proof of this is left as a homework problem.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html#homework-problems",
    "href": "10-Discrete-Random-Variables.html#homework-problems",
    "title": "10  Random Variables",
    "section": "10.4 Homework Problems",
    "text": "10.4 Homework Problems\n\nSuppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads.\n\n\n\nIs \\(X\\) discrete or continuous? What is the domain, support, of \\(X\\)?\n\nWhat values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Don’t actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads.\n\nAdvanced: In R, generate 10,000 observations from \\(X\\). What is the empirical, from the simulation, pmf? What is the average value of \\(X\\) based on this simulation? Create a bar chart of the proportions. Note: Unlike the example in the Notes, we don’t have the pmf, so you will have to simulate the experiment and using R to find the number of flips until the first heads.\n\nNote: There are many ways to do this. Below is a description of one approach. It assumes we are extremely unlikely to go past 1000 flips.\n\nFirst, let’s sample with replacement from the vector c(“H”,“T”), 1000 times with replacement, use sample().\nAs we did in the reading, use which() and a logical argument to find the first occurrence of a heads.\n\n\nFind the theoretical distribution, use math to come up with a closed for solution for the pmf.\n\n \n\nRepeat Problem 1,except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads.\n\n \n\n\nSuppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the “timeliness” at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task.\n\nPrior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables,\n\nDetermine whether it is discrete or continuous.\n\nReport its domain.\n\nWhat is the experimental unit?\n\nExplain how this random variable will be useful in addressing timeliness at the airport.\n\nWe will provide one example:\nLet \\(D\\) be the difference between a flight’s actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a “very late” departure.\n\nConsider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice.\n\n\n\nWhat is the domain/support of \\(Y\\)?\n\nWhat values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Don’t actually do any formal math, just think about the experiment.\n\nFind the probability mass function and cumulative distribution function of \\(Y\\).\n\nFind the expected value and variance of \\(Y\\).\n\nAdvanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d?\n\n \n\nProve the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\).\n\n \n\nWe saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "10-Discrete-Random-Variables.html#solutions-manual",
    "href": "10-Discrete-Random-Variables.html#solutions-manual",
    "title": "10  Random Variables",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html",
    "href": "11-Continuous-Random-Variables.html",
    "title": "11  Continuous Random Variables",
    "section": "",
    "text": "11.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html#objectives",
    "href": "11-Continuous-Random-Variables.html#objectives",
    "title": "11  Continuous Random Variables",
    "section": "",
    "text": "Define and properly use in context all new terminology, to include: probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables.\nGiven a continuous random variable, find probabilities using the pdf and/or the cdf.\nFind the mean and variance of a continuous random variable.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html#continuous-random-variables",
    "href": "11-Continuous-Random-Variables.html#continuous-random-variables",
    "title": "11  Continuous Random Variables",
    "section": "11.2 Continuous random variables",
    "text": "11.2 Continuous random variables\nIn the last chapter, we introduced random variables, and explored discrete random variables. In this chapter, we will move into continuous random variables, their properties, their distribution functions, and how they differ from discrete random variables.\nRecall that a continuous random variable has a domain that is a continuous interval (or possibly a group of intervals). For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. While our measurement will necessitate “discretizing” height to some degree, technically, height is a continuous random variable since a person could measure 67.3 inches or 67.4 inches or anything in between.\n\n11.2.1 Continuous distribution functions\nSo how do we describe the randomness of continuous random variables? In the case of discrete random variables, the probability mass function (pmf) and the cumulative distribution function (cdf) are used to describe randomness. However, recall that the pmf is a function that returns the probability that the random variable takes the inputted value. Due to the nature of continuous random variables, the probability that a continuous random variable takes on any one individual value is technically 0. Thus, a pmf cannot apply to a continuous random variable.\nRather, we describe the randomness of continuous random variables with the probability density function (pdf) and the cumulative distribution function (cdf). Note that the cdf has the same interpretation and application as in the discrete case.\n\n\n11.2.2 Probability density function\nLet \\(X\\) be a continuous random variable. The probability density function (pdf) of \\(X\\), given by \\(f_X(x)\\) is a function that describes the behavior of \\(X\\). It is important to note that in the continuous case, \\(f_X(x)\\neq \\mbox{P}(X=x)\\), as the probability of \\(X\\) taking any one individual value is 0.\nThe pdf is a function. The input of a pdf is any real number. The output is known as the density. The pdf has three main properties:\n\n\\(f_X(x)\\geq 0\\)\n\\(\\int_{S_X} f_X(x)\\mbox{d}x = 1\\)\n\\(\\mbox{P}(X\\in A)=\\int_{x\\in A} f_X(x)\\mbox{d}x\\) or another way to write this \\(\\mbox{P}(a \\leq X \\leq b)=\\int_{a}^{b} f_X(x)\\mbox{d}x\\)\n\nProperties 2) and 3) imply that the area underneath a pdf represents probability. The pdf is a non-negative function, it cannot have negative values.\n\n\n11.2.3 Cumulative distribution function\nThe cumulative distribution function (cdf) of a continuous random variable has the same interpretation as it does for a discrete random variable. It is a function. The input of a cdf is any real number, and the output is the probability that the random variable takes a value less than or equal to the inputted value. It is denoted as \\(F\\) and is given by: \\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\int_{-\\infty}^x f_x(t) \\mbox{d}t\n\\]\n\nExample:\nLet \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\). Verify that \\(f\\) is a valid pdf. Find the cdf of \\(X\\). Also, find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\), \\(\\mbox{P}(X&gt;0.5)\\), and \\(\\mbox{P}(0.1\\leq X &lt; 0.75)\\). Finally, find the median of \\(X\\).\n\nTo verify that \\(f\\) is a valid pdf, we simply note that \\(f_X(x) \\geq 0\\) on the range \\(0 \\leq x \\leq 1\\). Also, we note that \\(\\int_0^1 2x \\mbox{d}x = x^2\\bigg|_0^1 = 1\\).\nUsing R, we find\n\nintegrate(function(x)2*x,0,1)\n\n1 with absolute error &lt; 1.1e-14\n\n\nOr we can use the mosaicCalc package to find the anti-derivative. If the package is not installed, you can use the Packages tab in RStudio or type install.packages(\"mosaicCalc\") at the command prompt. Load the library.\n\nlibrary(mosaicCalc)\n\n\n(Fx&lt;-antiD(2*x~x))\n\nfunction (x, C = 0) \nx^2 + C\n\n\n\nFx(1)-Fx(0)\n\n[1] 1\n\n\nGraphically, the pdf is displayed in Figure 11.1:\n\n\n\n\n\n\n\n\nFigure 11.1: pdf of \\(X\\)\n\n\n\n\n\n\nThe cdf of \\(X\\) is found by \\[\n\\int_0^x 2t \\mbox{d}t = t^2\\bigg|_0^x = x^2\n\\] This is antiD found from the calculations above.\nSo, \\[\nF_X(x)=\\left\\{ \\begin{array}{ll} 0, & x&lt;0 \\\\ x^2, & 0\\leq x \\leq 1 \\\\ 1, & x&gt;1 \\end{array}\\right.\n\\]\nThe plot of the cdf of \\(X\\) is shown in Figure 11.2.\n\n\n\n\n\n\n\n\nFigure 11.2: CDF of \\(X\\)\n\n\n\n\n\nProbabilities are found either by integrating the pdf or using the cdf:\n\\(\\mbox{P}(X &lt; 0.5)=\\mbox{P}(X\\leq 0.5)=F_X(0.5)=0.5^2=0.25\\).\nSee Figure 11.3.\n\n\n\n\n\n\n\n\nFigure 11.3: Probability represented by shaded area\n\n\n\n\n\n\\(\\mbox{P}(X &gt; 0.5) = 1-\\mbox{P}(X\\leq 0.5)=1-0.25 = 0.75\\).\nSee Figure 11.4.\n\n\n\n\n\n\n\n\nFigure 11.4: Probability represented by shaded area\n\n\n\n\n\n\\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\int_{0.1}^{0.75}2x\\mbox{d}x = 0.75^2 - 0.1^2 = 0.5525\\).\nSee Figure 11.5.\n\nintegrate(function(x)2*x,.1,.75)\n\n0.5525 with absolute error &lt; 6.1e-15\n\n\nAlternatively, \\(\\mbox{P}(0.1 \\leq X &lt; 0.75) = \\mbox{P}(X &lt; 0.75) -\\mbox{P}(x \\leq 0.1)\\)\n\\(= F(0.75)-F(0.1)=0.75^2-0.1^2 =0.5525\\)\n\nFx(0.75)-Fx(0.1)\n\n[1] 0.5525\n\n\nNotice for a continuous random variable, we are loose with the use of the = sign. This is because for a continuous random variable \\(\\mbox{P}(X=x)=0\\). Do not get sloppy when working with discrete random variables.\n\n\n\n\n\n\n\n\nFigure 11.5: Probability represented by shaded area\n\n\n\n\n\nThe median of \\(X\\) is the value \\(x\\) such that \\(\\mbox{P}(X\\leq x)=0.5\\), the area under a single point is 0. So we simply solve \\(x^2=0.5\\) for \\(x\\). Thus, the median of \\(X\\) is \\(\\sqrt{0.5}=0.707\\).\nOr using R\n\nuniroot(function(x)(Fx(x)-.5),c(0,1))$root\n\n[1] 0.7071067\n\n\n\n\n11.2.4 Simulation\nAs in the case of the discrete random variable, we can simulate a continuous random variable if we have an inverse for the cdf. The range of the cdf is \\([0,1]\\), so we generate a random number in this interval and then apply the inverse cdf to obtain a random variable. In a similar manner, for a continuous random variable, we use the following pseudo code:\n1. Generate a random number in the interval \\([0,1]\\), \\(U\\).\n2. Find the random variable \\(X\\) from \\(F_{X}^{-1}(U)\\).\nIn R for our example, this looks like the following.\n\nsqrt(runif(1))\n\n[1] 0.6137365\n\n\n\nresults &lt;- do(10000)*sqrt(runif(1))\n\n\n\ninspect(results)\n\n\nquantitative variables:  \n  name   class         min        Q1    median        Q3       max      mean\n1 sqrt numeric 0.005321359 0.4977011 0.7084257 0.8656665 0.9999873 0.6669452\n         sd     n missing\n1 0.2358056 10000       0\n\n\n\nFigure 11.6 is a density plot of the simulated density function.\n\nresults %&gt;%\n  gf_density(~sqrt,xlab=\"X\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"\")\n\n\n\n\n\n\n\nFigure 11.6: Density plot of the simulated random variable.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html#moments",
    "href": "11-Continuous-Random-Variables.html#moments",
    "title": "11  Continuous Random Variables",
    "section": "11.3 Moments",
    "text": "11.3 Moments\nAs with discrete random variables, moments can be calculated to summarize characteristics such as center and spread. In the discrete case, expectation is found by multiplying each possible value by its associated probability and summing across the domain (\\(\\mbox{E}(X)=\\sum_x x\\cdot f_X(x)\\)). In the continuous case, the domain of \\(X\\) consists of an infinite set of values. From your calculus days, recall that the sum across an infinite domain is represented by an integral.\nLet \\(g(X)\\) be any function of \\(X\\). The expectation of \\(g(X)\\) is found by: \\[\n\\mbox{E}(g(X)) = \\int_{S_X} g(x)f_X(x)\\mbox{d}x\n\\]\n\n11.3.1 Mean and variance\nLet \\(X\\) be a continuous random variable. The mean of \\(X\\), or \\(\\mu_X\\), is simply \\(\\mbox{E}(X)\\). Thus, \\[\n\\mbox{E}(X)=\\int_{S_X}x\\cdot f_X(x)\\mbox{d}x\n\\]\nAs in the discrete case, the variance of \\(X\\) is the expected squared difference from the mean, or \\(\\mbox{E}[(X-\\mu_X)^2]\\). Thus, \\[\n\\sigma^2_X = \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]= \\int_{S_X} (x-\\mu_X)^2\\cdot f_X(x) \\mbox{d}x\n\\]\nRecall homework problem 6 from the last chapter. In this problem, you showed that \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\). Thus, \\[\n\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_{S_X} x^2\\cdot f_X(x)\\mbox{d}x - \\mu_X^2\n\\]\n\nExample:\nConsider the random variable \\(X\\) from above. Find the mean and variance of \\(X\\). \\[\n\\mu_X= \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mbox{d}x = \\frac{2x^3}{3}\\bigg|_0^1 = \\frac{2}{3}=0.667\n\\]\n\nSide note: Since the mean of \\(X\\) is smaller than the median of \\(X\\), we say that \\(X\\) is skewed to the left, or negatively skewed.\nUsing R.\n\nintegrate(function(x)x*2*x,0,1)\n\n0.6666667 with absolute error &lt; 7.4e-15\n\n\nOr using antiD()\n\nEx&lt;-antiD(2*x^2~x)\nEx(1)-Ex(0)\n\n[1] 0.6666667\n\n\nUsing our simulation.\n\nmean(~sqrt,data=results)\n\n[1] 0.6669452\n\n\n\\[\n\\sigma^2_X = \\mbox{Var}(X)= \\mbox{E}(X^2)-\\mbox{E}(X)^2\n\\] \\[= \\int_0^1 x^2\\cdot 2x\\mbox{d}x - \\left(\\frac{2}{3}\\right)^2 = \\frac{2x^4}{4}\\bigg|_0^1-\\frac{4}{9}=\\frac{1}{2}-\\frac{4}{9}=\\frac{1}{18}=0.056\n\\]\n\nintegrate(function(x)x^2*2*x,0,1)$value-(2/3)^2\n\n[1] 0.05555556\n\n\nor\n\nVx&lt;-antiD(x^2*2*x~x)\nVx(1)-Vx(0)-(2/3)^2\n\n[1] 0.05555556\n\n\n\nvar(~sqrt,data=results)*9999/10000\n\n[1] 0.05559873\n\n\nAnd finally, the standard deviation of \\(X\\) is \\(\\sigma_X = \\sqrt{\\sigma^2_X}=\\sqrt{1/18}=0.236\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html#homework-problems",
    "href": "11-Continuous-Random-Variables.html#homework-problems",
    "title": "11  Continuous Random Variables",
    "section": "11.4 Homework Problems",
    "text": "11.4 Homework Problems\n\nLet \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\).\n\n\n\nAssume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\).\n\nPlot the pdf of \\(X\\).\n\nFind and plot the cdf of \\(X\\).\n\nFind \\(\\mbox{P}(X&lt;1)\\).\n\nFind \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\).\n\nFind the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value).\n\nFind the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\).\n\nFind the mean and variance of \\(X\\).\n\nSimulate 10000 values from this distribution and plot the density.\n\n\n\nLet \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).)",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "11-Continuous-Random-Variables.html#solutions-manual",
    "href": "11-Continuous-Random-Variables.html#solutions-manual",
    "title": "11  Continuous Random Variables",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "12-Named-Discrete-Distributions.html",
    "href": "12-Named-Discrete-Distributions.html",
    "title": "12  Named Discrete Distributions",
    "section": "",
    "text": "12.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "12-Named-Discrete-Distributions.html#objectives",
    "href": "12-Named-Discrete-Distributions.html#objectives",
    "title": "12  Named Discrete Distributions",
    "section": "",
    "text": "Recognize and set up for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments.\nUse R to calculate probabilities and quantiles involving random variables with common discrete distributions.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "12-Named-Discrete-Distributions.html#named-distributions",
    "href": "12-Named-Discrete-Distributions.html#named-distributions",
    "title": "12  Named Discrete Distributions",
    "section": "12.2 Named distributions",
    "text": "12.2 Named distributions\nIn the previous two chapters, we introduced the concept of random variables, distribution functions, and expectations. In some cases, the nature of an experiment may yield random variables with common distributions. In these cases, we can rely on easy-to-use distribution functions and built-in R functions in order to calculate probabilities and quantiles.\n\n12.2.1 Discrete uniform distribution\nThe first distribution we will discuss is the discrete uniform distribution. It is not a very commonly used distribution, especially compared to its continuous counterpart. A discrete random variable has the discrete uniform distribution if probability is evenly allocated to each value in the sample space. A variable with this distribution has parameters \\(a\\) and \\(b\\) representing the minimum and maximum of the sample space, respectively. (By default, that sample space is assumed to consist of integers only, but that is by no means always the case.)\n\nExample:\nRolling a fair die is an example of the discrete uniform. Each side of the die has an equal probability.\n\nLet \\(X\\) be a discrete random variable with the uniform distribution. If the sample space is consecutive integers, this distribution is denoted as \\(X\\sim\\textsf{DUnif}(a,b)\\). The pmf of \\(X\\) is given by: \\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{b-a+1}, & x \\in \\{a, a+1,...,b\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nFor the die:\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6-1+1} = \\frac{1}{6}, & x \\in \\{1, 2,...,6\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nThe expected value of \\(X\\) is found by: \\[\n\\mbox{E}(X)=\\sum_{x=a}^b x\\cdot\\frac{1}{b-a+1}= \\frac{1}{b-a+1} \\cdot \\sum_{x=a}^b x=\\frac{1}{b-a+1}\\cdot\\frac{b-a+1}{2}\\cdot (a+b) = \\frac{a+b}{2}\n\\]\nWhere the sum of consecutive integers is a common result from discrete math, research it for more information.\nThe variance of \\(X\\) is found by: (derivation not included) \\[\n\\mbox{Var}(X)=\\mbox{E}[(X-\\mbox{E}(X))^2]=\\frac{(b-a+1)^2-1}{12}\n\\]\nSummarizing for the die:\nLet \\(X\\) be the result of a single roll of a fair die. We will report the distribution of \\(X\\), the pmf, \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\).\nThe sample space of \\(X\\) is \\(S_X=\\{1,2,3,4,5,6\\}\\). Since each of those outcomes is equally likely, \\(X\\) follows the discrete uniform distribution with \\(a=1\\) and \\(b=6\\). Thus, \\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6}, & x \\in \\{1,2,3,4,5,6\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nFinally, \\(\\mbox{E}(X)=\\frac{1+6}{2}=3.5\\). Also, \\(\\mbox{Var}(X)=\\frac{(6-1+1)^2-1}{12}=\\frac{35}{12}=2.917\\).\n\n\n12.2.2 Simulating\nTo simulate the discrete uniform, we use sample().\n\nExample:\nTo simulate rolling a die 4 times, we use sample().\n\n\nset.seed(61)\nsample(1:6,4,replace=TRUE)\n\n[1] 4 2 2 1\n\n\nLet’s roll it 10,000 times and find\n\nresults&lt;-do(10000)*sample(1:6,1,replace=TRUE)\n\n\ntally(~sample,data=results,format=\"percent\")\n\nsample\n    1     2     3     4     5     6 \n16.40 16.46 16.83 17.15 16.92 16.24 \n\n\n\nmean(~sample,data=results)\n\n[1] 3.5045\n\n\n\nvar(~sample,data=results)*(10000-1)/10000\n\n[1] 2.87598\n\n\nAgain as a reminder, we multiply by \\(\\frac{(10000-1)}{10000}\\) because the function var() is calculating a sample variance using \\(n-1\\) in the denominator but we need the population variance.\n\n\n12.2.3 Binomial distribution\nThe binomial distribution is extremely common, and appears in many situations. In fact, we have already discussed several examples where the binomial distribution is heavily involved.\nConsider an experiment involving repeated independent trials of a binary process (two outcomes), where in each trial, there is a constant probability of “success” (one of the outcomes which is arbitrary). If the random variable \\(X\\) represents the number of successes out of \\(n\\) independent trials, then \\(X\\) is said to follow the binomial distribution with parameters \\(n\\) and \\(p\\) (the probability of a success in each trial).\nThe pmf of \\(X\\) is given by: \\[\nf_X(x)=\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}\n\\]\nfor \\(x \\in \\{0,1,...,n\\}\\) and 0 otherwise.\nLet’s take a moment to dissect this pmf. We are looking for the probability of obtaining \\(x\\) successes out of \\(n\\) trials. The \\(p^x\\) represents the probability of \\(x\\) successes, using the multiplication rule because of the independence assumption. The term \\((1-p)^{n-x}\\) represents the probability of the remainder of the trials as failures. Finally, the \\(n\\choose x\\) term represents the number of ways to obtain \\(x\\) successes out of \\(n\\) trials. For example, there are three ways to obtain 1 success out of 3 trials (one success followed by two failures; one failure, one success and then one failure; or two failures followed by a success).\nThe expected value of a binomially distributed random variable is given by \\(\\mbox{E}(X)=np\\) and the variance is given by \\(\\mbox{Var}(X)=np(1-p)\\).\n\nExample:\nLet \\(X\\) be the number of heads out of 20 independent flips of a fair coin. Note that this is a binomial because the trials are independent and the probability of success, in this case a heads, is constant, and there are two outcomes. Find the distribution, mean and variance of \\(X\\). Find \\(\\mbox{P}(X=8)\\). Find \\(\\mbox{P}(X\\leq 8)\\).\n\n\\(X\\) has the binomial distribution with \\(n=20\\) and \\(p=0.5\\). The pmf is given by: \\[\nf_X(x)=\\mbox{P}(X=x)={20 \\choose x}0.5^x (1-0.5)^{20-x}\n\\]\nAlso, \\(\\mbox{E}(X)=20*0.5=10\\) and \\(\\mbox{Var}(X)=20*0.5*0.5=5\\).\nTo find \\(\\mbox{P}(X=8)\\), we can simply use the pmf: \\[\n\\mbox{P}(X=8)=f_X(8)={20\\choose 8}0.5^8 (1-0.5)^{12}\n\\]\n\nchoose(20,8)*0.5^8*(1-0.5)^12\n\n[1] 0.1201344\n\n\nTo find \\(\\mbox{P}(X\\leq 8)\\), we would need to find the cumulative probability: \\[\n\\mbox{P}(X\\leq 8)=\\sum_{x=0}^8 {20\\choose 8}0.5^x (1-0.5)^{20-x}\n\\]\n\nx&lt;-0:8\nsum(choose(20,x)*0.5^x*(1-.5)^(20-x))\n\n[1] 0.2517223\n\n\n\n\n12.2.4 Software Functions\nOne of the advantages of using named distributions is that most software packages have built-in functions that compute probabilities and quantiles for common named distributions. Over the course of this chapter, you will notice that each named distribution is treated similarly in R. There are four main functions tied to each distribution. For the binomial distribution, these are dbinom(), pbinom(), qbinom(), and rbinom().\ndbinom(): This function is equivalent to the probability mass function. We use this to find \\(\\mbox{P}(X=x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes three inputs: x (the value of the random variable), size (the number of trials, \\(n\\)), and prob (the probability of success, \\(p\\)). So, \\[\n\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}=\\textsf{dbinom(x,n,p)}\n\\]\npbinom(): This function is equivalent to the cumulative distribution function. We use this to find \\(\\mbox{P}(X\\leq x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes the same inputs as dbinom() but returns the cumulative probability: \\[\n\\mbox{P}(X\\leq x)=\\sum_{k=0}^x{n\\choose{k}}p^k(1-p)^{n-k}=\\textsf{pbinom(x,n,p)}\n\\]\nqbinom(): This is the inverse of the cumulative distribution function and will return a percentile. This function has three inputs: p (a probability), size and prob. It returns the smallest value \\(x\\) such that \\(\\mbox{P}(X\\leq x) \\geq p\\).\nrbinom(): This function is used to randomly generate values from the binomial distribution. It takes three inputs: n (the number of values to generate), size and prob. It returns a vector containing the randomly generated values.\nTo learn more about these functions, type ? followed the function in the console.\n\nExercise:\nUse the built-in functions for the binomial distribution to plot the pmf of \\(X\\) from the previous example. Also, use the built-in functions to compute the probabilities from the example.\n\nFigure 12.1\n\ngf_dist(\"binom\",size=20,prob=.5) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n\n\n\n\n\n\n\nFigure 12.1: The pmf of a binomial random variable\n\n\n\n\n\n\n###P(X=8)\ndbinom(8,20,0.5)\n\n[1] 0.1201344\n\n###P(X&lt;=8)\npbinom(8,20,0.5)\n\n[1] 0.2517223\n\n## or \nsum(dbinom(0:8,20,0.5))\n\n[1] 0.2517223\n\n\n\n\n12.2.5 Poisson distribution\nThe Poisson distribution is very common when considering count or arrival data. Consider a random process where events occur according to some rate over time (think arrivals to a retail register). Often, these events are modeled with the Poisson process. The Poisson process assumes a consistent rate of arrival and a memoryless arrival process (the time until the next arrival is independent of time since the last arrival). If we assume a particular process is a Poisson process, then there are two random variables that take common named distributions. The number of arrivals in a specified amount of time follows the Poisson distribution. Also, the amount of time until the next arrival follows the exponential distribution. We will defer discussion of the exponential distribution until the next chapter. What is random in the Poisson is the number of occurrences while the interval is fixed. That is why it is a discrete distribution. The parameter \\(\\lambda\\) is the average number of occurrences in the specific interval, note that the interval must be the same as is specified in the random variable.\nLet \\(X\\) be the number of arrivals in a length of time, \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals in length of time \\(T\\). Then \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\): \\[\nX\\sim \\textsf{Poisson}(\\lambda)\n\\]\nThe pmf of \\(X\\) is given by: \\[\nf_X(x)=\\mbox{P}(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}, \\hspace{0.5cm} x=0,1,2,...\n\\]\nOne unique feature of the Poisson distribution is that \\(\\mbox{E}(X)=\\mbox{Var}(X)=\\lambda\\).\n\nExample:\nSuppose fleet vehicles arrive to a maintenance garage at an average rate of 0.4 per day. Let’s assume that these vehicles arrive according to a Poisson process. Let \\(X\\) be the number of vehicles that arrive to the garage in a week (7 days). Notice that the time interval has changed! What is the random variable \\(X\\)? What is the distribution (with parameter) of \\(X\\). What are \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\)? Find \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), and \\(\\mbox{P}(2 \\leq X \\leq 8)\\). Also, find the median of \\(X\\), and the 95th percentile of \\(X\\) (the value of \\(x\\) such that \\(\\mbox{P}(X\\leq x)\\geq 0.95\\)). Further, plot the pmf of \\(X\\).\n\nSince vehicles arrive according to a Poisson process, the probability question leads us to define the random variable \\(X\\) as The number of vehicles that arrive in a week.\nWe know that \\(X\\sim \\textsf{Poisson}(\\lambda=0.4*7=2.8)\\). Thus, \\(\\mbox{E}(X)=\\mbox{Var}(X)=2.8\\).\nThe parameter is the average number of vehicles that arrive in a week.\n\\[\n\\mbox{P}(X=0)=\\frac{2.8^0 e^{-2.8}}{0!}=e^{-2.8}=0.061\n\\]\nAlternatively, we can use the built-in R functions for the Poisson distribution:\n\n##P(X=0)\ndpois(0,2.8)\n\n[1] 0.06081006\n\n##P(X&lt;=6)\nppois(6,2.8)\n\n[1] 0.9755894\n\n## or\nsum(dpois(0:6,2.8))\n\n[1] 0.9755894\n\n##P(X&gt;=2)=1-P(X&lt;2)=1-P(X&lt;=1)\n1-ppois(1,2.8)\n\n[1] 0.7689218\n\n## or\nsum(dpois(2:1000,2.8))\n\n[1] 0.7689218\n\n\nNote that when considering \\(\\mbox{P}(X\\geq 2)\\), we recognize that this is equivalent to \\(1-\\mbox{P}(X\\leq 1)\\). We can use ppois() to find this probability.\nWhen considering \\(\\mbox{P}(2\\leq X \\leq 8)\\), we need to make sure we formulate this correctly. Below are two possible methods:\n\n##P(2 &lt;= X &lt;= 8) = P(X &lt;= 8)-P(X &lt;= 1)\nppois(8,2.8)-ppois(1,2.8)\n\n[1] 0.766489\n\n## or\nsum(dpois(2:8,2.8))\n\n[1] 0.766489\n\n\nTo find the median and the 95th percentiles, we use qpois:\n\nqpois(0.5,2.8)\n\n[1] 3\n\nqpois(0.95,2.8)\n\n[1] 6\n\n\nFigure 12.2 is a plot of the pmf of a Poisson random variable.\n\ngf_dist(\"pois\",lambda=2.8) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n\n\n\n\n\n\n\nFigure 12.2: The pmf of a Poisson random variable.\n\n\n\n\n\nFigure 12.3 is the cdf of the same Poisson random variable in Figure 12.2.\n\ngf_dist(\"pois\",lambda=2.8,kind=\"cdf\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"P(X&lt;=x)\")\n\n\n\n\n\n\n\nFigure 12.3: The cdf of the Poisson random variable in Figure 12.2\n\n\n\n\n\n\n\n12.2.6 Hypergeometric\nConsider an experiment where \\(k\\) objects are to be selected from a larger, but finite, group consisting of \\(m\\) “successes” and \\(n\\) “failures”. This is similar to the binomial process; after all, we are selecting successes and failures. However, in this case, the results are effectively selected without replacement. If the random variable \\(X\\) represents the number of successes selected in our sample of size \\(k\\), then \\(X\\) follows a hypergeometric distribution with parameters \\(m\\), \\(n\\), and \\(k\\). The pmf of \\(X\\) is given by:\n\\[\nf_X(x) = \\frac{{m \\choose{x}}{n \\choose{k-x}}}{{m+n \\choose{k}}}, \\qquad x = 0,1,...,m\n\\]\nAlso, \\(\\mbox{E}(X)=\\frac{km}{m+n}\\) and \\(\\mbox{Var}(X)=k\\frac{m}{m+n}\\frac{n}{m+n}\\frac{m+n-k}{m+n-1}\\)\nIf you draw on your knowledge of combinations, you can see why this pmf makes sense.\n\nExample:\nSuppose a bag contains 12 red chips and 8 black chips. I reach in blindly and randomly select 6 chips. What is the probability I select no black chips? All black chips? Between 2 and 5 black chips?\n\nFirst we should identify a random variable that will help us with this problem. Let \\(X\\) be the number of black chips selected when randomly selecting 6 from the bag. Then \\(X\\sim \\textsf{HyperGeom}(8,12,6)\\). We can use R to find these probabilities.\nFirst, the plot of the pmf of the hypergeometric is in #fig-hyper.\n\ngf_dist(\"hyper\",m=8,n=12,k=6) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n\n\n\n\n\n\n\nFigure 12.4: The pmf of a hypergeometric random variable.\n\n\n\n\n\n\n##P(X=0)\ndhyper(0,8,12,6)\n\n[1] 0.02383901\n\n##P(X=6)\ndhyper(6,8,12,6)\n\n[1] 0.0007223942\n\n##P(2 &lt;= X &lt;=5)\nsum(dhyper(2:5,8,12,6))\n\n[1] 0.8119711",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "12-Named-Discrete-Distributions.html#homework-problems",
    "href": "12-Named-Discrete-Distributions.html#homework-problems",
    "title": "12  Named Discrete Distributions",
    "section": "12.3 Homework Problems",
    "text": "12.3 Homework Problems\nFor each of the problems below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question.\nWe will demonstrate using 1a and 1b.\n\nThe T-6 training aircraft is used during UPT. Suppose that on each training sortie, aircraft return with a maintenance-related failure at a rate of 1 per 100 sorties.\n\n\n\nFind the probability of no maintenance failures in 15 sorties.\n\n\\(X\\): the number of maintenance failures in 15 sorties.\n\\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\)\n\\(\\mbox{E}(X)=15*0.01=0.15\\) and \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\).\n\\(\\mbox{P}(\\mbox{No mainteance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\)\n\n0.99^15\n\n[1] 0.8600584\n\n## or \ndbinom(0,15,0.01)\n\n[1] 0.8600584\n\n\nThis probability makes sense, since the expected value is fairly low. Because, on average, only 0.15 failures would occur every 15 trials, 0 failures would be a very common result. Graphically, the pmf looks like Figure 12.5.\n\ngf_dist(\"binom\",size=15,prob=0.01) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n\n\n\n\n\n\n\nFigure 12.5: The pmf for binomail in Homework Problem 1a.\n\n\n\n\n\n\nFind the probability of at least two maintenance failures in 15 sorties.\n\nWe can use the same \\(X\\) as above. Now, we are looking for \\(\\mbox{P}(X\\geq 2)\\). This is equivalent to finding \\(1-\\mbox{P}(X\\leq 1)\\):\n\n## Directly\n1-(0.99^15 + 15*0.01*0.99^14)\n\n[1] 0.009629773\n\n## or, using R\nsum(dbinom(2:15,15,0.01))\n\n[1] 0.009629773\n\n## or\n1-sum(dbinom(0:1,15,0.01))\n\n[1] 0.009629773\n\n## or\n1-pbinom(1,15,0.01)\n\n[1] 0.009629773\n\n## or \npbinom(1,15,0.01,lower.tail = F)\n\n[1] 0.009629773\n\n\n\nFind the probability of at least 30 successful (no mx failures) sorties before the first failure.\n\nFind the probability of at least 50 successful sorties before the third failure.\n\n\n\nOn a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour.\n\n\n\nFind the probability no vehicles arrive in 10 minutes.\n\nFind the probability at least 50 vehicles arrive in an hour.\n\nFind the probability that at least 5 minutes will pass before the next arrival.\n\n\n\nSuppose there are 12 male and 7 female cadets in a classroom. I select 5 completely at random (without replacement).\n\n\n\nFind the probability I select no female cadets.\n\nFind the probability I select more than 2 female cadets.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "12-Named-Discrete-Distributions.html#solutions-manual",
    "href": "12-Named-Discrete-Distributions.html#solutions-manual",
    "title": "12  Named Discrete Distributions",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Named Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "13-Named-Continuous-Distributions.html",
    "href": "13-Named-Continuous-Distributions.html",
    "title": "13  Named Continuous Distributions",
    "section": "",
    "text": "13.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "13-Named-Continuous-Distributions.html#objectives",
    "href": "13-Named-Continuous-Distributions.html#objectives",
    "title": "13  Named Continuous Distributions",
    "section": "",
    "text": "Recognize when to use common continuous distributions (Uniform, Exponential Normal, and Beta), identify parameters, and find moments.\nUse R to calculate probabilities and quantiles involving random variables with common continuous distributions.\nUnderstand the relationship between the Poisson process and the Poisson & Exponential distributions.\nKnow when to apply and then use the memory-less property.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "13-Named-Continuous-Distributions.html#continuous-distributions",
    "href": "13-Named-Continuous-Distributions.html#continuous-distributions",
    "title": "13  Named Continuous Distributions",
    "section": "13.2 Continuous distributions",
    "text": "13.2 Continuous distributions\nIn this chapter we will explore continuous distributions. This means we work with probability density functions and use them to find probabilities. Thus we must integrate, either numerically, graphically, or mathematically. The cumulative distribution function will also play an important role in this chapter.\nThere are many more distributions than the ones in this chapter but these are the most common and will set you up to learn and use any others in the future.\n\n13.2.1 Uniform distribution\nThe first continuous distribution we will discuss is the uniform distribution. By default, when we refer to the uniform distribution, we are referring to the continuous version. When referring to the discrete version, we use the full term “discrete uniform distribution.”\nA continuous random variable has the uniform distribution if probability density is constant, uniform. The parameters of this distribution are \\(a\\) and \\(b\\), representing the minimum and maximum of the sample space. This distribution is commonly denoted as \\(U(a,b)\\).\nLet \\(X\\) be a continuous random variable with the uniform distribution. This is denoted as \\(X\\sim \\textsf{Unif}(a,b)\\). The pdf of \\(X\\) is given by: \\[\nf_X(x)=\\left\\{\\begin{array}{ll} \\frac{1}{b-a}, & a\\leq x \\leq b \\\\ 0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nThe mean of \\(X\\) is \\(\\mbox{E}(X)=\\frac{a+b}{2}\\) and the variance is \\(\\mbox{Var}(X)=\\frac{(b-a)^2}{12}\\). The derivation of the mean is left to the exercises.\nThe most common uniform distribution is \\(U(0,1)\\) which we have already used several times in this book. Again, notice in Figure 13.1 that the plot of the pdf is a constant or uniform value.\n\n\n\n\n\n\n\n\nFigure 13.1: The pdf of Uniform random variable.\n\n\n\n\n\nTo check that it is a proper pdf, all values must be non-negative and the total probability must be 1. In R the function for probability density will start with the letter d and have some short descriptor for the distribution. For the uniform we use dunif().\n\nintegrate(function(x)dunif(x),0,1)\n\n1 with absolute error &lt; 1.1e-14\n\n\n\n\n13.2.2 Exponential distribution\nRecall from the chapter on named discrete distributions, we discussed the Poisson process. If arrivals follow a Poisson process, we know that the number of arrivals in a specified amount of time follows a Poisson distribution, and the time until the next arrival follows the exponential distribution. In the Poisson distribution, the number of arrivals is random and the interval is fixed. In the exponential distribution we change this, the interval is random and the arrivals are fixed at 1. This is a subtle point but worth the time to make sure you understand.\nLet \\(X\\) be the number of arrivals in a time interval \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals per unit time interval. From the previous chapter, we know that \\(X\\sim \\textsf{Poisson}(\\lambda T)\\). Now let \\(Y\\) be the time until the next arrival. Then \\(Y\\) follows the exponential distribution with parameter \\(\\lambda\\) which has units of inverse base time:\n\\[\nY \\sim \\textsf{Expon}(\\lambda)\n\\]\nNote on \\(\\lambda\\): One point of confusion involving the parameters of the Poisson and exponential distributions. The parameter of the Poisson distribution (usually denoted as \\(\\lambda\\)) represents the average number of arrivals in whatever amount of time specified by the random variable. In the case of the exponential distribution, the parameter (also denoted as \\(\\lambda\\)) represents the average number of arrivals per unit time. For example, suppose arrivals follow a Poisson process with an average of 10 arrivals per day. \\(X\\), the number of arrivals in 5 days, follows a Poisson distribution with parameter \\(\\lambda=50\\), since that is the average number of arrivals in the amount of time specified by \\(X\\). Meanwhile, \\(Y\\), the time in days until the next arrival, follows an exponential distribution with parameter \\(\\lambda=10\\) (the average number of arrivals per day).\nThe pdf of \\(Y\\) is given by:\n\\[\nf_Y(y)=\\lambda e^{-\\lambda y}, \\hspace{0.3cm} y&gt;0\n\\]\nThe mean and variance of \\(Y\\) are: \\(\\mbox{E}(Y)=\\frac{1}{\\lambda}\\) and \\(\\mbox{Var}(Y)=\\frac{1}{\\lambda^2}\\). You should be able to derive these results but they require integration by parts and can be lengthy algebraic exercises.\n\nExample:\nSuppose at a local retail store, customers arrive to a checkout counter according to a Poisson process with an average of one arrival every three minutes. Let \\(Y\\) be the time (in minutes) until the next customer arrives to the counter. What is the distribution (and parameter) of \\(Y\\)? What are \\(\\mbox{E}(Y)\\) and \\(\\mbox{Var}(Y)\\)? Find \\(\\mbox{P}(Y&gt;5)\\), \\(\\mbox{P}(Y\\leq 3)\\), and \\(\\mbox{P}(1 \\leq Y &lt; 5)\\)? Also, find the median and 95th percentile of \\(Y\\). Finally, plot the pdf of \\(Y\\).\n\nSince one arrival shows up every three minutes, the average number of arrivals per unit time is 1/3 arrival per minute. Thus, \\(Y\\sim \\textsf{Expon}(\\lambda=1/3)\\). This means that \\(\\mbox{E}(Y)=3\\) and \\(\\mbox{Var}(Y)=9\\).\nTo find \\(\\mbox{P}(Y&gt;5)\\), we could integrate the pdf of \\(Y\\):\n\\[\n\\mbox{P}(Y&gt;5)=\\int_5^\\infty \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y = \\lim_{a \\to +\\infty}\\int_5^a \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y =\n\\]\n\\[\n\\lim_{a \\to +\\infty} -e^{-\\frac{1}{3}y}\\bigg|_5^a=\\lim_{a \\to +\\infty} -e^{-\\frac{a}{3}}-(-e^{-\\frac{5}{3}})= 0 + 0.189 = 0.189\n\\]\nAlternatively, we could use R:\n\n##Prob(Y&gt;5)=1-Prob(Y&lt;=5)\n1-pexp(5,1/3)\n\n[1] 0.1888756\n\n\nOr using integrate()\n\nintegrate(function(x)1/3*exp(-1/3*x),5,Inf)\n\n0.1888756 with absolute error &lt; 8.5e-05\n\n\nFor the remaining probabilities, we will use R:\n\n##Prob(Y&lt;=3)\npexp(3,1/3)\n\n[1] 0.6321206\n\n##Prob(1&lt;=Y&lt;5)\npexp(5,1/3)-pexp(1,1/3)\n\n[1] 0.5276557\n\n\nThe median is \\(y\\) such that \\(\\mbox{P}(Y\\leq y)=0.5\\). We can find this by solving the following for \\(y\\): \\[\n\\int_0^y \\frac{1}{3}e^{-\\frac{1}{3}y}\\mbox{d} y = 0.5\n\\]\nAlternatively, we can use qexp in R:\n\n##median\nqexp(0.5,1/3)\n\n[1] 2.079442\n\n##95th percentile\nqexp(0.95,1/3)\n\n[1] 8.987197\n\n\n\n\n\n\n\n\n\n\nFigure 13.2: The pdf of exponential random variable \\(Y\\)\n\n\n\n\n\nBoth from Figure 13.2 and the mean and median, we know that the exponential distribution is skewed to the right.\n\n\n13.2.3 Memory-less property\nThe Poisson process is known for its memory-less property. Essentially, this means that the time until the next arrival is independent of the time since last arrival. Thus, the probability of an arrival within the next 5 minutes is the same regardless of whether an arrival just occurred or an arrival has not occurred for a long time.\nTo show this let’s consider random variable \\(Y\\) ( time until the next arrival in minutes) where \\(Y\\sim\\textsf{Expon}(\\lambda)\\). We will show that, given it has been at least \\(t\\) minutes since the last arrival, the probability we wait at least \\(y\\) additional minutes is equal to the marginal probability that we wait \\(y\\) additional minutes.\nFirst, note that the cdf of \\(Y\\), \\(F_Y(y)=\\mbox{P}(Y\\leq y)=1-e^{-\\lambda y}\\), you should be able to derive this. So, \\[\n\\mbox{P}(Y\\geq y+t|Y\\geq t) = \\frac{\\mbox{P}(Y\\geq y+t \\cap Y\\geq t)}{\\mbox{P}(Y\\geq t)}=\\frac{\\mbox{P}(Y\\geq y +t)}{\\mbox{P}(Y\\geq t)} = \\frac{1-(1-e^{-(y+t)\\lambda})}{1-(1-e^{-t\\lambda})}\n\\] \\[\n=\\frac{e^{-\\lambda y }e^{-\\lambda t}}{e^{-\\lambda t }}=e^{-\\lambda y} = 1-(1-e^{-\\lambda y})=\\mbox{P}(Y\\geq y).\n\\blacksquare\n\\]\nLet’s simulate values for a Poisson. The Poisson is often used in modeling customer service situations such as service at Chipotle. However, some people have the mistaken idea that arrivals will be equally spaced. In fact, arrivals will come in clusters and bunches. Maybe this is the root of the common expression, “Bad news comes in threes”?\n\n\n\n\n\n\n\n\nFigure 13.3: Simulations of Poisson random variable.\n\n\n\n\n\nIn Figure 13.3, the number of events in a box is \\(X\\sim \\textsf{Poisson}(\\lambda = 5)\\). As you can see, some boxes have more than 5 and some less because 5 is the average number of arrivals. Also note that the spacing is not equal. The 8 different runs are just repeated simulations of the same process. We can see spacing and clusters in each run.\n\n\n13.2.4 Normal distribution\nThe normal distribution (also referred to as Gaussian) is a common distribution found in natural processes. You have likely seen a bell curve in various contexts. The bell curve is often indicative of an underlying normal distribution. There are two parameters of the normal distribution: \\(\\mu\\) (the mean of \\(X\\)) and \\(\\sigma\\) (the standard deviation of \\(X\\)).\nSuppose a random variable \\(X\\) has a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The pdf of \\(X\\) is given by:\n\\[\nf_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\hspace{0.3cm} -\\infty &lt; x &lt;\\infty\n\\]\nSome plots of normal distributions for different parameters are plotted in Figure 13.4.\n\n\n\n\n\n\n\n\nFigure 13.4: The pdf of Normal for various values of mu and sigma\n\n\n\n\n\n\n13.2.4.1 Standard normal\nWhen random variable \\(X\\) is normally distributed with \\(\\mu=0\\) and \\(\\sigma=1\\), \\(X\\) is said to follow the standard normal distribution. Sometimes, the standard normal pdf is denoted by \\(\\phi(x)\\).\nNote that any normally distributed random variable can be transformed to have the standard normal distribution. Let \\(X \\sim \\textsf{Norm}(\\mu,\\sigma)\\). Then, \\[\nZ=\\frac{X-\\mu}{\\sigma} \\sim \\textsf{Norm}(0,1)\n\\]\nPartially, one can show this is true by noting that the mean of \\(Z\\) is 0 and the variance (and standard deviation) of \\(Z\\) is 1: \\[\n\\mbox{E}(Z)=\\mbox{E}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\left(\\mbox{E}(X)-\\mu\\right)=\\frac{1}\\sigma(\\mu-\\mu)=0\n\\] \\[\n\\mbox{Var}(Z)=\\mbox{Var}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma^2}\\left(\\mbox{Var}(X)-0\\right)=\\frac{1}{\\sigma^2} \\sigma^2=1\n\\]\nNote that this does not prove that \\(Z\\) follows the standard normal distribution; we have merely shown that \\(Z\\) has a mean of 0 and a variance of 1. We will discuss transformation of random variables in a later chapter.\n\nExample:\nLet \\(X \\sim \\textsf{Norm}(\\mu=200,\\sigma=15)\\). Compute \\(\\mbox{P}(X\\leq 160)\\), \\(\\mbox{P}(180\\leq X &lt; 230)\\), and \\(\\mbox{P}(X&gt;\\mu+\\sigma)\\). Find the median and 95th percentile of \\(X\\).\n\nAs with the gamma distribution, to find probabilities and quantiles, integration will be difficult, so it’s best to use the built-in R functions:\n\n## Prob(X&lt;=160)\npnorm(160,200,15)\n\n[1] 0.003830381\n\n##Prob(180 &lt;= X &lt; 230)\npnorm(230,200,15)-pnorm(180,200,15)\n\n[1] 0.8860386\n\n##Prob(X&gt;mu+sig)\n1-pnorm(215,200,15)\n\n[1] 0.1586553\n\n## median\nqnorm(0.5,200,15)\n\n[1] 200\n\n## 95th percentile\nqnorm(0.95,200,15)\n\n[1] 224.6728",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "13-Named-Continuous-Distributions.html#homework-problems",
    "href": "13-Named-Continuous-Distributions.html#homework-problems",
    "title": "13  Named Continuous Distributions",
    "section": "13.3 Homework Problems",
    "text": "13.3 Homework Problems\nFor problems 1-3 below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question.\n\nOn a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour.\n\n\n\nFind the probability no vehicles arrive in 10 minutes.\n\nFind the probability that at least 5 minutes will pass before the next arrival.\n\nFind the probability that the next vehicle will arrive between 2 and 10 minutes from now.\n\nFind the probability that at least 7 minutes will pass before the next arrival, given that 2 minutes have already passed. Compare this answer to part (b). This is an example of the memory-less property of the exponential distribution.\n\nFill in the blank. There is a probability of 90% that the next vehicle will arrive within __ minutes. This value is known as the 90% percentile of the random variable.\n\nUse the function stripplot() to visualize the arrival of 30 vehicles using a random sample from the appropriate exponential distribution.\n\n\n\nSuppose PFT scores in the cadet wing follow a normal distribution with mean 330 and standard deviation 50.\n\n\n\nFind the probability a randomly selected cadet has a PFT score higher than 450.\n\nFind the probability a randomly selected cadet has a PFT score within 2 standard deviations of the mean.\n\nFind \\(a\\) and \\(b\\) such that 90% of PFT scores will be between \\(a\\) and \\(b\\).\n\nFind the probability a randomly selected cadet has a PFT score higher than 450 given he/she is among the top 10% of cadets.\n\n\n\nAdvanced. You may have heard of the 68-95-99.7 rule. This is a helpful rule of thumb that says if a population has a normal distribution, then 68% of the data will be within one standard deviation of the mean, 95% of the data will be within two standard deviations and 99.7% of the data will be within three standard deviations. Create a function in R that has two inputs (a mean and a standard deviation). It should return a vector with three elements: the probability that a randomly selected observation from the normal distribution with the inputted mean and standard deviation lies within one, two and three standard deviations. Test this function with several values of the mu and sd. You should get the same answer each time.\nDerive the mean of a general uniform distribution, \\(U(a,b)\\).",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "13-Named-Continuous-Distributions.html#solutions-manual",
    "href": "13-Named-Continuous-Distributions.html#solutions-manual",
    "title": "13  Named Continuous Distributions",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Named Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html",
    "href": "14-Multivariate-Distributions.html",
    "title": "14  Multivariate Distributions",
    "section": "",
    "text": "14.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html#objectives",
    "href": "14-Multivariate-Distributions.html#objectives",
    "title": "14  Multivariate Distributions",
    "section": "",
    "text": "Define (and distinguish between) the terms joint probability mass/density function, marginal pmf/pdf, and conditional pmf/pdf.\nGiven a joint pmf/pdf, obtain the marginal and conditional pmfs/pdfs.\nUse joint, marginal and conditional pmfs/pdfs to obtain probabilities.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html#multivariate-distributions",
    "href": "14-Multivariate-Distributions.html#multivariate-distributions",
    "title": "14  Multivariate Distributions",
    "section": "14.2 Multivariate distributions",
    "text": "14.2 Multivariate distributions\nMultivariate situations are the more common in practice. We are often dealing with more than one variable. We have seen this in the previous block of material and will see multivariate distributions in the remainder of the book.\nThe basic idea is that we want to determine the relationship between two or more variables to include variable(s) conditional on variables.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html#joint-probability",
    "href": "14-Multivariate-Distributions.html#joint-probability",
    "title": "14  Multivariate Distributions",
    "section": "14.3 Joint probability",
    "text": "14.3 Joint probability\nThus far, we have only considered situations involving one random variable. In some cases, we might be concerned with the behavior of multiple random variables simultaneously. This chapter and the next are dedicated to jointly distributed random variables.\n\n14.3.1 Discrete random variables\nIn the discrete case, joint probability is described by the joint probability mass function. In the bivariate case, suppose \\(X\\) and \\(Y\\) are discrete random variables. The joint pmf is given by \\(f_{X,Y}(x,y)\\) and represents \\(\\mbox{P}(X=x,Y=y) = \\mbox{P}(X=x \\cap Y=y)\\). Note: it is common in statistical and probability models to use a comma to represent and, in fact the select() function in tidyverse does this.\nThe same rules of probability apply to the joint pmf. Each value of \\(f\\) must be between 0 and 1, and the total probability must sum to 1: \\[\n\\sum_{x\\in S_X}\\sum_{y \\in S_Y} f_{X,Y}(x,y) = 1\n\\] This notation means that if we sum the joint probabilities over all values of the random variables \\(X\\) and \\(Y\\) we will get 1.\nIf given a joint pmf, one can obtain the marginal pmf of individual variables. The marginal pmf is simply the mass function of an individual random variable, summing over the possible values of all the other variables. In the bivariate case, the marginal pmf of \\(X\\), \\(f_X(x)\\) is found by: \\[\nf_X(x)=\\sum_{y \\in S_Y}f_{X,Y}(x,y)\n\\]\nNotice that in the above summation, we summed over only the \\(y\\) values.\nSimilarly, \\[\nf_Y(y)=\\sum_{x \\in S_X}f_{X,Y}(x,y)\n\\]\nThe marginal pmf must be distinguished from the conditional pmf. The conditional pmf describes a discrete random variable given other random variables have taken particular values. In the bivariate case, the conditional pmf of \\(X\\), given \\(Y=y\\), is denoted as \\(f_{X|Y=y}(x)\\) and is found by: \\[\nf_{X|Y=y}(x)=\\mbox{P}(X=x|Y=y)=\\frac{\\mbox{P}(X=x,Y=y)}{\\mbox{P}(Y=y)}=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\n\nExample:\nLet \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below.\n\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\ & \\hline 0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &2 & 0.18 & 0.20 & 0.12  \n\\\\ & 4 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\n\nFind the marginal pmfs of \\(X\\) and \\(Y\\).\nFind \\(f_{X|Y=0}(x)\\) and \\(f_{Y|X=2}(y)\\).\n\nThe marginal pmfs can be found by summing across the other variable. So, to find \\(f_X(x)\\), we simply sum across the rows:\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=2 \\\\\n0.07+0.05+0.09, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=2 \\\\\n0.21, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nSimilarly, \\(f_Y(y)\\) can be found by summing down the columns of the joint pmf: \\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nTo find the conditional pmf of \\(X\\) given \\(Y=0\\), it helps to recognize that once we know that \\(Y=0\\), the overall sample space has changed. Now the only outcomes we consider are in the first column (corresponding to \\(Y=0\\)):\n\n\n\n\n\n\n\n\n\nWe are looking for the distribution of \\(X\\) within the circled area. So, we need to find the proportion of probability assigned to each outcome of \\(X\\). Mathematically: \\[\nf_{X|Y=0}(x)=\\mbox{P}(X=x|Y=0)=\\frac{\\mbox{P}(X=x,Y=0)}{\\mbox{P}(Y=0)}=\\frac{f_{X,Y}(x,0)}{f_Y(0)}\n\\]\nAbove, we found the marginal pmf of \\(Y\\). We know that \\(f_Y(0)=0.35\\). So, \\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{X|Y=0}(x)=\\left\\{\\begin{array}{ll} \\frac{0.10}{0.35}, & x=0 \\\\\n\\frac{0.18}{0.35}, & x=2 \\\\\n\\frac{0.07}{0.35}, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.286, & x=0 \\\\\n0.514, & x=2 \\\\\n0.200, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nNote that the probabilities in this pmf sum to 1. It is always wise to confirm this to ensure we did not make a simple computational error along the way.\nSimilarly, we can find \\(f_{Y|X=2}(y)\\). First we recognize that \\(f_X(2)=0.5\\). \\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{Y|X=2}(x)=\\left\\{\\begin{array}{ll} \\frac{0.18}{0.50}, & y=0 \\\\\n\\frac{0.20}{0.50}, & y=1 \\\\\n\\frac{0.12}{0.50}, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.36, & y=0 \\\\\n0.40, & y=1 \\\\\n0.24, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nTogether, these pmfs can be used to find relevant probabilities. For example, see the homework exercises.\n\n\n14.3.2 Continuous random variables\nMany of the ideas above for discrete random variables also apply to the case of multiple continuous random variables. Suppose \\(X\\) and \\(Y\\) are continuous random variables. Their joint probability is described by the joint probability density function. As in the discrete case, the joint pdf is represented by \\(f_{X,Y}(x,y)\\). Recall that while pmfs return probabilities, pdfs return densities, which are not equivalent to probabilities. In order to obtain probability from a pdf, one has to integrate the pdf across the applicable subset of the domain.\nThe rules of a joint pdf are analogous to the univariate case. For all \\(x\\) and \\(y\\), \\(f_{X,Y}(x,y)\\geq 0\\) and the probability must sum to one: \\[\n\\int_{S_X}\\int_{S_Y}f_{X,Y}(x,y)\\mbox{d}y \\mbox{d}x = 1\n\\]\nThe marginal pdf is the density function of an individual random variable, integrating out all others. In the bivariate case, the marginal pdf of \\(X\\), \\(f_X(x)\\), is found by summing, integrating, across the other variable: \\[\nf_X(x)=\\int_{S_Y}f_{X,Y}(x,y)\\mbox{d}y\n\\]\nSimilarly, \\[\nf_Y(y)=\\int_{S_X}f_{X,Y}(x,y)\\mbox{d}y\n\\]\nThe conditional pdf of \\(X\\), given \\(Y=y\\) is denoted as \\(f_{X|Y=y}(x)\\) and is found in the same way as in the discrete case: \\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\nSimilarly, \\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\]\nNote that we are working with the pdf and not probabilities in this case. That is because we can’t determine the probability at a point for a continuous random variable. Thus we work with conditional pdfs to find probabilities for conditional statements.\n\nExample:\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf:\n\\[\nf_{X,Y}(x,y)=xy\n\\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\).\n\n\nVerify \\(f\\) is a valid joint pdf.\n\nWe need to ensure the total volume under the pdf is 1. Note that the double integral with constant limits of integration is just like doing single integrals. We just treat the other variable as a constant. In this book we will not work with limits of integration that have variables in them, this is the material of Calc III.\nFor our simple case of constant limits of integration, the order of integration does not matter. We will arbitrarily integrate \\(x\\) first, treating \\(y\\) as a constant. Then integrate with respect to \\(y\\).\n\\[\n\\int_0^1 \\int_0^2 xy \\mbox{d}x \\mbox{d}y = \\int_0^1 \\frac{x^2y}{2}\\bigg|_0^2 \\mbox{d}y = \\int_0^1 2y\\mbox{d}y = y^2\\bigg|_0^1 = 1\n\\]\nUsing R to do this requires a new package cubature. You can install it from RStudio package tab or the command line using install.packages(\"cubature\"). Then we can use it as follows:\n\n library(cubature) # load the package \"cubature\"\n\n\nf &lt;- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 2))\n\n$integral\n[1] 1\n\n$error\n[1] 0\n\n$functionEvaluations\n[1] 17\n\n$returnCode\n[1] 0\n\n\nNotice the function adaptIntegrate returned four objects. You can read the help menu to learn more about them but we are only interested in the result contained in the object integral.\n\nFind \\(\\mbox{P}(X &gt; 1, Y \\leq 0.5)\\). \\[\n\\mbox{P}(X&gt;1,Y\\leq 0.5)=\\int_0^{0.5}\\int_1^2 xy \\mbox{d}x \\mbox{d}y = \\int_0^{0.5} \\frac{x^2 y}{2}\\bigg|_1^2 \\mbox{d}y = \\int_0^{0.5}2y - \\frac{y}{2}\\mbox{d}y\n\\] \\[\n= \\frac{3y^2}{4}\\bigg|_0^{0.5}=0.1875\n\\]\n\n\nf &lt;- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(1, 0), upperLimit = c(2, 1/2))\n\n$integral\n[1] 0.1875\n\n$error\n[1] 2.775558e-17\n\n$functionEvaluations\n[1] 17\n\n$returnCode\n[1] 0\n\n\n\nFind the marginal pdfs of \\(X\\) and \\(Y\\). \\[\nf_X(x)=\\int_0^1 xy \\mbox{d}y = \\frac{xy^2}{2}\\bigg|_0^1=\\frac{x}{2}\n\\]\n\nwhere \\(0 \\leq x \\leq 2\\).\n\\[\nf_Y(y)=\\int_0^2 xy \\mbox{d}x = \\frac{x^2y}{2}\\bigg|_0^2= 2y\n\\]\nwhere \\(0 \\leq y \\leq 1\\).\n\nFind the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{xy}{2y}=\\frac{x}{2}\n\\]\n\nwhere \\(0 \\leq x \\leq 2\\).\nSimilarly, \\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}=\\frac{xy}{\\frac{x}{2}}=2y\\]\nwhere \\(0 \\leq y \\leq 1\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html#homework-problems",
    "href": "14-Multivariate-Distributions.html#homework-problems",
    "title": "14  Multivariate Distributions",
    "section": "14.4 Homework Problems",
    "text": "14.4 Homework Problems\n\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[\nf_{X,Y}(x,y)=x + y\n\\]\n\nwhere \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\).\n\nVerify that \\(f\\) is a valid pdf.\n\nFind the marginal pdfs of \\(X\\) and \\(Y\\).\n\nFind the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\).\n\nFind the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;0.8)\\); \\(\\mbox{P}(X&lt;0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\).\n\n\n\nIn the reading, we saw an example where \\(f_X(x)=f_{X|Y=y}(x)\\) and \\(f_Y(y)=f_{Y|X=x}(y)\\). This is not common and is important. What does this imply about \\(X\\) and \\(Y\\)?\nADVANCED: Recall on an earlier assignment, we came up with random variables to describe timeliness at an airport. Suppose over the course of 210 days, on each day we recorded the number of customer complaints regarding timeliness. Also on each day, we recorded the weather (our airport is located somewhere without snow and without substantial wind). The data are displayed below.\n\n\\[\n\\begin{array}{cc|cc} & & &\\textbf{Weather Status}  \n\\\\ & & \\textbf{Clear} & \\textbf{Light Rain} & \\textbf{Rain}  \n\\\\ & \\hline0 & 28 & 11 & 4  \n\\\\ & 1 & 18 & 15 & 8  \n\\\\ & 2 & 17 & 25 & 12  \n\\\\ \\textbf{# of complaints} & 3 & 13 & 15 & 16  \\\\\n& 4 & 8 & 8 & 10  \n\\\\ & 5 & 0 & 1 & 1\n\\\\ \\end{array}\n\\]\nFirst, define two random variables for this scenario. One of them (# of complaints) is essentially already a random variable. For the other (weather status) you will need to assign a number to each status.\n\nUse the table above to build an empirical joint pmf of the two random variables.\n\nFind the marginal pmfs of each random variable.\n\nFind the probability of fewer than 3 complaints.\n\nFind the probability of fewer than 3 complaints given there is no rain.\n\n \nOptional for those of you that like Calc III and want a challenge.\n\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[\nf_{X,Y}(x,y)=1\n\\]\n\nwhere \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\).\n\nVerify that \\(f\\) is a valid pdf.\n\nFind the marginal pdfs of \\(X\\) and \\(Y\\).\n\nFind the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\).\n\nFind the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;1)\\); \\(\\mbox{P}(X&lt;0.5,Y\\leq 0.8)\\); Optional \\(\\mbox{P}(X&lt;0.5|Y= 0.8)\\); \\(\\mbox{P}(Y\\leq 1-X)\\). (It would probably help to draw some pictures.)",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "14-Multivariate-Distributions.html#solutions-manual",
    "href": "14-Multivariate-Distributions.html#solutions-manual",
    "title": "14  Multivariate Distributions",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Distributions</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html",
    "href": "15-Multivariate-Expectation.html",
    "title": "15  Multivariate Expectation",
    "section": "",
    "text": "15.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#objectives",
    "href": "15-Multivariate-Expectation.html#objectives",
    "title": "15  Multivariate Expectation",
    "section": "",
    "text": "Given a joint pmf/pdf, obtain means and variances of random variables and functions of random variables.\nDefine the terms covariance and correlation, and given a joint pmf/pdf, obtain the covariance and correlation between two random variables.\nGiven a joint pmf/pdf, determine whether random variables are independent of one another.\nFind conditional expectations.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#expectation---moments",
    "href": "15-Multivariate-Expectation.html#expectation---moments",
    "title": "15  Multivariate Expectation",
    "section": "15.2 Expectation - moments",
    "text": "15.2 Expectation - moments\nComputing expected values of random variables in the joint context is similar to the univariate case. Let \\(X\\) and \\(Y\\) be discrete random variables with joint pmf \\(f_{X,Y}(x,y)\\). Let \\(g(X,Y)\\) be some function of \\(X\\) and \\(Y\\). Then: \\[\n\\mbox{E}[g(X,Y)]=\\sum_x\\sum_y g(x,y)f_{X,Y}(x,y)\n\\]\n(Note that \\(\\sum\\limits_{x}\\) is shorthand for the sum across all possible values of \\(x\\).)\nIn the case of continuous random variables with a joint pdf \\(f_{X,Y}(x,y)\\), expectation becomes: \\[\n\\mbox{E}[g(X,Y)]=\\int_x\\int_y g(x,y)f_{X,Y}(x,y)\\mbox{d} y \\mbox{d} x\n\\]\n\n15.2.1 Expectation of discrete random variables\nGiven a joint pmf, one can find the mean of \\(X\\) by using the joint function or by finding the marginal pmf first and then using that to find \\(\\mbox{E}(X)\\). In the end, both ways are the same. For the discrete case: \\[\n\\mbox{E}(X)=\\sum_x\\sum_y xf_{X,Y}(x,y) = \\sum_x x \\sum_y f_{X,Y}(x,y)\n\\]\nThe \\(x\\) can be moved outside the inner sum since the inner sum is with respect to variable \\(y\\) and \\(x\\) is a constant with respect to \\(y\\). Note that the inner sum is the marginal pmf of \\(X\\). So, \\[\n\\mbox{E}(X)=\\sum_x x \\sum_y f_{X,Y}(x,y)=\\sum_x x f_X(x)\n\\]\n\n\nExample:\nLet \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below.\n\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\nFind \\(\\mbox{E}(X)\\)\nFirst we will use the joint pmf directly, then we find the marginal pmf of \\(X\\) and use that as we would in a univariate case.\n\\[\n\\mbox{E}(X)=\\sum_{x=0}^2 \\sum_{y=0}^2 x f_{X,Y}(x,y)\n\\] \\[\n=0*0.10+0*0.08+0*0.11+1*0.18+...+2*0.09 = 0.92\n\\]\nThe marginal pmf of \\(X\\) is \\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=1 \\\\\n0.07+0.05+0.09, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nSo, \\(\\mbox{E}(X)=0*0.29+1*0.5+2*0.21=0.92\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#exercises-to-apply-what-we-learned",
    "href": "15-Multivariate-Expectation.html#exercises-to-apply-what-we-learned",
    "title": "15  Multivariate Expectation",
    "section": "15.3 Exercises to apply what we learned",
    "text": "15.3 Exercises to apply what we learned\n\nExercise: Let \\(X\\) and \\(Y\\) be defined above. Find \\(\\mbox{E}(Y)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right)\\).\n\n\n15.3.1 E(Y)\nAs with \\(\\mbox{E}(X)\\), \\(\\mbox{E}(Y)\\) can be found in two ways. We will use the marginal pmf of \\(Y\\), which we will not derive:\n\\[\n\\mbox{E}(Y)=\\sum_{y=0}^2 y \\cdot f_Y(y)=0*0.35+1*0.33+2*0.32 = 0.97\n\\]\n\n\n15.3.2 E(X+Y)\nTo find \\(\\mbox{E}(X+Y)\\) we will use the joint pmf. In the discrete case, it helps to first identify all of the possible values of \\(X+Y\\) and then figure out what probabilities are associated with each value. This problem is really a transformation problem where we are finding the distribution of \\(X+Y\\). In this example, \\(X+Y\\) can take on values 0, 1, 2, 3, and 4. The value 0 only happens when \\(X=Y=0\\) and the probability of this outcome is 0.10. The value 1 occurs when \\(X=0\\) and \\(Y=1\\) or when \\(X=1\\) and \\(Y=0\\). This occurs with probability 0.08 + 0.18. We continue in this manner: \\[\n\\mbox{E}(X+Y)=\\sum_{x=0}^2\\sum_{y=0}^2 (x+y)f_{X,Y}(x,y)\n\\] \\[\n= 0*0.1+1*(0.18+0.08)+2*(0.11+0.07+0.20)+3*(0.12+0.05)+4*0.09\n\\] \\[\n= 1.89\n\\]\nNote that \\(\\mbox{E}(X+Y)=\\mbox{E}(X)+\\mbox{E}(Y)\\). (The proof of this is left to the reader.)\n\n\n15.3.3 E(XY)\n\\[\n\\mbox{E}(XY)=\\sum_{x=0}^2\\sum_{y=0}^2 xyf_{X,Y}(x,y)\n\\] \\[\n= 0*(0.1+0.08+0.11+0.18+0.07)+1*0.20 +2*(0.12+0.05)+4*0.09\n\\] \\[\n= 0.9\n\\]\nNote that \\(\\mbox{E}(XY)\\) is not necessarily equal to \\(\\mbox{E}(X)\\mbox{E}(Y)\\).\n\n\n15.3.4 E(1/2X+Y+1)\n\\[\n\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right) = \\sum_{x=0}^2\\sum_{y=0}^2 \\frac{1}{2x+y+1}f_{X,Y}(x,y)\n\\] \\[\n= 1*0.1+\\frac{1}{2}*0.08+\\frac{1}{3}*(0.11+0.18)+\\frac{1}{4}*0.20 + \\] \\[\n\\frac{1}{5}*(0.12+0.07)+\\frac{1}{6}*0.05+\\frac{1}{7}*0.09\n\\] \\[\n= 0.3125\n\\]\n\n\n15.3.5 Expectation of continuous random variables\nLet’s consider an example with continuous random variables where summation is replaced with integration:\n\nExample:\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[\nf_{X,Y}(x,y)=xy\n\\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#exercises-to-apply-what-we-learned-1",
    "href": "15-Multivariate-Expectation.html#exercises-to-apply-what-we-learned-1",
    "title": "15  Multivariate Expectation",
    "section": "15.4 Exercises to apply what we learned",
    "text": "15.4 Exercises to apply what we learned\n\nExercise: Find \\(\\mbox{E}(X)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{Var}(XY)\\).\n\n\n15.4.1 E(X)\nWe found the marginal pdf of \\(X\\) in a previous chapter, so we should use that now: \\[\n\\mbox{E}(X)=\\int_0^2 x\\frac{x}{2}\\mbox{d} x = \\frac{x^3}{6}\\bigg|_0^2= \\frac{4}{3}\n\\] Or using R\n\nfractions(integrate(function(x){x^2/2},0,2)$value)\n\n[1] 4/3\n\n\n\n\n15.4.2 E(X+Y)\nTo find \\(\\mbox{E}(X+Y)\\), we could use the joint pdf directly, or use the marginal pdf of \\(Y\\) to find \\(\\mbox{E}(Y)\\) and then add the result to \\(\\mbox{E}(X)\\). The reason this is valid is because when we integrate \\(x\\) with the joint pdf, integrating with respect to \\(y\\) first, we can treat \\(x\\) as a constant and bring it out side the integral. Then we are integrating the joint pdf with respect \\(y\\) which results in the marginal pdf of \\(X\\).\nWe’ll use the joint pdf: \\[\n\\mbox{E}(X+Y)=\\int_0^2\\int_0^1 (x+y)xy\\mbox{d} y \\mbox{d} x\n\\] \\[\n=\\int_0^2\\int_0^1 (x^2y+xy^2)\\mbox{d} y \\mbox{d} x = \\int_0^2 \\frac{x^2y^2}{2}+\\frac{xy^3}{3} \\bigg|_{y=0}^{y=1}\\mbox{d} x\n\\]\n\\[\n= \\int_0^2 \\frac{x^2}{2}+\\frac{x}{3} \\mbox{d} x= \\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^2=\\frac{8}{6}+\\frac{4}{6}=2\n\\]\nOr using R:\n\nadaptIntegrate(function(x){(x[1]+x[2])*x[1]*x[2]},\n    lowerLimit = c(0,0),\n    upperLimit = c(1,2))$integral\n\n[1] 2\n\n\nIf we wanted to use simulation to find this expectation, we could simulate variables from the marginal of \\(X\\) and \\(Y\\) and then add them together to create a new variable.\nThe cdf for \\(X\\) is \\(\\frac{x^2}{4}\\) so we simulate a random variable from \\(X\\) by sampling from a random uniform and then taking the inverse of the cdf. For \\(Y\\) the cdf is \\(y^2\\) and do a similar simulation.\n\nset.seed(1820)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=x+y) %&gt;%\n  summarize(Ex=mean(x),Ey=mean(y),Explusy = mean(z))\n\n        Ex        Ey  Explusy\n1 1.338196 0.6695514 2.007748\n\n\nWe can see that \\(E(X + Y) = E(X) + E(Y)\\).\n\n\n15.4.3 E(XY)\nNext, we have\n\\[\n\\mbox{E}(XY)=\\int_0^2\\int_0^1 xy*xy\\mbox{d} y \\mbox{d} x = \\int_0^2 \\frac{x^2y^3}{3}\\bigg|_0^1 \\mbox{d} x = \\int_0^2 \\frac{x^2}{3}\\mbox{d} x\n\\] \\[\n=\\frac{x^3}{9}\\bigg|_0^2 = \\frac{8}{9}\n\\]\nUsing R:\n\nfractions(adaptIntegrate(function(x){(x[1]*x[2])*x[1]*x[2]},\n      lowerLimit = c(0,0),\n      upperLimit = c(1,2))$integral)\n\n[1] 8/9\n\n\nOr by simulating, we have:\n\nset.seed(191)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=x*y) %&gt;%\n  summarize(Ex=mean(x),Ey=mean(y),Extimesy = mean(z))\n\n       Ex        Ey  Extimesy\n1 1.33096 0.6640436 0.8837552\n\n\n\n\n15.4.4 V(XY)\nRecall that the variance of a random variable is the expected value of the squared difference from its mean. So, \\[\n\\mbox{Var}(XY)=\\mbox{E}\\left[\\left(XY-\\mbox{E}(XY)\\right)^2\\right]=\\mbox{E}\\left[\\left(XY-\\frac{8}{9}\\right)^2\\right]\n\\] \\[\n=\\int_0^2\\int_0^1 \\left(xy-\\frac{8}{9}\\right)^2 xy\\mbox{d} y \\mbox{d} x =\\int_0^2\\int_0^1 \\left(x^2y^2-\\frac{16xy}{9}+\\frac{64}{81}\\right)xy\\mbox{d} y \\mbox{d} x\n\\]\nYuck!! But we will continue because we are determined to integrate after so much Calculus in our core curriculum.\n\\[\n=\\int_0^2\\int_0^1 \\left(x^3y^3-\\frac{16x^2y^2}{9}+\\frac{64xy}{81}\\right)\\mbox{d} y \\mbox{d} x\n\\]\n\\[\n=\\int_0^2 \\frac{x^3y^4}{4}-\\frac{16x^2y^3}{27}+\\frac{32xy^2}{81}\\bigg|_0^1 \\mbox{d} x\n\\]\n\\[\n= \\int_0^2 \\frac{x^3}{4}-\\frac{16x^2}{27}+\\frac{32x}{81}\\mbox{d} x\n\\]\n\\[\n= \\frac{x^4}{16}-\\frac{16x^3}{81}+\\frac{16x^2}{81}\\bigg|_0^2\n\\] \\[\n=\\frac{16}{16}-\\frac{128}{81}+\\frac{64}{81}=\\frac{17}{81}\n\\]\nUsing R:\n\nfractions(adaptIntegrate(function(x){(x[1]*x[2]-8/9)^2*x[1]*x[2]},\n    lowerLimit = c(0,0),\n    upperLimit = c(1,2))$integral)\n\n[1] 17/81\n\n\nNext we will estimate the variance using a simulation:\n\nset.seed(816)\nnew_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %&gt;%\n  mutate(z=(x*y-8/9)^2) %&gt;%\n  summarize(Var = mean(z))\n\n        Var\n1 0.2098769\n\n\nThat was much easier. Notice that we are really just estimating these expectations with the simulations. The mathematical answers are the true population values while our simulations are sample estimates. In a few chapters we will discuss estimators in more detail.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#covariancecorrelation",
    "href": "15-Multivariate-Expectation.html#covariancecorrelation",
    "title": "15  Multivariate Expectation",
    "section": "15.5 Covariance/Correlation",
    "text": "15.5 Covariance/Correlation\nWe have discussed expected values of random variables and functions of random variables in a joint context. It would be helpful to have some kind of consistent measure to describe how two random variables are related to one another. Covariance and correlation do just that. It is important to understand that these are measures of a linear relationship between variables.\nConsider two random variables \\(X\\) and \\(Y\\). (We could certainly consider more than two, but for demonstration, let’s consider only two for now). The covariance between \\(X\\) and \\(Y\\) is denoted as \\(\\mbox{Cov}(X,Y)\\) and is found by: \\[\n\\mbox{Cov}(X,Y)=\\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right]\n\\]\nWe can simplify this expression to make it a little more usable: \\[\n\\begin{align}\n\\mbox{Cov}(X,Y) & = \\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right] \\\\\n& = \\mbox{E}\\left[XY - Y\\mbox{E}(X) - X\\mbox{E}(Y) + \\mbox{E}(X)\\mbox{E}(Y)\\right] \\\\\n& = \\mbox{E}(XY) - \\mbox{E}(Y\\mbox{E}(X)) - \\mbox{E}(X\\mbox{E}(Y)) + \\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)-\\mbox{E}(X)\\mbox{E}(Y)+\\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\end{align}\n\\]\nThus, \\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\]\nThis expression is a little easier to use, since it’s typically straightforward to find each of these quantities.\nIt is important to note that while variance is a positive quantity, covariance can be positive or negative. A positive covariance implies that as the value of one variable increases, the other tends to increase. This is a statement about a linear relationship. Likewise, a negative covariance implies that as the value of one variable increases, the other tends to decrease.\n\nExample:\nAn example of positive covariance is human height and weight. As height increase, weight tends to increase. An example of negative covariance is gas mileage and car weight. As car weight increases, gas mileage decreases.\n\nRemember that if \\(a\\) and \\(b\\) are constants, \\(\\mbox{E}(aX+b) =a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\). Similarly, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are all constants, \\[\n\\mbox{Cov}(aX+b,cY+d)=ac\\mbox{Cov}(X,Y)\n\\]\nOne disadvantage of covariance is its dependence on the scales of the random variables involved. This makes it difficult to compare covariances of multiple sets of variables. Correlation avoids this problem. Correlation is a scaled version of covariance. It is denoted by \\(\\rho\\) and found by: \\[\n\\rho = \\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]\nWhile covariance could take on any real number, correlation is bounded by -1 and 1. Two random variables with a correlation of 1 are said to be perfectly positively correlated, while a correlation of -1 implies perfect negative correlation. Two random variables with a correlation (and thus covariance) of 0 are said to be uncorrelated, that is they do not have a linear relationship but could have a non-linear relationship. This last point is important; random variables with no relationship will have a 0 covariance. However, a 0 covariance only implies that the random variables do not have a linear relationship.\nLet’s look at some plots, Figures @ref(fig:cor1-fig), @ref(fig:cor2-fig), @ref(fig:cor3-fig), and @ref(fig:cor4-fig) of different correlations. Remember that the correlation we are calculating in this section is for the population, while the plots are showing sample points from a population.\n\n\n\n\n\nCorrelation of 1\n\n\n\n\n\n\n\n\n\nCorrelation of .8\n\n\n\n\n\n\n\n\n\nCorrelation of .5\n\n\n\n\n\n\n\n\n\nCorrelation of 0\n\n\n\n\n\n\n15.5.1 Variance of sums\nSuppose \\(X\\) and \\(Y\\) are two random variables. Then, \\[\n\\mbox{Var}(X+Y)=\\mbox{E}\\left[(X+Y-\\mbox{E}(X+Y))^2\\right]=\\mbox{E}[(X+Y)^2]-\\left[\\mbox{E}(X+Y)\\right]^2\n\\]\nIn the last step, we are using the alternative expression for variance (\\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\)). Evaluating: \\[\n\\mbox{Var}(X+Y)=\\mbox{E}(X^2)+\\mbox{E}(Y^2)+2\\mbox{E}(XY)-\\mbox{E}(X)^2-\\mbox{E}(Y)^2-2\\mbox{E}(X)\\mbox{E}(Y)\n\\]\nRegrouping the terms: \\[\n=\\mbox{E}(X^2)-\\mbox{E}(X)^2+\\mbox{E}(Y^2)-\\mbox{E}(Y)^2+2\\left(\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\\right)\n\\]\n\\[\n=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)\n\\]\n\nExample:\nLet \\(X\\) and \\(Y\\) be defined as above. Find \\(\\mbox{Cov}(X,Y)\\), \\(\\rho\\), and \\(\\mbox{Var}(X+Y)\\).\n\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0.9-0.92*0.97=0.0076\n\\]\n\\[\n\\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]\nQuickly, \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2= 1.34-0.92^2 =0.4936\\) and \\(\\mbox{Var}(Y)=0.6691\\). So, \\[\n\\rho=\\frac{0.0076}{\\sqrt{0.4936*0.6691}}=0.013\n\\]\nWith such a low \\(\\rho\\), we would say that \\(X\\) and \\(Y\\) are only slightly positively correlated.\n\\[\n\\mbox{Var}(X+Y)=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=0.4936+0.6691+2*0.0076=1.178\n\\]",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#independence",
    "href": "15-Multivariate-Expectation.html#independence",
    "title": "15  Multivariate Expectation",
    "section": "15.6 Independence",
    "text": "15.6 Independence\nTwo random variables \\(X\\) and \\(Y\\) are said to be independent if their joint pmf/pdf is the product of their marginal pmfs/pdfs: \\[\nf_{X,Y}(x,y)=f_X(x)f_Y(y)\n\\]\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\mbox{Cov}(X,Y) = 0\\). The converse is not necessarily true, however because they could have a non-linear relationship.\nFor a discrete distribution, you must check that each cell, joint probabilities, are equal to the product of the marginal probability. Back to our joint pmf from above:\n\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]\nThe marginal pmf of \\(X\\) is\n\\[\nf_X(x) = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\] The marginal pmf of \\(Y\\) is\n\\[\nf_Y(y) = \\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\] Checking the first cell, we immediately see that \\(f_{X,Y}(x=0,y=0) \\neq f_{X}(x=0)\\cdot f_{Y}(y=0)\\) so \\(X\\) and \\(Y\\) are not independent.\nAn easy way to determine if continuous variables are independent is to first check that the domain only contains constants, it is rectangular, and second that the joint pdf can be written as a product of a function of \\(X\\) only and a function of \\(Y\\) only.\nThus for our examples above even though the domains were rectangular, in \\(f(x,y)=xy\\), \\(X\\) and \\(Y\\) were independent while \\(f(x,y)=x+y\\) they were not.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#conditional-expectation",
    "href": "15-Multivariate-Expectation.html#conditional-expectation",
    "title": "15  Multivariate Expectation",
    "section": "15.7 Conditional expectation",
    "text": "15.7 Conditional expectation\nAn important idea in graph theory, network analysis, Bayesian networks, and queuing theory is conditional expectation. We will only briefly introduce the ideas here so that you have a basic understanding. This does not imply it is not an important topic.\nLet’s start with a simple example to illustrate the ideas.\n\nExample:\nSam will read either one chapter of his history book or one chapter of his philosophy book. If the number of misprints in a chapter of his history book is Poisson with mean 2 and if the number of misprints in a chapter of his philosophy book is Poisson with mean 5, then assuming Sam is equally likely to choose either book, what is the expected number of misprints that Sam will find?\n\nNote: in the next chapter we are working with transformations and could attack the problem using that method.\nFirst let’s use simulation to get an idea what value the answer should be and then use algebraic techniques and definitions we have learned in this book.\nSimulate 5000 reads from the history book and 5000 from philosophy and combine:\n\nset.seed(2011)\nmy_data&lt;-data.frame(misprints=c(rpois(5000,2),rpois(5000,5)))\n\n\nhead(my_data)\n\n  misprints\n1         1\n2         1\n3         2\n4         1\n5         2\n6         4\n\n\n\ndim(my_data)\n\n[1] 10000     1\n\n\nFigure @ref(fig:hist151-fig) is a histogram of the data.\n\ngf_histogram(~misprints,data=my_data,breaks=seq(-0.5, 15.5, by=1)) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x=\"Number of Misprints\")\n\n\n\n\nMisprints from combined history and philosphy books.\n\n\n\n\nOr as a bar chart in Figure @ref(fig:bar151-fig)\n\ngf_bar(~misprints,data=my_data)%&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x=\"Number of Misprints\")\n\n\n\n\nMisprints from combined history and philosphy books as a bar chart.\n\n\n\n\nAnd now find the average.\n\nmean(~misprints,data=my_data)\n\n[1] 3.4968\n\n\nNow for a mathematical solution. Let \\(X\\) stand for the number of misprints and \\(Y\\) for the book, to make \\(Y\\) a random variable let’s call 0 history and 1 philosophy. Then \\(E[X|Y]\\) is the expected number of misprints given the book. This is a conditional expectation. For example \\(E[X|Y=0]\\) is the expected number of misprints in the history book.\nNow here is the tricky part, without specifying a value of \\(Y\\), called a realization, this expectation function is a random variable that depends on \\(Y\\). In other words, if we don’t know the book, the expected number of misprints depends on \\(Y\\) and thus is a random variable. If we take the expected value of this random variable we get\n\\[E[X]=E[E[X|Y]]\\]\nThe inner expectation in the right hand side is for the conditional distribution and the outer is for the marginal with respect to \\(Y\\). This seems confusing, so let’s go back to our example.\n\\[E[X]=E[E[X|Y]]\\] \\[=E[X|Y=0] \\cdot \\mbox{P}(Y=0)+E[X|Y=1] \\cdot \\mbox{P}(Y=1)\\] \\[=2*\\frac{1}{2}+5*\\frac{1}{2}=\\frac{7}{2}=3.5\\] These ideas are going to be similar for continuous random variables. There is so much more that you can do with conditional expectations, but these are beyond the scope of the book.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#homework-problems",
    "href": "15-Multivariate-Expectation.html#homework-problems",
    "title": "15  Multivariate Expectation",
    "section": "15.8 Homework Problems",
    "text": "15.8 Homework Problems\n\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[\nf_{X,Y}(x,y)=x + y\n\\]\n\nwhere \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\).\n\nFind \\(\\mbox{E}(X)\\) and \\(\\mbox{E}(Y)\\).\n\nFind \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\).\n\nFind \\(\\mbox{Cov}(X,Y)\\) and \\(\\rho\\). Are \\(X\\) and \\(Y\\) independent?\n\nFind \\(\\mbox{Var}(3X+2Y)\\).\n\n\n\nOptional - not difficult but does have small Calc III idea. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[\nf_{X,Y}(x,y)=1\n\\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\).\n\n\n\nFind \\(\\mbox{E}(X)\\) and \\(\\mbox{E}(Y)\\).\nFind \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\).\nFind \\(\\mbox{Cov}(X,Y)\\) and \\(\\rho\\). Are \\(X\\) and \\(Y\\) independent?\nFind \\(\\mbox{Var}\\left(\\frac{X}{2}+2Y\\right)\\).\n\n\n\nSuppose \\(X\\) and \\(Y\\) are independent random variables. Show that \\(\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\\).\nYou are playing a game with a friend. Each of you roll a fair sided die and record the result.\n\n\n\nWrite the joint probability mass function.\nFind the expected value of the product of your score and your friend’s score.\nVerify the previous part using simulation.\nUsing simulation, find the expected value of the maximum number on the two roles.\n\n\n\nA miner is trapped in a mine containing three doors. The first door leads to a tunnel that takes him to safety after two hours of travel. The second door leads to a tunnel that returns him to the mine after three hours of travel. The third door leads to a tunnel that returns him to his mine after five hours. Assuming that the miner is at all times equally likely to choose any one of the doors, yes a bad assumption but it makes for a nice problem, what is the expected length of time until the miner reaches safety?\nADVANCED: Let \\(X_1,X_2,...,X_n\\) be independent, identically distributed random variables. (This is often abbreviated as “i.i.d.”). Each \\(X_i\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\) (i.e., for all \\(i\\), \\(\\mbox{E}(X_i)=\\mu\\) and \\(\\mbox{Var}(X_i)=\\sigma^2\\)).\n\nLet \\(S=X_1+X_2+...+X_n=\\sum_{i=1}^n X_i\\). And let \\(\\bar{X}={\\sum_{i=1}^n X_i \\over n}\\).\nFind \\(\\mbox{E}(S)\\), \\(\\mbox{Var}(S)\\), \\(\\mbox{E}(\\bar{X})\\) and \\(\\mbox{Var}(\\bar{X})\\).",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "15-Multivariate-Expectation.html#solutions-manual",
    "href": "15-Multivariate-Expectation.html#solutions-manual",
    "title": "15  Multivariate Expectation",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate Expectation</span>"
    ]
  },
  {
    "objectID": "16-Transformations.html",
    "href": "16-Transformations.html",
    "title": "16  Transformations",
    "section": "",
    "text": "16.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "16-Transformations.html#objectives",
    "href": "16-Transformations.html#objectives",
    "title": "16  Transformations",
    "section": "",
    "text": "Given a discrete random variable, determine the distribution of a transformation of that random variable.\n\nGiven a continuous random variable, use the cdf method to determine the distribution of a transformation of that random variable.\n\nUse simulation methods to find the distribution of a transform of single or multivariate random variables.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "16-Transformations.html#transformations",
    "href": "16-Transformations.html#transformations",
    "title": "16  Transformations",
    "section": "16.2 Transformations",
    "text": "16.2 Transformations\nThroughout our coverage of random variables, we have mentioned transformations of random variables. These have been in the context of linear transformations. We have discussed expected value and variance of linear transformations. Recall that \\(\\mbox{E}(aX+b)=a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\).\nIn this chapter, we will discuss transformations of random variables in general, beyond the linear case.\n\n16.2.1 Transformations of discrete random variables\nLet \\(X\\) be a discrete random variable and let \\(g\\) be a function. The variable \\(Y=g(X)\\) is a discrete random variable with pmf: \\[\nf_Y(y)=\\mbox{P}(Y=y)=\\sum_{\\forall x: g(x)=y}\\mbox{P}(X=x)=\\sum_{\\forall x: g(x)=y}f_X(x)\n\\]\nAn example would help since the notation can be confusing.\n\nExample:\nSuppose \\(X\\) is a discrete random variable with pmf: \\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.05, & x=-2 \\\\\n0.10, & x=-1 \\\\\n0.35, & x=0 \\\\\n0.30, & x=1 \\\\\n0.20, & x=2 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\n\nFind the pmf for \\(Y=X^2\\).\nIt helps to identify the support of \\(Y\\), that is where \\(f_{Y}(y)&gt;0\\). Since the support of \\(X\\) is \\(S_X=\\{-2,-1,0,1,2\\}\\), the support of \\(Y\\) is \\(S_Y=\\{0,1,4\\}\\). \\[\nf_Y(0)=\\sum_{x^2=0}f_X(x)=f_X(0)=0.35\n\\]\n\\[\nf_Y(1)=\\sum_{x^2=1}f_X(x)=f_X(-1)+f_X(1)=0.1+0.3=0.4\n\\] \\[\nf_Y(4)=\\sum_{x^2=4}f_X(x)=f_X(-2)+f_X(2)=0.05+0.2=0.25\n\\]\nSo, \\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.4, & y=1 \\\\\n0.25, & y=4 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]\nIt also helps to confirm that these probabilities add to one, which they do. This is the pmf of \\(Y=X^2\\).\nThe key idea is to find the support of the new random variable and then go back to the original random variable and sum all the probabilities that get mapped into that new support element.\n\n\n16.2.2 Transformations of continuous random variables\nThe methodology above will not work directly in the case of continuous random variables. This is because in the continuous case, the pdf, \\(f_X(x)\\), represents density and not probability.\n\n\n16.2.3 The cdf method\nThe cdf method is one of several methods that can be used for transformations of continuous random variables. The idea is to find the cdf of the new random variable and then find the pdf by way of the fundamental theorem of calculus.\nSuppose \\(X\\) is a continuous random variable with cdf \\(F_X(x)\\). Let \\(Y=g(X)\\). We can find the cdf of \\(Y\\) as:\n\\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X)\\leq y)=\\mbox{P}(X\\leq g^{-1}(y))=F_X(g^{-1}(y))\n\\]\nTo get the pdf of \\(Y\\) we would need to take the derivative of the cdf. Note that \\(g^{-1}(y)\\) is the function inverse while \\(g(y)^{-1}\\) is the multiplicative inverse.\nThis method requires the transformation function to have an inverse. Sometimes we can break the domain of the original random variables into regions where an inverse of the transformation function exists.\n\nExample: Let \\(X\\sim \\textsf{Unif}(0,1)\\) and let \\(Y=X^2\\). Find the pdf of \\(Y\\).\n\nBefore we start, let’s think about the distribution of \\(Y\\). We are randomly taking numbers between 0 and 1 and then squaring them. Squaring a positive number less than 1 makes it even smaller. We thus suspect the pdf of \\(Y\\) will have larger density near 0 than 1. The shape is hard to determine so let’s do some math.\nSince \\(X\\) has the uniform distribution, we know that \\(f_X(x)\\) and \\(F_X(x)=x\\) for \\(0\\leq x \\leq 1\\). So, \\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(X^2\\leq y)=\\mbox{P}(X\\leq \\sqrt{y})=F_X\\left(\\sqrt{y}\\right)=\\sqrt{y}\n\\]\nTaking the derivative of this yields: \\[\nf_Y(y)=\\frac{1}{2\\sqrt{y}}\n\\]\nfor \\(0 &lt; y \\leq 1\\) and 0 otherwise. Notice we can’t have \\(y=0\\) since we would be dividing by zero. This is not a problem since we have a continuous distribution. We could verify this a proper pdf by determining if the pdf integrates to 1 over the domain: \\[\n\\int_0^1 \\frac{1}{2\\sqrt{y}} \\,\\mathrm{d}y = \\sqrt{y}\\bigg|_0^1 = 1\n\\] We can also do this using R but we first have to create a function that can take vector input.\n\ny_pdf &lt;- function(y) {\n  1/(2*sqrt(y))\n}\n\n\ny_pdf&lt;- Vectorize(y_pdf)\n\n\nintegrate(y_pdf,0,1)\n\n1 with absolute error &lt; 2.9e-15\n\n\nNotice that since the domain of the original random variable was non-negative, the squared function had an inverse.\nThe pdf of the random variable \\(Y\\) is plotted in Figure 16.1.\n\ngf_line(y_pdf(seq(0.01,1,.01))~seq(0.01,1,.01),xlab=\"Y\",ylab=expression(f(y))) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 16.1: he pdf of the transformed random variable \\(Y\\)\n\n\n\n\n\nWe can see that the density is much larger at we approach 0.\n\n\n16.2.4 The pdf method\nThe cdf method of transforming continuous random variables also yields to another method called the pdf method. Recall that the cdf method tells us that if \\(X\\) is a continuous random variable with cdf \\(F_X\\), and \\(Y=g(X)\\), then\n\\[\nF_Y(y)=F_X(g^{-1}(y))\n\\]\nWe can find the pdf of \\(Y\\) by differentiating the cdf: \\[\nf_Y(y)=\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y}F_Y(y)=\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} F_X(g^{-1}(y)) = f_X(g^{-1}(y))\\bigg| \\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y}  g^{-1}(y) \\bigg|\n\\]\nSo, as long as \\(g^{-1}\\) is differentiable, we can use this method to directly obtain the pdf of \\(Y\\).\nNote that in some texts, the portion of this expression \\(\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} g^{-1}(y)\\) is sometimes referred to as the Jacobian. We need to take the absolute value of the transformation function \\(g(x)\\) because if it is a decreasing function, we have\n\\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X) \\leq y)=\\mbox{P}(X \\geq g^{-1}(y))= 1 - F_X(g^{-1}(y))\n\\]\n\nExercise:\nRepeat the previous example using the pdf method.\n\nSince \\(X\\) has the uniform distribution, we know that \\(f_X(x)=1\\) for \\(0\\leq x \\leq 1\\). Also, \\(g(x)=x^2\\) and \\(g^{-1}(y)=\\sqrt{y}\\), which is differentiable. So,\n\\[\nf_Y(y)=f_X(\\sqrt{y})\\bigg|\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} \\sqrt{y}\\bigg| = \\frac{1}{2\\sqrt{y}}\n\\]\n\n\n16.2.5 Simulation\nWe can also get an estimate of the distribution by simulating the random variable. If we have the cdf and can find its inverse, then just like we did in an earlier chapter, we sample from a uniform distribution and apply the inverse to get the distribution.\nIn an earlier chapter we had the example:\n\nLet \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\).\n\nNow let’s find an approximation to the distribution of \\(Y = \\ln{X}\\) using simulation.\nThe cdf of \\(X\\) is \\(F_X(x)=x^2\\) where \\(0 \\leq x \\leq 1\\). We will draw a uniform random variable and then take the square root to simulate a random variable from \\(f_X(x)\\). We will replicate this 10,000 times. In R our code, which we have done before, is:\n\nset.seed(1107)\nresults &lt;- do(10000)*sqrt(runif(1))\n\nRemember, we are using the square root because we want the inverse of the cdf and not, for this method, the inverse of the transformation function as when we were using the mathematical method. This can be a point of confusion.\n\ninspect(results)\n\n\nquantitative variables:  \n  name   class        min        Q1    median        Q3       max      mean\n1 sqrt numeric 0.01126465 0.5027012 0.7069864 0.8670356 0.9999379 0.6672924\n         sd     n missing\n1 0.2341494 10000       0\n\n\nFigure 16.2 is a density plot of the simulated original random variable.\n\nresults %&gt;%\n  gf_density(~sqrt,xlab=\"X\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(y=\"\")\n\n\n\n\n\n\n\nFigure 16.2: The density plot of the original using simulation.\n\n\n\n\n\nNow to find the distribution of \\(Y\\) we just apply the transformation.\n\ny_results &lt;- results %&gt;%\n  transmute(y=log(sqrt))\n\nFigure 16.3 is the density plot of the transformed random variable from the simulation. We can see that the support for \\(Y\\) is \\(-\\infty &lt; y \\leq 0\\) and the density is tight near zero but skewed to the left.\n\ny_results %&gt;%\n  gf_density(~y,xlab=\"X\")  %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(y=\"\")\n\n\n\n\n\n\n\nFigure 16.3: The density plot of the transformed random variable from the simulation.\n\n\n\n\n\n\ninspect(y_results)\n\n\nquantitative variables:  \n  name   class       min         Q1     median         Q3           max\n1    y numeric -4.486086 -0.6877593 -0.3467439 -0.1426753 -6.207173e-05\n        mean        sd     n missing\n1 -0.4969103 0.4933701 10000       0\n\n\n\n\n16.2.6 Multivariate Transformations\nFor the discrete case, if you have the joint pmf, then if the transformation is to a univariate random variable, the process is similar to what see learned above. For continuous random variables, the mathematics are a little more difficult so we will just use simulation.\nHere’s the scenario. Suppose \\(X\\) and \\(Y\\) are independent random variables, both uniformly distributed on \\([5,6]\\). \\[\nX\\sim \\textsf{Unif}(5,6)\\hspace{1.5cm} Y\\sim \\textsf{Unif}(5,6)\n\\]\nLet \\(X\\) be your arrival time for dinner and \\(Y\\) your friends arrival time. We picked 5 to 6 because this is the time in the evening we want to meet. Also assume you both travel independently.\nDefine \\(Z\\) as a transformation of \\(X\\) and \\(Y\\) such that \\(Z=|X-Y|\\). Thus \\(Z\\) is the absolute value of the difference between your arrival times. The units for \\(Z\\) are hours. We would like to explore the distribution of \\(Z\\). We could do this via calc III methods but we will simulate instead.\nWe can use R to obtain simulated values from \\(X\\) and \\(Y\\) (and thus find \\(Z\\)).\nFirst, simulate 100,000 observations from the uniform distribution with parameters 5 and 6. Assign those random observations to a variable. Next, repeat that process, assigning those to a different variable. These two vectors represent your simulated values from \\(X\\) and \\(Y\\). Finally, obtain your simulated values of \\(Z\\) by taking the absolute value of the difference.\n\nExercise:\n\nComplete the code on your own before looking at the code below.\n\nset.seed(354)\nresults &lt;- do(100000)*abs(diff(runif(2,5,6)))\n\n\nhead(results)\n\n         abs\n1 0.03171229\n2 0.77846706\n3 0.29111599\n4 0.06700434\n5 0.08663187\n6 0.40622840\n\n\nFigure 16.4 is a plot of the estimated density of the transformation.\n\nresults %&gt;%\n  gf_density(~abs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"|X-Y|\",y=\"\")\n\n\n\n\n\n\n\nFigure 16.4: The density of the absolute value of the difference in uniform random variables.\n\n\n\n\n\nOr as a histogram in Figure 16.5 .\n\nresults %&gt;%\n  gf_histogram(~abs)%&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x=\"|X-Y|\",y=\"\")\n\n\n\n\n\n\n\nFigure 16.5: Histogram of the absolute value of the difference in random variables.\n\n\n\n\n\n\ninspect(results)\n\n\nquantitative variables:  \n  name   class          min       Q1    median        Q3       max     mean\n1  abs numeric 1.265667e-06 0.133499 0.2916012 0.4990543 0.9979459 0.332799\n         sd      n missing\n1 0.2358863 100000       0\n\n\n\nExercise:\nNow suppose whomever arrives first will only wait 5 minutes and then leave. What is the probability you eat together?\n\n\ndata.frame(results) %&gt;%\n  summarise(mean(abs&lt;=5/60))\n\n  mean(abs &lt;= 5/60)\n1           0.15966\n\n\n\nExercise:\nHow long should the first person wait so that there is at least a 50% probability of you eating together?\n\nLet’s write a function to find the cdf.\n\nz_cdf &lt;- function(x) {\n  mean(results$abs&lt;=x)\n}\n\n\nz_cdf&lt;- Vectorize(z_cdf)\n\nNow test for 5 minutes to make sure our function is correct since we determined above that this value should be 0.15966.\n\nz_cdf(5/60)\n\n[1] 0.15966\n\n\nLet’s plot to see what the cdf looks like.\n\ngf_line(z_cdf(seq(0,1,.01))~seq(0,1,.01),xlab=\"Time Difference\",ylab=\"CDF\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\n\nIt looks like somewhere around 15 minutes, a quarter of an hour. But we will find a better answer by finding the root. In the code that follows we want to find where the cdf equals 0.5. The function uniroot() solves the given equations for roots so we want to put in the cdf minus 0.5. In other words, uniroot() solves \\(f(x)=0\\) for x.\n\nuniroot(function(x)z_cdf(x)-.5,c(.25,35))$root\n\n[1] 0.2916077\n\n\nSo it is actually 0.292 hours, 17.5 minutes. So round up and wait 18 minutes.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "16-Transformations.html#homework-problems",
    "href": "16-Transformations.html#homework-problems",
    "title": "16  Transformations",
    "section": "16.3 Homework Problems",
    "text": "16.3 Homework Problems\n\nLet \\(X\\) be a random variable and let \\(g\\) be a function. By this point, it should be clear that \\(\\mbox{E}[g(X)]\\) is not necessarily equal to \\(g(\\mbox{E}[X])\\).\n\nLet \\(X\\sim \\textsf{Expon}(\\lambda=0.5)\\) and \\(g(X)=X^2\\). We know that \\(\\mbox{E}(X)=\\frac{1}{0.5}=2\\) so \\(g(\\mbox{E}(X))=\\mbox{E}(X)^2=4\\). Use R to find \\(\\mbox{E}[g(X)]\\). Make use of the fact that R has rexp() built into it, so you don’t have to create your own random variable generator.\n\nLet \\(X\\sim \\textsf{Binom}(n,\\pi)\\). What is the pmf for \\(Y = X+3\\)? Make sure you specify the domain of \\(Y\\). [Note, we have used \\(p\\) for the probability of success in a binomial distribution in past chapters but some references use \\(\\pi\\) instead.]\nLet \\(X\\sim \\textsf{Expon}(\\lambda)\\). Let \\(Y=X^2\\). Find the pdf of \\(Y\\).\nOPTIONAL: In exercise 3, you found the pdf of \\(Y=X^2\\) when \\(X\\sim \\textsf{Expon}(\\lambda)\\). Rearrange the pdf to show that \\(Y\\sim \\textsf{Weibull}\\) and find the parameters of that distribution.\nYou are on a team of two. You are both tasked to complete an exercise. The time it takes you, \\(T_1\\), and likewise, your teammate, \\(T_2\\), to complete the exercise are independent random variables. Exercise completion time, in minutes, is distributed with the following pdf:\n\n\\[\nf_T(t)= \\frac{-t}{200}+\\frac{3}{20}; 10 \\leq t \\leq30\n\\]\nFigure 16.6 is a plot of the pdf.\n\n\n\n\n\n\n\n\nFigure 16.6: pdf of \\(T\\)\n\n\n\n\n\nWe want to find the probability our combined time is less than 40 minutes, \\(\\mbox{P}(T_1 + T_2 &lt; 40)\\). We will solve this in steps in this problem. We are going to use a computational method because the mathematics is long and algebra intensive. You are welcome to try a mathematical solution if you like but we will not provide a mathematical solution.\n\nUse the integrate() function to confirm this is a valid pdf.\nFind the cdf of \\(T\\) mathematically.\nTo use the cdf to simulate random variables from this distribution, we need the inverse of the cdf which means we have to solve a quadratic equation. We can do this mathematically or just use the function uniroot(). So first, we will make sure we understand how to use uniroot().\n\nAs a check, we know the median of the distribution is approximately 15.857. Here is code to show that 15.857 is approximately the median. We are integrating the pdf from 10 to 15.857 to confirm that this is 0.5.\n\nintegrate(function(x)-x/200+3/20,10,15.857)\n\n0.4999389 with absolute error &lt; 5.6e-15\n\n\nUse uniroot() and your cdf to confirm that 15.857 is the median.\n\nWe will create a function to take a random uniform variable on the interval \\([0,1]\\) and return a value of our random variable, \\(T\\), exercise time. We can then use this function to simulate each of the exercise times and then create a new random variable that is the sum. Complete the R code and check that it returns the median.\n\nT &lt;- function(y){\n  uniroot(function(x)\"YOUR CDF HERE as a function of x\"-y,c(10,30))$root\n}\nWe made it a function of \\(y\\) since we are using \\(x\\) in our cdf. There are two function calls here, can you see why?\n\nVectorize the function you just created using the Vectorize() function. Check that it is vectorized by entering c(.5,.75) into the function. You should get 15.85786 20.00000 as an output.\nWe are ready. Let’s create a data frame with 10000 simulation for our time and another 10000 for our teammates. Remember to set a seed. At this point it may be hard to remember what we have done. The function we created takes as input a vector of random number from a uniform distribution and then applies the inverse cdf to generate a random sample from our given pdf.\nDo a numerical summary of the data and plot a density plot of your exercise times to give us confidence that we simulated the process correctly.\nCreate the new variable that is the sum of the two exercise time and then find the probability that the sum is less than 40.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "16-Transformations.html#solutions-manual",
    "href": "16-Transformations.html#solutions-manual",
    "title": "16  Transformations",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html",
    "href": "17-Estimation-Methods.html",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#objectives",
    "href": "17-Estimation-Methods.html#objectives",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "Obtain a method of moments estimate of a parameter or set of parameters.\n\nGiven a random sample from a distribution, obtain the likelihood function.\n\nObtain a maximum likelihood estimate of a parameter or set of parameters.\n\nDetermine if an estimator is unbiased.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#transition",
    "href": "17-Estimation-Methods.html#transition",
    "title": "17  Estimation Methods",
    "section": "17.2 Transition",
    "text": "17.2 Transition\nWe started this book with descriptive models of data and then moved onto probability models. In these probability models, we have been characterizing experiments and random processes using both theory and simulation. These models are using a model about a random event to make decisions about data. These models are about the population and are used to make decisions about samples and data. For example, suppose we flip a fair coin 10 times, and record the number of heads. The population is the collection of all possible outcomes of this experiment. In this case, the population is infinite, as we could run this experiment repeatedly without limit. If we assume, model, the number of heads as a binomial distribution, we know the exact distribution of the outcomes. For example, we know that exactly 24.61% of the time, we will obtain 5 heads out of 10 flips of a fair coin. We can also use the model to characterize the variance, that is when it does not equal 5 and how much different from 5 it will be. However, these probability models are highly dependent on the assumptions and the values of the parameters.\nFrom this point on in the book, we will focus on statistical models. Statistical models describe one or more variables and their relationships. We use these models to make decisions about the population, to predict future outcomes, or both. Often we don’t know the true underlying process; all we have is a sample of observations and perhaps some context. Using inferential statistics, we can draw conclusions about the underlying process. For example, suppose we are given a coin and we don’t know whether it is fair. So, we flip it a number of times to obtain a sample of outcomes. We can use that sample to decide whether the coin could be fair.\nIn some sense, we’ve already explored some of these concepts. In our simulation examples, we have drawn observations from a population of interest and used those observations to estimate characteristics of another population or segment of the experiment. For example, we explored random variable \\(Z\\), where \\(Z=|X - Y|\\) and \\(X\\) and \\(Y\\) were both uniform random variables. Instead of dealing with the distribution of \\(Z\\) directly, we simulated many observations from \\(Z\\) and used this simulation to describe the behavior of \\(Z\\).\nStatistical models and probability models are not separate. In statistical models we find relationships, the explained portion of variation, and use probability models for the remaining random variation. In Figure 17.1, we demonstrate this relationship between the two types of models. In the first part of our studies, we will use univariate data in statistical models to estimate the parameters of a probability model. From there we will develop more sophisticated models to include multivariate models.\n\n\n\n\n\n\n\n\nFigure 17.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we don’t know the underlying process, and must infer based on representative samples.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#estimation",
    "href": "17-Estimation-Methods.html#estimation",
    "title": "17  Estimation Methods",
    "section": "17.3 Estimation",
    "text": "17.3 Estimation\nRecall that in probability models, we have complete information about the population and we use that to describe the expected behavior of samples from that population. In statistics we are given a sample from a population about which we know little or nothing.\nIn this chapter, we will discuss estimation. Given a sample, we would like to estimate population parameters. There are several ways to do that. We will discuss two methods: method of moments and maximum likelihood.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#method-of-moments",
    "href": "17-Estimation-Methods.html#method-of-moments",
    "title": "17  Estimation Methods",
    "section": "17.4 Method of Moments",
    "text": "17.4 Method of Moments\nRecall earlier we discussed moments. We can refer to \\(\\mbox{E}(X) = \\mu\\) as the first moment or mean. Further, we can refer to \\(\\mbox{E}(X^k)\\) as the \\(k\\)th central moment and \\(\\mbox{E}[(X-\\mu)^k]\\) as the \\(k\\) moment around the mean. The second moment around the mean is also known as variance. It is important to point out that these are POPULATION moments and are typically some function of the parameters of a probability model.\nSuppose \\(X_1,X_2,...,X_n\\) is a sequence of independent, identically distributed random variables with some distribution and parameters \\(\\boldsymbol{\\theta}\\). When provided with a random sample of data, we will not know the population moments. However, we can obtain sample moments. The \\(k\\)th central sample moment is denoted by \\(\\hat{\\mu}_k\\) and is given by \\[\n\\hat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n x_i^k\n\\]\nThe \\(k\\)th sample moment around the mean is denoted by \\(\\hat{\\mu}'_k\\) and is given by \\[\n\\hat{\\mu}'_k=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^k\n\\]\nThe value \\(\\hat{\\mu}\\) is read “mu-hat”. The hat denotes that the value is an estimate.\nWe can use the sample moments to estimate the population moments since the population moments are usually functions of a distribution’s parameters, \\(\\boldsymbol{\\theta}\\). Thus, we can solve for the parameters to obtain method of moments estimates of \\(\\boldsymbol{\\theta}\\).\nThis is all technical, so let’s look at an example.\n\nExample:\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d., independent and identically distributed, sample from a uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), and we don’t know \\(\\theta\\). That is, our data consists of positive random numbers but we don’t know the upper bound. Find the method of moments estimator for \\(\\theta\\), the upper bound.\n\nWe know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{E}(X)=\\frac{a+b}{2}\\). So, in this case, \\(\\mbox{E}(X)={\\theta \\over 2}\\). This is the first population moment. We can estimate this with the first sample moment, which is just the sample mean: \\[\n\\hat{\\mu}_1=\\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x}\n\\]\nOur best guess for the first population moment (\\(\\theta/2\\)) is the first sample moment (\\(\\bar{x}\\)). From a common sense perspective, we are hoping that the sample moment will be close in value to the population moment, so we can set them equal and solve for the unknown population parameter. This is essentially what we were doing in our simulations of probability models. Solving for \\(\\theta\\) yields our method of moments estimator for \\(\\theta\\): \\[\n\\hat{\\theta}_{MoM}=2\\bar{x}\n\\]\nNote that we could have used the second moments about the mean as well. This is less intuitive but still applicable. In this case we know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{Var}(X)=\\frac{(b - a)^2}{12}\\). So, in this case, \\(\\mbox{Var}(X)=\\frac{\\theta ^2}{ 12}\\). We use the second sample moment about the mean \\(\\hat{\\mu}'_2=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^2\\) which is not quite the sample variance. In fact, the sample variance is related to the second sample moment about the mean by \\(\\hat{\\mu}'_2 = s^2 \\frac{n}{n-1}\\). Setting the population moment and sample moment equal and solving we get\n\\[\n\\hat{\\theta}_{MoM}=\\sqrt{\\frac{12n}{n-1}}s\n\\]\nTo decide which is better we need a criteria of comparison. This is beyond the scope of this book, but some common criteria are unbiased and minimum variance.\nThe method of moments can be used to estimate more than one parameter as well. We simply would have to incorporate higher order moments.\n\nExample:\nSuppose we take an i.i.d. sample from the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Find method of moments estimates of \\(\\mu\\) and \\(\\sigma\\).\n\nFirst, we remember that we know two population moments for the normal distribution: \\[\n\\mbox{E}(X)=\\mu \\hspace{1cm} \\mbox{Var}(X)=\\mbox{E}[(X-\\mu)^2]=\\sigma^2\n\\]\nSetting these equal to the sample moments yields: \\[\n\\hat{\\mu}_{MoM}=\\bar{x} \\hspace{1cm} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\nAgain, we notice that the estimate for \\(\\sigma\\) is different from sample standard deviation discussed earlier in the book. The reason for this is a property of estimators called unbiased. Notice that if we treat the data points as random variables then the estimators are random variables. We can then take the expected value of the estimator and if this equals the parameter being estimated, then it is unbiased. Mathematically, this is written \\[\nE(\\hat{\\theta})=\\theta\n\\] Unbiased is not a required property for an estimator, but many practitioners find it desirable. In words, unbiased means that on average the estimator will equal the true value. Sample variance using \\(n-1\\) in the denominator is an unbiased estimate of the population variance.\n\nExercise:\nYou shot 25 free throws and make 21. Assuming a binomial model fits. Find an estimate of the probability of making a free throw.\n\nThere are two ways to approach this problem depending on how we define the random variable. In the first case we will use a binomial random variable, \\(X\\) the number of made free throws in 25 attempts. In this case, we only ran the experiment once and have the observed result of 21. Recall for the binomial \\(E(X)=np\\) where \\(n\\) is the number of attempts and \\(p\\) is the probability of success. The sample mean is 21 since we only have one data point. Using the method of moments, we set the first population mean equal to the first sample mean \\(np=\\frac{\\sum{x_i}}{m}\\), notice \\(n\\) is the number of trials 25 and \\(m\\) is the number of data points 1, or \\(25 \\hat{p} = 21\\). Thus \\(\\hat{p} = \\frac{21}{25}\\).\nA second approach is to let \\(X_i\\) be a single free throw, we have a Bernoulli random variable. This variable takes on the values of 0 if we miss and 1 if we make the free throw. Thus we have 25 data points. For a Bernoulli random variable \\(E(X)=p\\). The sample is \\(\\bar{x} = \\frac{21}{25}\\). Using the method of moments, we set the sample mean equal to the population mean. We have \\(E(X) = \\hat{p} = \\bar{x} = \\frac{21}{25}\\). This is a natural estimate; we estimate our probability of success as the number of made free throws divided by the number of shots. As a side note, this is an unbiased estimator since \\[\nE(\\hat{p})=E\\left( \\sum{\\frac{X_i}{n}} \\right)\n\\]\n\\[\n=  \\sum{E\\left( \\frac{X_i}{n} \\right)}= \\sum{ \\frac{E\\left(X_i\\right)}{n}}=\\sum{\\frac{p}{n}}=\\frac{np}{n}=p\n\\]",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#maximum-likelihood",
    "href": "17-Estimation-Methods.html#maximum-likelihood",
    "title": "17  Estimation Methods",
    "section": "17.5 Maximum likelihood",
    "text": "17.5 Maximum likelihood\nRecall that using method of moments involves finding values of the parameters that cause the population moments to be equal to the sample moments. Solving for the parameters yields method of moments estimates.\nNext we will discuss one more estimation method, maximum likelihood estimation. In this method, we are finding values of parameters that would make the observed data most “likely”. In order to do this, we first need to introduce the likelihood function.\n\n17.5.1 Likelihood Function\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d. random sample from a distribution with mass/density function \\(f_{X}(x;\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) are the parameters. Let’s take a second to explain this notation. We are using a bold symbol for \\(\\boldsymbol{\\theta}\\) to indicate it is a vector, that it can be one or more values. However, in the pmf/pdf \\(x\\) is not bold since it is a scalar variable. In our probability models we know \\(\\boldsymbol{\\theta}\\) and then use to model to make decision about the random variable \\(X\\).\nThe likelihood function is denoted as \\(L(\\boldsymbol{\\theta};x_1,x_2,...,x_n) = L(\\boldsymbol{\\theta};\\boldsymbol{x})\\). Now we have multiple instances of the random variable, we use \\(\\boldsymbol{x}\\). Since our random sample is i.i.d., independent and identically distributed, we can write the likelihood function as a product of the pmfs/pdfs: \\[\nL(\\boldsymbol{\\theta};\\boldsymbol{x})=\\prod_{i=1}^n f_X(x_i;\\boldsymbol{\\theta})\n\\]\nThe likelihood function is really the pmf/pdf except instead of the variables being random and the parameter(s) fixed, the values of the variable are known and the parameter(s) are unknown. A note on notation, we are using the semicolon in the pdf and likelihood function to denote what is known or given. In the pmf/pdf the parameters are known and thus follow the semicolon. The opposite is the case in the likelihood function.\nLet’s do an example to help understand these ideas.\n\nExample:\nSuppose we are presented with a coin and are unsure of its fairness. We toss the coin 50 times and obtain 18 heads and 32 tails. Let \\(\\pi\\) be the probability that a coin flip results in heads, we could use \\(p\\) but we are getting you used to the two different common ways to represent a binomial parameter. What is the likelihood function of \\(\\pi\\)?\n\nThis is a binomial process, but each individual coin flip can be thought of as a Bernoulli experiment. That is, \\(x_1,x_2,...,x_{50}\\) is an i.i.d. sample from \\(\\textsf{Binom}(1,\\pi)\\) or, in other words, \\(\\textsf{Bernoulli}(\\pi)\\). Each \\(x_i\\) is either 1 or 0. The pmf of \\(X\\), a Bernoulli random variable, is simply: \\[\nf_X(x;\\pi)= \\binom{1}{x} \\pi^x(1-\\pi)^{1-x} = \\pi^x(1-\\pi)^{1-x}\n\\]\nNotice this makes sense\n\\[\nf_X(1)=P(X=1)= \\pi^1(1-\\pi)^{1-1}=\\pi\n\\]\nand\n\\[\nf_X(0)=P(X=0)= \\pi^0(1-\\pi)^{1-0}=(1-\\pi)\n\\]\nGeneralizing for any sample size \\(n\\), the likelihood function is: \\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{n} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{\\sum_{i=1}^{n} x_i}(1-\\pi)^{n-\\sum_{i=1}^{n} x_i}\n\\]\nFor our example \\(n=50\\) and the\n\\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{50} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{18}(1-\\pi)^{32}\n\\]\nwhich makes sense because we had 18 successes, heads, and 32 failures, tails. The likelihood function is a function of the unknown parameter \\(\\pi\\).\n\n\n17.5.2 Maximum Likelihood Estimation\nOnce we have a likelihood function \\(L(\\boldsymbol{\\theta},\\boldsymbol{x})\\), we need to figure out which value of \\(\\boldsymbol{\\theta}\\) makes the data most likely. In other words, we need to maximize \\(L\\) with respect to \\(\\boldsymbol{\\theta}\\).\nMost of the time (but not always), this will involve simple optimization through calculus (i.e., take the derivative with respect to the parameter, set to 0 and solve for the parameter). When maximizing the likelihood function through calculus, it is often easier to maximize the log of the likelihood function, denoted as \\(l\\) and often referred to as the “log-likelihood function”: \\[\nl(\\boldsymbol{\\theta};\\boldsymbol{x})= \\log L(\\boldsymbol{\\theta};\\boldsymbol{x})\n\\] Note that since logarithm is one-to-one, onto and increasing, maximizing the log-likelihood function is equivalent to maximizing the likelihood function, and the maximum will occur at the same values of the parameters. We are using log because now we can take the derivative of a sum instead of a product, thus making it much easier.\n\nExample:\nContinuing our example. Find the maximum likelihood estimator for \\(\\pi\\).\n\nRecall that our likelihood function is \\[\nL(\\pi;\\boldsymbol{x})= \\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\n\\]\nFigure 17.2 is a plot of the likelihood function as a function of the unknown parameter \\(\\pi\\).\n\n\n\n\n\n\n\n\nFigure 17.2: Likelihood function for 18 successes in 50 trials\n\n\n\n\n\nBy visual inspection, the value of \\(\\pi\\) that makes our data most likely, maximizes the likelihood function, is something a little less than 0.4, the actual value is 0.36 as indicated by the blue line in Figure 17.2.\nTo maximize by mathematical methods, we need to take the derivative of the likelihood function with respect to \\(\\pi\\). We can do this because the likelihood function is a continuous function. Even though the binomial is a discrete random variable, its likelihood is a continuous function.\nWe can find the derivative of the likelihood function by applying the product rule: \\[\n{\\,\\mathrm{d}L(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi} = \\left(\\sum x_i\\right) \\pi^{\\sum x_i -1}(1-\\pi)^{n-\\sum x_i} + \\pi^{\\sum x_i}\\left(\\sum x_i -n\\right)(1-\\pi)^{n-\\sum x_i -1}\n\\]\nWe could simplify this, set to 0, and solve for \\(\\pi\\). However, it may be easier to use the log-likelihood function: \\[\nl(\\pi;\\boldsymbol{x})=\\log L(\\pi;\\boldsymbol{x})= \\log \\left(\\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\\right) = \\sum x_i \\log \\pi + (n-\\sum x_i)\\log (1-\\pi)\n\\]\nNow, taking the derivative does not require the product rule: \\[\n{\\,\\mathrm{d}l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi}= {\\sum x_i \\over \\pi} - {n-\\sum x_i\\over (1-\\pi)}\n\\]\nSetting equal to 0 yields: \\[\n{\\sum x_i \\over \\pi} ={n-\\sum x_i\\over (1-\\pi)}\n\\]\nSolving for \\(\\pi\\) yields \\[\n\\hat{\\pi}_{MLE}={\\sum x_i \\over n}\n\\]\nNote that technically, we should confirm that the function is concave down at our critical value, ensuring that \\(\\hat{\\pi}_{MLE}\\) is, in fact, a maximum: \\[\n{\\,\\mathrm{d}^2 l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi^2}= {-\\sum x_i \\over \\pi^2} - {n-\\sum x_i\\over (1-\\pi)^2}\n\\]\nThis value is negative for all relevant values of \\(\\pi\\), so \\(l\\) is concave down and \\(\\hat{\\pi}_{MLE}\\) is a maximum.\nIn the case of our example (18 heads out of 50 trials), \\(\\hat{\\pi}_{MLE}=18/50=0.36\\).\nThis seems to make sense. Our best guess for the probability of heads is the number of observed heads divided by our number of trials. That was a great deal of algebra and calculus for what appears to be an obvious answer. However, in more difficult problems, it is not as obvious what to use for a MLE.\n\n\n17.5.3 Numerical Methods\nWhen obtaining MLEs, there are times when analytical methods (calculus) are not feasible or not possible. In the Pruim book (Pruim 2011), there is a good example regarding data from Old Faithful at Yellowstone National Park. We need to load the fastR2 package for this example.\n\nlibrary(fastR2)\n\nThe faithful data set is preloaded into R and contains 272 observations of 2 variables: eruption time in minutes and waiting time until next eruption. If we plot eruption durations, we notice that the distribution appears bimodal, see Figure 17.3.\n\n\n\n\n\n\n\n\nFigure 17.3: Histogram of eruption durations of Old Faithful.\n\n\n\n\n\nWithin each section, the distribution appears somewhat bell-curve-ish so we’ll model the eruption time with a mixture of two normal distributions. In this mixture, a proportion \\(\\alpha\\) of our eruptions belong to one normal distribution and the remaining \\(1-\\alpha\\) belong to the other normal distribution. The density function of eruptions is given by: \\[\n\\alpha f(x;\\mu_1,\\sigma_1)+(1-\\alpha)f(x;\\mu_2,\\sigma_2)\n\\]\nwhere \\(f\\) is the pdf of the normal distribution with parameters specified.\nWe have five parameters to estimate: \\(\\alpha, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). Obviously, estimation through differentiation is not feasible and thus we will use numerical methods. This code is less in the spirit of tidyverse but we want you to see the example. Try to work your way through the code below:\n\n# Define function for pdf of eruptions as a mixture of normals\ndmix&lt;-function(x,alpha,mu1,mu2,sigma1,sigma2){\n  if(alpha &lt; 0) dnorm(x,mu2,sigma2)\n  if(alpha &gt; 1) dnorm(x,mu1,sigma1)\n  if(alpha &gt;= 0 && alpha &lt;=1){\n    alpha*dnorm(x,mu1,sigma1)+(1-alpha)*dnorm(x,mu2,sigma2)\n  }\n}\n\nNext write a function for the log-likelihood function. R is a vector based programming language so we send theta into the function as a vector argument.\n\n# Create the log-likelihood function\nloglik&lt;-function(theta,x){\n  alpha=theta[1]\n  mu1=theta[2]\n  mu2=theta[3]\n  sigma1=theta[4]\n  sigma2=theta[5]\n  density&lt;-function(x){\n    if(alpha&lt;0) return (Inf)\n    if(alpha&gt;1) return (Inf)\n    if(sigma1&lt;0) return (Inf)\n    if(sigma2&lt;0) return (Inf)\n    dmix(x,alpha,mu1,mu2,sigma1,sigma2)\n  }\n  sum(log(sapply(x,density)))\n}\n\nFind the sample mean and standard deviation of the eruption data to use as starting points in the optimization routine.\n\nm&lt;-mean(faithful$eruptions)\ns&lt;-sd(faithful$eruptions)\n\nUse the function nlmax() to maximize the non-linear log-likelihood function.\n\nmle&lt;-nlmax(loglik,p=c(0.5,m-1,m+1,s,s),x=faithful$eruptions)$estimate\nmle\n\n[1] 0.3484040 2.0186065 4.2733410 0.2356208 0.4370633\n\n\nSo, according to our MLEs, about 34.84% of the eruptions belong to the first normal distribution (the one on the left). Furthermore the parameters of that first distribution are a mean of 2.019 and a standard deviation of 0.236. Likewise, 65.16% of the eruptions belong to the second normal with mean of 4.27 and standard deviation of 0.437.\nPlotting the density atop the histogram shows a fairly good fit:\n\ndmix2&lt;-function(x) dmix(x,mle[1],mle[2],mle[3],mle[4],mle[5])\n#y_old&lt;-dmix2(seq(1,6,.01))\n#x_old&lt;-seq(1,6,.01)\n#dens_data&lt;-data.frame(x=x_old,y=y_old)\n#faithful%&gt;%\n#gf_histogram(~eruptions,fill=\"cyan\",color = \"black\") %&gt;%\n#  gf_curve(y~x,data=dens_data)%&gt;%\n#  gf_theme(theme_bw()) %&gt;%\n#  gf_labs(x=\"Duration in minutes\",y=\"Count\") \nhist(faithful$eruptions,breaks=40,freq=F,main=\"\",xlab=\"Duration in minutes.\")\ncurve(dmix2,from=1,to=6,add=T)\n\n\n\n\nHistogram of eruption duration with estimated mixture of normals plotted on top.\n\n\n\n\nThis is a fairly elaborate example but it is cool. You can see the power of the method and the software.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#homework-problems",
    "href": "17-Estimation-Methods.html#homework-problems",
    "title": "17  Estimation Methods",
    "section": "17.6 Homework Problems",
    "text": "17.6 Homework Problems\n\nIn the Notes, we found that if we take a sample from the uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), the method of moments estimate of \\(\\theta\\) is \\(\\hat{\\theta}_{MoM}=2\\bar{x}\\). Suppose our sample consists of the following values: \\[\n0.2 \\hspace{0.4cm} 0.9 \\hspace{0.4cm} 1.9 \\hspace{0.4cm} 2.2 \\hspace{0.4cm} 4.7 \\hspace{0.4cm} 5.1\n\\]\n\n\n\nWhat is \\(\\hat{\\theta}_{MoM}\\) for this sample?\n\nWhat is an wrong with this estimate?\n\nShow that this estimator is unbiased.\n\nADVANCED: Use simulation in R to find out how often the method of moment estimator is less the maximum observed value, (\\(\\hat{\\theta}_{MoM} &lt; \\max x\\)). Report an answer for various sizes of samples. You can just pick an arbitrary value for \\(\\theta\\) when you sample from the uniform. However, the minimum must be 0.\n\n\n\nLet \\(x_1,x_2,...,x_n\\) be a simple random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\).\nLet \\(x_1,x_2,...,x_n\\) be an i.i.d. random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\).\nIt is mathematically difficult to determine if the estimators found in questions 2 and 3 are unbiased. Since the sample mean is in the denominator; mathematically we may have to work with the joint pdf. So instead, use simulation to get an sense of whether the method of moments estimator for the exponential distribution is unbiased.\nFind a maximum likelihood estimator for \\(\\theta\\) when \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare this to the method of moments estimator we found. Hint: Do not take the derivative of the likelihood function.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#solutions-manual",
    "href": "17-Estimation-Methods.html#solutions-manual",
    "title": "17  Estimation Methods",
    "section": "Solutions Manual",
    "text": "Solutions Manual\n\n\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html",
    "href": "18-Inference-Case-Study.html",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#objectives",
    "href": "18-Inference-Case-Study.html#objectives",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: point estimate, null hypothesis, alternative hypothesis, hypothesis test, randomization, permutation test, test statistic, and \\(p\\)-value.\nConduct a hypothesis test using a randomization test, to include all 4 steps.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#introduction",
    "href": "18-Inference-Case-Study.html#introduction",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nWe now have the foundation to move on to statistical modeling, both inferential and prediction. First we will begin with inference, where we use the ideas of estimation and the variance of estimates to make decisions about the population. We will also briefly introduce the ideas of prediction. Then in the final block of material, we will examine some common linear models and use them for both prediction and inference.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#foundation-for-inference",
    "href": "18-Inference-Case-Study.html#foundation-for-inference",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.3 Foundation for inference",
    "text": "18.3 Foundation for inference\nSuppose a professor randomly splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)?\nWhile the proportions would probably be close to each other, they are probably not exactly the same. We would probably observe a small difference due to chance.\n\nExercise:\nIf we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?1\n\nStudying randomness of this form is a key focus of statistical modeling. In this block, we’ll explore this type of randomness in the context of several applications, and we’ll learn new tools and ideas that can be applied to help make decisions from data.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#randomization-case-study-gender-discrimination",
    "href": "18-Inference-Case-Study.html#randomization-case-study-gender-discrimination",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.4 Randomization case study: gender discrimination",
    "text": "18.4 Randomization case study: gender discrimination\nWe consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.2 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?”\n\n18.4.1 Variability within data\nThe participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects.\n\nExercise:\nIs this an observational study or an experiment? How does the type of study impact what can be inferred from the results?3\n\nFor each supervisor, we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in the table below, we would like to evaluate whether females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against.\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Decision}\\\\\n& & \\mbox{Promoted} & \\mbox{Not Promoted} & \\mbox{Total}  \\\\\n& \\hline \\mbox{male} & 21 & 3 & 24  \\\\\n\\textbf{Gender}& \\mbox{female} & 14 & 10 & 24  \\\\\n& \\mbox{Total} & 35 & 13 & 48  \\\\\n\\end{array}\n\\]\n\nThought Question:\nStatisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against?\n\nThe large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. Most people come to this conclusion because they think these sample statistics are the actual population parameters. We cannot yet be sure if the observed difference represents discrimination or is just from random variability. Generally, there is fluctuation in sample data; if we conducted the experiment again, we would likely get different values. We also wouldn’t expect the sample proportions for males and females to be exactly equal, even if the truth was that the promotion decisions were independent of gender. To make a decision, we must understand the random variability and use it to compare with the observed difference.\nThis question is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. The table shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). This observed difference is what we call a point estimate of the true effect. The point estimate of the difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance.\nWhat would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%.\nConsider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files.\nWe will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Again, we will do this by determining how much difference in promotion rates would happen by random variation and compare this with the observed difference. We will make a decision based on probability considerations.\n\n\n18.4.2 Simulating the study\nThe table of data shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers’ decisions were independent of gender, that is the null hypothesis is true. Then, if we conducted the experiment again with a different random assignment of files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers’ decisions had been independent of gender but we had distributed the files differently.4 We will walk through the steps next.\nFirst let’s import the data.\n\ndiscrim &lt;- read_csv(\"data/discrimination_study.csv\")\n\nLet’s inspect the data set.\n\ninspect(discrim)\n\n\ncategorical variables:  \n      name     class levels  n missing\n1   gender character      2 48       0\n2 decision character      2 48       0\n                                   distribution\n1 female (50%), male (50%)                     \n2 promoted (72.9%), not_promoted (27.1%)       \n\n\nLet’s look at a table of the data, showing gender versus decision.\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   not_promoted promoted Total\n  female           10       14    24\n  male              3       21    24\n  Total            13       35    48\n\n\nLet’s do some categorical data cleaning. To get the tally() results to look like our initial table, we need to change the variables from characters to factors and reorder the levels. By default, factor levels are ordered alphabetically, but we want promoted and male to appear as the first levels in the table.\nWe will use mutate_if() to convert character variables to factors and fct_relevel() to reorder the levels.\n\ndiscrim &lt;- discrim %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  mutate(gender = fct_relevel(gender, \"male\"),\n         decision = fct_relevel(decision, \"promoted\"))\n\n\nhead(discrim)\n\n# A tibble: 6 × 2\n  gender decision    \n  &lt;fct&gt;  &lt;fct&gt;       \n1 female not_promoted\n2 female not_promoted\n3 male   promoted    \n4 female promoted    \n5 female promoted    \n6 female promoted    \n\n\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   promoted not_promoted Total\n  male         21            3    24\n  female       14           10    24\n  Total        35           13    48\n\n\nNow that we have the data in the form that we want, we are ready to conduct the permutation test, a simulation of what would have happened if the bankers’ decisions had been independent of gender but we had distributed the files differently. To think about this simulation, imagine we actually had the personnel files. We thoroughly shuffle 48 personnel files, 24 labeled male and 24 labeled female, and deal these files into two stacks. We will deal 35 files into the first stack, which will represent the 35 supervisors who recommended promotion. The second stack will have 13 files, and it will represent the 13 supervisors who recommended against promotion. That is, we keep the same number of files in the promoted and not_promoted categories, and imagine simply shuffling the male and female labels around. Remember that the files are identical except for the listed gender. This simulation then assumes that gender is not important and, thus, we can randomly assign the files to any of the supervisors. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female candidates who were promoted. Since we don’t actually physically have the files, we will do this shuffle via computer code.\nSince the randomization of files in this simulation is independent of the promotion decisions, any difference in the two fractions is entirely due to chance. The following code shows the results of such a simulation.\n\nset.seed(101)\ntally(~shuffle(gender) + decision, discrim, margins = TRUE)\n\n               decision\nshuffle(gender) promoted not_promoted Total\n         male         18            6    24\n         female       17            7    24\n         Total        35           13    48\n\n\nThe shuffle() function randomly rearranges the gender column while keeping the decision column the same. It is really a sampling without replacement. That is, we randomly sample 35 personnel files to be promoted and the other 13 personnel files are not_promoted.\n\nExercise: What is the difference in promotion rates between the two simulated groups? How does this compare to the observed difference, 29.2%, from the actual study?5\n\nCalculating by hand will not help in a simulation, so we must write a function or use an existing one. We will use diffprop() from the mosaic package. The code to find the difference for the original data is:\n\n(obs &lt;- diffprop(decision ~ gender, data = discrim))\n\n  diffprop \n-0.2916667 \n\n\nNotice that this is subtracting the proportion of males promoted from the proportion of females promoted. This does not impact our results as this is an arbitrary decision. We just need to be consistent in our analysis. If we prefer to use positive values, we can adjust the order easily.\n\ndiffprop(decision ~ fct_relevel(gender, \"female\"), data = discrim)\n\n diffprop \n0.2916667 \n\n\nNotice what we have done here; we developed a single value metric to measure the relationship between gender and decision. This single value metric is called the test statistic. We could have used a number of different metrics, to include just the difference in number of promoted males and females. The key idea in hypothesis testing is that once you decide on a test statistic, you need to find the distribution of that test statistic, assuming the null hypothesis is true.\n\n\n18.4.3 Checking for independence\nWe computed one possible difference under the null hypothesis in the exercise above, which represents one difference due to chance. Repeating the simulation, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. That is, the difference if there really is no relationship between gender and the promotion decision. We are using a simulation when there is actually a finite number of permutations of the gender label. From Chapter @ref(PROBRULES) on counting, we have 48 labels of which 24 are male and 24 are female. Thus the total number of ways to arrange the labels differently is:\n\\[\n\\frac{48!}{24!\\cdot24!} \\approx 3.2 \\cdot 10^{13}\n\\]\n\nfactorial(48) / (factorial(24)*factorial(24))\n\n[1] 3.22476e+13\n\n\nAs is often the case, the number of all possible permutations is too large to find by hand or even via code. Thus, we will use a simulation, a subset of all possible permutations, to approximate the permutation test. Using simulation in this way is called a randomization test.\nLet’s simulate the experiment and plot the simulated values of the difference in the proportions of male and female files recommended for promotion.\n\nset.seed(2022)\nresults &lt;- do(10000)*diffprop(decision ~ shuffle(gender), data = discrim)\n\nIn Figure @ref(fig:teststat1-fig), we will insert a vertical line at the value of our observed difference.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = -0.2916667 ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Difference in proportions\", y = \"Counts\",\n          title = \"Gender discrimination in hiring permutation test\",\n          subtitle = \"Test statistic is difference in promotion for female and male\")\n\n\n\n\nDistribution of test statistic.\n\n\n\n\nNote that the distribution of these simulated differences is centered around 0 and is roughly symmetrical. It is centered on zero because we simulated differences in a way that made no distinction between men and women. This makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation under the assumption of the null hypothesis. The histogram also looks like a normal distribution; this is not a coincidence, but a result of the Central Limit Theorem, which we will learn about later in this block.\n\nExample:\nHow often would you observe a difference as extreme as -29.2% (-0.292) according to the figure? (Often, sometimes, rarely, or never?)\n\nIt appears that a difference as extreme as -29.2% due to chance alone would only happen rarely. We can estimate the probability using the results object.\n\nresults %&gt;%\n  summarise(p_value = mean(diffprop &lt;= obs))\n\n  p_value\n1  0.0257\n\n\nIn our simulations, only 2.6% of the simulated test statistics were less than or equal to the observed test statistic, as or more extreme relative to the null hypothesis. Such a low probability indicates that observing such a large difference in proportions from chance alone is rare. This probability is known as a \\(p\\)-value. The \\(p\\)-value is a conditional probability, the probability of the observed value or more extreme given that the null hypothesis is true.\nWe could have also found the exact \\(p\\)-value using the hypergeometric distribution. We have 13 not_promoted positions, so we could have anywhere between 0 and 13 females not promoted. We observed 10 females not promoted. Thus, the exact \\(p\\)-value from the hypergeometric distribution is the probability of 10 or more females not promoted (as or more extreme than the observed) when we select 13 people from a pool of 24 males and 24 females, and the selection is done without replacement.\n\n1 - phyper(9, 24, 24, 13)\n\n[1] 0.02449571\n\n\nAgain, we see a low probability, only 2.4%, of observing 10 or more females not promoted, given that the null hypothesis is true.\nThe observed difference of -29.2% is a rare (low probability) event if there truly is no impact from listing gender in the personnel files. This provides us with two possible interpretations of the study results, in context of our hypotheses:\n\\(H_0\\): Null hypothesis. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely.\n\\(H_A\\): Alternative hypothesis. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of -29.2%.\nWhen we conduct formal studies, we reject a skeptical position (\\(H_0\\)) if the data strongly conflict with that position.6\nIn our analysis, we determined that there was only a ~ 2% probability of obtaining a test statistic where the difference between female and male promotion proportions was 29.2% or larger assuming gender had no impact. So we conclude the data provide sufficient evidence of gender discrimination against women by the supervisors. In this case, we reject the null hypothesis in favor of the alternative hypothesis.\nStatistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur.\nLet’s summarize what we did in this case study. We had a research question and some data to test the question. We then performed 4 steps:\n\nState the null and alternative hypotheses.\n\nCompute a test statistic.\n\nDetermine the \\(p\\)-value.\n\nDraw a conclusion.\n\nWe decided to use a randomization test, a simulation, to answer the question. When creating a randomization distribution, we attempted to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis.\nWe need to simulate a world in which the null hypothesis is true. If we don’t do this, we won’t be testing our null hypothesis. In our problem, we assumed gender and promotion were independent.\n\nUse the data in the original sample.\nThe original data should shed light on some aspects of the distribution that are not determined by the null hypothesis. For our problem, we used the difference in promotion rates. The data does not give us the distribution direction, but it gives us an idea that there is a large difference.\n\nReflect the way the original data were collected.\nThere were 48 files and 48 supervisors. A total of 35 files were recommended for promotion. We keep this the same in our simulation.\n\nThe remainder of this block expands on the ideas of this case study.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#homework-problems",
    "href": "18-Inference-Case-Study.html#homework-problems",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.5 Homework Problems",
    "text": "18.5 Homework Problems\n\nSide effects of Avandia. Rosiglitazone is the active ingredient in the controversial type~2 diabetes medicine Avandia and has been linked to an increased risk of serious cardiovascular problems such as stroke, heart failure, and death. A common alternative treatment is pioglitazone, the active ingredient in a diabetes medicine called Actos. In a nationwide retrospective observational study of 227,571 Medicare beneficiaries aged 65 years or older, it was found that 2,593 of the 67,593 patients using rosiglitazone and 5,386 of the 159,978 using pioglitazone had serious cardiovascular problems. These data are summarized in the contingency table below.\n\n\\[\n\\begin{array}{cc|ccc} & & &\\textit{Cardiovascular problems}\\\\\n& & \\text{Yes}  & \\text{No} & \\textbf{Total}  \\\\\n& \\hline \\text{Rosiglitazone}   & 2,593 & 65,000        & 67,593 \\\\\n\\textit{Treatment}& \\text{Pioglitazone}     & 5,386     & 154,592   & 159,978  \\\\\n& \\textbf{Total}            & 7,979 & 219,592       & 227,571 \\\\\n\\end{array}\n\\]\nDetermine if each of the following statements is true or false. If false, explain why. The reasoning may be wrong even if the statement’s conclusion is correct. In such cases, the statement should be considered false.\n\nSince more patients on pioglitazone had cardiovascular problems (5,386 vs. 2,593), we can conclude that the rate of cardiovascular problems for those on a pioglitazone treatment is higher.\nThe data suggest that diabetic patients who are taking rosiglitazone are more likely to have cardiovascular problems since the rate of incidence was (2,593 / 67,593 = 0.038) 3.8% for patients on this treatment, while it was only (5,386 / 159,978 = 0.034) 3.4% for patients on pioglitazone.\nThe fact that the rate of incidence is higher for the rosiglitazone group proves that rosiglitazone causes serious cardiovascular problems.\nBased on the information provided so far, we cannot tell if the difference between the rates of incidences is due to a relationship between the two variables or due to chance.\n\n\n\nHeart transplants. The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called was used to indicate whether or not the patient was alive at the end of the study.\nIn the study, of the 34 patients in the control group, 4 were alive at the end of the study. Of the 69 patients in the treatment group, 24 were alive. The contingency table below summarizes these results.\n\n\\[\n\\begin{array}{cc|ccc} & & &\\textit{Group}\\\\\n& & \\text{Control}  & \\text{Treatment}  & \\textbf{Total}  \\\\\n& \\hline \\text{Alive}   & 4     & 24            & 28 \\\\\n\\textit{Outcome}& \\text{Dead}   & 30        & 45            & 75  \\\\\n& \\textbf{Total}            & 34        & 69            & 103\\\\\n\\end{array}\n\\]\nThe data is in a file called Stanford_heart_study.csv. Read the data in and answer the following questions.\n\nWhat proportion of patients in the treatment group and what proportion of patients in the control group died? Note: One approach for investigating whether or not the treatment is effective is to use a randomization technique.\nWhat are the claims being tested? Use the same null and alternative hypothesis notation used in the chapter notes.\nThe paragraph below describes the set up for such approach, if we were to do it without using statistical software. Fill in the blanks with a number or phrase, whichever is appropriate.\n\n\nWe write alive on _______ cards representing patients who were alive at the end of the study, and dead on _______ cards representing patients who were not. Then, we shuffle these cards and split them into two groups: one group of size _______ representing treatment, and another group of size _______ representing control. We calculate the difference between the proportion of cards in the control and treatment groups (control - treatment), this is just so we have positive observed value, and record this value. We repeat this many times to build a distribution centered at _______. Lastly, we calculate the fraction of simulations where the simulated differences in proportions are _______ or _______. If this fraction of simulations, the empirical \\(p\\)-value, is low, we conclude that it is unlikely to have observed such an outcome by chance and that the null hypothesis should be rejected in favor of the alternative.\n\nNext we will perform the simulation and use results to decide the effectiveness of the transplant program.\n\nFind observed value of the test statistic, which we decided to use the difference in proportions.\nSimulate 1000 values of the test statistic by using shuffle() on the variable group.\nPlot distribution of results. Include a vertical line for the observed value. Clean up the plot as if you were presenting to a decision maker.\nFind \\(p\\)-value. Think carefully about what more extreme would mean.\nDecide if the treatment is effective.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#solutions-manual",
    "href": "18-Inference-Case-Study.html#solutions-manual",
    "title": "18  Inferential Thinking Case Study",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#footnotes",
    "href": "18-Inference-Case-Study.html#footnotes",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "We would be assuming that these two variables, side of the room and use of Apple product, are independent, meaning they are unrelated.↩︎\nRosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.↩︎\nThe study is an experiment, as subjects were randomly assigned a male personnel file or a female personnel file. Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.↩︎\nThe test procedure we employ in this section is formally called a permutation test.↩︎\n\\(18/24 - 17/24 = 0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups.↩︎\nThis reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html",
    "href": "19-Hypothesis-Testing-Simulation.html",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#objectives",
    "href": "19-Hypothesis-Testing-Simulation.html#objectives",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "Know and properly use the terminology of a hypothesis test, to include: null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type I error, type II error, false positive, false negative, null distribution, and sampling distribution.\nConduct all four steps of a hypothesis test using randomization.\nDiscuss and explain the ideas of decision errors, one-sided versus two-sided tests, and the choice of a significance level.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#decision-making-under-uncertainty",
    "href": "19-Hypothesis-Testing-Simulation.html#decision-making-under-uncertainty",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.2 Decision making under uncertainty",
    "text": "19.2 Decision making under uncertainty\nAt this point, it is useful to take a look at where we have been in this book and where we are going. We did this in the case study, but we want to discuss it again in a little more detail. We first looked at descriptive models to help us understand our data. This also required us to get familiar with software. We learned about graphical summaries, data collection methods, and summary metrics.\nNext we learned about probability models. These models allowed us to use assumptions and a small number of parameters to make statements about data (a sample) and also to simulate data. We found that there is a close tie between probability models and statistical models. In our first efforts at statistical modeling, we started to use data to create estimates for parameters of a probability model.\nNow we are moving more in depth into statistical models. This is going to tie all the ideas together. We are going to use data from a sample and ideas of randomization to make conclusions about a population. This will require probability models, descriptive models, and some new ideas and terminology. We will generate point estimates for a metric designed to answer the research question and then find ways to determine the variability of the metric. In Figure 19.1, we demonstrate this relationship between probability and statistical models.\n\n\n\n\n\n\n\n\nFigure 19.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen in a sample if we know the underlying process; in statistics, we don’t know the underlying process, and must infer about the population based on representative samples.\n\n\n\n\n\nComputational/Mathematical and hypothesis testing/confidence intervals context\nWe are going to be using data from a sample of the population to make decisions about the population. There are many approaches and techniques for this. In this book, we will be introducing and exploring different approaches; we are establishing foundations. As you can imagine, these ideas are varied, subtle, and at times difficult. We will just be exposing you to the foundational ideas. We want to make sure you understand that to become an accomplished practitioner, you must master the fundamentals and continue to learn the advanced ideas.\nHistorically, there have been two approaches to statistical decision making, hypothesis testing and confidence intervals. At their mathematical foundation, they are equivalent, but, sometimes in practice, they offer different perspectives on the problem. We will learn about both of these approaches.\nThe engines that drive the numeric results of a decision making model are either mathematical or computational. In reality, computational methods have mathematics behind them, and mathematical methods often require computer computations. The real distinction between them is the assumptions we are making about our population. Mathematical solutions typically have stricter assumptions, thus leading to a tractable mathematical solution to the sampling distribution of the test statistic, while computational models relax assumptions but may require extensive computational power. Like all problems, there is a trade off to consider when trying to choose which approach is better. There is no one universal best method. Some methods perform better in certain contexts. It is important to understand that computational methods such as the bootstrap are NOT all you need to know.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#introduction",
    "href": "19-Hypothesis-Testing-Simulation.html#introduction",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.3 Introduction",
    "text": "19.3 Introduction\nIn this chapter we will introduce hypothesis testing. It is really an extension of our last chapter, the case study. We will put more emphasis on terms and core concepts. In this chapter, we will use a computational solution but this will lead us into thinking of mathematical solutions.1 The role of the analyst is always key regardless of the perceived power of the computer. The analyst must take the research question and translate it into a numeric metric for evaluation. The analyst must decide on the type of data and its collection to evaluate the question. The analyst must evaluate the variability in the metric and determine what that means in relation to the original research question. The analyst must propose an answer.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "href": "19-Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.4 Hypothesis testing",
    "text": "19.4 Hypothesis testing\nWe will continue to emphasize the ideas of hypothesis testing through a data-driven example, but also via analogy to the US court system. So let’s begin our journey.\n\nExample:\nYou are annoyed by TV commercials. You suspect that there were more commercials in the basic TV channels, typically the local area channels, than in the premium channels you pay extra for. To test this claim, hypothesis, you want to collect some data and draw a conclusion. How would you collect this data?\n\nHere is one approach: we watch 20 random half hour shows of live TV. Ten of those hours are basic TV and the other ten are premium. In each case, you record the total length of commercials during each show.\n\nExercise: Is this enough data? You decide to have your friends help you, so you actually only watched 5 hours and got the rest of the data from your friends. Is this a problem?\n\nWe cannot determine if this is enough data without some type of subject matter knowledge. First, we need to decide what metric to use to determine if a difference exists (more to come on this). Second, we need to decide how big of a difference, from a practical standpoint, is of interest. Is a loss of 1 minute of TV show enough to say there is a difference? How about 5 minutes? These are not statistical questions, but depend on the context of the problem and often require subject matter expertise to answer. Often, data is collected without thought to these considerations. There are several methods that attempt to answer these questions. They are loosely called sample size calculations. This book will not focus on sample size calculations and will leave it to the reader to learn more from other sources. For the second exercise question, the answer depends on the protocol and operating procedures used. If your friends are trained on how to measure the length of commercials, what counts as an ad, and their skills are verified, then it is probably not a problem to use them to collect data. Consistency in measurement is the key.\nThe file ads.csv contains the data. Let’s read the data into R and start to summarize. Remember to load the appropriate R packages.\n\nads &lt;- read_csv(\"data/ads.csv\")\n\n\nads\n\n# A tibble: 10 × 2\n   basic premium\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  6.95    3.38\n 2 10.0     7.8 \n 3 10.6     9.42\n 4 10.2     4.66\n 5  8.58    5.36\n 6  7.62    7.63\n 7  8.23    4.95\n 8 10.4     8.01\n 9 11.0     7.8 \n10  8.52    9.58\n\n\n\nglimpse(ads)\n\nRows: 10\nColumns: 2\n$ basic   &lt;dbl&gt; 6.950, 10.013, 10.620, 10.150, 8.583, 7.620, 8.233, 10.350, 11…\n$ premium &lt;dbl&gt; 3.383, 7.800, 9.416, 4.660, 5.360, 7.630, 4.950, 8.013, 7.800,…\n\n\nNotice that this data may not be tidy; what does each row represent and is it a single observation? We don’t know how the data was obtained, but if each row represents a different friend who watches one basic and one premium channel, then it is possible this data is tidy. We want each observation to be a single TV show, so let’s clean up, or tidy, our data. Remember to ask yourself “What do I want R to do?” and “What does it need to do this?” We want one column that specifies the channel type and another to specify the total length of the commercials.\nWe need R to put, or pivot, the data into a longer form. We need the function pivot_longer(). For more information type vignette(\"pivot\") at the command prompt in R.\n\nads &lt;- ads %&gt;%\n  pivot_longer(cols = everything(), names_to = \"channel\", values_to = \"length\")\nads\n\n# A tibble: 20 × 2\n   channel length\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 basic     6.95\n 2 premium   3.38\n 3 basic    10.0 \n 4 premium   7.8 \n 5 basic    10.6 \n 6 premium   9.42\n 7 basic    10.2 \n 8 premium   4.66\n 9 basic     8.58\n10 premium   5.36\n11 basic     7.62\n12 premium   7.63\n13 basic     8.23\n14 premium   4.95\n15 basic    10.4 \n16 premium   8.01\n17 basic    11.0 \n18 premium   7.8 \n19 basic     8.52\n20 premium   9.58\n\n\nLooks good. Let’s summarize the data.\n\ninspect(ads)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 channel character      2 20       0\n                                   distribution\n1 basic (50%), premium (50%)                   \n\nquantitative variables:  \n    name   class   min     Q1 median      Q3    max    mean       sd  n missing\n1 length numeric 3.383 7.4525  8.123 9.68825 11.016 8.03215 2.121412 20       0\n\n\nThis summary is not what we want, since we want to break it down by channel type.\n\nfavstats(length ~ channel, data = ads)\n\n  channel   min      Q1 median       Q3    max   mean       sd  n missing\n1   basic 6.950 8.30375  9.298 10.30000 11.016 9.2051 1.396126 10       0\n2 premium 3.383 5.05250  7.715  7.95975  9.580 6.8592 2.119976 10       0\n\n\n\nExercise: Visualize the data using a boxplot.\n\n\nads %&gt;%\n  gf_boxplot(channel ~ length) %&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\n\nIt appears that the premium channels are skewed to the left. A density plot may help us compare the distributions and see the skewness, Figure 19.2.\n\nads %&gt;%\n  gf_dens(~length, color = ~channel)%&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Density\", color = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 19.2: Commercial length broken down by channel type.\n\n\n\n\n\nFrom this data, it looks like there is a difference between the two type of channels, but we must put the research question into a metric that will allow us to reach a decision. We will do this in a hypothesis test. As a reminder, the steps are\n\nState the null and alternative hypotheses.\n\nCompute a test statistic.\n\nDetermine the \\(p\\)-value.\n\nDraw a conclusion.\n\nBefore doing this, let’s visit an example of hypothesis testing that has become common knowledge for us, the US criminal trial system. We could also use the cadet honor system. This analogy allows us to remember and apply the steps.\n\n19.4.1 Hypothesis testing in the US court system\nA US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.\nThe jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).\nJurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty.\nThis is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.\nThere are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.\nNow back to our problem.\n\n\n19.4.2 Step 1- State the null and alternative hypotheses\nThe first step is to translate the research question into hypotheses. As a reminder, our research question is do premium channels have less ad time than basic channels? In collecting the data, we already decided the total length of time of commercials in a 30 minute show was the correct data for answering this question. We believe that premium channels have less commercial time. However, the null hypothesis, the straw man, has to be the default case that makes it possible to generate a sampling distribution.\n\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different.\n\nThese hypotheses are vague. What does it mean for two distributions to be different and how do we measure and summarize this? Let’s move to the second step and then come back and modify our hypotheses. Notice that the null hypothesis states the distributions are the same. When we generate our sampling distribution of the test statistic, we will do so under this null hypothesis.\n\n\n19.4.3 Step 2 - Compute a test statistic.\n\nExercise:\nWhat type of metric could we use to test for a difference in the distributions of commercial lengths between the two types of channels?\n\nThere are many ways for the distributions of lengths of commercials to differ. The easiest is to think of the summary statistics such as mean, median, standard deviation, or some combination of all of these. Historically, for mathematical reasons, it has been common to look at differences in measures of centrality, mean or median. The second consideration is what kind of difference? For example, should we consider a ratio or an actual difference (subtraction)? Again for historical reasons, the difference in means has been used as a measure. To keep things interesting, and to force those with some high school stats experience to think about this problem differently, we are going to use a different metric than has historically been used and taught. This also requires us to write some of our own code. Later, we will ask you to complete the same analysis with a different test statistic, either with your own code or using code from the mosaic package.\nOur metric is the ratio of the median length of commercials in basic channels to premium. Thus, our hypotheses are now:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nFirst, let’s calculate the median length of commercials, by channel type, for our data.\n\nmedian(length ~ channel, data = ads) \n\n  basic premium \n  9.298   7.715 \n\n\nSo, the ratio of median lengths is\n\nmedian(length ~ channel, data = ads)[1] / median(length ~ channel, data = ads)[2]\n\n   basic \n1.205185 \n\n\nLet’s put the calculation of the ratio into a function.\n\nmetric &lt;- function(x){\n  temp &lt;- x[1] / x[2]\n  names(temp) &lt;- \"test_stat\"\n  return(temp)\n}\n\n\nmetric(median(length ~ channel, data = ads))\n\ntest_stat \n 1.205185 \n\n\nNow, let’s save the observed value of the test statistic in an object.\n\nobs &lt;- metric(median(length ~ channel, data = ads))\nobs\n\ntest_stat \n 1.205185 \n\n\nHere is what we have done; we needed a single number metric to use in evaluating the null and alternative hypotheses. The null hypothesis is that the commercial lengths for the two channel types have the same distribution and the alternative is that they don’t. To measure the alternative hypothesis, we decided to use a ratio of the medians. If the ratio is close to 1, then the medians are not different. There may be other ways in which the distributions are different but we have decided on the ratio of medians for this example.\n\n\n19.4.4 Step 3 - Determine the \\(p\\)-value.\nAs a reminder, the \\(p\\)-value is the probability of our observed test statistic or something more extreme, given the null hypothesis is true. Since our null hypothesis is that the distributions are the same, we can use a randomization test. We will shuffle the channel labels since under the null hypothesis, they are irrelevant. Here is the code for one randomization.\n\nset.seed(371)\nmetric(median(length ~ shuffle(channel), data = ads))\n\ntest_stat \n0.9957097 \n\n\nLet’s generate the empirical sampling distribution of the test statistic we developed. We will perform 1,000 simulations now.\n\nset.seed(371)\nresults &lt;- do(1000)*metric(median(length ~ shuffle(channel), data = ads))\n\nNext we create a plot of the distribution of the ratio of median commercial lengths in basic and premium channels, assuming they come from the same population, Figure 19.3.\n\nresults %&gt;%\n  gf_histogram(~test_stat) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 19.3: Historgram of the sampling distribution by an approxiamte permutation test.\n\n\n\n\n\nNotice that this distribution is centered on 1 and appears to be roughly symmetrical. The vertical line is our observed value of the test statistic. It seems to be in the tail, and is larger than expected if the channels came from the same distribution. Let’s calculate the \\(p\\)-value.\n\nresults %&gt;%\n  summarise(p_value = mean(test_stat &gt;= obs))\n\n  p_value\n1   0.026\n\n\nBefore proceeding, we have a technical question: Should we include the observed data in the calculation of the \\(p\\)-value?\nThe answer is that most people would conclude that the original data is one of the possible permutations and thus include it. This practice will also ensure that the \\(p\\)-value from a randomization test is never zero. In practice, this simply means adding 1 to both the numerator and denominator. The mosaic package has done this for us with the prop1() function.\n\nprop1(~(test_stat &gt;= obs), data = results)\n\n prop_TRUE \n0.02697303 \n\n\nThe test we performed is called a one-sided test because we only checked if the median length for basic channels is larger than that for premium channels. In this case of a one-sided test, more extreme meant a ratio much bigger than 1. A two-sided test is also common, in fact it is more common, and is used if we did not apriori think one channel type had longer commercials than the other. In this case, we find the \\(p\\)-value by doubling the single-sided value. This is because more extreme could have happened in either tail of the sampling distribution.\n\n\n19.4.5 Step 4 - Draw a conclusion\nOur research question – do premium channels have less ad time than basic channels? – was framed in context of the following hypotheses:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nIn our simulations, less than 2.7% of the simulated test statistics were greater than or equal (more extreme relative to the null hypothesis) to the observed test statistic. That is, the observed ratio of 1.2 is a rare event if the distributions of commercial lengths for premium and basic channels truly are the same. By chance alone, we would only expect an observed ratio this large to occur less than 3 in 100 times. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we reject \\(H_0\\) and conclude there is evidence that the median length of basic channel ads is bigger than that for premium channels.\nThe less than 3-in-100 chance is the \\(p\\)-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.\nWhen the \\(p\\)-value is small, i.e. less than a previously set threshold, we say the results are statistically significant2. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The threshold, called the significance level and often represented by the Greek letter \\(\\alpha\\), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application. Using a significance level of \\(\\alpha = 0.05\\) in the TV channel study, we can say that the data provided statistically significant evidence against the null hypothesis.\n\nWe say that the data provide statistically significant evidence against the null hypothesis if the \\(p\\)-value is less than some reference value, usually \\(\\alpha=0.05\\).\n\nIf the null hypothesis is true, unknown to us, the significance level \\(\\alpha\\) defines the probability that we will make a Type 1 Error. We will define decision errors in the next section.\n\nSide note: What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye – good job! There are many video clips that explain the use of 0.05. Sometimes it’s also a good idea to deviate from the standard. It really depends on the risk that the decision maker wants to accept in terms of the two types of decision errors.\n\n\nExercise:\nUse our \\(p\\)-value and a significance level of 0.05 to make a decision.\n\nBased on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed ratio of medians is 0.027. Since this is less than our significance level of \\(\\alpha = 0.05\\), we reject the null in favor of the alternative that the basic channel has longer commercials.\n\n\n19.4.6 Decision errors\nHypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\nThere are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\nExample:\nIn a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 error represent in this context? What does a Type 2 error represent?\n\nIf the court makes a Type 1 error, this means the defendant is truly innocent (\\(H_0\\) true) but is wrongly convicted. A Type 2 error means the court failed to reject \\(H_0\\) (i.e. failed to convict the person) when she was in fact guilty (\\(H_A\\) true).\n\nExercise:\nConsider the commercial length study where we concluded basic channels had longer commercials than premium channels. What would a Type 1 error represent in this context?3\n\n\nExercise:\nHow could we reduce the Type 1 error rate in US courts? What influence would this have on the Type 2 error rate?\n\nTo lower the Type 1 error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 errors.\n\nExercise:\nHow could we reduce the Type 2 error rate in US courts? What influence would this have on the Type 1 error rate?\n\nTo lower the Type 2 error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 error rate.\n\nExercise: Think about the cadet honor system, its metric of evaluation, and the impact on the types of decision errors.\n\nThese exercises provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type for a given amount of data, information.\n\n\n19.4.7 Choosing a significance level\nChoosing a significance level for a test is important in many contexts, and the traditional level is \\(\\alpha = 0.05\\). However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.\nIf making a Type 1 error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01 or 0.001). Under this scenario, we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\).\nIf making a Type 2 error is relatively more dangerous or much more costly than a Type 1 error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null hypothesis is actually false.\nThe significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 error.\n\n\n19.4.8 Introducing two-sided hypotheses\nSo far we have explored whether women were discriminated against and whether commercials were longer depending on the type of channel. In these two case studies, we’ve actually ignored some possibilities:\n\nWhat if men are actually discriminated against?\n\nWhat if ads on premium channels are actually longer?\n\nThese possibilities weren’t considered in our hypotheses or analyses. This may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our worldview:\n\nFraming an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.\n\nIf we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!\n\nThe previous hypothesis tests we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in a single direction, but usually we want to consider all possibilities. To do so, let’s discuss two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone cardiopulmonary resuscitation, CPR.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "href": "19-Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.5 Two-sided hypothesis test",
    "text": "19.5 Two-sided hypothesis test\nIt is important to distinguish between a two-sided hypothesis test and a one-sided hypothesis test. In a two-sided test, we are concerned with whether or not the population parameter could take a particular value. For parameter \\(\\theta\\), a set of two-sided hypotheses looks like:\n\\[\nH_0: \\theta=\\theta_0 \\hspace{1.5cm} H_1: \\theta \\neq \\theta_0\n\\] where \\(\\theta_0\\) is a particular value the parameter could take on.\nIn a one-sided test, we are concerned with whether a parameter exceeds or does not exceed a specific value. A set of one-sided hypotheses looks like:\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &gt; \\theta_0\n\\] or\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &lt; \\theta_0\n\\]\nIn some texts, one-sided null hypotheses include an inequality (\\(H_0 \\leq \\theta_0\\) or \\(H_0 \\geq \\theta_0\\)). We have already demonstrated one-sided tests and, in the next example, we will use a two-sided test.\n\n19.5.1 CPR example\nCardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.\nHere we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.4 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\n\n\n19.5.2 Step 1 - State the null and alternative hypotheses\n\nExercise: Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).\n\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\\(H_0\\): Blood thinners do not have an overall survival effect; survival rate is independent of experimental treatment group. \\(p_c - p_t = 0\\).\n\\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\).\nNotice here that we accelerated the process by already defining our test statistic, our metric, in the hypothesis. It is the difference in survival rates for the control and treatment groups. This is a similar metric to what we used in the case study. We could use others but this will allow us to use functions from the mosaic package and will also help us to understand metrics for mathematically derived sampling distributions.\nThere were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are in the file blood_thinner.csv.\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\nthinner\n\n# A tibble: 90 × 2\n   group     outcome \n   &lt;chr&gt;     &lt;chr&gt;   \n 1 treatment survived\n 2 control   survived\n 3 control   died    \n 4 control   died    \n 5 control   died    \n 6 treatment survived\n 7 control   died    \n 8 control   died    \n 9 treatment died    \n10 treatment survived\n# ℹ 80 more rows\n\n\nLet’s put the data in a table.\n\ntally(~group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\n\n19.5.3 Step 2 - Compute a test statistic.\nThe test statistic we have selected is the difference in survival rate between the control group and the treatment group. The following R command finds the observed proportions.\n\ntally(outcome ~ group, data = thinner, margins = TRUE, format = \"proportion\")\n\n          group\noutcome    control treatment\n  died        0.78      0.65\n  survived    0.22      0.35\n  Total       1.00      1.00\n\n\nNotice the formula we used to get the correct variable in the column for the summary proportions.\nThe observed test statistic can now be found.5\n\nobs &lt;- diffprop(outcome ~ group, data = thinner)\nobs\n\ndiffprop \n   -0.13 \n\n\nBased on the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. We wonder if this difference is easily explainable by chance.\n\n\n19.5.4 Step 3 - Determine the \\(p\\)-value.\nAs we did in the previous two studies, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning treatment and control stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences. This is our empirical sampling distribution, the distribution of differences simulated under the null hypothesis.\n\nset.seed(655)\nresults &lt;- do(10000)*diffprop(outcome ~ shuffle(group), data = thinner)\n\nFigure 19.4 is a histogram of the estimated sampling distribution.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 19.4: Histogram of the estimated sampling distribution.\n\n\n\n\n\nNotice how it is centered on zero, the assumption of no difference. Also notice that it is unimodal and symmetric. We will use this when we develop mathematical sampling distributions. We now calculate the proportion of simulated differences that are less than or equal to the observed difference.\n\nprop1(~(diffprop &lt;= obs), data = results)\n\nprop_TRUE \n0.1283872 \n\n\nThe left tail area is about 0.128. (Note: it is only a coincidence that our \\(p\\)-value is approximately 0.13 and we also have \\(\\hat{p}_c - \\hat{p}_t= -0.13\\).) However, contrary to how we calculated the \\(p\\)-value in previous studies, the \\(p\\)-value of this test is not 0.128!\nThe \\(p\\)-value is defined as the probability we observe a result at least as favorable to the alternative hypothesis as the observed result (i.e. the observed difference). In this case, any differences greater than or equal to 0.13 would provide equally strong evidence favoring the alternative hypothesis as differences less than or equal to -0.13. A difference of 0.13 would correspond to 13% higher survival rate in the treatment group than the control group.\nThere is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase or decrease the risk of death in patients who undergo CPR before arriving at the hospital.6\nFor a two-sided test, we take the single tail (in this case, 0.128) and double it to get the \\(p\\)-value: 0.256.\n\n\n19.5.5 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment.\n\nDefault to a two-sided test We want to be rigorous and keep an open mind when we analyze data and evidence. In general, you should default to using a two-sided test when conducting hypothesis tests. Use a one-sided hypothesis test only if you truly have interest in only one direction.\n\n\nComputing a \\(p\\)-value for a two-sided test\nFirst compute the \\(p\\)-value for one tail of the distribution, then double that value to get the two-sided \\(p\\)-value. That’s it!\n\nIt is never okay to change two-sided tests to one-sided tests after observing the data.\n\nHypothesis tests should be set up before seeing the data\nAfter observing data, it can be tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses and the significance level should be set up before observing the data.\n\n\n\n19.5.6 How to use a hypothesis test\nThis is a summary of the general framework for using hypothesis testing. These are the same steps as before, with just slightly different wording.\n\nFrame the research question in terms of hypotheses.\nHypothesis tests are appropriate for research questions that can be summarized in two competing hypotheses. The null hypothesis (\\(H_0\\)) usually represents a skeptical perspective or a perspective of no difference. The alternative hypothesis (\\(H_A\\)) usually represents a new view or a difference.\nCollect data with an observational study or experiment.\nIf a research question can be formed into two hypotheses, we can collect data to run a hypothesis test. If the research question focuses on associations between variables but does not concern causation, we would run an observational study. If the research question seeks a causal connection between two or more variables, then an experiment should be used if possible.\nAnalyze the data.\nChoose an analysis technique appropriate for the data and identify the \\(p\\)-value. So far, we’ve only seen one analysis technique: randomization. We’ll encounter several new methods suitable for many other contexts.\nForm a conclusion.\nUsing the \\(p\\)-value from the analysis, determine whether the data provide statistically significant evidence against the null hypothesis. Also, be sure to write the conclusion in plain language so casual readers can understand the results.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#homework-problems",
    "href": "19-Hypothesis-Testing-Simulation.html#homework-problems",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.6 Homework Problems",
    "text": "19.6 Homework Problems\n\nRepeat the analysis of the commercial lengths for basic and premium TV channels in the notes. This time, use a different test statistic.\n\n\n\nState the null and alternative hypotheses.\nCompute a test statistic. Remember to use something different than what was used in the text.\nDetermine the \\(p\\)-value.\nDraw a conclusion.\n\n\n\nIs yawning contagious?\n\nAn experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested whether a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasn’t a person yawning near them (control). The following table shows the results of this experiment.\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Group}\\\\\n& & \\text{Treatment } &  \\text{Control} & \\text{Total}  \\\\\n& \\hline \\text{Yawn}    &   10      & 4     & 14  \\\\\n\\textbf{Result} & \\text{Not Yawn}   & 24        & 12        & 36   \\\\\n    &\\text{Total}       & 34        & 16        & 50 \\\\\n\\end{array}\n\\]\nThe data is in the file yawn.csv.\n\nWhat are the hypotheses?\nCalculate the observed difference between the yawning rates under the two scenarios. Yes, we are giving you the test statistic.\nEstimate the \\(p\\)-value using randomization.\nPlot the empirical sampling distribution.\nDetermine the conclusion of the hypothesis test.\nThe traditional belief is that yawning is contagious – one yawn can lead to another yawn, which might lead to another, and so on. In this exercise, there was the option of selecting a one-sided or two-sided test. Which would you recommend (or which did you choose)? Justify your answer in 1-3 sentences.\nHow did you select your level of significance? Explain in 1-3 sentences.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#solutions-manual",
    "href": "19-Hypothesis-Testing-Simulation.html#solutions-manual",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#footnotes",
    "href": "19-Hypothesis-Testing-Simulation.html#footnotes",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "In our opinion, this is how things developed historically. However, since computational tools prior to machine computers, humans in most cases, were limited and expensive, there was a shift to mathematical solutions. The relatively recent increase and availability in machine computational power has led to a shift back to computational methods. Thus, some people think mathematical methods predate computational methods, but that is not the case.↩︎\nSome experts in the field of statistics are uncomfortable with using a significance level to declare that something is statistically significant. The choice of a significance level (discussed more in a later section) can be somewhat arbitrary. If you conduct a hypothesis test with a \\(p\\)-value of 0.051 and chose a significance level of 0.05, are your results really non-significant compared to getting a \\(p\\)-value of 0.049? For this reason, some experts shy away from the term statistically significant and instead describe detectable or discernable effects/differences.↩︎\nMaking a Type 1 error in this context would mean that there is no difference in commercial length between basic and premium channels, despite the strong evidence (the data suggesting otherwise) found in the observational study. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings. Replication is part of the scientific method.↩︎\n“Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎\nObserved control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Observed treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_c - \\hat{p}_t = 0.22 - 0.35 = -0.13\\).↩︎\nRealistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html",
    "href": "20-Hypothesis-Testing-ProbDist.html",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#objectives",
    "href": "20-Hypothesis-Testing-ProbDist.html#objectives",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "Know and properly use the terminology of a hypothesis test, to include: permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, and power.\nConduct all four steps of a hypothesis test using probability models.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "href": "20-Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.2 Hypothesis testing using probability models",
    "text": "20.2 Hypothesis testing using probability models\nAs a lead into the Central Limit Theorem in @ref(HYPTESTCLT) and mathematical sampling distributions, we will look at a class of hypothesis testing where the null hypothesis specifies a probability model. In some cases, we can get an exact answer, and in others, we will use simulation to get an empirical \\(p\\)-value. By the way, a permutation test is an exact test; by this we mean we are finding all possible permutations in the calculation of the \\(p\\)-value. However, since the complete enumeration of all permutations is often difficult, we approximate it with randomization, simulation. Thus, the \\(p\\)-value from a randomization test is an approximation of the exact (permutation) test.\nLet’s use three examples to illustrate the ideas of this chapter.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "href": "20-Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.3 Tappers and listeners",
    "text": "20.3 Tappers and listeners\nHere’s a game you can try with your friends or family. Pick a simple, well-known song. Tap that tune on your desk, and see if the other person can guess the song. In this simple game, you are the tapper, and the other person is the listener.\nA Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.1 In her study, she recruited 120 tappers and 120 listeners into the study. About 50% of the tappers expected that the listener would be able to guess the song. Newton wondered, is 50% a reasonable expectation?\n\n20.3.1 Step 1- State the null and alternative hypotheses\nNewton’s research question can be framed into two hypotheses:\n\\(H_0\\): The tappers are correct, and, in general, 50% of listeners are able to guess the tune. \\(p = 0.50\\)\n\\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners are able to guess the tune. \\(p \\neq 0.50\\)\n\nExercise: Is this a one-sided or two-sided hypothesis test? How many variables are in this model?\n\nThe tappers think that listeners will guess the song correctly 50% of the time, and this is a two-sided test since we don’t know beforehand if listeners will be better or worse than 50%.\nThere is only one variable of interest, whether the listener is correct.\n\n\n20.3.2 Step 2 - Compute a test statistic\nIn Newton’s study, only 42 (we changed the number to make this problem more interesting from an educational perspective) out of 120 listeners (\\(\\hat{p} = 0.35\\)) were able to guess the tune! From the perspective of the null hypothesis, we might wonder, how likely is it that we would get this result from chance alone? That is, what’s the chance we would happen to see such a small fraction if \\(H_0\\) were true and the true correct-guess rate is 0.50?\nNow before we use simulation, let’s frame this as a probability model. The random variable \\(X\\) is the number of correct guesses out of 120. If the observations are independent and the probability of success is constant (each listener has the same probability of guessing correctly), then we could use a binomial model. We can’t assess the validity of these assumptions without knowing more about the experiment, the subjects, and the data collection. For educational purposes, we will assume they are valid. Thus, our test statistic is the number of successes in 120 trials. The observed value is 42.\n\n\n20.3.3 Step 3 - Determine the \\(p\\)-value\nWe now want to find the \\(p\\)-value as \\(2 \\cdot \\Prob(X \\leq 42)\\) where \\(X\\) is a binomial random variable with \\(p = 0.5\\) and \\(n = 120\\). Again, the \\(p\\)-value is the probability of the observed data or something more extreme, given the null hypothesis is true. Here, the null hypothesis being true implies that the probability of success is 0.50. We will use R to get the one-sided \\(p\\)-value and then double it to get the two-sided \\(p\\)-value for the problem. We selected \\(\\Prob(X \\leq 42)\\) because “more extreme” means the observed values and values further from what you would get if the null hypothesis were true, which is 60 for this problem.\n\n2*pbinom(42, 120, prob = 0.5)\n\n[1] 0.001299333\n\n\nThat is a small \\(p\\)-value.\n\n\n20.3.4 Step 4 - Draw a conclusion\nBased on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0013\\) probability that only 42 or less (or 78 or more) listeners would get correctly. This is the probability of what we observed or something more extreme, given the null hypothesis is true. This probability is much less than 0.05, so we reject the null hypothesis that the listeners are guessing correctly half of the time and conclude that the correct-guess rate rate is different from 50%.\nThis decision region looks like the pmf in Figure 20.1. Any observed values inside the red boundary lines would be consistent with the null hypothesis. That is, any observed values inside the red boundary lines would result in a \\(p\\)-value larger than 0.05. Any values at the red line or more extreme would be in the rejection region, resulting in a \\(p\\)-value smaller than 0.05. We also plotted the observed value in black.\n\ngf_dist(\"binom\", size = 120, prob = 0.5, xlim = c(50, 115)) %&gt;%\n  gf_vline(xintercept = c(48, 72), color = \"red\") %&gt;%\n  gf_vline(xintercept = c(42), color = \"black\") %&gt;%\n  gf_theme(theme_bw) %&gt;%\n  gf_labs(title = \"Binomial pmf\", subtitle = \"Probability of success is 0.5\", \n          y = \"Probability\")\n\n\n\n\n\n\n\nFigure 20.1: Binomial pmf\n\n\n\n\n\n\n\n20.3.5 Repeat using simulation\nWe will repeat the analysis using an empirical (observed from simulated data) \\(p\\)-value. Step 1, stating the null and alternative hypothesis, is the same.\n\n\n20.3.6 Step 2 - Compute a test statistic\nWe will use the proportion of listeners that get the song correct instead of the number of listeners that get it correct. This is a minor change since we are simply dividing by 120.\n\nobs &lt;- 42 / 120\nobs\n\n[1] 0.35\n\n\n\n\n20.3.7 Step 3 - Determine the \\(p\\)-value\nTo simulate 120 games under the null hypothesis where \\(p = 0.50\\), we could flip a coin 120 times. Each time the coin comes up heads, this could represent the listener guessing correctly, and tails would represent the listener guessing incorrectly. For example, we can simulate 5 tapper-listener pairs by flipping a coin 5 times:\n\\[\n\\begin{array}{ccccc}\nH & H & T & H & T \\\\\nCorrect & Correct & Wrong & Correct & Wrong \\\\\n\\end{array}\n\\]\nAfter flipping the coin 120 times, we got 56 heads for a proportion of \\(\\hat{p}_{sim} = 0.467\\). As we did with the randomization technique, seeing what would happen with one simulation isn’t enough. In order to evaluate whether our originally observed proportion of 0.35 is unusual or not, we should generate more simulations. Here, we’ve repeated this simulation 10,000 times:\n\nset.seed(604)\nresults &lt;- rbinom(10000, 120, 0.5) / 120\n\nNote, we could simulate it a number of ways. Here is a way using do() that will look like what we’ve done for other randomization tests.\n\nset.seed(604)\nresults &lt;- do(10000)*mean(sample(c(0, 1), size = 120, replace = TRUE))\n\n\nhead(results)\n\n       mean\n1 0.4250000\n2 0.5250000\n3 0.5916667\n4 0.5000000\n5 0.5250000\n6 0.5083333\n\n\n\nresults %&gt;%\n  gf_histogram(~mean, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = c(obs, 1 - obs), color = \"black\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 20.2: The estimated sampling distribution\n\n\n\n\n\nNotice in Figure 20.2 how the sampling distribution is centered at 0.5 and looks symmetrical.\nThe \\(p\\)-value is found using the prop1 function. In this problem, we really need the observed value to be included to prevent a \\(p\\)-value of zero.\n\n2*prop1(~(mean &lt;= obs), data = results)\n\n prop_TRUE \n0.00119988 \n\n\n\n\n20.3.8 Step 4 - Draw a conclusion\nIn these 10,000 simulations, we see very few results close to 0.35. Based on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0012\\) probability that only 35% or less or 65% or more listeners would get it right. This \\(p\\)-value is much less than 0.05, so we reject that the listeners are guessing correctly half of the time and conclude that the correct-guess rate is different from 50%.\n\nExercise: In the context of the experiment, what is the \\(p\\)-value for the hypothesis test?2\n\n\nExercise:\nDo the data provide statistically significant evidence against the null hypothesis? State an appropriate conclusion in the context of the research question.3",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#cardiopulmonary-resuscitation-cpr",
    "href": "20-Hypothesis-Testing-ProbDist.html#cardiopulmonary-resuscitation-cpr",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.4 Cardiopulmonary resuscitation (CPR)",
    "text": "20.4 Cardiopulmonary resuscitation (CPR)\nLet’s return to the CPR example from last chapter. As a reminder, we will repeat some of the background material.\nCardiopulmonary resuscitation (CPR) is a procedure sometimes used on individuals suffering a heart attack. It is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries, which complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack, but blood thinners negatively affect internal injuries.\nPatients who underwent CPR for a heart attack and were subsequently admitted to a hospital4 were randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\n\n20.4.1 Step 1- State the null and alternative hypotheses\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\\(H_0\\): Blood thinners do not have an overall survival effect; survival rate is independent of experimental treatment group. \\(p_c - p_t = 0\\).\n\\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\).\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\nhead(thinner)\n\n# A tibble: 6 × 2\n  group     outcome \n  &lt;chr&gt;     &lt;chr&gt;   \n1 treatment survived\n2 control   survived\n3 control   died    \n4 control   died    \n5 control   died    \n6 treatment survived\n\n\nLet’s put the data in a table.\n\ntally(~group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\n\n20.4.2 Step 2 - Compute a test statistic.\nIn this example, we can think of the data as coming from a hypergeometric distribution. This is really a binomial from a finite population. We can calculate the \\(p\\)-value using this probability distribution. The random variable is the number of control patients that survived from a total population of 90 patients, where 50 are control patients and 40 are treatment patients, and where a total of 25 survived.\n\n\n20.4.3 Step 3 - Determine the \\(p\\)-value.\nIn this case, we want to find \\(\\Prob(X \\leq 11)\\) (the observed number of control patients that survived) and double it since it is a two-sided test.\n\n2*phyper(11, 50, 40, 25)\n\n[1] 0.2581356\n\n\nNote: We could have picked the lower right cell as the reference cell. But now I want the \\(\\Prob(X \\geq 14)\\) (the observed number of treatment patients that survived) with the appropriate change in parameter values. Notice we get the same answer.\n\n2*(1 - phyper(13, 40, 50, 25))\n\n[1] 0.2581356\n\n\nWe could do the same thing for the other two cells. Here we find \\(\\Prob(X \\leq 26)\\) (the observed number of treatment patients that did not survive).\n\n2*phyper(26, 40, 50, 65)\n\n[1] 0.2581356\n\n\nHere we find \\(\\Prob(X \\geq 39)\\) (the observed number of control patients that did not survive).\n\n2*(1 - phyper(38, 50, 40, 65))\n\n[1] 0.2581356\n\n\nR also has a built in function, fisher.test(), that we could use. This function calculates Fisher’s exact test, where \\(p\\)-values are obtained using the hypergeometric distribution.\n\nfisher.test(tally(~group + outcome, data = thinner))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tally(~group + outcome, data = thinner)\np-value = 0.2366\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.6794355 5.4174460\nsample estimates:\nodds ratio \n  1.895136 \n\n\nThe \\(p\\)-value is slightly different since the hypergeometric distribution is not symmetric. For this reason, doubling the \\(p\\)-value from the single side result is not quite right. The algorithm in fisher.test() finds and adds all probabilities less than or equal to value of \\(\\Prob(X = 11)\\), see Figure 20.3. Using fisher.test() gives the correct \\(p\\)-value.\n\ngf_dist(\"hyper\", m = 50, n = 40, k = 25) %&gt;%\n  gf_hline(yintercept = dhyper(11, 50, 40, 25), color = \"red\") %&gt;%\n  gf_labs(title = \"Hypergeometric pmf\", subtitle = \"Red line is P(X = 11)\", \n          y = \"Probability\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 20.3: Hypergeometric pmf showing the cutoff for \\(p\\)-value calculation.\n\n\n\n\n\nThis is how fisher.test() is calculating the \\(p\\)-value:\n\ntemp &lt;- dhyper(0:25, 50, 40, 25)\nsum(temp[temp &lt;= dhyper(11, 50, 40, 25)])\n\n[1] 0.2365928\n\n\nThe randomization test in the last chapter yielded a \\(p\\)-value of 0.257 so all tests are consistent.\n\n\n20.4.4 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment.\nNotice that in these first two examples, we had a test of a single proportion and a test of two proportions. The single proportion test did not have an equivalent randomization test since there is not a second variable to shuffle. We were able to get answers since we found a probability model that we could use instead.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#golf-balls",
    "href": "20-Hypothesis-Testing-ProbDist.html#golf-balls",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.5 Golf Balls",
    "text": "20.5 Golf Balls\nOur last example will be interesting because the distribution has multiple parameters and a test metric is not obvious at this point.\nThe owners of a residence located along a golf course collected the first 500 golf balls that landed on their property. Most golf balls are labeled with the make of the golf ball and a number, for example “Nike 1” or “Titleist 3”. The numbers are typically between 1 and 4, and the owners of the residence wondered if these numbers are equally likely (at least among golf balls used by golfers of poor enough quality that they lose them in the yards of the residences along the fairway.)\nWe will use a significance level of \\(\\alpha = 0.05\\) since there is no reason to favor one decision error over the other.\n\n20.5.1 Step 1- State the null and alternative hypotheses\nWe think that the numbers are not all equally likely. The question of one-sided versus two-sided is not relevant in this test. You will see this when we write the hypotheses.\n\\(H_0\\): All of the numbers are equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) Or \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\)\n\\(H_A\\): There is some other distribution of the numbers in the population. At least one population proportion is not \\(\\frac{1}{4}\\).\nNotice that we switched to using \\(\\pi\\) instead of \\(p\\) for the population parameter. There is no reason other than to make you aware that both are used.\nThis problem is an extension of the binomial. Instead of two outcomes, there are four outcomes. This is called a multinomial distribution. You can read more about it if you like, but our methods will not make it necessary to learn the probability mass function.\nOut of the 500 golf balls collected, 486 of them had a number between 1 and 4. We will deal with only these 486 golf balls. Let’s get the data from `golf_balls.csv”.\n\ngolf_balls &lt;- read_csv(\"data/golf_balls.csv\")\n\n\ninspect(golf_balls)\n\n\nquantitative variables:  \n    name   class min Q1 median Q3 max     mean       sd   n missing\n1 number numeric   1  1      2  3   4 2.366255 1.107432 486       0\n\n\n\ntally(~number, data = golf_balls)\n\nnumber\n  1   2   3   4 \n137 138 107 104 \n\n\n\n\n20.5.2 Step 2 - Compute a test statistic.\nIf all numbers were equally likely, we would expect to see 121.5 golf balls of each number. This is a point estimate and thus not an actual value that could be realized. Of course, in a sample we will have variation and thus departure from this state. We need a test statistic that will help us determine if the observed values are reasonable under the null hypothesis. Remember that the test statistic is a single number metric used to evaluate the hypothesis.\n\nExercise:\nWhat would you propose for the test statistic?\n\nWith four proportions, we need a way to combine them. This seems tricky, so let’s just use a simple approach. Let’s take the maximum number of balls across all cells of the table and subtract the minimum. This is called the range and we will denote the parameter as \\(R\\). Under the null hypothesis, this should be zero. We could re-write our hypotheses as:\n\\(H_0\\): \\(R=0\\)\n\\(H_A\\): \\(R&gt;0\\)\nNotice that \\(R\\) will always be non-negative, thus this test is one-sided.\nThe observed range is 34, \\(138 - 104\\).\n\nobs &lt;- diff(range(tally(~number, data = golf_balls)))\nobs\n\n[1] 34\n\n\n\n\n20.5.3 Step 3 - Determine the \\(p\\)-value.\nWe don’t know the distribution of our test statistic, so we will use simulation. We will simulate data from a multinomial distribution under the null hypothesis and calculate a new value of the test statistic. We will repeat this 10,000 times and this will give us an estimate of the sampling distribution.\nWe will use the sample() function again to simulate the distribution of numbers under the null hypothesis. To help us understand the process and build the code, we are only initially using a sample size of 12 to keep the printout reasonable and easy to read.\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nNotice this is not using tidyverse coding ideas. We don’t think we need tibbles or data frames so we went with straight nested R code. You can break this code down by starting with the code in the center.\n\nset.seed(3311)\nsample(1:4, size = 12, replace = TRUE)\n\n [1] 3 1 2 3 2 3 1 3 3 4 1 1\n\n\n\nset.seed(3311)\ntable(sample(1:4, size = 12, replace = TRUE))\n\n\n1 2 3 4 \n4 2 5 1 \n\n\n\nset.seed(3311)\nrange(table(sample(1:4, size = 12, replace = TRUE)))\n\n[1] 1 5\n\n\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nWe are now ready to ramp up to the full problem. Let’s simulate the data under the null hypothesis. We are sampling 486 golf balls (instead of 12) with the numbers 1 through 4 on them. Each number is equally likely. We then find the range, our test statistic. Finally we repeat this 10,000 to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(3311)\nresults &lt;- do(10000)*diff(range(table(sample(1:4, size = 486, replace = TRUE))))\n\nFigure 20.4 is a plot of the sampling distribution of the range.\n\nresults %&gt;%\n  gf_histogram(~diff, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Range\", \n          subtitle = \"Multinomial with equal probability\",\n          x = \"Range\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 20.4: Sampling distribution of the range.\n\n\n\n\n\nNotice how this distribution is skewed to the right. The \\(p\\)-value is 0.14.\n\nprop1(~(diff &gt;= obs), data = results)\n\nprop_TRUE \n0.1393861 \n\n\n\n\n20.5.4 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, based on our data, we do not find statistically significant evidence against the claim that the number on the golf balls are equally likely. We can’t say that the proportion of golf balls with each number differs from 0.25.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "href": "20-Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.6 Repeat with a different test statistic",
    "text": "20.6 Repeat with a different test statistic\nThe test statistic we developed was helpful, but it seems weak because we did not use the information in all four cells. So let’s devise a metric that does this. The hypotheses are the same, so we will jump to step 2.\n\n20.6.1 Step 2 - Compute a test statistic.\nIf each number were equally likely, we would have 121.5 balls in each bin. We can find a test statistic by looking at the deviation in each cell from 121.5.\n\ntally(~number, data = golf_balls) - 121.5\n\nnumber\n    1     2     3     4 \n 15.5  16.5 -14.5 -17.5 \n\n\nNow we need to collapse these into a single number. Just adding will always result in a value of 0, why? So let’s take the absolute value and then add the cells together.\n\nobs &lt;- sum(abs(tally(~number, data = golf_balls) - 121.5))\nobs\n\n[1] 64\n\n\nThis will be our test statistic.\n\n\n20.6.2 Step 3 - Determine the \\(p\\)-value.\nWe will use similar code from above with our new metric. Now we sample 486 golf balls with the numbers 1 through 4 on them, and find our test statistic, the sum of the absolute deviations of each cell of the table from the expected count, 121.5. We repeat this process 10,000 times to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(9697)\nresults &lt;- do(10000)*sum(abs(table(sample(1:4, size = 486, replace = TRUE)) - 121.5))\n\nFigure 20.5 is a plot of the sampling distribution of the absolute value of deviations.\n\nresults %&gt;%\n  gf_histogram(~sum, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Absolute Deviations\",\n          subtitle = \"Multinomial with equal probability\",\n          x = \"Absolute deviations\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 20.5: Sampling distribution of the absolute deviations.\n\n\n\n\n\nNotice how this distribution is skewed to the right and our test statistic seems to be more extreme.\n\nprop1(~(sum &gt;= obs), data = results)\n\n prop_TRUE \n0.01359864 \n\n\nThe \\(p\\)-value is 0.014. This value is much smaller than our previous result. The test statistic matters in our decision process as nothing about this problem has changed except the test statistic.\n\n\n20.6.3 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is smaller than 0.05, we reject the null hypothesis. That is, based on our data, we find statistically significant evidence against the claim that the numbers on the golf balls are equally likely. We conclude that the numbers on the golf balls are not all equally likely, or that at least one is different.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#summary",
    "href": "20-Hypothesis-Testing-ProbDist.html#summary",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.7 Summary",
    "text": "20.7 Summary\nIn this chapter, we used probability models to help us make decisions from data. This chapter is different from the randomization section in that randomization had two variables (one of which we could shuffle) and a null hypothesis of no difference. In the case of a single proportion, we were able to use the binomial distribution to get an exact \\(p\\)-value under the null hypothesis. In the case of a \\(2 \\times 2\\) table, we were able to show that we could use the hypergeometric distribution to get an exact \\(p\\)-value under the assumptions of the model.\nWe also found that the choice of test statistic has an impact on our decision. Even though we get valid \\(p\\)-values and the desired Type 1 error rate, if the information in the data is not used to its fullest, we will lose power. Note: power is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\nIn the next chapter, we will learn about mathematical solutions to finding the sampling distribution. The key difference in all these methods is the selection of the test statistic and the assumptions made to derive a sampling distribution.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#homework-problems",
    "href": "20-Hypothesis-Testing-ProbDist.html#homework-problems",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.8 Homework Problems",
    "text": "20.8 Homework Problems\n\nRepeat the analysis of the yawning data from last chapter, but this time use the hypergeometric distribution.\nIs yawning contagious?\nAn experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasn’t a person yawning near them (control). The following table shows the results of this experiment.\n\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Group}\\\\\n& & \\text{Treatment } &  \\text{Control} & \\text{Total}  \\\\\n& \\hline \\text{Yawn}    &   10      & 4     & 14  \\\\\n\\textbf{Result} & \\text{Not Yawn}   & 24        & 12        & 36   \\\\\n    &\\text{Total}       & 34        & 16        & 50 \\\\\n\\end{array}\n\\]\nThe data is in the file yawn.csv.\n\nWhat are the hypotheses?\nChoose a cell, and calculate the observed statistic.\nFind the \\(p\\)-value using the hypergeometric distribution.\nPlot the the sampling distribution.\nDetermine the conclusion of the hypothesis test.\nCompare your results with the randomization test.\n\n\n\nRepeat the analysis of the golf ball data using a different test statistic.\nUse a level of significance of 0.05.\n\n\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the \\(p\\)-value.\nDraw a conclusion.\n\n\n\nBody Temperature\n\nShoemaker5 cites a paper from the American Medical Association6 that questions conventional wisdom that the average body temperature of a human is 98.6 degrees Fahrenheit. One of the main points of the original article is that the traditional mean of 98.6 is, in essence, 100 years out of date. The authors cite problems with the original study’s methodology, diurnal fluctuations (up to 0.9 degrees F per day), and unreliable thermometers. The authors believe the average human body temperature is less than 98.6. Conduct a hypothesis test.\n\nState the null and alternative hypotheses.\nState the significance level that will be used.\nLoad the data from the file “temperature.csv” and generate summary statistics and a boxplot of the temperature data. We will not be using gender or heart rate for this problem.\nCompute a test statistic. We are going to help you with this part. We cannot do a randomization test since we don’t have a second variable. It would be nice to use the mean as a test statistic but we don’t yet know the sampling distribution of the sample mean.\nLet’s get clever. If the distribution of the sample is symmetric (this is an assumption but look at the boxplot and summary statistics to determine if you are comfortable with it), then under the null hypothesis, the observed values should be equally likely to either be greater or less than 98.6. Thus, our test statistic is the number of cases that have a positive difference between 98.6 and the observed value. This will be a binomial distribution with a probability of success (having a positive difference) of 0.5. You must also account for the possibility that there are observations of 98.6 in the data.\nDetermine the \\(p\\)-value.\nDraw a conclusion.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#solutions-manual",
    "href": "20-Hypothesis-Testing-ProbDist.html#solutions-manual",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#footnotes",
    "href": "20-Hypothesis-Testing-ProbDist.html#footnotes",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "This case study is described in Made to Stick: Why Some Ideas Survive and Others Die by Chip and Dan Heath.↩︎\nThe \\(p\\)-value is the chance of seeing the observed data or something more in favor of the alternative hypothesis (something as or more extreme) given that guessing has a probability of success of 0.5. Since we didn’t observe many simulations with even close to just 42 listeners correct, the \\(p\\)-value will be small, around 1-in-1000.↩︎\nThe \\(p\\)-value is less than 0.05, so we reject the null hypothesis. There is statistically significant evidence, and the data provide strong evidence that the chance a listener will guess the correct tune is different from 50%.↩︎\n“Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎\nL. Shoemaker Allen (1996) What’s Normal? – Temperature, Gender, and Heart Rate, Journal of Statistics Education, 4:2↩︎\nMackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), “A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich,” Journal of the American Medical Association, 268, 1578-1580.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic\nDocuments for r. https://github.com/rstudio/rmarkdown.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben\nBaumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2022.\nOpenintro: Data Sets and Supplemental Functions from OpenIntro\nTextbooks and Labs. http://openintrostat.github.io/openintro/.\n\n\nDiez, David, Christopher Barr, and Mine Çetinkaya-Rundel. 2014.\nIntroductory Statistics with Randomization and Simulation. 1st\ned. Openintro. https://www.openintro.org/book/isrs/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021.\nISLR: Data for an Introduction to Statistical Learning with\nApplications in r. https://www.statlearning.com.\n\n\nKerns, Jay. 2010. Introductory to Probability and Statistics with\nr. 1st ed. http://ipsur.r-forge.r-project.org/book/download/IPSUR.pdf.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023.\nVcd: Visualizing Categorical Data.\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics:\nAn Introduction Using r. Vol. 13. American Mathematical Soc.\n\n\nPruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2024.\nMosaic: Project MOSAIC Statistics and Mathematics Teaching\nUtilities. https://github.com/ProjectMOSAIC/mosaic.\n\n\nRipley, Brian. 2024. MASS: Support Functions and Datasets for\nVenables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the\nTidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey\nDunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic\nReport Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Joe Cheng, and Xianying Tan. 2024. DT: A Wrapper of the\nJavaScript Library DataTables. https://github.com/rstudio/DT.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with Kable and\nPipe Syntax. http://haozhu233.github.io/kableExtra/.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "References"
    ]
  }
]