[
  {
    "objectID": "01-Data-Case-Study.html",
    "href": "01-Data-Case-Study.html",
    "title": "1  Data Case Study",
    "section": "",
    "text": "1.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#objectives",
    "href": "01-Data-Case-Study.html#objectives",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Use R for basic analysis and visualization.\nCompile a pdf file report from a RMD or qmd file in R.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "href": "01-Data-Case-Study.html#introduction-to-descriptive-statistical-modeling",
    "title": "1  Data Case Study",
    "section": "1.2 Introduction to descriptive statistical modeling",
    "text": "1.2 Introduction to descriptive statistical modeling\nIn this first block of material, we will focus on data types, collection methods, summaries, and visualizations. We also intend to introduce computing via the R package. Programming in R requires some focus early in this book and we will supplement with some online courses. There is relatively little mathematics in this first block.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#the-data-analytic-process",
    "href": "01-Data-Case-Study.html#the-data-analytic-process",
    "title": "1  Data Case Study",
    "section": "1.3 The data analytic process",
    "text": "1.3 The data analytic process\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. It is helpful to put statistics in the context of a general process of investigation:\n\nIdentify a question or problem.\nCollect relevant data on the topic.\nExplore and understand the data.\nAnalyze the data.\nForm a conclusion.\nMake decisions based on the conclusion.\n\nThis is typical of an explanatory process because it starts with a research question and proceeds. However, sometimes an analysis is exploratory in nature. There is data but not necessarily a research question. The purpose of the analysis is to find interesting features in the data and sometimes generate hypotheses. In this book, we focus on the explanatory aspects of analysis.\nStatistics as a subject focuses on making stages 2-5 objective, rigorous, and efficient. That is, statistics has three primary components:\n\nHow best can we collect data?\n\nHow should it be analyzed?\n\nAnd what can we infer from the analysis?\n\nThe topics scientists investigate are as diverse as the questions they ask. However, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference. This chapter provides a glimpse into these and other themes we will encounter throughout the rest of the book.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#case-study",
    "href": "01-Data-Case-Study.html#case-study",
    "title": "1  Data Case Study",
    "section": "1.4 Case study",
    "text": "1.4 Case study\nIn this chapter, we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke. 1 2 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer:\n\n1.4.1 Research question\n\nDoes the use of stents reduce the risk of stroke?\n\n\n\n1.4.2 Collect the relevant data\nThe researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups:\nTreatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification.\nControl group. Patients in the control group received the same medical management as the treatment group but did not receive stents.\nResearchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group.\nThis is an experiment and not an observational study. We will learn more about these ideas in this block.\nResearchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment.\n\n\n1.4.3 Import data\nWe begin our first use of R.\nIf you need to install a package, most likely it will be on CRAN, the Comprehensive R Archive Network. Before a package can be used, it must be installed on the computer (once per computer or account) and loaded into a session (once per R session). When you exit R, the package stays installed on the computer but will not be reloaded when R is started again.\nIn summary, R has packages that can be downloaded and installed from online repositories such as CRAN. When you install a package, which only needs to be done once per computer or account, in R all it is doing is placing the source code in a library folder designated during the installation of R. Packages are typically collections of functions and variables that are specific to a certain task or subject matter.\nFor example, to install the mosaic package, enter:\ninstall.packages(\"mosaic\") # fetch package from CRAN\nIn RStudio, there is a Packages tab that makes it easy to add and maintain packages.\nTo use a package in a session, we must load it. This makes it available to the current session only. When you start R again, you will have to load packages again. The command library() with the package name supplied as the argument is all that is needed. For this session, we will load tidyverse and mosaic. Note: the box below is executing the R commands, this is known as reproducible research since you can see the code and then you can run or modify as you need.\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nNext read in the data into the working environment.\n\n# This code reads the `stent_study.csv` file into the `stent_study` object.\nstent_study &lt;- read_csv(\"data/stent_study.csv\")\n\nNote on commenting code: It is good practice to comment code. Here are some of the best practices for commenting computer code:\nComments should explain why code is written the way it is, rather than explaining what the code does. This means that you should explain the intent of the code, not just the steps that it takes to achieve that intent.\nComments should be brief and to the point. There is no need to write long, rambling comments. Just write enough to explain what the code is doing and why.\nComments should be clear and concise. Use plain language that is easy to understand. Avoid jargon and technical terms that the reader may not be familiar with.\nComments should be consistent with the style of the code. If the code is written in a formal style, then the comments should also be formal. If the code is written in a more informal style, then the comments should be informal.\nComments should be up-to-date. If you make changes to the code, then you should also update the comments to reflect those changes.\nIn additional, consider the following practices in writing your code:\nUsing a consistent comment style. This will make it easier for other people to read and understand your code.\nUsing meaningful names for variables and functions. This will help to reduce the need for comments.\nUse indentation and whitespace to make your code easier to read. This will also help to reduce the need for comments.\nDocument your code. This means writing a separate document that explains the purpose of the code, how to use it, and any known limitations.\nBy following these best practices, you can write code that is easy to understand and maintain. This will make your code more reusable and will help to prevent errors.\nNow back to our code. Let’s break this code down. We are reading from a .csv file and assigning the results into an object called stent_study. The assignment arrow &lt;- means we assign what is on the right to what is on the left. The R function we use in this case is read_csv(). When using R functions, you should ask yourself:\n\nWhat do I want R to do?\nWhat information must I provide for R to do this?\n\nWe want R to read in a .csv file. We can get help on this function by typing ?read_csv or help(read_csv) at the prompt. The only required input to read_csv() is the file location. We have our data stored in a folder called “data” under the working directory. We can determine the working directory by typing getwd() at the prompt.\n\ngetwd()\n\nSimilarly, if we wish to change the working directory, we can do so by using the setwd() function:\n\nsetwd('C:/Users/Brianna.Hitt/Documents/ProbStat/Another Folder')\n\nIn R if you use the view(), you will see the data in what looks like a standard spreadsheet.\n\nView(stent_study)\n\n\n\n1.4.4 Explore data\nBefore we attempt to answer the research question, let’s look at the data. We want R to print out the first 10 rows of the data. The appropriate function is head() and it needs the data object. By default, R will output the first 6 rows. By using the n = argument, we can specify how many rows we want to view.\n\nhead(stent_study, n = 10)\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nWe also want to “inspect” the data. The function is inspect() and R needs the data object stent_study.\n\ninspect(stent_study)\n\n\ncategorical variables:  \n        name     class levels   n missing\n1      group character      2 451       0\n2  outcome30 character      2 451       0\n3 outcome365 character      2 451       0\n                                   distribution\n1 control (50.3%), trmt (49.7%)                \n2 no_event (89.8%), stroke (10.2%)             \n3 no_event (83.8%), stroke (16.2%)             \n\n\nTo keep things simple, we will only look at the outcome30 variable in this case study. We will summarize the data in a table. Later in the book, we will learn to do this using the tidy package; for now we use the mosaic package. This package makes use of the modeling formula that you will use extensively later in this book. The modeling formula is also used in Math 378.\nWe want to summarize the data by making a table. From mosaic, we use the tally() function. Before using this function, we have to understand the basic formula notation that mosaic uses. The basic format is:\ngoal(y ~ x, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x as “y tilde x” and interpret it in the equivalent forms: “y broken down by x”; “y modeled by x”; “y explained by x”; “y depends on x”; or “y accounted for by x.” For graphics, it’s reasonable to read the formula as “y vs. x”, which is exactly the convention used for coordinate axes.\nFor this exercise, we want to apply tally() to the variables group and outcome30. In this case it does not matter which we call y and x; however, it is more natural to think of outcome30 as a dependent variable.\n\ntally(outcome30 ~ group, data = stent_study, margins = TRUE)\n\n          group\noutcome30  control trmt\n  no_event     214  191\n  stroke        13   33\n  Total        227  224\n\n\nThe margins option totals the columns.\nOf the 224 patients in the treatment group, 33 had a stroke by the end of the first month. Using these two numbers, we can use R to compute the proportion of patients in the treatment group who had a stroke by the end of their first month.\n\n33 / (33 + 191)\n\n[1] 0.1473214\n\n\n\nExercise:\nWhat proportion of the control group had a stroke in the first 30 days of the study? And why is this proportion different from the proportion reported by inspect()?\n\nLet’s have R calculate proportions for us. Use ? or help() to look at the help menu for tally(). Note that one of the option arguments of the tally() function is format =. Setting this equal to proportion will output the proportions instead of the counts.\n\ntally(outcome30 ~ group, data = stent_study, format = 'proportion', margins = TRUE)\n\n          group\noutcome30     control       trmt\n  no_event 0.94273128 0.85267857\n  stroke   0.05726872 0.14732143\n  Total    1.00000000 1.00000000\n\n\nWe can compute summary statistics from the table. A summary statistic is a single number summarizing a large amount of data.3 For instance, the primary results of the study after 1 month could be described by two summary statistics: the proportion of people who had a stroke in the treatment group and the proportion of people who had a stroke in the control group.\n\nProportion who had a stroke in the treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\)\nProportion who had a stroke in the control group: \\(13/227 = 0.06 = 6\\%\\)\n\n\n\n1.4.5 Visualize the data\nIt is often important to visualize the data. The table is a type of visualization, but in this section we will introduce a graphical method called bar charts.\nWe will use the ggformula package to visualize the data. It is a wrapper to the ggplot2 package which is becoming the industry standard for generating professional graphics. However, the interface for ggplot2 can be difficult to learn and we will ease into it by using ggformula, which makes use of the formula notation introduced above. The ggformula package was loaded when we loaded mosaic.4\nTo generate a basic graphic, we need to ask ourselves what information we are trying to see, what particular type of graph is best, what corresponding R function to use, and what information that R function needs in order to build a plot. For categorical data, we want a bar chart and the R function gf_bar() needs the data object and the variable(s) of interest.\nHere is our first attempt. In Figure 1.1, we leave the y portion of our formula blank. Doing this implies that we simply want to view the number/count of outcome30 by type. We will see the two levels of outcome30 on the x-axis and counts on the y-axis.\n(ref:ggfbold) Using ggformula to create a bar chart.\n\ngf_bar(~outcome30, data = stent_study)\n\n\n\n\n\n\n\nFigure 1.1: Using ggformula to create a bar chart.\n\n\n\n\n\n\nExercise:\nExplain Figure 1.1.\n\nThis plot graphically shows us the total number of “stroke” and the total number of “no_event”. However, this is not what we want. We want to compare the 30-day outcomes for both treatment groups. So, we need to break the data into different groups based on treatment type. In the formula notation, we now update it to the form:\ngoal(y ~ x|z, data = MyData, ...) # pseudo-code for the formula template\nWe read y ~ x|z as “y tilde x by z” and interpret it in the equivalent forms: “y modeled by x for each z”; “y explained by x within each z”; or “y accounted for by x within z.” For graphics, it’s reasonable to read the formula as “y vs. x for each z”. Figure Figure 1.2 shows the results.\n\ngf_bar(~outcome30|group, data = stent_study) \n\n\n\n\n\n\n\nFigure 1.2: Bar charts conditioned on the group variable.\n\n\n\n\n\n\n1.4.5.1 More advanced graphics\nAs a prelude for things to come, the above graphic needs work. The labels don’t help and there is no title. We could add color. Does it make more sense to use proportions? Here is the code and results for a better graph, see Figure Figure 1.3. Don’t worry if this seems a bit advanced, but feel free to examine each new component of this code.\n\n# This code creates a graph showing the impact of stents on stroke.\n# The `gf_props()` function creates a bar graph showing the number of events\n# for each experimental group. The `fill` argument specifies the fill color\n# for each group. The `position = 'fill'` argument specifies that the bars\n# should be filled to the top.\n\n# The `gf_labs()` function adds the title, subtitle, x-axis label, and y-axis\n# label to the graph.\n\n# The `gf_theme()` function applies a black-and-white theme to the graph.\n\nstent_study %&gt;%\ngf_props(~group, fill = ~outcome30, position = 'fill') %&gt;%\n  gf_labs(title = \"Impact of Stents of Stroke\",\n          subtitle = 'Experiment with 451 Patients',\n          x = \"Experimental Group\",\n          y = \"Number of Events\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 1.3: Better graph.\n\n\n\n\n\nNotice that we used the pipe operator, %&gt;%. This operator allows us to string functions together in a manner that makes it easier to read the code. In the above code, we are sending the data object stent_study into the function gf_props() to use as data, so we don’t need the data = argument. In math, this is a composition of functions. Instead of f(g(x)) we could use a pipe f(g(x)) = g(x) %&gt;% f().\n\n\n\n1.4.6 Conclusion\nThese two summary statistics (the proportions of people who had a stroke) are useful in looking for differences in the groups, and we are in for a surprise: an additional 9% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a real difference due to the treatment?\nThis second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 9% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance?\nThis is a preview of step 4, analyze the data, and step 5, form a conclusion, of the analysis cycle. While we haven’t yet covered statistical tools to fully address these steps, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients.\nBe careful: Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "01-Data-Case-Study.html#footnotes",
    "href": "01-Data-Case-Study.html#footnotes",
    "title": "1  Data Case Study",
    "section": "",
    "text": "Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003.↩︎\nNY Times article reporting on the study: http://www.nytimes.com/2011/09/08/health/research/08stent.html↩︎\nFormally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others.↩︎\nhttps://cran.r-project.org/web/packages/ggformula/vignettes/ggformula-blog.html↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Case Study</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html",
    "href": "17-Estimation-Methods.html",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#objectives",
    "href": "17-Estimation-Methods.html#objectives",
    "title": "17  Estimation Methods",
    "section": "",
    "text": "Obtain a method of moments estimate of a parameter or set of parameters.\n\nGiven a random sample from a distribution, obtain the likelihood function.\n\nObtain a maximum likelihood estimate of a parameter or set of parameters.\n\nDetermine if an estimator is unbiased.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#transition",
    "href": "17-Estimation-Methods.html#transition",
    "title": "17  Estimation Methods",
    "section": "17.2 Transition",
    "text": "17.2 Transition\nWe started this book with descriptive models of data and then moved onto probability models. In these probability models, we have been characterizing experiments and random processes using both theory and simulation. These models are using a model about a random event to make decisions about data. These models are about the population and are used to make decisions about samples and data. For example, suppose we flip a fair coin 10 times, and record the number of heads. The population is the collection of all possible outcomes of this experiment. In this case, the population is infinite, as we could run this experiment repeatedly without limit. If we assume, model, the number of heads as a binomial distribution, we know the exact distribution of the outcomes. For example, we know that exactly 24.61% of the time, we will obtain 5 heads out of 10 flips of a fair coin. We can also use the model to characterize the variance, that is when it does not equal 5 and how much different from 5 it will be. However, these probability models are highly dependent on the assumptions and the values of the parameters.\nFrom this point on in the book, we will focus on statistical models. Statistical models describe one or more variables and their relationships. We use these models to make decisions about the population, to predict future outcomes, or both. Often we don’t know the true underlying process; all we have is a sample of observations and perhaps some context. Using inferential statistics, we can draw conclusions about the underlying process. For example, suppose we are given a coin and we don’t know whether it is fair. So, we flip it a number of times to obtain a sample of outcomes. We can use that sample to decide whether the coin could be fair.\nIn some sense, we’ve already explored some of these concepts. In our simulation examples, we have drawn observations from a population of interest and used those observations to estimate characteristics of another population or segment of the experiment. For example, we explored random variable \\(Z\\), where \\(Z=|X - Y|\\) and \\(X\\) and \\(Y\\) were both uniform random variables. Instead of dealing with the distribution of \\(Z\\) directly, we simulated many observations from \\(Z\\) and used this simulation to describe the behavior of \\(Z\\).\nStatistical models and probability models are not separate. In statistical models we find relationships, the explained portion of variation, and use probability models for the remaining random variation. In Figure 17.1, we demonstrate this relationship between the two types of models. In the first part of our studies, we will use univariate data in statistical models to estimate the parameters of a probability model. From there we will develop more sophisticated models to include multivariate models.\n\n\n\n\n\n\n\n\nFigure 17.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we don’t know the underlying process, and must infer based on representative samples.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#estimation",
    "href": "17-Estimation-Methods.html#estimation",
    "title": "17  Estimation Methods",
    "section": "17.3 Estimation",
    "text": "17.3 Estimation\nRecall that in probability models, we have complete information about the population and we use that to describe the expected behavior of samples from that population. In statistics we are given a sample from a population about which we know little or nothing.\nIn this chapter, we will discuss estimation. Given a sample, we would like to estimate population parameters. There are several ways to do that. We will discuss two methods: method of moments and maximum likelihood.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#method-of-moments",
    "href": "17-Estimation-Methods.html#method-of-moments",
    "title": "17  Estimation Methods",
    "section": "17.4 Method of Moments",
    "text": "17.4 Method of Moments\nRecall earlier we discussed moments. We can refer to \\(\\mbox{E}(X) = \\mu\\) as the first moment or mean. Further, we can refer to \\(\\mbox{E}(X^k)\\) as the \\(k\\)th central moment and \\(\\mbox{E}[(X-\\mu)^k]\\) as the \\(k\\) moment around the mean. The second moment around the mean is also known as variance. It is important to point out that these are POPULATION moments and are typically some function of the parameters of a probability model.\nSuppose \\(X_1,X_2,...,X_n\\) is a sequence of independent, identically distributed random variables with some distribution and parameters \\(\\boldsymbol{\\theta}\\). When provided with a random sample of data, we will not know the population moments. However, we can obtain sample moments. The \\(k\\)th central sample moment is denoted by \\(\\hat{\\mu}_k\\) and is given by \\[\n\\hat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n x_i^k\n\\]\nThe \\(k\\)th sample moment around the mean is denoted by \\(\\hat{\\mu}'_k\\) and is given by \\[\n\\hat{\\mu}'_k=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^k\n\\]\nThe value \\(\\hat{\\mu}\\) is read “mu-hat”. The hat denotes that the value is an estimate.\nWe can use the sample moments to estimate the population moments since the population moments are usually functions of a distribution’s parameters, \\(\\boldsymbol{\\theta}\\). Thus, we can solve for the parameters to obtain method of moments estimates of \\(\\boldsymbol{\\theta}\\).\nThis is all technical, so let’s look at an example.\n\nExample:\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d., independent and identically distributed, sample from a uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), and we don’t know \\(\\theta\\). That is, our data consists of positive random numbers but we don’t know the upper bound. Find the method of moments estimator for \\(\\theta\\), the upper bound.\n\nWe know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{E}(X)=\\frac{a+b}{2}\\). So, in this case, \\(\\mbox{E}(X)={\\theta \\over 2}\\). This is the first population moment. We can estimate this with the first sample moment, which is just the sample mean: \\[\n\\hat{\\mu}_1=\\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x}\n\\]\nOur best guess for the first population moment (\\(\\theta/2\\)) is the first sample moment (\\(\\bar{x}\\)). From a common sense perspective, we are hoping that the sample moment will be close in value to the population moment, so we can set them equal and solve for the unknown population parameter. This is essentially what we were doing in our simulations of probability models. Solving for \\(\\theta\\) yields our method of moments estimator for \\(\\theta\\): \\[\n\\hat{\\theta}_{MoM}=2\\bar{x}\n\\]\nNote that we could have used the second moments about the mean as well. This is less intuitive but still applicable. In this case we know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{Var}(X)=\\frac{(b - a)^2}{12}\\). So, in this case, \\(\\mbox{Var}(X)=\\frac{\\theta ^2}{ 12}\\). We use the second sample moment about the mean \\(\\hat{\\mu}'_2=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^2\\) which is not quite the sample variance. In fact, the sample variance is related to the second sample moment about the mean by \\(\\hat{\\mu}'_2 = s^2 \\frac{n}{n-1}\\). Setting the population moment and sample moment equal and solving we get\n\\[\n\\hat{\\theta}_{MoM}=\\sqrt{\\frac{12n}{n-1}}s\n\\]\nTo decide which is better we need a criteria of comparison. This is beyond the scope of this book, but some common criteria are unbiased and minimum variance.\nThe method of moments can be used to estimate more than one parameter as well. We simply would have to incorporate higher order moments.\n\nExample:\nSuppose we take an i.i.d. sample from the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Find method of moments estimates of \\(\\mu\\) and \\(\\sigma\\).\n\nFirst, we remember that we know two population moments for the normal distribution: \\[\n\\mbox{E}(X)=\\mu \\hspace{1cm} \\mbox{Var}(X)=\\mbox{E}[(X-\\mu)^2]=\\sigma^2\n\\]\nSetting these equal to the sample moments yields: \\[\n\\hat{\\mu}_{MoM}=\\bar{x} \\hspace{1cm} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\nAgain, we notice that the estimate for \\(\\sigma\\) is different from sample standard deviation discussed earlier in the book. The reason for this is a property of estimators called unbiased. Notice that if we treat the data points as random variables then the estimators are random variables. We can then take the expected value of the estimator and if this equals the parameter being estimated, then it is unbiased. Mathematically, this is written \\[\nE(\\hat{\\theta})=\\theta\n\\] Unbiased is not a required property for an estimator, but many practitioners find it desirable. In words, unbiased means that on average the estimator will equal the true value. Sample variance using \\(n-1\\) in the denominator is an unbiased estimate of the population variance.\n\nExercise:\nYou shot 25 free throws and make 21. Assuming a binomial model fits. Find an estimate of the probability of making a free throw.\n\nThere are two ways to approach this problem depending on how we define the random variable. In the first case we will use a binomial random variable, \\(X\\) the number of made free throws in 25 attempts. In this case, we only ran the experiment once and have the observed result of 21. Recall for the binomial \\(E(X)=np\\) where \\(n\\) is the number of attempts and \\(p\\) is the probability of success. The sample mean is 21 since we only have one data point. Using the method of moments, we set the first population mean equal to the first sample mean \\(np=\\frac{\\sum{x_i}}{m}\\), notice \\(n\\) is the number of trials 25 and \\(m\\) is the number of data points 1, or \\(25 \\hat{p} = 21\\). Thus \\(\\hat{p} = \\frac{21}{25}\\).\nA second approach is to let \\(X_i\\) be a single free throw, we have a Bernoulli random variable. This variable takes on the values of 0 if we miss and 1 if we make the free throw. Thus we have 25 data points. For a Bernoulli random variable \\(E(X)=p\\). The sample is \\(\\bar{x} = \\frac{21}{25}\\). Using the method of moments, we set the sample mean equal to the population mean. We have \\(E(X) = \\hat{p} = \\bar{x} = \\frac{21}{25}\\). This is a natural estimate; we estimate our probability of success as the number of made free throws divided by the number of shots. As a side note, this is an unbiased estimator since \\[\nE(\\hat{p})=E\\left( \\sum{\\frac{X_i}{n}} \\right)\n\\]\n\\[\n=  \\sum{E\\left( \\frac{X_i}{n} \\right)}= \\sum{ \\frac{E\\left(X_i\\right)}{n}}=\\sum{\\frac{p}{n}}=\\frac{np}{n}=p\n\\]",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "17-Estimation-Methods.html#maximum-likelihood",
    "href": "17-Estimation-Methods.html#maximum-likelihood",
    "title": "17  Estimation Methods",
    "section": "17.5 Maximum likelihood",
    "text": "17.5 Maximum likelihood\nRecall that using method of moments involves finding values of the parameters that cause the population moments to be equal to the sample moments. Solving for the parameters yields method of moments estimates.\nNext we will discuss one more estimation method, maximum likelihood estimation. In this method, we are finding values of parameters that would make the observed data most “likely”. In order to do this, we first need to introduce the likelihood function.\n\n17.5.1 Likelihood Function\nSuppose \\(x_1,x_2,...,x_n\\) is an i.i.d. random sample from a distribution with mass/density function \\(f_{X}(x;\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) are the parameters. Let’s take a second to explain this notation. We are using a bold symbol for \\(\\boldsymbol{\\theta}\\) to indicate it is a vector, that it can be one or more values. However, in the pmf/pdf \\(x\\) is not bold since it is a scalar variable. In our probability models we know \\(\\boldsymbol{\\theta}\\) and then use to model to make decision about the random variable \\(X\\).\nThe likelihood function is denoted as \\(L(\\boldsymbol{\\theta};x_1,x_2,...,x_n) = L(\\boldsymbol{\\theta};\\boldsymbol{x})\\). Now we have multiple instances of the random variable, we use \\(\\boldsymbol{x}\\). Since our random sample is i.i.d., independent and identically distributed, we can write the likelihood function as a product of the pmfs/pdfs: \\[\nL(\\boldsymbol{\\theta};\\boldsymbol{x})=\\prod_{i=1}^n f_X(x_i;\\boldsymbol{\\theta})\n\\]\nThe likelihood function is really the pmf/pdf except instead of the variables being random and the parameter(s) fixed, the values of the variable are known and the parameter(s) are unknown. A note on notation, we are using the semicolon in the pdf and likelihood function to denote what is known or given. In the pmf/pdf the parameters are known and thus follow the semicolon. The opposite is the case in the likelihood function.\nLet’s do an example to help understand these ideas.\n\nExample:\nSuppose we are presented with a coin and are unsure of its fairness. We toss the coin 50 times and obtain 18 heads and 32 tails. Let \\(\\pi\\) be the probability that a coin flip results in heads, we could use \\(p\\) but we are getting you used to the two different common ways to represent a binomial parameter. What is the likelihood function of \\(\\pi\\)?\n\nThis is a binomial process, but each individual coin flip can be thought of as a Bernoulli experiment. That is, \\(x_1,x_2,...,x_{50}\\) is an i.i.d. sample from \\(\\textsf{Binom}(1,\\pi)\\) or, in other words, \\(\\textsf{Bernoulli}(\\pi)\\). Each \\(x_i\\) is either 1 or 0. The pmf of \\(X\\), a Bernoulli random variable, is simply: \\[\nf_X(x;\\pi)= \\binom{1}{x} \\pi^x(1-\\pi)^{1-x} = \\pi^x(1-\\pi)^{1-x}\n\\]\nNotice this makes sense\n\\[\nf_X(1)=P(X=1)= \\pi^1(1-\\pi)^{1-1}=\\pi\n\\]\nand\n\\[\nf_X(0)=P(X=0)= \\pi^0(1-\\pi)^{1-0}=(1-\\pi)\n\\]\nGeneralizing for any sample size \\(n\\), the likelihood function is: \\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{n} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{\\sum_{i=1}^{n} x_i}(1-\\pi)^{n-\\sum_{i=1}^{n} x_i}\n\\]\nFor our example \\(n=50\\) and the\n\\[\nL(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{50} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{18}(1-\\pi)^{32}\n\\]\nwhich makes sense because we had 18 successes, heads, and 32 failures, tails. The likelihood function is a function of the unknown parameter \\(\\pi\\).\n\n\n17.5.2 Maximum Likelihood Estimation\nOnce we have a likelihood function \\(L(\\boldsymbol{\\theta},\\boldsymbol{x})\\), we need to figure out which value of \\(\\boldsymbol{\\theta}\\) makes the data most likely. In other words, we need to maximize \\(L\\) with respect to \\(\\boldsymbol{\\theta}\\).\nMost of the time (but not always), this will involve simple optimization through calculus (i.e., take the derivative with respect to the parameter, set to 0 and solve for the parameter). When maximizing the likelihood function through calculus, it is often easier to maximize the log of the likelihood function, denoted as \\(l\\) and often referred to as the “log-likelihood function”: \\[\nl(\\boldsymbol{\\theta};\\boldsymbol{x})= \\log L(\\boldsymbol{\\theta};\\boldsymbol{x})\n\\] Note that since logarithm is one-to-one, onto and increasing, maximizing the log-likelihood function is equivalent to maximizing the likelihood function, and the maximum will occur at the same values of the parameters. We are using log because now we can take the derivative of a sum instead of a product, thus making it much easier.\n\nExample:\nContinuing our example. Find the maximum likelihood estimator for \\(\\pi\\).\n\nRecall that our likelihood function is \\[\nL(\\pi;\\boldsymbol{x})= \\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\n\\]\nFigure 17.2 is a plot of the likelihood function as a function of the unknown parameter \\(\\pi\\).\n\n\n\n\n\n\n\n\nFigure 17.2: Likelihood function for 18 successes in 50 trials\n\n\n\n\n\nBy visual inspection, the value of \\(\\pi\\) that makes our data most likely, maximizes the likelihood function, is something a little less than 0.4, the actual value is 0.36 as indicated by the blue line in Figure 17.2.\nTo maximize by mathematical methods, we need to take the derivative of the likelihood function with respect to \\(\\pi\\). We can do this because the likelihood function is a continuous function. Even though the binomial is a discrete random variable, its likelihood is a continuous function.\nWe can find the derivative of the likelihood function by applying the product rule: \\[\n{\\,\\mathrm{d}L(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi} = \\left(\\sum x_i\\right) \\pi^{\\sum x_i -1}(1-\\pi)^{n-\\sum x_i} + \\pi^{\\sum x_i}\\left(\\sum x_i -n\\right)(1-\\pi)^{n-\\sum x_i -1}\n\\]\nWe could simplify this, set to 0, and solve for \\(\\pi\\). However, it may be easier to use the log-likelihood function: \\[\nl(\\pi;\\boldsymbol{x})=\\log L(\\pi;\\boldsymbol{x})= \\log \\left(\\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\\right) = \\sum x_i \\log \\pi + (n-\\sum x_i)\\log (1-\\pi)\n\\]\nNow, taking the derivative does not require the product rule: \\[\n{\\,\\mathrm{d}l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi}= {\\sum x_i \\over \\pi} - {n-\\sum x_i\\over (1-\\pi)}\n\\]\nSetting equal to 0 yields: \\[\n{\\sum x_i \\over \\pi} ={n-\\sum x_i\\over (1-\\pi)}\n\\]\nSolving for \\(\\pi\\) yields \\[\n\\hat{\\pi}_{MLE}={\\sum x_i \\over n}\n\\]\nNote that technically, we should confirm that the function is concave down at our critical value, ensuring that \\(\\hat{\\pi}_{MLE}\\) is, in fact, a maximum: \\[\n{\\,\\mathrm{d}^2 l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi^2}= {-\\sum x_i \\over \\pi^2} - {n-\\sum x_i\\over (1-\\pi)^2}\n\\]\nThis value is negative for all relevant values of \\(\\pi\\), so \\(l\\) is concave down and \\(\\hat{\\pi}_{MLE}\\) is a maximum.\nIn the case of our example (18 heads out of 50 trials), \\(\\hat{\\pi}_{MLE}=18/50=0.36\\).\nThis seems to make sense. Our best guess for the probability of heads is the number of observed heads divided by our number of trials. That was a great deal of algebra and calculus for what appears to be an obvious answer. However, in more difficult problems, it is not as obvious what to use for a MLE.\n\n\n17.5.3 Numerical Methods\nWhen obtaining MLEs, there are times when analytical methods (calculus) are not feasible or not possible. In the Pruim book (Pruim 2011), there is a good example regarding data from Old Faithful at Yellowstone National Park. We need to load the fastR2 package for this example.\n\nlibrary(fastR2)\n\nThe faithful data set is preloaded into R and contains 272 observations of 2 variables: eruption time in minutes and waiting time until next eruption. If we plot eruption durations, we notice that the distribution appears bimodal, see Figure 17.3.\n\n\n\n\n\n\n\n\nFigure 17.3: Histogram of eruption durations of Old Faithful.\n\n\n\n\n\nWithin each section, the distribution appears somewhat bell-curve-ish so we’ll model the eruption time with a mixture of two normal distributions. In this mixture, a proportion \\(\\alpha\\) of our eruptions belong to one normal distribution and the remaining \\(1-\\alpha\\) belong to the other normal distribution. The density function of eruptions is given by: \\[\n\\alpha f(x;\\mu_1,\\sigma_1)+(1-\\alpha)f(x;\\mu_2,\\sigma_2)\n\\]\nwhere \\(f\\) is the pdf of the normal distribution with parameters specified.\nWe have five parameters to estimate: \\(\\alpha, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). Obviously, estimation through differentiation is not feasible and thus we will use numerical methods. This code is less in the spirit of tidyverse but we want you to see the example. Try to work your way through the code below:\n\n# Define function for pdf of eruptions as a mixture of normals\ndmix&lt;-function(x,alpha,mu1,mu2,sigma1,sigma2){\n  if(alpha &lt; 0) dnorm(x,mu2,sigma2)\n  if(alpha &gt; 1) dnorm(x,mu1,sigma1)\n  if(alpha &gt;= 0 && alpha &lt;=1){\n    alpha*dnorm(x,mu1,sigma1)+(1-alpha)*dnorm(x,mu2,sigma2)\n  }\n}\n\nNext write a function for the log-likelihood function. R is a vector based programming language so we send theta into the function as a vector argument.\n\n# Create the log-likelihood function\nloglik&lt;-function(theta,x){\n  alpha=theta[1]\n  mu1=theta[2]\n  mu2=theta[3]\n  sigma1=theta[4]\n  sigma2=theta[5]\n  density&lt;-function(x){\n    if(alpha&lt;0) return (Inf)\n    if(alpha&gt;1) return (Inf)\n    if(sigma1&lt;0) return (Inf)\n    if(sigma2&lt;0) return (Inf)\n    dmix(x,alpha,mu1,mu2,sigma1,sigma2)\n  }\n  sum(log(sapply(x,density)))\n}\n\nFind the sample mean and standard deviation of the eruption data to use as starting points in the optimization routine.\n\nm&lt;-mean(faithful$eruptions)\ns&lt;-sd(faithful$eruptions)\n\nUse the function nlmax() to maximize the non-linear log-likelihood function.\n\nmle&lt;-nlmax(loglik,p=c(0.5,m-1,m+1,s,s),x=faithful$eruptions)$estimate\nmle\n\n[1] 0.3484040 2.0186065 4.2733410 0.2356208 0.4370633\n\n\nSo, according to our MLEs, about 34.84% of the eruptions belong to the first normal distribution (the one on the left). Furthermore the parameters of that first distribution are a mean of 2.019 and a standard deviation of 0.236. Likewise, 65.16% of the eruptions belong to the second normal with mean of 4.27 and standard deviation of 0.437.\nPlotting the density atop the histogram shows a fairly good fit:\n\ndmix2&lt;-function(x) dmix(x,mle[1],mle[2],mle[3],mle[4],mle[5])\n#y_old&lt;-dmix2(seq(1,6,.01))\n#x_old&lt;-seq(1,6,.01)\n#dens_data&lt;-data.frame(x=x_old,y=y_old)\n#faithful%&gt;%\n#gf_histogram(~eruptions,fill=\"cyan\",color = \"black\") %&gt;%\n#  gf_curve(y~x,data=dens_data)%&gt;%\n#  gf_theme(theme_bw()) %&gt;%\n#  gf_labs(x=\"Duration in minutes\",y=\"Count\") \nhist(faithful$eruptions,breaks=40,freq=F,main=\"\",xlab=\"Duration in minutes.\")\ncurve(dmix2,from=1,to=6,add=T)\n\n\n\n\nHistogram of eruption duration with estimated mixture of normals plotted on top.\n\n\n\n\nThis is a fairly elaborate example but it is cool. You can see the power of the method and the software.\n\n\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.",
    "crumbs": [
      "Advanced Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Estimation Methods</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html",
    "href": "18-Inference-Case-Study.html",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#objectives",
    "href": "18-Inference-Case-Study.html#objectives",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: point estimate, null hypothesis, alternative hypothesis, hypothesis test, randomization, permutation test, test statistic, and \\(p\\)-value.\nConduct a hypothesis test using a randomization test, to include all 4 steps.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#introduction",
    "href": "18-Inference-Case-Study.html#introduction",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nWe now have the foundation to move on to statistical modeling, both inferential and prediction. First we will begin with inference, where we use the ideas of estimation and the variance of estimates to make decisions about the population. We will also briefly introduce the ideas of prediction. Then in the final block of material, we will examine some common linear models and use them for both prediction and inference.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#foundation-for-inference",
    "href": "18-Inference-Case-Study.html#foundation-for-inference",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.3 Foundation for inference",
    "text": "18.3 Foundation for inference\nSuppose a professor randomly splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)?\nWhile the proportions would probably be close to each other, they are probably not exactly the same. We would probably observe a small difference due to chance.\n\nExercise:\nIf we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?1\n\nStudying randomness of this form is a key focus of statistical modeling. In this block, we’ll explore this type of randomness in the context of several applications, and we’ll learn new tools and ideas that can be applied to help make decisions from data.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#randomization-case-study-gender-discrimination",
    "href": "18-Inference-Case-Study.html#randomization-case-study-gender-discrimination",
    "title": "18  Inferential Thinking Case Study",
    "section": "18.4 Randomization case study: gender discrimination",
    "text": "18.4 Randomization case study: gender discrimination\nWe consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.2 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?”\n\n18.4.1 Variability within data\nThe participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects.\n\nExercise:\nIs this an observational study or an experiment? How does the type of study impact what can be inferred from the results?3\n\nFor each supervisor, we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in the table below, we would like to evaluate whether females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against.\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Decision}\\\\\n& & \\mbox{Promoted} & \\mbox{Not Promoted} & \\mbox{Total}  \\\\\n& \\hline \\mbox{male} & 21 & 3 & 24  \\\\\n\\textbf{Gender}& \\mbox{female} & 14 & 10 & 24  \\\\\n& \\mbox{Total} & 35 & 13 & 48  \\\\\n\\end{array}\n\\]\n\nThought Question:\nStatisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against?\n\nThe large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. Most people come to this conclusion because they think these sample statistics are the actual population parameters. We cannot yet be sure if the observed difference represents discrimination or is just from random variability. Generally, there is fluctuation in sample data; if we conducted the experiment again, we would likely get different values. We also wouldn’t expect the sample proportions for males and females to be exactly equal, even if the truth was that the promotion decisions were independent of gender. To make a decision, we must understand the random variability and use it to compare with the observed difference.\nThis question is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. The table shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). This observed difference is what we call a point estimate of the true effect. The point estimate of the difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance.\nWhat would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%.\nConsider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files.\nWe will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Again, we will do this by determining how much difference in promotion rates would happen by random variation and compare this with the observed difference. We will make a decision based on probability considerations.\n\n\n18.4.2 Simulating the study\nThe table of data shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers’ decisions were independent of gender, that is the null hypothesis is true. Then, if we conducted the experiment again with a different random assignment of files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers’ decisions had been independent of gender but we had distributed the files differently.4 We will walk through the steps next.\nFirst let’s import the data.\n\ndiscrim &lt;- read_csv(\"data/discrimination_study.csv\")\n\nLet’s inspect the data set.\n\ninspect(discrim)\n\n\ncategorical variables:  \n      name     class levels  n missing\n1   gender character      2 48       0\n2 decision character      2 48       0\n                                   distribution\n1 female (50%), male (50%)                     \n2 promoted (72.9%), not_promoted (27.1%)       \n\n\nLet’s look at a table of the data, showing gender versus decision.\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   not_promoted promoted Total\n  female           10       14    24\n  male              3       21    24\n  Total            13       35    48\n\n\nLet’s do some categorical data cleaning. To get the tally() results to look like our initial table, we need to change the variables from characters to factors and reorder the levels. By default, factor levels are ordered alphabetically, but we want promoted and male to appear as the first levels in the table.\nWe will use mutate_if() to convert character variables to factors and fct_relevel() to reorder the levels.\n\ndiscrim &lt;- discrim %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  mutate(gender = fct_relevel(gender, \"male\"),\n         decision = fct_relevel(decision, \"promoted\"))\n\n\nhead(discrim)\n\n# A tibble: 6 × 2\n  gender decision    \n  &lt;fct&gt;  &lt;fct&gt;       \n1 female not_promoted\n2 female not_promoted\n3 male   promoted    \n4 female promoted    \n5 female promoted    \n6 female promoted    \n\n\n\ntally(~gender + decision, discrim, margins = TRUE)\n\n        decision\ngender   promoted not_promoted Total\n  male         21            3    24\n  female       14           10    24\n  Total        35           13    48\n\n\nNow that we have the data in the form that we want, we are ready to conduct the permutation test, a simulation of what would have happened if the bankers’ decisions had been independent of gender but we had distributed the files differently. To think about this simulation, imagine we actually had the personnel files. We thoroughly shuffle 48 personnel files, 24 labeled male and 24 labeled female, and deal these files into two stacks. We will deal 35 files into the first stack, which will represent the 35 supervisors who recommended promotion. The second stack will have 13 files, and it will represent the 13 supervisors who recommended against promotion. That is, we keep the same number of files in the promoted and not_promoted categories, and imagine simply shuffling the male and female labels around. Remember that the files are identical except for the listed gender. This simulation then assumes that gender is not important and, thus, we can randomly assign the files to any of the supervisors. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female candidates who were promoted. Since we don’t actually physically have the files, we will do this shuffle via computer code.\nSince the randomization of files in this simulation is independent of the promotion decisions, any difference in the two fractions is entirely due to chance. The following code shows the results of such a simulation.\n\nset.seed(101)\ntally(~shuffle(gender) + decision, discrim, margins = TRUE)\n\n               decision\nshuffle(gender) promoted not_promoted Total\n         male         18            6    24\n         female       17            7    24\n         Total        35           13    48\n\n\nThe shuffle() function randomly rearranges the gender column while keeping the decision column the same. It is really a sampling without replacement. That is, we randomly sample 35 personnel files to be promoted and the other 13 personnel files are not_promoted.\n\nExercise: What is the difference in promotion rates between the two simulated groups? How does this compare to the observed difference, 29.2%, from the actual study?5\n\nCalculating by hand will not help in a simulation, so we must write a function or use an existing one. We will use diffprop() from the mosaic package. The code to find the difference for the original data is:\n\n(obs &lt;- diffprop(decision ~ gender, data = discrim))\n\n  diffprop \n-0.2916667 \n\n\nNotice that this is subtracting the proportion of males promoted from the proportion of females promoted. This does not impact our results as this is an arbitrary decision. We just need to be consistent in our analysis. If we prefer to use positive values, we can adjust the order easily.\n\ndiffprop(decision ~ fct_relevel(gender, \"female\"), data = discrim)\n\n diffprop \n0.2916667 \n\n\nNotice what we have done here; we developed a single value metric to measure the relationship between gender and decision. This single value metric is called the test statistic. We could have used a number of different metrics, to include just the difference in number of promoted males and females. The key idea in hypothesis testing is that once you decide on a test statistic, you need to find the distribution of that test statistic, assuming the null hypothesis is true.\n\n\n18.4.3 Checking for independence\nWe computed one possible difference under the null hypothesis in the exercise above, which represents one difference due to chance. Repeating the simulation, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. That is, the difference if there really is no relationship between gender and the promotion decision. We are using a simulation when there is actually a finite number of permutations of the gender label. From Chapter @ref(PROBRULES) on counting, we have 48 labels of which 24 are male and 24 are female. Thus the total number of ways to arrange the labels differently is:\n\\[\n\\frac{48!}{24!\\cdot24!} \\approx 3.2 \\cdot 10^{13}\n\\]\n\nfactorial(48) / (factorial(24)*factorial(24))\n\n[1] 3.22476e+13\n\n\nAs is often the case, the number of all possible permutations is too large to find by hand or even via code. Thus, we will use a simulation, a subset of all possible permutations, to approximate the permutation test. Using simulation in this way is called a randomization test.\nLet’s simulate the experiment and plot the simulated values of the difference in the proportions of male and female files recommended for promotion.\n\nset.seed(2022)\nresults &lt;- do(10000)*diffprop(decision ~ shuffle(gender), data = discrim)\n\nIn Figure @ref(fig:teststat1-fig), we will insert a vertical line at the value of our observed difference.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = -0.2916667 ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_labs(x = \"Difference in proportions\", y = \"Counts\",\n          title = \"Gender discrimination in hiring permutation test\",\n          subtitle = \"Test statistic is difference in promotion for female and male\")\n\n\n\n\nDistribution of test statistic.\n\n\n\n\nNote that the distribution of these simulated differences is centered around 0 and is roughly symmetrical. It is centered on zero because we simulated differences in a way that made no distinction between men and women. This makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation under the assumption of the null hypothesis. The histogram also looks like a normal distribution; this is not a coincidence, but a result of the Central Limit Theorem, which we will learn about later in this block.\n\nExample:\nHow often would you observe a difference as extreme as -29.2% (-0.292) according to the figure? (Often, sometimes, rarely, or never?)\n\nIt appears that a difference as extreme as -29.2% due to chance alone would only happen rarely. We can estimate the probability using the results object.\n\nresults %&gt;%\n  summarise(p_value = mean(diffprop &lt;= obs))\n\n  p_value\n1  0.0257\n\n\nIn our simulations, only 2.6% of the simulated test statistics were less than or equal to the observed test statistic, as or more extreme relative to the null hypothesis. Such a low probability indicates that observing such a large difference in proportions from chance alone is rare. This probability is known as a \\(p\\)-value. The \\(p\\)-value is a conditional probability, the probability of the observed value or more extreme given that the null hypothesis is true.\nWe could have also found the exact \\(p\\)-value using the hypergeometric distribution. We have 13 not_promoted positions, so we could have anywhere between 0 and 13 females not promoted. We observed 10 females not promoted. Thus, the exact \\(p\\)-value from the hypergeometric distribution is the probability of 10 or more females not promoted (as or more extreme than the observed) when we select 13 people from a pool of 24 males and 24 females, and the selection is done without replacement.\n\n1 - phyper(9, 24, 24, 13)\n\n[1] 0.02449571\n\n\nAgain, we see a low probability, only 2.4%, of observing 10 or more females not promoted, given that the null hypothesis is true.\nThe observed difference of -29.2% is a rare (low probability) event if there truly is no impact from listing gender in the personnel files. This provides us with two possible interpretations of the study results, in context of our hypotheses:\n\\(H_0\\): Null hypothesis. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely.\n\\(H_A\\): Alternative hypothesis. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of -29.2%.\nWhen we conduct formal studies, we reject a skeptical position (\\(H_0\\)) if the data strongly conflict with that position.6\nIn our analysis, we determined that there was only a ~ 2% probability of obtaining a test statistic where the difference between female and male promotion proportions was 29.2% or larger assuming gender had no impact. So we conclude the data provide sufficient evidence of gender discrimination against women by the supervisors. In this case, we reject the null hypothesis in favor of the alternative hypothesis.\nStatistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur.\nLet’s summarize what we did in this case study. We had a research question and some data to test the question. We then performed 4 steps:\n\nState the null and alternative hypotheses.\n\nCompute a test statistic.\n\nDetermine the \\(p\\)-value.\n\nDraw a conclusion.\n\nWe decided to use a randomization test, a simulation, to answer the question. When creating a randomization distribution, we attempted to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis.\nWe need to simulate a world in which the null hypothesis is true. If we don’t do this, we won’t be testing our null hypothesis. In our problem, we assumed gender and promotion were independent.\n\nUse the data in the original sample.\nThe original data should shed light on some aspects of the distribution that are not determined by the null hypothesis. For our problem, we used the difference in promotion rates. The data does not give us the distribution direction, but it gives us an idea that there is a large difference.\n\nReflect the way the original data were collected.\nThere were 48 files and 48 supervisors. A total of 35 files were recommended for promotion. We keep this the same in our simulation.\n\nThe remainder of this block expands on the ideas of this case study.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "18-Inference-Case-Study.html#footnotes",
    "href": "18-Inference-Case-Study.html#footnotes",
    "title": "18  Inferential Thinking Case Study",
    "section": "",
    "text": "We would be assuming that these two variables, side of the room and use of Apple product, are independent, meaning they are unrelated.↩︎\nRosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.↩︎\nThe study is an experiment, as subjects were randomly assigned a male personnel file or a female personnel file. Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.↩︎\nThe test procedure we employ in this section is formally called a permutation test.↩︎\n\\(18/24 - 17/24 = 0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups.↩︎\nThis reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Inferential Thinking Case Study</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html",
    "href": "19-Hypothesis-Testing-Simulation.html",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#objectives",
    "href": "19-Hypothesis-Testing-Simulation.html#objectives",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "Know and properly use the terminology of a hypothesis test, to include: null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type I error, type II error, false positive, false negative, null distribution, and sampling distribution.\nConduct all four steps of a hypothesis test using randomization.\nDiscuss and explain the ideas of decision errors, one-sided versus two-sided tests, and the choice of a significance level.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#decision-making-under-uncertainty",
    "href": "19-Hypothesis-Testing-Simulation.html#decision-making-under-uncertainty",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.2 Decision making under uncertainty",
    "text": "19.2 Decision making under uncertainty\nAt this point, it is useful to take a look at where we have been in this book and where we are going. We did this in the case study, but we want to discuss it again in a little more detail. We first looked at descriptive models to help us understand our data. This also required us to get familiar with software. We learned about graphical summaries, data collection methods, and summary metrics.\nNext we learned about probability models. These models allowed us to use assumptions and a small number of parameters to make statements about data (a sample) and also to simulate data. We found that there is a close tie between probability models and statistical models. In our first efforts at statistical modeling, we started to use data to create estimates for parameters of a probability model.\nNow we are moving more in depth into statistical models. This is going to tie all the ideas together. We are going to use data from a sample and ideas of randomization to make conclusions about a population. This will require probability models, descriptive models, and some new ideas and terminology. We will generate point estimates for a metric designed to answer the research question and then find ways to determine the variability of the metric. In Figure 19.1, we demonstrate this relationship between probability and statistical models.\n\n\n\n\n\n\n\n\nFigure 19.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen in a sample if we know the underlying process; in statistics, we don’t know the underlying process, and must infer about the population based on representative samples.\n\n\n\n\n\nComputational/Mathematical and hypothesis testing/confidence intervals context\nWe are going to be using data from a sample of the population to make decisions about the population. There are many approaches and techniques for this. In this book, we will be introducing and exploring different approaches; we are establishing foundations. As you can imagine, these ideas are varied, subtle, and at times difficult. We will just be exposing you to the foundational ideas. We want to make sure you understand that to become an accomplished practitioner, you must master the fundamentals and continue to learn the advanced ideas.\nHistorically, there have been two approaches to statistical decision making, hypothesis testing and confidence intervals. At their mathematical foundation, they are equivalent, but, sometimes in practice, they offer different perspectives on the problem. We will learn about both of these approaches.\nThe engines that drive the numeric results of a decision making model are either mathematical or computational. In reality, computational methods have mathematics behind them, and mathematical methods often require computer computations. The real distinction between them is the assumptions we are making about our population. Mathematical solutions typically have stricter assumptions, thus leading to a tractable mathematical solution to the sampling distribution of the test statistic, while computational models relax assumptions but may require extensive computational power. Like all problems, there is a trade off to consider when trying to choose which approach is better. There is no one universal best method. Some methods perform better in certain contexts. It is important to understand that computational methods such as the bootstrap are NOT all you need to know.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#introduction",
    "href": "19-Hypothesis-Testing-Simulation.html#introduction",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.3 Introduction",
    "text": "19.3 Introduction\nIn this chapter we will introduce hypothesis testing. It is really an extension of our last chapter, the case study. We will put more emphasis on terms and core concepts. In this chapter, we will use a computational solution but this will lead us into thinking of mathematical solutions.1 The role of the analyst is always key regardless of the perceived power of the computer. The analyst must take the research question and translate it into a numeric metric for evaluation. The analyst must decide on the type of data and its collection to evaluate the question. The analyst must evaluate the variability in the metric and determine what that means in relation to the original research question. The analyst must propose an answer.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "href": "19-Hypothesis-Testing-Simulation.html#hypothesis-testing",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.4 Hypothesis testing",
    "text": "19.4 Hypothesis testing\nWe will continue to emphasize the ideas of hypothesis testing through a data-driven example, but also via analogy to the US court system. So let’s begin our journey.\n\nExample:\nYou are annoyed by TV commercials. You suspect that there were more commercials in the basic TV channels, typically the local area channels, than in the premium channels you pay extra for. To test this claim, hypothesis, you want to collect some data and draw a conclusion. How would you collect this data?\n\nHere is one approach: we watch 20 random half hour shows of live TV. Ten of those hours are basic TV and the other ten are premium. In each case, you record the total length of commercials during each show.\n\nExercise: Is this enough data? You decide to have your friends help you, so you actually only watched 5 hours and got the rest of the data from your friends. Is this a problem?\n\nWe cannot determine if this is enough data without some type of subject matter knowledge. First, we need to decide what metric to use to determine if a difference exists (more to come on this). Second, we need to decide how big of a difference, from a practical standpoint, is of interest. Is a loss of 1 minute of TV show enough to say there is a difference? How about 5 minutes? These are not statistical questions, but depend on the context of the problem and often require subject matter expertise to answer. Often, data is collected without thought to these considerations. There are several methods that attempt to answer these questions. They are loosely called sample size calculations. This book will not focus on sample size calculations and will leave it to the reader to learn more from other sources. For the second exercise question, the answer depends on the protocol and operating procedures used. If your friends are trained on how to measure the length of commercials, what counts as an ad, and their skills are verified, then it is probably not a problem to use them to collect data. Consistency in measurement is the key.\nThe file ads.csv contains the data. Let’s read the data into R and start to summarize. Remember to load the appropriate R packages.\n\nads &lt;- read_csv(\"data/ads.csv\")\n\n\nads\n\n# A tibble: 10 × 2\n   basic premium\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  6.95    3.38\n 2 10.0     7.8 \n 3 10.6     9.42\n 4 10.2     4.66\n 5  8.58    5.36\n 6  7.62    7.63\n 7  8.23    4.95\n 8 10.4     8.01\n 9 11.0     7.8 \n10  8.52    9.58\n\n\n\nglimpse(ads)\n\nRows: 10\nColumns: 2\n$ basic   &lt;dbl&gt; 6.950, 10.013, 10.620, 10.150, 8.583, 7.620, 8.233, 10.350, 11…\n$ premium &lt;dbl&gt; 3.383, 7.800, 9.416, 4.660, 5.360, 7.630, 4.950, 8.013, 7.800,…\n\n\nNotice that this data may not be tidy; what does each row represent and is it a single observation? We don’t know how the data was obtained, but if each row represents a different friend who watches one basic and one premium channel, then it is possible this data is tidy. We want each observation to be a single TV show, so let’s clean up, or tidy, our data. Remember to ask yourself “What do I want R to do?” and “What does it need to do this?” We want one column that specifies the channel type and another to specify the total length of the commercials.\nWe need R to put, or pivot, the data into a longer form. We need the function pivot_longer(). For more information type vignette(\"pivot\") at the command prompt in R.\n\nads &lt;- ads %&gt;%\n  pivot_longer(cols = everything(), names_to = \"channel\", values_to = \"length\")\nads\n\n# A tibble: 20 × 2\n   channel length\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 basic     6.95\n 2 premium   3.38\n 3 basic    10.0 \n 4 premium   7.8 \n 5 basic    10.6 \n 6 premium   9.42\n 7 basic    10.2 \n 8 premium   4.66\n 9 basic     8.58\n10 premium   5.36\n11 basic     7.62\n12 premium   7.63\n13 basic     8.23\n14 premium   4.95\n15 basic    10.4 \n16 premium   8.01\n17 basic    11.0 \n18 premium   7.8 \n19 basic     8.52\n20 premium   9.58\n\n\nLooks good. Let’s summarize the data.\n\ninspect(ads)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 channel character      2 20       0\n                                   distribution\n1 basic (50%), premium (50%)                   \n\nquantitative variables:  \n    name   class   min     Q1 median      Q3    max    mean       sd  n missing\n1 length numeric 3.383 7.4525  8.123 9.68825 11.016 8.03215 2.121412 20       0\n\n\nThis summary is not what we want, since we want to break it down by channel type.\n\nfavstats(length ~ channel, data = ads)\n\n  channel   min      Q1 median       Q3    max   mean       sd  n missing\n1   basic 6.950 8.30375  9.298 10.30000 11.016 9.2051 1.396126 10       0\n2 premium 3.383 5.05250  7.715  7.95975  9.580 6.8592 2.119976 10       0\n\n\n\nExercise: Visualize the data using a boxplot.\n\n\nads %&gt;%\n  gf_boxplot(channel ~ length) %&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\n\nIt appears that the premium channels are skewed to the left. A density plot may help us compare the distributions and see the skewness, Figure 19.2.\n\nads %&gt;%\n  gf_dens(~length, color = ~channel)%&gt;%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Density\", color = \"Channel Type\" ) %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 19.2: Commercial length broken down by channel type.\n\n\n\n\n\nFrom this data, it looks like there is a difference between the two type of channels, but we must put the research question into a metric that will allow us to reach a decision. We will do this in a hypothesis test. As a reminder, the steps are\n\nState the null and alternative hypotheses.\n\nCompute a test statistic.\n\nDetermine the \\(p\\)-value.\n\nDraw a conclusion.\n\nBefore doing this, let’s visit an example of hypothesis testing that has become common knowledge for us, the US criminal trial system. We could also use the cadet honor system. This analogy allows us to remember and apply the steps.\n\n19.4.1 Hypothesis testing in the US court system\nA US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.\nThe jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).\nJurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty.\nThis is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.\nThere are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.\nNow back to our problem.\n\n\n19.4.2 Step 1- State the null and alternative hypotheses\nThe first step is to translate the research question into hypotheses. As a reminder, our research question is do premium channels have less ad time than basic channels? In collecting the data, we already decided the total length of time of commercials in a 30 minute show was the correct data for answering this question. We believe that premium channels have less commercial time. However, the null hypothesis, the straw man, has to be the default case that makes it possible to generate a sampling distribution.\n\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different.\n\nThese hypotheses are vague. What does it mean for two distributions to be different and how do we measure and summarize this? Let’s move to the second step and then come back and modify our hypotheses. Notice that the null hypothesis states the distributions are the same. When we generate our sampling distribution of the test statistic, we will do so under this null hypothesis.\n\n\n19.4.3 Step 2 - Compute a test statistic.\n\nExercise:\nWhat type of metric could we use to test for a difference in the distributions of commercial lengths between the two types of channels?\n\nThere are many ways for the distributions of lengths of commercials to differ. The easiest is to think of the summary statistics such as mean, median, standard deviation, or some combination of all of these. Historically, for mathematical reasons, it has been common to look at differences in measures of centrality, mean or median. The second consideration is what kind of difference? For example, should we consider a ratio or an actual difference (subtraction)? Again for historical reasons, the difference in means has been used as a measure. To keep things interesting, and to force those with some high school stats experience to think about this problem differently, we are going to use a different metric than has historically been used and taught. This also requires us to write some of our own code. Later, we will ask you to complete the same analysis with a different test statistic, either with your own code or using code from the mosaic package.\nOur metric is the ratio of the median length of commercials in basic channels to premium. Thus, our hypotheses are now:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nFirst, let’s calculate the median length of commercials, by channel type, for our data.\n\nmedian(length ~ channel, data = ads) \n\n  basic premium \n  9.298   7.715 \n\n\nSo, the ratio of median lengths is\n\nmedian(length ~ channel, data = ads)[1] / median(length ~ channel, data = ads)[2]\n\n   basic \n1.205185 \n\n\nLet’s put the calculation of the ratio into a function.\n\nmetric &lt;- function(x){\n  temp &lt;- x[1] / x[2]\n  names(temp) &lt;- \"test_stat\"\n  return(temp)\n}\n\n\nmetric(median(length ~ channel, data = ads))\n\ntest_stat \n 1.205185 \n\n\nNow, let’s save the observed value of the test statistic in an object.\n\nobs &lt;- metric(median(length ~ channel, data = ads))\nobs\n\ntest_stat \n 1.205185 \n\n\nHere is what we have done; we needed a single number metric to use in evaluating the null and alternative hypotheses. The null hypothesis is that the commercial lengths for the two channel types have the same distribution and the alternative is that they don’t. To measure the alternative hypothesis, we decided to use a ratio of the medians. If the ratio is close to 1, then the medians are not different. There may be other ways in which the distributions are different but we have decided on the ratio of medians for this example.\n\n\n19.4.4 Step 3 - Determine the \\(p\\)-value.\nAs a reminder, the \\(p\\)-value is the probability of our observed test statistic or something more extreme, given the null hypothesis is true. Since our null hypothesis is that the distributions are the same, we can use a randomization test. We will shuffle the channel labels since under the null hypothesis, they are irrelevant. Here is the code for one randomization.\n\nset.seed(371)\nmetric(median(length ~ shuffle(channel), data = ads))\n\ntest_stat \n0.9957097 \n\n\nLet’s generate the empirical sampling distribution of the test statistic we developed. We will perform 1,000 simulations now.\n\nset.seed(371)\nresults &lt;- do(1000)*metric(median(length ~ shuffle(channel), data = ads))\n\nNext we create a plot of the distribution of the ratio of median commercial lengths in basic and premium channels, assuming they come from the same population, Figure 19.3.\n\nresults %&gt;%\n  gf_histogram(~test_stat) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 19.3: Historgram of the sampling distribution by an approxiamte permutation test.\n\n\n\n\n\nNotice that this distribution is centered on 1 and appears to be roughly symmetrical. The vertical line is our observed value of the test statistic. It seems to be in the tail, and is larger than expected if the channels came from the same distribution. Let’s calculate the \\(p\\)-value.\n\nresults %&gt;%\n  summarise(p_value = mean(test_stat &gt;= obs))\n\n  p_value\n1   0.026\n\n\nBefore proceeding, we have a technical question: Should we include the observed data in the calculation of the \\(p\\)-value?\nThe answer is that most people would conclude that the original data is one of the possible permutations and thus include it. This practice will also ensure that the \\(p\\)-value from a randomization test is never zero. In practice, this simply means adding 1 to both the numerator and denominator. The mosaic package has done this for us with the prop1() function.\n\nprop1(~(test_stat &gt;= obs), data = results)\n\n prop_TRUE \n0.02697303 \n\n\nThe test we performed is called a one-sided test because we only checked if the median length for basic channels is larger than that for premium channels. In this case of a one-sided test, more extreme meant a ratio much bigger than 1. A two-sided test is also common, in fact it is more common, and is used if we did not apriori think one channel type had longer commercials than the other. In this case, we find the \\(p\\)-value by doubling the single-sided value. This is because more extreme could have happened in either tail of the sampling distribution.\n\n\n19.4.5 Step 4 - Draw a conclusion\nOur research question – do premium channels have less ad time than basic channels? – was framed in context of the following hypotheses:\n\\(H_0\\): Null hypothesis. The distribution of length of commercials for premium and basic channels is the same.\n\\(H_A\\): Alternative hypothesis. The distribution of length of commercials for premium and basic channels is different because the median length of basic channels ads is bigger than for premium channel ads.\nIn our simulations, less than 2.7% of the simulated test statistics were greater than or equal (more extreme relative to the null hypothesis) to the observed test statistic. That is, the observed ratio of 1.2 is a rare event if the distributions of commercial lengths for premium and basic channels truly are the same. By chance alone, we would only expect an observed ratio this large to occur less than 3 in 100 times. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we reject \\(H_0\\) and conclude there is evidence that the median length of basic channel ads is bigger than that for premium channels.\nThe less than 3-in-100 chance is the \\(p\\)-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.\nWhen the \\(p\\)-value is small, i.e. less than a previously set threshold, we say the results are statistically significant2. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The threshold, called the significance level and often represented by the Greek letter \\(\\alpha\\), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application. Using a significance level of \\(\\alpha = 0.05\\) in the TV channel study, we can say that the data provided statistically significant evidence against the null hypothesis.\n\nWe say that the data provide statistically significant evidence against the null hypothesis if the \\(p\\)-value is less than some reference value, usually \\(\\alpha=0.05\\).\n\nIf the null hypothesis is true, unknown to us, the significance level \\(\\alpha\\) defines the probability that we will make a Type 1 Error. We will define decision errors in the next section.\n\nSide note: What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye – good job! There are many video clips that explain the use of 0.05. Sometimes it’s also a good idea to deviate from the standard. It really depends on the risk that the decision maker wants to accept in terms of the two types of decision errors.\n\n\nExercise:\nUse our \\(p\\)-value and a significance level of 0.05 to make a decision.\n\nBased on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed ratio of medians is 0.027. Since this is less than our significance level of \\(\\alpha = 0.05\\), we reject the null in favor of the alternative that the basic channel has longer commercials.\n\n\n19.4.6 Decision errors\nHypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\nThere are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\nExample:\nIn a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 error represent in this context? What does a Type 2 error represent?\n\nIf the court makes a Type 1 error, this means the defendant is truly innocent (\\(H_0\\) true) but is wrongly convicted. A Type 2 error means the court failed to reject \\(H_0\\) (i.e. failed to convict the person) when she was in fact guilty (\\(H_A\\) true).\n\nExercise:\nConsider the commercial length study where we concluded basic channels had longer commercials than premium channels. What would a Type 1 error represent in this context?3\n\n\nExercise:\nHow could we reduce the Type 1 error rate in US courts? What influence would this have on the Type 2 error rate?\n\nTo lower the Type 1 error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 errors.\n\nExercise:\nHow could we reduce the Type 2 error rate in US courts? What influence would this have on the Type 1 error rate?\n\nTo lower the Type 2 error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 error rate.\n\nExercise: Think about the cadet honor system, its metric of evaluation, and the impact on the types of decision errors.\n\nThese exercises provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type for a given amount of data, information.\n\n\n19.4.7 Choosing a significance level\nChoosing a significance level for a test is important in many contexts, and the traditional level is \\(\\alpha = 0.05\\). However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.\nIf making a Type 1 error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01 or 0.001). Under this scenario, we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\).\nIf making a Type 2 error is relatively more dangerous or much more costly than a Type 1 error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null hypothesis is actually false.\nThe significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 error.\n\n\n19.4.8 Introducing two-sided hypotheses\nSo far we have explored whether women were discriminated against and whether commercials were longer depending on the type of channel. In these two case studies, we’ve actually ignored some possibilities:\n\nWhat if men are actually discriminated against?\n\nWhat if ads on premium channels are actually longer?\n\nThese possibilities weren’t considered in our hypotheses or analyses. This may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our worldview:\n\nFraming an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.\n\nIf we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!\n\nThe previous hypothesis tests we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in a single direction, but usually we want to consider all possibilities. To do so, let’s discuss two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone cardiopulmonary resuscitation, CPR.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "href": "19-Hypothesis-Testing-Simulation.html#two-sided-hypothesis-test",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "19.5 Two-sided hypothesis test",
    "text": "19.5 Two-sided hypothesis test\nIt is important to distinguish between a two-sided hypothesis test and a one-sided hypothesis test. In a two-sided test, we are concerned with whether or not the population parameter could take a particular value. For parameter \\(\\theta\\), a set of two-sided hypotheses looks like:\n\\[\nH_0: \\theta=\\theta_0 \\hspace{1.5cm} H_1: \\theta \\neq \\theta_0\n\\] where \\(\\theta_0\\) is a particular value the parameter could take on.\nIn a one-sided test, we are concerned with whether a parameter exceeds or does not exceed a specific value. A set of one-sided hypotheses looks like:\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &gt; \\theta_0\n\\] or\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta &lt; \\theta_0\n\\]\nIn some texts, one-sided null hypotheses include an inequality (\\(H_0 \\leq \\theta_0\\) or \\(H_0 \\geq \\theta_0\\)). We have already demonstrated one-sided tests and, in the next example, we will use a two-sided test.\n\n19.5.1 CPR example\nCardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.\nHere we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.4 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\n\n\n19.5.2 Step 1 - State the null and alternative hypotheses\n\nExercise: Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).\n\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\\(H_0\\): Blood thinners do not have an overall survival effect; survival rate is independent of experimental treatment group. \\(p_c - p_t = 0\\).\n\\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\).\nNotice here that we accelerated the process by already defining our test statistic, our metric, in the hypothesis. It is the difference in survival rates for the control and treatment groups. This is a similar metric to what we used in the case study. We could use others but this will allow us to use functions from the mosaic package and will also help us to understand metrics for mathematically derived sampling distributions.\nThere were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are in the file blood_thinner.csv.\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\nthinner\n\n# A tibble: 90 × 2\n   group     outcome \n   &lt;chr&gt;     &lt;chr&gt;   \n 1 treatment survived\n 2 control   survived\n 3 control   died    \n 4 control   died    \n 5 control   died    \n 6 treatment survived\n 7 control   died    \n 8 control   died    \n 9 treatment died    \n10 treatment survived\n# ℹ 80 more rows\n\n\nLet’s put the data in a table.\n\ntally(~group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\n\n19.5.3 Step 2 - Compute a test statistic.\nThe test statistic we have selected is the difference in survival rate between the control group and the treatment group. The following R command finds the observed proportions.\n\ntally(outcome ~ group, data = thinner, margins = TRUE, format = \"proportion\")\n\n          group\noutcome    control treatment\n  died        0.78      0.65\n  survived    0.22      0.35\n  Total       1.00      1.00\n\n\nNotice the formula we used to get the correct variable in the column for the summary proportions.\nThe observed test statistic can now be found.5\n\nobs &lt;- diffprop(outcome ~ group, data = thinner)\nobs\n\ndiffprop \n   -0.13 \n\n\nBased on the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. We wonder if this difference is easily explainable by chance.\n\n\n19.5.4 Step 3 - Determine the \\(p\\)-value.\nAs we did in the previous two studies, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning treatment and control stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences. This is our empirical sampling distribution, the distribution of differences simulated under the null hypothesis.\n\nset.seed(655)\nresults &lt;- do(10000)*diffprop(outcome ~ shuffle(group), data = thinner)\n\nFigure 19.4 is a histogram of the estimated sampling distribution.\n\nresults %&gt;%\n  gf_histogram(~diffprop) %&gt;%\n  gf_vline(xintercept = obs) %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 19.4: Histogram of the estimated sampling distribution.\n\n\n\n\n\nNotice how it is centered on zero, the assumption of no difference. Also notice that it is unimodal and symmetric. We will use this when we develop mathematical sampling distributions. We now calculate the proportion of simulated differences that are less than or equal to the observed difference.\n\nprop1(~(diffprop &lt;= obs), data = results)\n\nprop_TRUE \n0.1283872 \n\n\nThe left tail area is about 0.128. (Note: it is only a coincidence that our \\(p\\)-value is approximately 0.13 and we also have \\(\\hat{p}_c - \\hat{p}_t= -0.13\\).) However, contrary to how we calculated the \\(p\\)-value in previous studies, the \\(p\\)-value of this test is not 0.128!\nThe \\(p\\)-value is defined as the probability we observe a result at least as favorable to the alternative hypothesis as the observed result (i.e. the observed difference). In this case, any differences greater than or equal to 0.13 would provide equally strong evidence favoring the alternative hypothesis as differences less than or equal to -0.13. A difference of 0.13 would correspond to 13% higher survival rate in the treatment group than the control group.\nThere is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase or decrease the risk of death in patients who undergo CPR before arriving at the hospital.6\nFor a two-sided test, we take the single tail (in this case, 0.128) and double it to get the \\(p\\)-value: 0.256.\n\n\n19.5.5 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment.\n\nDefault to a two-sided test We want to be rigorous and keep an open mind when we analyze data and evidence. In general, you should default to using a two-sided test when conducting hypothesis tests. Use a one-sided hypothesis test only if you truly have interest in only one direction.\n\n\nComputing a \\(p\\)-value for a two-sided test\nFirst compute the \\(p\\)-value for one tail of the distribution, then double that value to get the two-sided \\(p\\)-value. That’s it!\n\nIt is never okay to change two-sided tests to one-sided tests after observing the data.\n\nHypothesis tests should be set up before seeing the data\nAfter observing data, it can be tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses and the significance level should be set up before observing the data.\n\n\n\n19.5.6 How to use a hypothesis test\nThis is a summary of the general framework for using hypothesis testing. These are the same steps as before, with just slightly different wording.\n\nFrame the research question in terms of hypotheses.\nHypothesis tests are appropriate for research questions that can be summarized in two competing hypotheses. The null hypothesis (\\(H_0\\)) usually represents a skeptical perspective or a perspective of no difference. The alternative hypothesis (\\(H_A\\)) usually represents a new view or a difference.\nCollect data with an observational study or experiment.\nIf a research question can be formed into two hypotheses, we can collect data to run a hypothesis test. If the research question focuses on associations between variables but does not concern causation, we would run an observational study. If the research question seeks a causal connection between two or more variables, then an experiment should be used if possible.\nAnalyze the data.\nChoose an analysis technique appropriate for the data and identify the \\(p\\)-value. So far, we’ve only seen one analysis technique: randomization. We’ll encounter several new methods suitable for many other contexts.\nForm a conclusion.\nUsing the \\(p\\)-value from the analysis, determine whether the data provide statistically significant evidence against the null hypothesis. Also, be sure to write the conclusion in plain language so casual readers can understand the results.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "19-Hypothesis-Testing-Simulation.html#footnotes",
    "href": "19-Hypothesis-Testing-Simulation.html#footnotes",
    "title": "19  Hypothesis Testing with Simulation",
    "section": "",
    "text": "In our opinion, this is how things developed historically. However, since computational tools prior to machine computers, humans in most cases, were limited and expensive, there was a shift to mathematical solutions. The relatively recent increase and availability in machine computational power has led to a shift back to computational methods. Thus, some people think mathematical methods predate computational methods, but that is not the case.↩︎\nSome experts in the field of statistics are uncomfortable with using a significance level to declare that something is statistically significant. The choice of a significance level (discussed more in a later section) can be somewhat arbitrary. If you conduct a hypothesis test with a \\(p\\)-value of 0.051 and chose a significance level of 0.05, are your results really non-significant compared to getting a \\(p\\)-value of 0.049? For this reason, some experts shy away from the term statistically significant and instead describe detectable or discernable effects/differences.↩︎\nMaking a Type 1 error in this context would mean that there is no difference in commercial length between basic and premium channels, despite the strong evidence (the data suggesting otherwise) found in the observational study. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings. Replication is part of the scientific method.↩︎\n“Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎\nObserved control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Observed treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_c - \\hat{p}_t = 0.22 - 0.35 = -0.13\\).↩︎\nRealistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis Testing with Simulation</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html",
    "href": "20-Hypothesis-Testing-ProbDist.html",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#objectives",
    "href": "20-Hypothesis-Testing-ProbDist.html#objectives",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "Know and properly use the terminology of a hypothesis test, to include: permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, \\(p\\)-value, and power.\nConduct all four steps of a hypothesis test using probability models.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "href": "20-Hypothesis-Testing-ProbDist.html#hypothesis-testing-using-probability-models",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.2 Hypothesis testing using probability models",
    "text": "20.2 Hypothesis testing using probability models\nAs a lead into the Central Limit Theorem in @ref(HYPTESTCLT) and mathematical sampling distributions, we will look at a class of hypothesis testing where the null hypothesis specifies a probability model. In some cases, we can get an exact answer, and in others, we will use simulation to get an empirical \\(p\\)-value. By the way, a permutation test is an exact test; by this we mean we are finding all possible permutations in the calculation of the \\(p\\)-value. However, since the complete enumeration of all permutations is often difficult, we approximate it with randomization, simulation. Thus, the \\(p\\)-value from a randomization test is an approximation of the exact (permutation) test.\nLet’s use three examples to illustrate the ideas of this chapter.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "href": "20-Hypothesis-Testing-ProbDist.html#tappers-and-listeners",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.3 Tappers and listeners",
    "text": "20.3 Tappers and listeners\nHere’s a game you can try with your friends or family. Pick a simple, well-known song. Tap that tune on your desk, and see if the other person can guess the song. In this simple game, you are the tapper, and the other person is the listener.\nA Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.1 In her study, she recruited 120 tappers and 120 listeners into the study. About 50% of the tappers expected that the listener would be able to guess the song. Newton wondered, is 50% a reasonable expectation?\n\n20.3.1 Step 1- State the null and alternative hypotheses\nNewton’s research question can be framed into two hypotheses:\n\\(H_0\\): The tappers are correct, and, in general, 50% of listeners are able to guess the tune. \\(p = 0.50\\)\n\\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners are able to guess the tune. \\(p \\neq 0.50\\)\n\nExercise: Is this a one-sided or two-sided hypothesis test? How many variables are in this model?\n\nThe tappers think that listeners will guess the song correctly 50% of the time, and this is a two-sided test since we don’t know beforehand if listeners will be better or worse than 50%.\nThere is only one variable of interest, whether the listener is correct.\n\n\n20.3.2 Step 2 - Compute a test statistic\nIn Newton’s study, only 42 (we changed the number to make this problem more interesting from an educational perspective) out of 120 listeners (\\(\\hat{p} = 0.35\\)) were able to guess the tune! From the perspective of the null hypothesis, we might wonder, how likely is it that we would get this result from chance alone? That is, what’s the chance we would happen to see such a small fraction if \\(H_0\\) were true and the true correct-guess rate is 0.50?\nNow before we use simulation, let’s frame this as a probability model. The random variable \\(X\\) is the number of correct guesses out of 120. If the observations are independent and the probability of success is constant (each listener has the same probability of guessing correctly), then we could use a binomial model. We can’t assess the validity of these assumptions without knowing more about the experiment, the subjects, and the data collection. For educational purposes, we will assume they are valid. Thus, our test statistic is the number of successes in 120 trials. The observed value is 42.\n\n\n20.3.3 Step 3 - Determine the \\(p\\)-value\nWe now want to find the \\(p\\)-value as \\(2 \\cdot \\Prob(X \\leq 42)\\) where \\(X\\) is a binomial random variable with \\(p = 0.5\\) and \\(n = 120\\). Again, the \\(p\\)-value is the probability of the observed data or something more extreme, given the null hypothesis is true. Here, the null hypothesis being true implies that the probability of success is 0.50. We will use R to get the one-sided \\(p\\)-value and then double it to get the two-sided \\(p\\)-value for the problem. We selected \\(\\Prob(X \\leq 42)\\) because “more extreme” means the observed values and values further from what you would get if the null hypothesis were true, which is 60 for this problem.\n\n2*pbinom(42, 120, prob = 0.5)\n\n[1] 0.001299333\n\n\nThat is a small \\(p\\)-value.\n\n\n20.3.4 Step 4 - Draw a conclusion\nBased on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0013\\) probability that only 42 or less (or 78 or more) listeners would get correctly. This is the probability of what we observed or something more extreme, given the null hypothesis is true. This probability is much less than 0.05, so we reject the null hypothesis that the listeners are guessing correctly half of the time and conclude that the correct-guess rate rate is different from 50%.\nThis decision region looks like the pmf in Figure 20.1. Any observed values inside the red boundary lines would be consistent with the null hypothesis. That is, any observed values inside the red boundary lines would result in a \\(p\\)-value larger than 0.05. Any values at the red line or more extreme would be in the rejection region, resulting in a \\(p\\)-value smaller than 0.05. We also plotted the observed value in black.\n\ngf_dist(\"binom\", size = 120, prob = 0.5, xlim = c(50, 115)) %&gt;%\n  gf_vline(xintercept = c(48, 72), color = \"red\") %&gt;%\n  gf_vline(xintercept = c(42), color = \"black\") %&gt;%\n  gf_theme(theme_bw) %&gt;%\n  gf_labs(title = \"Binomial pmf\", subtitle = \"Probability of success is 0.5\", \n          y = \"Probability\")\n\n\n\n\n\n\n\nFigure 20.1: Binomial pmf\n\n\n\n\n\n\n\n20.3.5 Repeat using simulation\nWe will repeat the analysis using an empirical (observed from simulated data) \\(p\\)-value. Step 1, stating the null and alternative hypothesis, is the same.\n\n\n20.3.6 Step 2 - Compute a test statistic\nWe will use the proportion of listeners that get the song correct instead of the number of listeners that get it correct. This is a minor change since we are simply dividing by 120.\n\nobs &lt;- 42 / 120\nobs\n\n[1] 0.35\n\n\n\n\n20.3.7 Step 3 - Determine the \\(p\\)-value\nTo simulate 120 games under the null hypothesis where \\(p = 0.50\\), we could flip a coin 120 times. Each time the coin comes up heads, this could represent the listener guessing correctly, and tails would represent the listener guessing incorrectly. For example, we can simulate 5 tapper-listener pairs by flipping a coin 5 times:\n\\[\n\\begin{array}{ccccc}\nH & H & T & H & T \\\\\nCorrect & Correct & Wrong & Correct & Wrong \\\\\n\\end{array}\n\\]\nAfter flipping the coin 120 times, we got 56 heads for a proportion of \\(\\hat{p}_{sim} = 0.467\\). As we did with the randomization technique, seeing what would happen with one simulation isn’t enough. In order to evaluate whether our originally observed proportion of 0.35 is unusual or not, we should generate more simulations. Here, we’ve repeated this simulation 10,000 times:\n\nset.seed(604)\nresults &lt;- rbinom(10000, 120, 0.5) / 120\n\nNote, we could simulate it a number of ways. Here is a way using do() that will look like what we’ve done for other randomization tests.\n\nset.seed(604)\nresults &lt;- do(10000)*mean(sample(c(0, 1), size = 120, replace = TRUE))\n\n\nhead(results)\n\n       mean\n1 0.4250000\n2 0.5250000\n3 0.5916667\n4 0.5000000\n5 0.5250000\n6 0.5083333\n\n\n\nresults %&gt;%\n  gf_histogram(~mean, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = c(obs, 1 - obs), color = \"black\") %&gt;%\n  gf_theme(theme_bw()) %&gt;%\n  gf_labs(x = \"Test statistic\")\n\n\n\n\n\n\n\nFigure 20.2: The estimated sampling distribution\n\n\n\n\n\nNotice in Figure 20.2 how the sampling distribution is centered at 0.5 and looks symmetrical.\nThe \\(p\\)-value is found using the prop1 function. In this problem, we really need the observed value to be included to prevent a \\(p\\)-value of zero.\n\n2*prop1(~(mean &lt;= obs), data = results)\n\n prop_TRUE \n0.00119988 \n\n\n\n\n20.3.8 Step 4 - Draw a conclusion\nIn these 10,000 simulations, we see very few results close to 0.35. Based on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0012\\) probability that only 35% or less or 65% or more listeners would get it right. This \\(p\\)-value is much less than 0.05, so we reject that the listeners are guessing correctly half of the time and conclude that the correct-guess rate is different from 50%.\n\nExercise: In the context of the experiment, what is the \\(p\\)-value for the hypothesis test?2\n\n\nExercise:\nDo the data provide statistically significant evidence against the null hypothesis? State an appropriate conclusion in the context of the research question.3",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#cardiopulmonary-resuscitation-cpr",
    "href": "20-Hypothesis-Testing-ProbDist.html#cardiopulmonary-resuscitation-cpr",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.4 Cardiopulmonary resuscitation (CPR)",
    "text": "20.4 Cardiopulmonary resuscitation (CPR)\nLet’s return to the CPR example from last chapter. As a reminder, we will repeat some of the background material.\nCardiopulmonary resuscitation (CPR) is a procedure sometimes used on individuals suffering a heart attack. It is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries, which complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack, but blood thinners negatively affect internal injuries.\nPatients who underwent CPR for a heart attack and were subsequently admitted to a hospital4 were randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\n\n20.4.1 Step 1- State the null and alternative hypotheses\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\\(H_0\\): Blood thinners do not have an overall survival effect; survival rate is independent of experimental treatment group. \\(p_c - p_t = 0\\).\n\\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\).\n\nthinner &lt;- read_csv(\"data/blood_thinner.csv\")\n\n\nhead(thinner)\n\n# A tibble: 6 × 2\n  group     outcome \n  &lt;chr&gt;     &lt;chr&gt;   \n1 treatment survived\n2 control   survived\n3 control   died    \n4 control   died    \n5 control   died    \n6 treatment survived\n\n\nLet’s put the data in a table.\n\ntally(~group + outcome, data = thinner, margins = TRUE)\n\n           outcome\ngroup       died survived Total\n  control     39       11    50\n  treatment   26       14    40\n  Total       65       25    90\n\n\n\n\n20.4.2 Step 2 - Compute a test statistic.\nIn this example, we can think of the data as coming from a hypergeometric distribution. This is really a binomial from a finite population. We can calculate the \\(p\\)-value using this probability distribution. The random variable is the number of control patients that survived from a total population of 90 patients, where 50 are control patients and 40 are treatment patients, and where a total of 25 survived.\n\n\n20.4.3 Step 3 - Determine the \\(p\\)-value.\nIn this case, we want to find \\(\\Prob(X \\leq 11)\\) (the observed number of control patients that survived) and double it since it is a two-sided test.\n\n2*phyper(11, 50, 40, 25)\n\n[1] 0.2581356\n\n\nNote: We could have picked the lower right cell as the reference cell. But now I want the \\(\\Prob(X \\geq 14)\\) (the observed number of treatment patients that survived) with the appropriate change in parameter values. Notice we get the same answer.\n\n2*(1 - phyper(13, 40, 50, 25))\n\n[1] 0.2581356\n\n\nWe could do the same thing for the other two cells. Here we find \\(\\Prob(X \\leq 26)\\) (the observed number of treatment patients that did not survive).\n\n2*phyper(26, 40, 50, 65)\n\n[1] 0.2581356\n\n\nHere we find \\(\\Prob(X \\geq 39)\\) (the observed number of control patients that did not survive).\n\n2*(1 - phyper(38, 50, 40, 65))\n\n[1] 0.2581356\n\n\nR also has a built in function, fisher.test(), that we could use. This function calculates Fisher’s exact test, where \\(p\\)-values are obtained using the hypergeometric distribution.\n\nfisher.test(tally(~group + outcome, data = thinner))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tally(~group + outcome, data = thinner)\np-value = 0.2366\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.6794355 5.4174460\nsample estimates:\nodds ratio \n  1.895136 \n\n\nThe \\(p\\)-value is slightly different since the hypergeometric distribution is not symmetric. For this reason, doubling the \\(p\\)-value from the single side result is not quite right. The algorithm in fisher.test() finds and adds all probabilities less than or equal to value of \\(\\Prob(X = 11)\\), see Figure 20.3. Using fisher.test() gives the correct \\(p\\)-value.\n\ngf_dist(\"hyper\", m = 50, n = 40, k = 25) %&gt;%\n  gf_hline(yintercept = dhyper(11, 50, 40, 25), color = \"red\") %&gt;%\n  gf_labs(title = \"Hypergeometric pmf\", subtitle = \"Red line is P(X = 11)\", \n          y = \"Probability\") %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nFigure 20.3: Hypergeometric pmf showing the cutoff for \\(p\\)-value calculation.\n\n\n\n\n\nThis is how fisher.test() is calculating the \\(p\\)-value:\n\ntemp &lt;- dhyper(0:25, 50, 40, 25)\nsum(temp[temp &lt;= dhyper(11, 50, 40, 25)])\n\n[1] 0.2365928\n\n\nThe randomization test in the last chapter yielded a \\(p\\)-value of 0.257 so all tests are consistent.\n\n\n20.4.4 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment.\nNotice that in these first two examples, we had a test of a single proportion and a test of two proportions. The single proportion test did not have an equivalent randomization test since there is not a second variable to shuffle. We were able to get answers since we found a probability model that we could use instead.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#golf-balls",
    "href": "20-Hypothesis-Testing-ProbDist.html#golf-balls",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.5 Golf Balls",
    "text": "20.5 Golf Balls\nOur last example will be interesting because the distribution has multiple parameters and a test metric is not obvious at this point.\nThe owners of a residence located along a golf course collected the first 500 golf balls that landed on their property. Most golf balls are labeled with the make of the golf ball and a number, for example “Nike 1” or “Titleist 3”. The numbers are typically between 1 and 4, and the owners of the residence wondered if these numbers are equally likely (at least among golf balls used by golfers of poor enough quality that they lose them in the yards of the residences along the fairway.)\nWe will use a significance level of \\(\\alpha = 0.05\\) since there is no reason to favor one decision error over the other.\n\n20.5.1 Step 1- State the null and alternative hypotheses\nWe think that the numbers are not all equally likely. The question of one-sided versus two-sided is not relevant in this test. You will see this when we write the hypotheses.\n\\(H_0\\): All of the numbers are equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) Or \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\)\n\\(H_A\\): There is some other distribution of the numbers in the population. At least one population proportion is not \\(\\frac{1}{4}\\).\nNotice that we switched to using \\(\\pi\\) instead of \\(p\\) for the population parameter. There is no reason other than to make you aware that both are used.\nThis problem is an extension of the binomial. Instead of two outcomes, there are four outcomes. This is called a multinomial distribution. You can read more about it if you like, but our methods will not make it necessary to learn the probability mass function.\nOut of the 500 golf balls collected, 486 of them had a number between 1 and 4. We will deal with only these 486 golf balls. Let’s get the data from `golf_balls.csv”.\n\ngolf_balls &lt;- read_csv(\"data/golf_balls.csv\")\n\n\ninspect(golf_balls)\n\n\nquantitative variables:  \n    name   class min Q1 median Q3 max     mean       sd   n missing\n1 number numeric   1  1      2  3   4 2.366255 1.107432 486       0\n\n\n\ntally(~number, data = golf_balls)\n\nnumber\n  1   2   3   4 \n137 138 107 104 \n\n\n\n\n20.5.2 Step 2 - Compute a test statistic.\nIf all numbers were equally likely, we would expect to see 121.5 golf balls of each number. This is a point estimate and thus not an actual value that could be realized. Of course, in a sample we will have variation and thus departure from this state. We need a test statistic that will help us determine if the observed values are reasonable under the null hypothesis. Remember that the test statistic is a single number metric used to evaluate the hypothesis.\n\nExercise:\nWhat would you propose for the test statistic?\n\nWith four proportions, we need a way to combine them. This seems tricky, so let’s just use a simple approach. Let’s take the maximum number of balls across all cells of the table and subtract the minimum. This is called the range and we will denote the parameter as \\(R\\). Under the null hypothesis, this should be zero. We could re-write our hypotheses as:\n\\(H_0\\): \\(R=0\\)\n\\(H_A\\): \\(R&gt;0\\)\nNotice that \\(R\\) will always be non-negative, thus this test is one-sided.\nThe observed range is 34, \\(138 - 104\\).\n\nobs &lt;- diff(range(tally(~number, data = golf_balls)))\nobs\n\n[1] 34\n\n\n\n\n20.5.3 Step 3 - Determine the \\(p\\)-value.\nWe don’t know the distribution of our test statistic, so we will use simulation. We will simulate data from a multinomial distribution under the null hypothesis and calculate a new value of the test statistic. We will repeat this 10,000 times and this will give us an estimate of the sampling distribution.\nWe will use the sample() function again to simulate the distribution of numbers under the null hypothesis. To help us understand the process and build the code, we are only initially using a sample size of 12 to keep the printout reasonable and easy to read.\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nNotice this is not using tidyverse coding ideas. We don’t think we need tibbles or data frames so we went with straight nested R code. You can break this code down by starting with the code in the center.\n\nset.seed(3311)\nsample(1:4, size = 12, replace = TRUE)\n\n [1] 3 1 2 3 2 3 1 3 3 4 1 1\n\n\n\nset.seed(3311)\ntable(sample(1:4, size = 12, replace = TRUE))\n\n\n1 2 3 4 \n4 2 5 1 \n\n\n\nset.seed(3311)\nrange(table(sample(1:4, size = 12, replace = TRUE)))\n\n[1] 1 5\n\n\n\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))\n\n[1] 4\n\n\nWe are now ready to ramp up to the full problem. Let’s simulate the data under the null hypothesis. We are sampling 486 golf balls (instead of 12) with the numbers 1 through 4 on them. Each number is equally likely. We then find the range, our test statistic. Finally we repeat this 10,000 to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(3311)\nresults &lt;- do(10000)*diff(range(table(sample(1:4, size = 486, replace = TRUE))))\n\nFigure 20.4 is a plot of the sampling distribution of the range.\n\nresults %&gt;%\n  gf_histogram(~diff, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Range\", \n          subtitle = \"Multinomial with equal probability\",\n          x = \"Range\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 20.4: Sampling distribution of the range.\n\n\n\n\n\nNotice how this distribution is skewed to the right. The \\(p\\)-value is 0.14.\n\nprop1(~(diff &gt;= obs), data = results)\n\nprop_TRUE \n0.1393861 \n\n\n\n\n20.5.4 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is larger than 0.05, we fail to reject the null hypothesis. That is, based on our data, we do not find statistically significant evidence against the claim that the number on the golf balls are equally likely. We can’t say that the proportion of golf balls with each number differs from 0.25.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "href": "20-Hypothesis-Testing-ProbDist.html#repeat-with-a-different-test-statistic",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.6 Repeat with a different test statistic",
    "text": "20.6 Repeat with a different test statistic\nThe test statistic we developed was helpful, but it seems weak because we did not use the information in all four cells. So let’s devise a metric that does this. The hypotheses are the same, so we will jump to step 2.\n\n20.6.1 Step 2 - Compute a test statistic.\nIf each number were equally likely, we would have 121.5 balls in each bin. We can find a test statistic by looking at the deviation in each cell from 121.5.\n\ntally(~number, data = golf_balls) - 121.5\n\nnumber\n    1     2     3     4 \n 15.5  16.5 -14.5 -17.5 \n\n\nNow we need to collapse these into a single number. Just adding will always result in a value of 0, why? So let’s take the absolute value and then add the cells together.\n\nobs &lt;- sum(abs(tally(~number, data = golf_balls) - 121.5))\nobs\n\n[1] 64\n\n\nThis will be our test statistic.\n\n\n20.6.2 Step 3 - Determine the \\(p\\)-value.\nWe will use similar code from above with our new metric. Now we sample 486 golf balls with the numbers 1 through 4 on them, and find our test statistic, the sum of the absolute deviations of each cell of the table from the expected count, 121.5. We repeat this process 10,000 times to get an estimate of the sampling distribution of our test statistic.\n\nset.seed(9697)\nresults &lt;- do(10000)*sum(abs(table(sample(1:4, size = 486, replace = TRUE)) - 121.5))\n\nFigure 20.5 is a plot of the sampling distribution of the absolute value of deviations.\n\nresults %&gt;%\n  gf_histogram(~sum, fill = \"cyan\", color = \"black\") %&gt;%\n  gf_vline(xintercept = obs, color = \"black\") %&gt;%\n  gf_labs(title = \"Sampling Distribution of Absolute Deviations\",\n          subtitle = \"Multinomial with equal probability\",\n          x = \"Absolute deviations\") %&gt;%\n  gf_theme(theme_bw)\n\n\n\n\n\n\n\nFigure 20.5: Sampling distribution of the absolute deviations.\n\n\n\n\n\nNotice how this distribution is skewed to the right and our test statistic seems to be more extreme.\n\nprop1(~(sum &gt;= obs), data = results)\n\n prop_TRUE \n0.01359864 \n\n\nThe \\(p\\)-value is 0.014. This value is much smaller than our previous result. The test statistic matters in our decision process as nothing about this problem has changed except the test statistic.\n\n\n20.6.3 Step 4 - Draw a conclusion\nSince this \\(p\\)-value is smaller than 0.05, we reject the null hypothesis. That is, based on our data, we find statistically significant evidence against the claim that the numbers on the golf balls are equally likely. We conclude that the numbers on the golf balls are not all equally likely, or that at least one is different.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#summary",
    "href": "20-Hypothesis-Testing-ProbDist.html#summary",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.7 Summary",
    "text": "20.7 Summary\nIn this chapter, we used probability models to help us make decisions from data. This chapter is different from the randomization section in that randomization had two variables (one of which we could shuffle) and a null hypothesis of no difference. In the case of a single proportion, we were able to use the binomial distribution to get an exact \\(p\\)-value under the null hypothesis. In the case of a \\(2 \\times 2\\) table, we were able to show that we could use the hypergeometric distribution to get an exact \\(p\\)-value under the assumptions of the model.\nWe also found that the choice of test statistic has an impact on our decision. Even though we get valid \\(p\\)-values and the desired Type 1 error rate, if the information in the data is not used to its fullest, we will lose power. Note: power is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\nIn the next chapter, we will learn about mathematical solutions to finding the sampling distribution. The key difference in all these methods is the selection of the test statistic and the assumptions made to derive a sampling distribution.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#homework-problems",
    "href": "20-Hypothesis-Testing-ProbDist.html#homework-problems",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "20.8 Homework Problems",
    "text": "20.8 Homework Problems\n\nRepeat the analysis of the yawning data from last chapter, but this time use the hypergeometric distribution.\nIs yawning contagious?\nAn experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasn’t a person yawning near them (control). The following table shows the results of this experiment.\n\n\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Group}\\\\\n& & \\text{Treatment } &  \\text{Control} & \\text{Total}  \\\\\n& \\hline \\text{Yawn}    &   10      & 4     & 14  \\\\\n\\textbf{Result} & \\text{Not Yawn}   & 24        & 12        & 36   \\\\\n    &\\text{Total}       & 34        & 16        & 50 \\\\\n\\end{array}\n\\]\nThe data is in the file yawn.csv.\n\nWhat are the hypotheses?\nChoose a cell, and calculate the observed statistic.\nFind the \\(p\\)-value using the hypergeometric distribution.\nPlot the the sampling distribution.\nDetermine the conclusion of the hypothesis test.\nCompare your results with the randomization test.\n\n\n\nRepeat the analysis of the golf ball data using a different test statistic.\nUse a level of significance of 0.05.\n\n\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the \\(p\\)-value.\nDraw a conclusion.\n\n\n\nBody Temperature\n\nShoemaker5 cites a paper from the American Medical Association6 that questions conventional wisdom that the average body temperature of a human is 98.6 degrees Fahrenheit. One of the main points of the original article is that the traditional mean of 98.6 is, in essence, 100 years out of date. The authors cite problems with the original study’s methodology, diurnal fluctuations (up to 0.9 degrees F per day), and unreliable thermometers. The authors believe the average human body temperature is less than 98.6. Conduct a hypothesis test.\n\nState the null and alternative hypotheses.\nState the significance level that will be used.\nLoad the data from the file “temperature.csv” and generate summary statistics and a boxplot of the temperature data. We will not be using gender or heart rate for this problem.\nCompute a test statistic. We are going to help you with this part. We cannot do a randomization test since we don’t have a second variable. It would be nice to use the mean as a test statistic but we don’t yet know the sampling distribution of the sample mean.\nLet’s get clever. If the distribution of the sample is symmetric (this is an assumption but look at the boxplot and summary statistics to determine if you are comfortable with it), then under the null hypothesis, the observed values should be equally likely to either be greater or less than 98.6. Thus, our test statistic is the number of cases that have a positive difference between 98.6 and the observed value. This will be a binomial distribution with a probability of success (having a positive difference) of 0.5. You must also account for the possibility that there are observations of 98.6 in the data.\nDetermine the \\(p\\)-value.\nDraw a conclusion.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#solutions-manual",
    "href": "20-Hypothesis-Testing-ProbDist.html#solutions-manual",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "Solutions Manual",
    "text": "Solutions Manual",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "20-Hypothesis-Testing-ProbDist.html#footnotes",
    "href": "20-Hypothesis-Testing-ProbDist.html#footnotes",
    "title": "20  Hypothesis Testing with Known Distributions",
    "section": "",
    "text": "This case study is described in Made to Stick: Why Some Ideas Survive and Others Die by Chip and Dan Heath.↩︎\nThe \\(p\\)-value is the chance of seeing the observed data or something more in favor of the alternative hypothesis (something as or more extreme) given that guessing has a probability of success of 0.5. Since we didn’t observe many simulations with even close to just 42 listeners correct, the \\(p\\)-value will be small, around 1-in-1000.↩︎\nThe \\(p\\)-value is less than 0.05, so we reject the null hypothesis. There is statistically significant evidence, and the data provide strong evidence that the chance a listener will guess the correct tune is different from 50%.↩︎\n“Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎\nL. Shoemaker Allen (1996) What’s Normal? – Temperature, Gender, and Heart Rate, Journal of Statistics Education, 4:2↩︎\nMackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), “A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich,” Journal of the American Medical Association, 268, 1578-1580.↩︎",
    "crumbs": [
      "Statistical Modeling - Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypothesis Testing with Known Distributions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pruim, Randall J. 2011. Foundations and Applications of Statistics:\nAn Introduction Using r. Vol. 13. American Mathematical Soc.",
    "crumbs": [
      "Statistical Modeling - Inference",
      "References"
    ]
  },
  {
    "objectID": "02-Data-Basics.html",
    "href": "02-Data-Basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#objectives",
    "href": "02-Data-Basics.html#objectives",
    "title": "2  Data Basics",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: case, variables, data frame, associated variables, independent, and discrete and continuous variables.\nIdentify and define the different types of variables.\nGiven a study description, describe the research question.\nIn R, create a scatterplot and determine the association of two numerical variables from the plot.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#data-basics",
    "href": "02-Data-Basics.html#data-basics",
    "title": "2  Data Basics",
    "section": "2.2 Data basics",
    "text": "2.2 Data basics\nEffective presentation and description of data is a first step in most analyses. This chapter introduces one structure for organizing data, as well as some terminology that will be used throughout this book.\n\n2.2.1 Observations, variables, and data matrices\nFor reference we will be using a data set concerning 50 emails received in 2012. These observations will be referred to as the email50 data set, and they are a random sample from a larger data set. This data is in the openintro package so let’s install and then load this package.\n\ninstall.packages(\"openintro\")\nlibrary(openintro)\n\nTable 2.1 shows 4 rows of the email50 data set and we have elected to only list 5 variables for ease of observation.\nEach row in the table represents a single email or case.1 The columns represent variables, which represent characteristics for each of the cases (emails). For example, the first row represents email 1, which is not spam, contains 21,705 characters, 551 line breaks, is written in HTML format, and contains only small numbers.\n\n\n\n\nTable 2.1: First 5 rows of email data frame\n\n\n\n\n\n\nspam\nnum_char\nline_breaks\nformat\nnumber\n\n\n\n\n0\n21.705\n551\n1\nsmall\n\n\n0\n7.011\n183\n1\nbig\n\n\n1\n0.631\n28\n0\nnone\n\n\n0\n15.829\n242\n1\nsmall\n\n\n\n\n\n\n\n\nLet’s look at the first 10 rows of data from email50 using R. Remember to ask the two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want the first 10 rows so we use head() and R needs the data object and the number of rows. The data object is called email50 and is accessible once the openintro package is loaded.\n\nhead(email50, n = 10)\n\n# A tibble: 10 × 21\n   spam  to_multiple from     cc sent_email time                image attach\n   &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 0     0           1         0 1          2012-01-04 13:19:16     0      0\n 2 0     0           1         0 0          2012-02-16 20:10:06     0      0\n 3 1     0           1         4 0          2012-01-04 15:36:23     0      2\n 4 0     0           1         0 0          2012-01-04 17:49:52     0      0\n 5 0     0           1         0 0          2012-01-27 09:34:45     0      0\n 6 0     0           1         0 0          2012-01-17 17:31:57     0      0\n 7 0     0           1         0 0          2012-03-18 04:18:55     0      0\n 8 0     0           1         0 1          2012-03-31 13:58:56     0      0\n 9 0     0           1         1 1          2012-01-11 01:57:54     0      0\n10 0     0           1         0 0          2012-01-07 19:29:16     0      0\n# ℹ 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;,\n#   password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;,\n#   re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;,\n#   number &lt;fct&gt;\n\n\nIn practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and the units of measurement. Descriptions of all variables in the email50 data set are given in its documentation which can be accessed in R by using the ? command:\n?email50\n(Note that not all data sets will have associated documentation; the authors of openintro package included this documentation with the email50 data set contained in the package.)\nThe data in email50 represent a data matrix, or in R terminology a data frame or tibble 2, which is a common way to organize data. Each row of a data matrix corresponds to a unique case, and each column corresponds to a variable. This is called tidy data.3 The data frame for the stroke study introduced in the previous chapter had patients as the cases and there were three variables recorded for each patient. If we are thinking of patients as the unit of observation, then this data is tidy.\n\n\n# A tibble: 10 × 3\n   group   outcome30 outcome365\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     \n 1 control no_event  no_event  \n 2 trmt    no_event  no_event  \n 3 control no_event  no_event  \n 4 trmt    no_event  no_event  \n 5 trmt    no_event  no_event  \n 6 control no_event  no_event  \n 7 trmt    no_event  no_event  \n 8 control no_event  no_event  \n 9 control no_event  no_event  \n10 control no_event  no_event  \n\n\nIf we think of an outcome as a unit of observation, then it is not tidy since the two outcome columns are variable values (month or year). The tidy data for this case would be:\n\n\n# A tibble: 10 × 4\n   patient_id group   time  result  \n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   \n 1          1 control month no_event\n 2          1 control year  no_event\n 3          2 trmt    month no_event\n 4          2 trmt    year  no_event\n 5          3 control month no_event\n 6          3 control year  no_event\n 7          4 trmt    month no_event\n 8          4 trmt    year  no_event\n 9          5 trmt    month no_event\n10          5 trmt    year  no_event\n\n\nThere are three interrelated rules which make a data set tidy:\n\nEach variable must have its own column.\n\nEach observation must have its own row.\n\nEach value must have its own cell.\n\nWhy ensure that your data is tidy? There are two main advantages:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. This will be more clear as we progress in our studies. Since most built-in R functions work with vectors of values, it makes transforming tidy data feel particularly natural.\n\nData frames are a convenient way to record and store data. If another individual or case is added to the data set, an additional row can be easily added. Similarly, another column can be added for a new variable.\n\nExercise:\nWe consider a publicly available data set that summarizes information about the 3,142 counties in the United States, and we create a data set called county_subset data set. This data set will include information about each county: its name, the state where it resides, its population in 2000 and 2010, per capita federal spending, poverty rate, and four additional characteristics. We create this data object in the code following this description. The parent data set is part of the usdata library and is called county_complete. The variables are summarized in the help menu built into the usdata package4. How might these data be organized in a data matrix? 5\n\nUsing R we will create our data object. First we load the library usdata.\n\nlibrary(usdata)\n\nWe only want a subset of the columns and we will use the select verb in dplyr to select and rename columns. We also create a new variable which is federal spending per capita using the mutate function.\n\ncounty_subset &lt;- county_complete %&gt;% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %&gt;%\n  mutate(fed_spend = fed_spend / pop2010)\n\nUsing R, we will display seven rows of the county_subset data frame.\n\nhead(county_subset, n = 7)\n\n            name   state pop2000 pop2010 fed_spend poverty homeownership\n1 Autauga County Alabama   43671   54571  6.068095    10.6          77.5\n2 Baldwin County Alabama  140415  182265  6.139862    12.2          76.7\n3 Barbour County Alabama   29038   27457  8.752158    25.0          68.0\n4    Bibb County Alabama   20826   22915  7.122016    12.6          82.9\n5  Blount County Alabama   51024   57322  5.130910    13.4          82.0\n6 Bullock County Alabama   11714   10914  9.973062    25.3          76.9\n7  Butler County Alabama   21399   20947  9.311835    25.0          69.0\n  multi_unit income med_income\n1        7.2  24568      53255\n2       22.6  26469      50147\n3       11.1  15875      33219\n4        6.6  19918      41770\n5        3.7  21070      45549\n6        9.9  20289      31602\n7       13.7  16916      30659\n\n\n\n\n2.2.2 Types of variables\nExamine the fed_spend, pop2010, and state variables in the county data set. Each of these variables is inherently different from the others, yet many of them share certain characteristics.\nFirst consider fed_spend. It is said to be a numerical variable (sometimes called a quantitative variable) since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical; even though area codes are made up of numerical digits, their average, sum, and difference have no clear meaning.\nThe pop2010 variable is also numerical; it is sensible to add, subtract, or take averages with those values, although it seems to be a little different than fed_spend. This variable of the population count can only be a whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). For this reason, the population variable is said to be discrete since it can only take specific numerical values. On the other hand, the federal spending variable is said to be continuous because it can take on any value in some interval. Now technically, there are no truly continuous numerical variables since all measurements are finite up to some level of accuracy or measurement precision (e.g., we typically measure federal spending in dollars and cents). However, in this book, we will treat both types of numerical variables the same, that is as continuous variables for statistical modeling. The only place this will be different in this book is in probability models, which we will see in the probability modeling block.\nThe variable state can take up to 51 values, after accounting for Washington, DC, and are summarized as: Alabama, Alaska, …, and Wyoming. Because the responses themselves are categories, state is a categorical variable (sometimes also called a qualitative variable), and the possible values are called the variable’s levels.\n\n\n\n\n\n\n\n\nFigure 2.1: Taxonomy of Variables.\n\n\n\n\n\nFinally, consider a hypothetical variable on education, which describes the highest level of education completed and takes on one of the values noHS, HS, College or Graduate_school. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable. A categorical variable with levels that do not have a natural ordering is called a nominal variable. To simplify analyses, any ordinal variables in this book will be treated as nominal categorical variables. In R, categorical variables can be treated in different ways; one of the key differences is that we can leave them as character values (character strings, or text) or as factors. A factor is essentially a categorical variable with defined levels. When R handles factors, it is only concerned about the levels of the factors. We will learn more about this as we progress.\nFigure 2.1 captures this classification of variables we have described.\n\nExercise:\nData were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical.6\n\n\nExercise:\nConsider the variables group and outcome30 from the stent study in the case study chapter. Are these numerical or categorical variables? 7\n\n\n\n2.2.3 Relationships between variables\nMany analyses are motivated by a researcher looking for a relationship between two or more variables. This is the heart of statistical modeling. A social scientist may like to answer some of the following questions:\n\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\n\nIf homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average?\n\nTo answer these questions, data must be collected, such as the county_complete data set. Examining summary statistics could provide insights for each of the two questions about counties. Graphs can be used to visually summarize data and are useful for answering such questions as well.\nScatterplots are one type of graph used to study the relationship between two numerical variables. Figure 2.2 compares the variables fed_spend and poverty. Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 1088 in the county_subset data set: Owsley County, Kentucky, which had a poverty rate of 41.5% and federal spending of $21.50 per capita. The dense cloud in the scatterplot suggests a relationship between the two variables: counties with a high poverty rate also tend to have slightly more federal spending. We might brainstorm as to why this relationship exists and investigate each idea to determine which is the most reasonable explanation.\n\n\n\n\n\n\n\n\nFigure 2.2: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted.\n\n\n\n\n\n\nExercise:\nExamine the variables in the email50 data set. Create two research questions about the relationships between these variables that are of interest to you.8\n\nThe fed_spend and poverty variables are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa.\n\nExample:\nThe relationship between the homeownership rate and the percent of units in multi-unit structures (e.g. apartments, condos) is visualized using a scatterplot in Figure 2.3. Are these variables associated?\n\nIt appears that the larger the fraction of units in multi-unit structures, the lower the homeownership rate. Since there is some relationship between the variables, they are associated.\n\n\n\n\n\n\n\n\nFigure 2.3: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties.\n\n\n\n\n\nBecause there is a downward trend in Figure 2.3 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. A positive association (upward trend) is shown in the relationship between the poverty and fed_spend variables represented in Figure 2.2, where counties with higher poverty rates tend to receive more federal spending per capita.\nIf two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two.\n\nA pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.\n\n\n\n2.2.4 Creating a scatterplot\nIn this section, we will create a simple scatterplot and then ask you to create one on your own. First, we will recreate the scatterplot seen in Figure 2.2. This figure uses the county_subset data set.\nHere are two questions:\nWhat do we want R to do? and\nWhat must we give R for it to do this?\nWe want R to create a scatterplot and to do this it needs, at a minimum, the data object, what we want on the \\(x\\)-axis, and what we want on the \\(y\\)-axis. More information on ggformula can be found here.\n\ncounty_subset %&gt;%\n  gf_point(fed_spend ~ poverty)\n\n\n\n\n\n\n\nFigure 2.4: Scatterplot with ggformula.\n\n\n\n\n\nFigure 2.4 is bad. There are poor axis labels, no title, dense clustering of points, and the \\(y\\)-axis is being driven by a couple of extreme points. We will need to clear this up. Again, try to read the code and use help() or ? to determine the purpose of each command in Figure 2.5.\n\ncounty_subset %&gt;%\n  filter(fed_spend &lt; 32) %&gt;%\n  gf_point(fed_spend ~ poverty,\n           xlab = \"Poverty Rate (Percent)\", \n           ylab = \"Federal Spending Per Capita\",\n           title = \"A scatterplot showing fed_spend against poverty\", \n           cex = 1, alpha = 0.2) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nFigure 2.5: Better example of a scatterplot.\n\n\n\n\n\n\nExercise:\nCreate the scatterplot in Figure 2.3.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "02-Data-Basics.html#footnotes",
    "href": "02-Data-Basics.html#footnotes",
    "title": "2  Data Basics",
    "section": "",
    "text": "A case is also sometimes called a unit of observation or an observational unit.↩︎\nA tibble is a data frame with attributes for such things as better display and printing.↩︎\nTidy data is data in which each row corresponds to a unique case and each column represents a single variable. For more information on tidy data, see the Simply Statistics blog and the R for Data Science book by Hadley Wickham and Garrett Grolemund.↩︎\nThese data were collected from the US Census website.↩︎\nEach county may be viewed as a case, and there are ten pieces of information recorded for each case. A table with 3,142 rows and 10 columns could hold these data, where each row represents a county and each column represents a particular piece of information.↩︎\nThe number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical.↩︎\nThere are only two possible values for each variable, and in both cases they describe categories. Thus, each is a categorical variable.↩︎\nTwo sample questions: (1) Intuition suggests that if there are many line breaks in an email then there would also tend to be many characters: does this hold true? (2) Is there a connection between whether an email format is plain text (versus HTML) and whether it is a spam message?↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html",
    "href": "03-Overview-of-Data-Collection-Principles.html",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "3.1 Objectives",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#objectives",
    "href": "03-Overview-of-Data-Collection-Principles.html#objectives",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "Define and use properly in context all new terminology, to include: population, sample, anecdotal evidence, bias, simple random sample, systematic sample, non-response bias, representative sample, convenience sample, explanatory variable, response variable, observational study, cohort, experiment, randomized experiment, and placebo.\nFrom a description of a research project, be able to describe the population of interest, the generalizability of the study, the explanatory and response variables, whether it is observational or experimental, and determine the type of sample.\nIn the context of a problem, explain how to conduct a sample for the different types of sampling procedures.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "href": "03-Overview-of-Data-Collection-Principles.html#overview-of-data-collection-principles",
    "title": "3  Overview of Data Collection Principles",
    "section": "3.2 Overview of data collection principles",
    "text": "3.2 Overview of data collection principles\nThe first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals.\n\n3.2.1 Populations and samples\nConsider the following three research questions:\n\nWhat is the average mercury content in swordfish in the Atlantic Ocean?\n\nOver the last 5 years, what is the average time to complete a degree for Duke undergraduate students?\n\nDoes a new drug reduce the number of deaths in patients with severe heart disease?\n\nEach research question refers to a target population, the entire collection of individuals about which we want information. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question.\n\nExercise:\nFor the second and third questions above, identify the target population and what represents an individual case.1\n\n\n\n3.2.2 Anecdotal evidence\nConsider the following possible responses to the three research questions:\n\nA man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high.\nI met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges.\nMy friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work.\n\nEach conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence.\n\n\n\n\n\nIn February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, It’s one storm, in one region, of one country.\n\n\n\n\n\nAnecdotal evidence: Be careful of data collected haphazardly. Such evidence may be true and verifiable, but it may only represent extraordinary cases.\n\nAnecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population.\n\n\n3.2.3 Sampling from a population\nWe might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. This is illustrated in Figure 3.1 .\n\n\n\n\n\n\n\n\nFigure 3.1: In this graphic, five graduates are randomly selected from the population to be included in the sample.\n\n\n\n\n\nWhy pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario.\n\nExample:\nSuppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates? 2\n\n\n\n\n\n\n\n\n\nFigure 3.2: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often.\n\n\n\n\n\nIf someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces sampling bias (see Figure 3.2), where some individuals in the population are more likely to be sampled than others. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample.\nSometimes a simple random sample is difficult to implement and an alternative method is helpful. One such substitute is a systematic sample, where one case is sampled after letting a fixed number of others, say 10 other cases, pass by. Since this approach uses a mechanism that is not easily subject to personal biases, it often yields a reasonably representative sample. This book will focus on simple random samples since the use of systematic samples is uncommon and requires additional considerations of the context.\nThe act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative3 of the entire population, the survey might suffer from non-response bias4.\n\n\n\n\n\n\n\n\nFigure 3.3: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem.\n\n\n\n\n\nAnother common pitfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample, see Figure 3.3. For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents.\n\nExercise:\nWe can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?5\n\n\n\n3.2.4 Explanatory and response variables\nConsider the following question for the county data set:\nIs federal spending, on average, higher or lower in counties with high rates of poverty?\nIf we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.6 If there are many variables, it may be possible to consider a number of them as explanatory variables.\n\nExplanatory and response variables\nTo identify the explanatory variable in a pair of variables, identify which of the two variables is suspected as explaining or causing changes in the other. In data sets with more than two variables, it is possible to have multiple explanatory variables. The response variable is the outcome or result of interest.\n\n\nCaution: Association does not imply causation. Labeling variables as explanatory and response does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other. We also use this language to help in our use of R and the formula notation.\n\nIn some cases, there is no explanatory or response variable. Consider the following question:\nIf homeownership in a particular county is lower than the national average, will the percent of multi-unit structures in that county likely be above or below the national average?\nIt is difficult to decide which of these variables should be considered the explanatory and response variable; i.e. the direction is ambiguous, so no explanatory or response labels are suggested here.\n\n\n3.2.5 Introducing observational studies and experiments\nThere are two primary types of data collection: observational studies and experiments.\nResearchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort7 of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but by themselves, they cannot show a causal connection.\nWhen researchers want to investigate the possibility of a causal connection, they conduct an experiment, a study in which the explanatory variables are assigned rather than observed. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a treatment group, and we are comparing at least two treatments, the experiment is called a randomized comparative experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. The case study at the beginning of the book is another example of an experiment, though that study did not employ a placebo. Math 359 is a course on the design and analysis of experimental data, DOE, at USAFA. In the Air Force, these types of experiments are an important part of test and evaluation. Many Air Force analysts are expert practitioners of DOE. In this book though, we will minimize our discussion of DOE.\n\nAssociation \\(\\neq\\) Causation\nAgain, association does not imply causation. In a data analysis, association does not imply causation, and causation can only be inferred from a randomized experiment. Although, a hot field is the analysis of causal relationships in observational data. This is important because consider cigarette smoking, how do we know it causes lung cancer? We only have observational data and clearly cannot do an experiment. We think analysts will be charged in the near future with using causal reasoning on observational data.",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "03-Overview-of-Data-Collection-Principles.html#footnotes",
    "href": "03-Overview-of-Data-Collection-Principles.html#footnotes",
    "title": "3  Overview of Data Collection Principles",
    "section": "",
    "text": "2) Notice that the second question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. 3) A person with severe heart disease represents a case. The population includes all people with severe heart disease.↩︎\nPerhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern.↩︎\nA representative sample accurately reflects the characteristics of the population.↩︎\nNon-response bias is bias that can be introduced when subjects elect not to participate in a study. Often, the individuals that do participate are systematically different from the individuals who do not.↩︎\nAnswers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind.↩︎\nSometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so be careful and consider the context when using or reading these words.↩︎\nA cohort is a group of individuals who are similar in some way.↩︎",
    "crumbs": [
      "Descriptive Statistical Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overview of Data Collection Principles</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html",
    "href": "09-Conditional-Probability.html",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "9.1 Objectives",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#objectives",
    "href": "09-Conditional-Probability.html#objectives",
    "title": "9  Conditional Probability",
    "section": "",
    "text": "Define conditional probability and distinguish it from joint probability.\n\nFind a conditional probability using its definition.\n\nUsing conditional probability, determine whether two events are independent.\n\nApply Bayes’ Rule mathematically and via simulation.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#conditional-probability",
    "href": "09-Conditional-Probability.html#conditional-probability",
    "title": "9  Conditional Probability",
    "section": "9.2 Conditional Probability",
    "text": "9.2 Conditional Probability\nSo far, we’ve covered the basic axioms of probability, the properties of events (set theory) and counting rules. Another important concept, perhaps one of the most important, is conditional probability. Often, we know a certain event or sequence of events has occurred and we are interested in the probability of another event.\n\nExample:\nSuppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]\n\n\nWhat is the probability that a blue vehicle is selected, given a sedan was selected?\n\nSince we know that a sedan was selected, our sample space has been reduced to just “red sedan” and “blue sedan”. The probability of selecting a blue vehicle out of this sample space is simply 1/2.\nIn set notation, let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. We are looking for \\(\\mbox{P}(A \\mbox{ given } B)\\), which is also written as \\(\\mbox{P}(A|B)\\). By definition, \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)}\n\\]\nIt is important to distinguish between the event \\(A|B\\) and \\(A \\cap B\\). This is a common misunderstanding about probability. \\(A \\cap B\\) is the event that an outcome was selected at random from the total sample space, and that outcome was contained in both \\(A\\) and \\(B\\). On the other hand, \\(A|B\\) assumes the \\(B\\) has occurred, and an outcome was drawn from the remaining sample space, and that outcome was contained in \\(A\\).\nAnother common misunderstanding involves the direction of conditional probability. Specifically, \\(A|B\\) is NOT the same event as \\(B|A\\). For example, consider a medical test for a disease. The probability that someone tests positive given they had the disease is different than the probability that someone has the disease given they tested positive. We will explore this example further in our Bayes’ Rule section.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#independence",
    "href": "09-Conditional-Probability.html#independence",
    "title": "9  Conditional Probability",
    "section": "9.3 Independence",
    "text": "9.3 Independence\nTwo events, \\(A\\) and \\(B\\), are said to be independent if the probability of one occurring does not change whether or not the other has occurred. We looked at this last chapter but now we have another way of looking at it using conditional probabilities. For example, let’s say the probability that a randomly selected student has seen the latest superhero movie is 0.55. What if we randomly select a student and we see that he/she is wearing a black backpack? Does that probability change? Likely not, since movie attendance is probably not related to choice of backpack color. These two events are independent.\nMathematically, \\(A\\) and \\(B\\) are considered independent if and only if \\[\n\\mbox{P}(A|B)=\\mbox{P}(A)\n\\]\nResult: \\(A\\) and \\(B\\) are independent if and only if \\[\n\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\n\\]\nThis follows from the definition of conditional probability and from above: \\[\n\\mbox{P}(A|B)=\\frac{\\mbox{P}(A\\cap B)}{\\mbox{P}(B)}=\\mbox{P}(A)\n\\]\nThus, \\(\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\\).\n\nExample: Consider the example above. Recall events \\(A\\) and \\(B\\). Let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. Are \\(A\\) and \\(B\\) independent?\n\nNo. First, recall that \\(\\mbox{P}(A|B)=0.5\\). The probability of selecting a blue vehicle (\\(\\mbox{P}(A)\\)) is \\(2/7\\) (the number of blue vehicles in our sample space divided by 7, the total number vehicles in \\(S\\)). This value is different from 0.5; thus, \\(A\\) and \\(B\\) are not independent.\nWe could also use the result above to determine whether \\(A\\) and \\(B\\) are independent. Note that \\(\\mbox{P}(A)= 2/7\\). Also, we know that \\(\\mbox{P}(B)=2/7\\). So, \\(\\mbox{P}(A)\\mbox{P}(B)=4/49\\). But, \\(\\mbox{P}(A\\cap B) = 1/7\\), since there is just one blue sedan in the sample space. \\(4/49\\) is not equal to \\(1/7\\); thus, \\(A\\) and \\(B\\) are not independent.",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "09-Conditional-Probability.html#bayes-rule",
    "href": "09-Conditional-Probability.html#bayes-rule",
    "title": "9  Conditional Probability",
    "section": "9.4 Bayes’ Rule",
    "text": "9.4 Bayes’ Rule\nAs mentioned in the introduction to this section, \\(\\mbox{P}(A|B)\\) is not the same quantity as \\(\\mbox{P}(B|A)\\). However, if we are given information about \\(A|B\\) and \\(B\\), we can use Bayes’ Rule to find \\(\\mbox{P}(B|A)\\). Let \\(B_1, B_2, ..., B_n\\) be mutually exclusive and exhaustive events and let \\(\\mbox{P}(A)&gt;0\\). Then, \\[\n\\mbox{P}(B_k|A)=\\frac{\\mbox{P}(A|B_k)\\mbox{P}(B_k)}{\\sum_{i=1}^n \\mbox{P}(A|B_i)\\mbox{P}(B_i)}\n\\]\nLet’s use an example to dig into where this comes from.\n\nExample:\nSuppose a doctor has developed a blood test for a certain rare disease (only one out of every 10,000 people have this disease). After careful and extensive evaluation of this blood test, the doctor determined the test’s sensitivity and specificity.\n\nSensitivity is the probability of detecting the disease for those who actually have it. Note that this is a conditional probability.\nSpecificity is the probability of correctly identifying “no disease” for those who do not have it. Again, another conditional probability.\nSee Figure 9.1 for a visual representation of these terms and others related to what is termed a confusion matrix.\n\n\n\n\n\n\n\n\nFigure 9.1: A table of true results and test results for a hypothetical disease. The terminology is included in the table. These ideas are important when evaluating machine learning classification models.\n\n\n\n\n\nIn fact, this test had a sensitivity of 100% and a specificity of 99.9%. Now suppose a patient walks in, the doctor administers the blood test, and it returns positive. What is the probability that that patient actually has the disease?\nThis is a classic example of how probability could be misunderstood. Upon reading this question, you might guess that the answer to our question is quite high. After all, this is a nearly perfect test. After exploring the problem more in depth, we find a different result.\n\n9.4.1 Approach using whole numbers\nWithout going directly to the formulaic expression above, let’s consider a collection of 100,000 randomly selected people. What do we know?\n\nBased on the prevalence of this disease (one out of every 10,000 people have this disease), we know that 10 of them should have the disease.\nThis test is perfectly sensitive. Thus, of the 10 people that have the disease, all of them test positive.\nThis test has a specificity of 99.9%. Of the 99,990 that don’t have the disease, \\(0.999*99990\\approx 99890\\) will test negative. The remaining 100 will test positive.\n\nThus, of our 100,000 randomly selected people, 110 will test positive. Of these 110, only 10 actually have the disease. Thus, the probability that someone has the disease given they’ve tested positive is actually around \\(10/110 = 0.0909\\).\n\n\n9.4.2 Mathematical approach\nNow let’s put this in context of Bayes’ Rule as stated above. First, let’s define some events. Let \\(D\\) be the event that someone has the disease. Thus, \\(D'\\) would be the event that someone does not have the disease. Similarly, let \\(T\\) be the event that someone has tested positive. What do we already know? \\[\n\\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D')=0.9999\n\\] \\[\n\\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T'|D)=0\n\\] \\[\n\\mbox{P}(T'|D')=0.999 \\hspace{1cm} \\mbox{P}(T|D') = 0.001\n\\]\nWe are looking for \\(\\mbox{P}(D|T)\\), the probability that someone has the disease, given he/she has tested positive. By the definition of conditional probability, \\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)}\n\\]\nThe numerator can be rewritten, again utilizing the definition of conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\).\nThe denominator can be rewritten using the Law of Total Probability (discussed [here][Probability properties]) and then the definition of conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D') = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')\\). So, putting it all together, \\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')}\n\\]\nNow we have stated our problem in the context of quantities we know: \\[\n\\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909\n\\]\nNote that in the original statement of Bayes’ Rule, we considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). In this example, we only have two: \\(D\\) and \\(D'\\).\n\n\n9.4.3 Simulation\nTo do the simulation, we can think of it as flipping a coin. First let’s assume we are pulling 1,000,000 people from the population. The probability that any one person has the disease is 0.0001. We will use rflip() to get the 1,000,000 people and designate as no disease or disease.\n\nset.seed(43)\nresults &lt;- rflip(1000000,0.0001,summarize = TRUE)\nresults\n\n      n heads  tails  prob\n1 1e+06   100 999900 1e-04\n\n\nIn this case 100 people had the disease. Now let’s find the positive test results. Of the 100 with the disease, all will test positive. Of those without disease, there is a 0.001 probability of testing positive.\n\nrflip(as.numeric(results['tails']),prob=.001,summarize = TRUE)\n\n       n heads  tails  prob\n1 999900   959 998941 0.001\n\n\nNow 959 tested positive. Thus the probability of having the disease given a positive test result is approximately:\n\n100/(100+959)\n\n[1] 0.09442871",
    "crumbs": [
      "Probability Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Probability and Statistics",
    "section": "",
    "text": "Preface\nThis book is based on the notes we created for our students as part of a one semester course on probability and statistics. We developed these notes from three primary resources. The most important is the Openintro Introductory Statistics with Randomization and Simulation (Diez, Barr, and Çetinkaya-Rundel 2014) book. In parts, we have used their notes and homework problems. However, in most cases we have altered their work to fit our needs. The second most important book for our work is Introduction to Probability and Statistics Using R (Kerns 2010). Finally, we have used some examples, code, and ideas from the first edition of Prium’s book, Foundations and Applications of Statistics: An Introduction Using R (R. J. Pruim 2011).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Computational Probability and Statistics",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nWe designed this book for the study of statistics that maximizes computational ideas while minimizing algebraic symbol manipulation. Although we do discuss traditional small-sample, normal-based inference and some of the classical probability distributions, we rely heavily on ideas such as simulation, permutations, and the bootstrap. This means that students with a background in differential and integral calculus will be successful with this book.\nThis book makes extensive using of the R programming language. In particular we focus both on the tidyverse and mosaic packages. We include a significant amount of code in our notes and frequently demonstrate multiple ways of completing a task. We have used this book for junior and sophomore college students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure-and-how-to-use-it",
    "href": "index.html#book-structure-and-how-to-use-it",
    "title": "Computational Probability and Statistics",
    "section": "Book structure and how to use it",
    "text": "Book structure and how to use it\nThis book is divided into four parts. Each part begins with a case study that introduces many of the main ideas of each part. Each chapter is designed to be a standalone 50 minute lesson. Within each chapter, we give exercises that can be worked in class and we provide learning objectives.\nThis book assumes students have access to R. Finally, we keep the number of homework problems to a reasonable level and assign all problems.\nThe four parts of the book are:\n\nDescriptive Statistical Modeling: This part introduces the student to data collection methods, summary statistics, visual summaries, and exploratory data analysis.\nProbability Modeling: We discuss the foundational ideas of probability, counting methods, and common distributions. We use both calculus and simulation to find moments and probabilities. We introduce basic ideas of multivariate probability. We include method of moments and maximum likelihood estimators.\nInferential Statistical Modeling: We discuss many of the basic inference ideas found in a traditional introductory statistics class but we add ideas of bootstrap and permutation methods.\nPredictive Statistical Modeling: The final part introduces prediction methods, mainly in the form of linear regression. This part also includes inference for regression.\n\nThe learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for:\n\nDeveloping probabilistic models.\n\nDeveloping statistical models for description, inference, and prediction.\n\nAdvancing practical and theoretical analytic experience and skills.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Computational Probability and Statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo take this course, students are expected to have completed calculus up through and including integral calculus. We do have multivariate ideas in the course, but they are easily taught and don’t require calculus III. We don’t assume the students have any programming experience and, thus, we include a great deal of code. We have historically supplemented the course with Data Camp courses. We have also used RStudio Cloud to help students get started in R without the burden of loading and maintaining software.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Computational Probability and Statistics",
    "section": "Packages",
    "text": "Packages\nThese notes make use of the following packages in R: knitr (Xie 2024), rmarkdown (Allaire et al. 2024), mosaic (R. Pruim, Kaplan, and Horton 2024), tidyverse (Wickham 2023), ISLR (James et al. 2021), vcd (Meyer et al. 2023), ggplot2 (Wickham et al. 2024), MASS (Ripley 2024), openintro (Çetinkaya-Rundel et al. 2024), broom (Robinson, Hayes, and Couch 2024), infer (R-infer?), kableExtra (Zhu 2024), and DT (Xie, Cheng, and Tan 2024).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#solutions-manual",
    "href": "index.html#solutions-manual",
    "title": "Computational Probability and Statistics",
    "section": "Solutions Manual",
    "text": "Solutions Manual\nThe accompanying solutions manual is available here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Probability and Statistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD We have been lucky to have numerous open sources to help facilitate this work. Thank you to those who helped to correct mistakes to include Skyler Royse. ======= We have been lucky to have numerous open sources to help facilitate this work. Thank you to those who helped to provide edits including Kris Pruitt, Matt Davis, and Skyler Royse. &gt;&gt;&gt;&gt;&gt;&gt;&gt; 9f35b31e4cd3f5f316488b29698f940e4b83eeef\n\n\n\n\n\n\n\n\n\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#file-creation-information",
    "href": "index.html#file-creation-information",
    "title": "Computational Probability and Statistics",
    "section": "File Creation Information",
    "text": "File Creation Information\n\nFile creation date: 2024-06-18\nR version 4.4.0 (2024-04-24)\n\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. Openintro: Datasets and Supplemental Functions from OpenIntro Textbooks and Labs. http://openintrostat.github.io/openintro/.\n\n\nDiez, David, Christopher Barr, and Mine Çetinkaya-Rundel. 2014. Introductory Statistics with Randomization and Simulation. 1st ed. Openintro. https://www.openintro.org/book/isrs/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://www.statlearning.com.\n\n\nKerns, Jay. 2010. Introductory to Probability and Statistics with r. 1st ed. http://ipsur.r-forge.r-project.org/book/download/IPSUR.pdf.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. Vcd: Visualizing Categorical Data.\n\n\nPruim, Randall J. 2011. Foundations and Applications of Statistics: An Introduction Using r. Vol. 13. American Mathematical Soc.\n\n\nPruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2024. Mosaic: Project MOSAIC Statistics and Mathematics Teaching Utilities. https://github.com/ProjectMOSAIC/mosaic.\n\n\nRipley, Brian. 2024. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2024. Broom: Convert Statistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Joe Cheng, and Xianying Tan. 2024. DT: A Wrapper of the JavaScript Library DataTables. https://github.com/rstudio/DT.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with Kable and Pipe Syntax. http://haozhu233.github.io/kableExtra/.",
    "crumbs": [
      "Preface"
    ]
  }
]