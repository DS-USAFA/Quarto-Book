# Studies {#sec-STUDY}

```{r}
#| echo: false
#| message: false
#| warning: false

library(kableExtra)
library(openintro)
library(tidyverse)
library(mosaic)
library(usdata)
```

## Objectives

1)  Differentiate between various statistical terminologies such as *observational study, confounding variable, prospective study, retrospective study, simple random sampling, stratified sampling, strata, cluster sampling, multistage sampling, experiment, randomized experiment, control, replicate, blocking, treatment group, control group, blinded study, placebo, placebo effect,* and *double-blind*, and construct examples to demonstrate their proper use in context.

2)  Evaluate study descriptions using appropriate terminology, and analyze the study design for potential biases or confounding variables.

3)  Given a scenario, identify and assess flaws in reasoning, and propose robust study and sampling methodologies to address these flaws.

## Observational studies, sampling strategies, and experiments

### Observational studies

Generally, data in **observational studies** are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers.

Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations.

> **Exercise**:\
> Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen *causes* skin cancer?[^studies-1]

[^studies-1]: No. See the paragraph following the exercise for an explanation.

Some previous research[^studies-2] tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen *and* more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation.

[^studies-2]: http://www.sciencedirect.com/science/article/pii/S0140673698121682\
    http://archderm.ama-assn.org/cgi/content/abstract/122/5/537\
    Study with a similar scenario to that described here:\
    http://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full

```{r fig.dim=c(5,3)}
#| label: fig-confound
#| echo: false
#| fig-cap: "un exposure is a confounding variable because it is related to both response and explanatory variables."
plot(c(-0.05, 1.2), c(0.39, 1), type = 'n', axes = FALSE, xlab = "", ylab = "")

text(0.59, 0.89, 'sun exposure', cex = 0.8)
rect(0.4, 0.8, 0.78, 1)

text(0.3, 0.49, 'use sunscreen', cex = 0.8)
rect(0.1, 0.4, 0.48, 0.6)
arrows(0.49, 0.78, 0.38, 0.62, length = 0.08, lwd = 1.5)

text(0.87, 0.5, 'skin cancer', cex = 0.8)
rect(0.71, 0.4, 1.01, 0.6)
arrows(0.67, 0.78, 0.8, 0.62, length = 0.08, lwd = 1.5)

arrows(0.5, 0.5, 0.69, 0.5, length = 0.08, col = COL[6, 2])
#text(0.6, 0.495, 'X', cex = 2, col = COL[4])
text(0.595, 0.565, "?", cex = 1.5, col = COL[4])

```

Sun exposure is what is called a **confounding variable**,[^studies-3] which is a variable that is correlated with both the explanatory and response variables, see @fig-confound. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured.

[^studies-3]: Also called a **lurking variable**, **confounding factor**, or a **confounder**.

Let's look at an example of confounding visually. Using the `SAT` data from the **mosaic** package let's look at expenditure per pupil versus SAT scores. @fig-confound2 is a plot of the data.

> **Exercise**:\
> What conclusion do you reach from the plot in @fig-confound2?[^studies-4]

[^studies-4]: It appears that average SAT score declines as expenditures per student increases.

```{r}
#| label: fig-confound2
#| echo: false
#| fig-cap: "Average SAT score versus expenditure per pupil; reminder: each observation represents an individual state."
#| warning: false
SAT %>%
  gf_point(sat ~ expend, xlab = "Expenditure per pupil", 
           ylab = "Average SAT score", 
           title = "SAT Scores versus Expenditures") %>%
  gf_theme(theme_classic())

```

The implication that spending less might give better results is not justified. Expenditures are confounded with the proportion of students who take the exam, and scores are higher in states where fewer students take the exam.

It is interesting to look at the original plot if we place the states into two groups depending on whether more or fewer than 40% of students take the SAT. @fig-conditional is a plot of the data broken down into the 2 groups.

```{r}
#| label: fig-conditional
#| echo: false
#| warning: false
#| fig-cap: "Average SAT score versus expenditure per pupil; broken down by level of participation."
SAT %>% 
  mutate(fracGroup = derivedFactor(
Hi = (frac > 40),
Low = (frac <= 40) )) %>%
  gf_point(sat ~ expend, color = ~fracGroup, 
           xlab = "Expenditure per pupil", ylab = "Average SAT score", 
           title = "SAT Scores versus Expenditures") %>%
  gf_lm() %>%
  gf_theme(theme_classic()) %>%
  gf_labs(color = "Fraction")
```

Once we account for the fraction of students taking the SAT, the relationship between expenditures and SAT scores changes.

In the same way, the `county` data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions.

> **Exercise**:\
> @fig-homeown2 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in the @fig-homeown2.[^studies-5]

[^studies-5]: Answers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents.

```{r}
#| message: false
#| eval: false
#| echo: false
#| warning: false
library(usdata)
```

```{r}
#| warning: false
#| echo: false
#| message: false
county_subset <- county_complete %>% 
  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, 
         poverty = poverty_2010, homeownership = homeownership_2010, 
         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, 
         med_income = median_household_income_2010) %>%
  mutate(fed_spend = fed_spend / pop2010)
```

```{r}
#| label: fig-homeown2
#| echo: false
#| fig-cap: "A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties."
pch <- 1
cex <- sqrt(county_subset$pop2010 / 1e6)
colPop <- fadeColor(ifelse(cex > 0.35, COL[4], COL[1]), 
                    substr(gray(0.6 + cex*0.1), 2, 3))
colSm <- colPop
cexF <- 2
cex <- 0.7
pch <- 20
gp1 <- cex < 0.32
colSm  <- COL[1, 3]
colPop <- COL[1, 3]
cexF   <- 1
plot(county_subset$multi_unit[gp1], county_subset$homeownership[gp1], 
     pch = pch, col = colSm, xlab = "", ylab = "Percent of Homeownership", 
     axes = FALSE, cex = ifelse(cex < 0.32, 0.32, cex)[gp1], 
     xlim = range(county_subset$multi_unit), 
     ylim = range(county_subset$homeownership))
at = seq(0, 100, 20)
axis(1, at, paste0(at, "%"))
axis(2, at, paste0(at, "%"))
box()
points(county_subset$multi_unit[!gp1], county_subset$homeownership[!gp1], 
       pch = pch, col = colPop, xlab = "", ylab = "Percent of Homeownership", 
       cex = ifelse(cex < 0.32, 0.32, cex)[!gp1])
points(county_subset$multi_unit[!gp1], county_subset$homeownership[!gp1], 
       pch = 20, cex = 0.2, col = COL[5, 3])
#points(cc$housing_multi_unit, cc$home_ownership, pch = ".", cex = 1)
#rug(county$poverty[county$fed_spend > 40], side = 3)
mtext("Percent of Units in Multi-Unit Structures", 1, 1.9, cex = 1)
```

Observational studies come in two forms: prospective and retrospective studies. A **prospective study** identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses Health Study, started in 1976 and expanded in 1989.[^studies-6] This prospective study recruits registered nurses and then collects data from them using questionnaires.

[^studies-6]: http://www.channing.harvard.edu/nhs/

**Retrospective studies** collect data after events have taken place; e.g. researchers may review past events in medical records. Some data sets, such as `county`, may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retail sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population).

### Three sampling methods

Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. @fig-simprand, @fig-stratsamp2, and @fig-clussamp4 provide a graphical representation of these techniques.

```{r}
#| label: fig-simprand
#| echo: false
#| message: false
#| fig-cap: "Examples of simple random sampling. In this figure, simple random sampling was used to randomly select the 18 cases."

data(COL)
set.seed(3)
N <- 108
n <- 18
colSamp <- COL[4]
PCH <- rep(c(1, 3, 20)[3], 3)

col <- rep(COL[1], N)
pch <- PCH[match(col, COL)]

#=====> SRS <=====#
plot(0, xlim = c(0, 2), ylim = 0:1, type = 'n', 
     axes = FALSE, xlab = "", ylab = "")
box()
x   <- runif(N, 0, 2)
y   <- runif(N)
inc <- n
points(x, y, col = col, pch = pch)

these <- sample(N, n)
points(x[these], y[these], pch = 20, cex = 0.8, col = colSamp)
points(x[these], y[these], cex = 1.4, col = colSamp)

```

```{r}
#| label: fig-stratsamp2
#| echo: false
#| message: false
#| fig-cap: "In this figure, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum."

#=====> Stratified <=====#
PCH <- rep(c(1, 3, 20)[3], 3)
plot(0, xlim = c(0, 2), ylim = 0:1, type = 'n', 
     axes = FALSE, xlab = "", ylab = "")
box()
X    <- c(0.18, 0.3, 0.68, 1.18, 1.4, 1.74)
Y    <- c(0.2, 0.78, 0.44, 0.7, 0.25, 0.65)
locs <- c(1, 4, 5, 3, 6, 2)
gps  <- list()
N    <- 1.1*c(15, 12, 35, 22, 13, 28)
R    <- sqrt(N / 500)
p    <- matrix(c(12, 2, NA, 1, 2, NA, 4, 30, NA, 
                 19, 1, NA, 11, 0, NA, 3, 24, NA), 3)
p     <- round(p*1.1)
p[3,] <- N - p[1,] - p[2,]
above <- c(1, 1, 1, 1, -1, 1)
for(i in 1:6){
	hold <- seq(0, 2*pi, len = 99)
	x    <- X[i] + (R[i] + 0.01)*cos(hold)
	y    <- Y[i] + (R[i] + 0.01)*sin(hold)
	polygon(x, y, border = COL[5, 4])
	x    <- rep(NA, N[i])
	y    <- rep(NA, N[i])
	for(j in 1:N[i]){
		inside <- FALSE
		while(!inside){
			xx <- runif(1, -R[i], R[i])
			yy <- runif(1, -R[i], R[i])
			if(sqrt(xx^2 + yy^2) < R[i]){
				inside <- TRUE
				x[j] <- xx
				y[j] <- yy
			}
		}
	}
	type <- sample(1, N[i], TRUE)
	pch  <- PCH[type]
	col  <- COL[type]
	x    <- X[i] + x
	y    <- Y[i] + y
	points(x, y, pch = pch, col = col)
	these  <- sample(N[i], 3)
	points(x[these], y[these], pch = 20, cex = 0.8, col = colSamp)
	points(x[these], y[these], cex = 1.4, col = colSamp)
}
text(X, Y + above*(R + 0.01), paste("Stratum", 1:6), pos = 2 + above, cex = 1.1)


```

```{r}
#| label: fig-clussamp4
#| echo: false
#| fig-cap: "In this figure, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected."
#| 
#=====> Cluster <=====#
PCH <- rep(c(1, 3, 20)[3], 3)
plot(0, xlim = c(0, 2), ylim = 0:1, type = 'n', 
     axes = FALSE, xlab = "", ylab = "")
box()
X    <- c(0.17, 0.19, 0.52, 0.85, 1, 1.22, 1.49, 1.79, 1.85)
Y    <- c(0.3, 0.75, 0.5, 0.26, 0.73, 0.38, 0.67, 0.3, 0.8)
locs <- c(1, 4, 5, 3, 6, 2)
gps  <- list()
N    <- c(18, 12, 11, 13, 16, 14, 15, 16, 12)
R    <- sqrt(N / 500)
p    <- matrix(c(6, 8, NA, 4, 4, NA, 4, 4, NA, 5, 4, NA, 8, 5, NA,
                 4, 5, NA, 5, 9, NA, 6, 5, NA, 4, 5, NA), 3)
p[3,] <- N - p[1,] - p[2,]
above <- c(-1, 1, 1, 1, 1, -1, 1, 1, 1)
for(i in 1:length(X)){
	hold <- seq(0, 2*pi, len = 99)
	x    <- X[i] + (R[i] + 0.02)*cos(hold)
	y    <- Y[i] + (R[i] + 0.02)*sin(hold)
	polygon(x, y, border = COL[5, 4])
	if(i %in% c(3, 4, 8)){
		polygon(x, y, border = COL[4], lty = 2, lwd = 1.5)
	}
	x    <- rep(NA, N[i])
	y    <- rep(NA, N[i])
	for(j in 1:N[i]){
		inside <- FALSE
		while(!inside){
			xx <- runif(1, -R[i], R[i])
			yy <- runif(1, -R[i], R[i])
			if(sqrt(xx^2 + yy^2) < R[i]){
				inside <- TRUE
				x[j] <- xx
				y[j] <- yy
			}
		}
	}
	type <- sample(1, N[i], TRUE)
	pch  <- PCH[type]
	col  <- COL[type]
	x    <- X[i] + x
	y    <- Y[i] + y
	points(x, y, pch = pch, col = col)
	these <- 1:N[i]
	if(i %in% c(3, 4, 8)){
	points(x[these], y[these], pch = 20, cex = 0.8, col = colSamp)
	points(x[these], y[these], cex = 1.4, col = colSamp)
		#points(x[these], y[these], pch = 19, col = colSamp)
	}
}
text(X, Y + above*(R+0.01), paste("Cluster", 1:length(X)), 
     pos = 2 + above, cex = 1.1)


```

**Simple random sampling** is probably the most intuitive form of random sampling, in which each individual in the population has an equal chance of being chosen. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league's 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that season's 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as "simple random" if each case in the population has an equal chance of being included in the final sample *and* knowing that a case is included in a sample does not provide useful information about which other cases are included or not.

**Stratified sampling** is a divide-and-conquer sampling strategy. The population is divided into groups called **strata**. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (we're looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players.

Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling.

> **Example**:\
> Why would it be good for cases within each stratum to be very similar?[^studies-7]

[^studies-7]: We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population.

In **cluster sampling**, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don't look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this book can be extended to handle such data.

> **Example**:\
> Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed?[^studies-8]

[^studies-8]: A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information.

Another technique called **multistage sampling** is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling.

### Experiments

Studies where the researchers assign treatments to cases are called **experiments**. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a **randomized experiment**. Randomized experiments are fundamentally important when trying to show a causal connection between two variables.

#### Principles of experimental design

Randomized experiments are generally built on four principles.

1.  **Controlling**. Researchers assign treatments to cases, and they do their best to **control** any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill.

2.  **Randomization**. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study.

3.  **Replication**. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we **replicate** by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. You replicate to the level of variability you want to estimate. For example, in flight test, we can run the same flight conditions again to get a replicate; however, if the same plane and pilot are being used, the replicate is not getting the pilot-to-pilot or the plane-to-plane variability.

4.  **Blocking**. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block, or group, to the treatments. This strategy is often referred to as **blocking**. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in @fig-exp4. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients.

```{r fig.dim=c(5,7)}
#| label: fig-exp4
#| echo: false
#| warning: false
#| fig-cap: "Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories."
set.seed(2)
xlim     <- c(0, 1)
slimBox3 <- 0.03
#COL      <- c("#225588", "#2266AA", "#882255", "#AA2266", "#000000", "#AAAAAA")
data(COL)

plot(c(0, 2.9), type = "n", axes = FALSE, 
     xlab = "", ylab = "", xlim = c(-0.1, 1.1))
rect(0, 2.2, 1, 2.9)
text(0.5, 2.885, "Numbered patients", pos = 3, cex = 0.9)
rect(0, 1.2, 0.45, 1.9)
rect(0.55, 1.2, 1, 1.9)
arrows(0.56, 2.17, 0.75, 2.02, length = 0.1, lwd = 1.37)
arrows(0.44, 2.17, 0.25, 2.02, length = 0.1, lwd = 1.37)
text(0.5, 2.07, "create\nblocks", cex = 0.8)
text(0.2, 1.89, "Low-risk patients", pos = 3, cex = 0.7)
text(0.2 + 0.55, 1.89, "High-risk patients", pos = 3, cex = 0.7)
rect(0, 0.48, 1, 0.9, border = COL[5])
rect(0, 0.00, 1, 0.42, border = COL[5])
#rect(0.55 + slimBox3, 0.48, 1, 0.9)
#rect(0.55 + slimBox3, 0.00, 1, 0.42)
arrows(0.09, 1.16, y1 = 1, length = 0.1, lwd = 1.37)
text(0.1, 1.08, "randomly\nsplit in half", cex = 0.7, pos = 4)
arrows(0.12 + 0.55, 1.16, y1 = 1, length = 0.1, lwd = 1.37)
text(0.13 + 0.55, 1.08, "randomly\nsplit in half", cex = 0.7, pos = 4)

#=====> Inner Box <=====#
rect(0.02, 0.50, 0.41, 0.88, border = COL[5, 3])
rect(0.02, 0.02, 0.41, 0.40, border = COL[5, 3])
rect(0.57 + slimBox3, 0.50, 0.98, 0.88, border = COL[5, 3])
rect(0.57 + slimBox3, 0.02, 0.98, 0.40, border = COL[5, 3])

#=====> Labels <=====#
rect(-0.05, 0.39 + 0.47, 0.14, 0.45 + 0.47, col = "#FFFFFF", border = COL[5])
text(0.02, 0.424 + 0.47, "Control", cex = 0.6, col = COL[5])

rect(-0.05, 0.39, 0.14, 0.45, col = "#FFFFFF", border = COL[5])
text(0.04, 0.424, "Treatment", cex = 0.6, col = COL[5])

n   <- 6*9
pch <- c(1, 20)[sample(2, n, TRUE, c(0.8, 1.2))]
cex <- rnorm(n, 1, 0.001)
k   <- 0
for(x in seq(0.1, 0.9, len = 9)){
	for(y in rev(seq(0.3, 0.8, len = 6))){
		k <- k + 1
		points(x, y + 2, pch = pch[k], cex = cex[k], col = COL[1 + 3*(pch[k] == 1)])
		text(x, y + 1.98, k, cex = 0.45, pos = 3, col = COL[1 + 3*(pch[k] == 1)])
	}
}

trmt  <- rep(NA, n)

these <- which(pch == 1)
trmt[sample(these, length(these) / 2)] <- "ctrl"
trmt[is.na(trmt) & pch == 1] <- "trmt"
k <- 0
x <- 0.078
y <- 1.83
for(i in these){
	k <- k + 1
	points(x, y, pch = pch[i], cex = cex[i], col = COL[4])
	text(x, y - 0.02, i, cex = 0.45, pos = 3, col = COL[4])
	if(y < 1.3){
		x <- x + 0.095
		y <- 1.83
	} else {
		y <- y - 0.11
	}
}
these <- which(pch != 1)
trmt[sample(these, length(these) / 2)] <- "ctrl"
trmt[is.na(trmt) & pch != 1] <- "trmt"
k <- 0
x <- 0.615
y <- 1.82
for(i in these){
	k <- k + 1
	points(x, y, pch = pch[i], cex = cex[i], col = COL[1])
	text(x, y - 0.02, i, cex = 0.45, pos = 3, col = COL[1])
	if(y < 1.3){
		x <- x + 0.08
		y <- 1.83
	} else {
		y <- y - 0.095
	}
}

#=====> Low Risk <=====#
k <- rep(0,4)
x <- c(0.10, 0.10, 0.665, 0.665)
y <- c(0.35, 0.83, 0.35, 0.83) - 0.03
for(i in 1:n){
	j <- 1
	if(trmt[i] == "trmt"){
		j <- j + 1
	}
	if(pch[i] != 1){
		j <- j + 2
	}
	k[j] <- k[j] + 1
	points(x[j], y[j], pch = pch[i], cex = cex[i], col = COL[1 + 3*(pch[i] == 1)])
	text(x[j], y[j] - 0.02, i, cex = 0.45, pos = 3, col = COL[1 + 3*(pch[i] == 1)])
	if(y[j] < 0.12 + 0.51*(j %in% c(2,4)) - 0.03){
		x[j] <- x[j] + 0.11 - 0.025*(j > 2)
		y[j] <- 0.35 + 0.48*(j %in% c(2,4)) - 0.03
	} else {
		y[j] <- y[j] - 0.085
	}
	#Sys.sleep(0.3)
}


```

It is important to incorporate the first three experimental design principles into any study, and this chapter describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this chapter may be extended to analyze data collected using blocking. Math 359 is an entire course at USAFA devoted to the design and analysis of experiments.

#### Reducing bias in human experiments

Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.[^studies-9] In particular, researchers wanted to know if the drug reduced deaths in patients.

[^studies-9]: Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.

These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug's effect. Study volunteers[^studies-10] were randomly placed into two study groups. One group, the **treatment group**, received the experimental treatment of interest (the new drug to treat heart attack patients). The other group, called the **control group**, did not receive any drug treatment. The comparison between the treatment and control groups allows researchers to determine whether the treatment really has an effect.

[^studies-10]: Human subjects are often called **patients**, **volunteers**, or **study participants**.

Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn't receive the drug and sits idly, hoping her participation doesn't increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify.

Researchers aren't usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be **blind**. But there is one problem: if a patient doesn't receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a **placebo**, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the **placebo effect**.

The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a **double-blind** setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.[^studies-11]

[^studies-11]: There are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the study's patients and do not tell the blinded health care professionals who is receiving which treatment.

> **Exercise**:\
> Look back to the stent study in the first chapter where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?[^studies-12]

[^studies-12]: The researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind.
