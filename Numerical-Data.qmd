# Numerical Data {#sec-NUMDATA}

## Objectives

1)  Differentiate between various statistical terminologies such as *scatterplot, mean, distribution, point estimate, weighted mean, histogram, data density, right skewed, left skewed, symmetric, mode, unimodal, bimodal, multimodal, density plot, variance, standard deviation, box plot, median, interquartile range, first quartile, third quartile, whiskers, outlier, robust estimate*, and *transformation*, and construct examples to demonstrate their proper use in context.

2)  Using `R`, generate and interpret summary statistics for numerical variables.

3)  Create and evaluate graphical summaries of numerical variables using `R`, choosing the most appropriate types of plots for different data characteristics and research questions.

4)  Synthesize numerical and graphical summaries to provide interpretations and explanations of a data set.

```{r}
#| echo: false
#| message: false
#| warning: false

library(kableExtra)
library(openintro)
library(tidyverse)
library(mosaic)
library(usdata)
```

```{r warning=FALSE,echo=FALSE}
county_M377 <- county_complete %>% 
  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, 
         poverty = poverty_2010, homeownership = homeownership_2010, 
         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, 
         med_income = median_household_income_2010) %>%
  mutate(fed_spend = fed_spend / pop2010)
```

## Numerical Data

This chapter introduces techniques for exploring and summarizing numerical variables. The `email50` and `mlb` data sets from the **openintro** package and a subset of `county_complete` from the **usdata** package provide rich opportunities for examples. Recall that outcomes of numerical variables are numbers on which it is reasonable to perform basic arithmetic operations. For example, the `pop2010` variable, which represents the population of counties in 2010, is numerical since we can sensibly discuss the difference or ratio of the populations in two counties. On the other hand, area codes and zip codes are not numerical.

### Scatterplots for paired data

A **scatterplot** provides a case-by-case view of data for two numerical variables. In @fig-scat5, we again present a scatterplot used to examine how federal spending and poverty are related in the `county` data set.

```{r}
#| label: fig-scat5
#| echo: false
#| fig-cap: "A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted."
plot(county_M377$poverty, county_M377$fed_spend, 
     pch = 20, cex = 0.7, col = COL[1,3], ylim = c(0, 31.25), 
     xlab = "", ylab = "Federal Spending Per Capita", axes = FALSE ,cex.lab = 0.75)
axis(1)
axis(2, at = seq(0, 30, 10))
box()
points(county_M377$poverty, county_M377$fed_spend, pch = 20, cex  =  0.18)
#rug(county$poverty[county$fed_spend > 40], side = 3)
mtext("Poverty Rate (Percent)", 1, 1.9)
t1 <- county_M377$poverty[1088]
t2 <- county_M377$fed_spend[1088]
lines(c(t1, t1), c(-10, t2), lty = 2, col = COL[4])
lines(c(-10, t1), c(t2, t2), lty = 2, col = COL[4])
points(t1, t2, col = COL[4])
text(43, 29, 
     "32 counties with higher\n federal spending are not shown", cex = 0.6)
```

Another scatterplot is shown in @fig-scat52, comparing the number of `line_breaks` and number of characters, `num_char`, in emails for the `email50` data set. In any scatterplot, each point represents a single case. Since there are 50 cases in `email50`, there are 50 points in @fig-scat52.

```{r}
#| label: fig-scat52
#| echo: false
#| fig-cap: "A scatterplot of `line_breaks` versus `num_char` for the `email50` data."
data(COL)
plot(email50$num_char, email50$line_breaks, 
     pch = 19, cex = 1.3, col = COL[1, 4], 
     xlab = "", ylab = "Number of Lines", axes = FALSE)
axis(2)
axis(1)
box()
points(email50$num_char, email50$line_breaks, cex = 1.3, col = COL[1, 1])
mtext("Number of Characters (in thousands)", 1, 1.9)
```

To put the number of characters in perspective, this paragraph in the text has 357 characters. Looking at @fig-scat52, it seems that some emails are incredibly long! Upon further investigation, we would actually find that most of the long emails use the HTML format, which means most of the characters in those emails are used to format the email rather than provide text.

> **Exercise**:\
> What do scatterplots reveal about the data, and how might they be useful?[^numerical-data-1]

[^numerical-data-1]: Answers may vary. Scatterplots are helpful in quickly spotting associations between variables, whether those associations represent simple or more complex relationships.

> *Example*:\
> Consider a new data set of 54 cars with two variables: vehicle price and weight.[^numerical-data-2] A scatterplot of vehicle price versus weight is shown in @fig-scat53. What can be said about the relationship between these variables?

[^numerical-data-2]: Subset of data from http://www.amstat.org/publications/jse/v1n1/datasets.lock.html

```{r}
#| label: fig-scat53
#| echo: false
#| fig-cap: "A scatterplot of *price* versus *weight* for 54 cars."
data(cars93)
data(COL)
plot(cars93$weight, cars93$price, 
     xlab = 'Weight (Pounds)', ylab = 'Price ($1000s)', 
     pch = 19, col = COL[1, 2], ylim = c(0, max(cars93$price)))

w  <- seq(1000, 5000, 100)
# Rough Model
g1 <- lm(price ~ weight, cars93, weights = 1 / weight^2)
g2 <- lm(price ~ weight + I(weight^2), cars93, weights = 1/weight^2)
p  <- predict(g2, data.frame(weight = w))
lines(w, p, lty = 2, col = COL[5, 3])
```

The relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we've seen which show relationships that are very linear.

> **Exercise**:\
> Describe two variables that would have a horseshoe-shaped association in a scatterplot.[^numerical-data-3]

[^numerical-data-3]: Consider the case where your vertical axis represents something "good" and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description since water becomes toxic when consumed in excessive quantities.

### The mean

The **mean**, sometimes called the average, is a common way to measure the center of a **distribution**[^numerical-data-4] of data. To find the mean number of characters in the 50 emails, we add up all the character counts and divide by the number of emails. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.

[^numerical-data-4]: The distribution of a variable is essentially the collection of all values of the variable in the data set. It tells us what values the variable takes on and how often. In the `email50` data set, we used a dotplot to view the distribution of `num_char`.

$$\bar{x} = \frac{21.7 + 7.0 + \cdots + 15.8}{50} = 11.6$$

The sample mean is often labeled $\bar{x}$. There is a bar over the letter, and the letter $x$ is being used as a generic placeholder for the variable of interest, `num_char`.

> **Mean**\
> The sample mean of a numerical variable is the sum of all of the observations divided by the number of observations, Equation 1.

```{=tex}
\begin{equation} 
  \bar{x} = \frac{x_1+x_2+\cdots+x_n}{n}
  \tag{1}
\end{equation}
```
where $x_1, x_2, \dots, x_n$ represent the $n$ observed values.

> **Exercise**:\
> Examine the two equations above. What does $x_1$ correspond to? And $x_2$? Can you infer a general meaning to what $x_i$ might represent?[^numerical-data-5]

[^numerical-data-5]: $x_1$ corresponds to the number of characters in the first email in the sample (21.7, in thousands), $x_2$ to the number of characters in the second email (7.0, in thousands), and $x_i$ corresponds to the number of characters in the $i^{th}$ email in the data set.

> **Exercise**:\
> What was $n$ in this sample of emails?[^numerical-data-6]

[^numerical-data-6]: The sample size, $n = 50$.

The `email50` data set is a sample from a larger population of emails that were received in January and March. We could compute a mean for this population in the same way as the sample mean. However, there is a difference in notation: the population mean has a special label: $\mu$. The symbol $\mu$ is the Greek letter *mu* and represents the average of all observations in the population. Sometimes a subscript, such as $_x$, is used to represent which variable the population mean refers to, e.g. $\mu_x$.

> *Example*: The average number of characters across all emails can be estimated using the sample data. Based on the sample of 50 emails, what would be a reasonable estimate of $\mu_x$, the mean number of characters in all emails in the `email` data set? (Recall that `email50` is a sample from `email`.)

The sample mean, 11.6, may provide a reasonable estimate of $\mu_x$. While this number will not be perfect, it provides a **point estimate**, a single plausible value, of the population mean. Later in the text, we will develop tools to characterize the accuracy of point estimates, and we will find that point estimates based on larger samples tend to be more accurate than those based on smaller samples.

> *Example*:\
> We might like to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes from the 3,143 counties in the `county` data set. What would be a better approach?

The `county` data set is special in that each county actually represents many individual people. If we were to simply average across the `income` variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties' totals, and then divide by the number of people in all the counties. If we completed these steps with the `county` data, we would find that the per capita income for the US is \$27,348.43. Had we computed the *simple* mean of per capita income across counties, the result would have been just \$22,504.70!

This previous example used what is called a **weighted mean**[^numerical-data-7], which will be a key topic in the probability section. As a look ahead, the probability mass function gives the population proportions of each county's mean value, and thus, to find the population mean $\mu$, we will use a weighted mean.

[^numerical-data-7]: A weighted mean is an average in which some observations contribute more "weight" than others. In the `county` data set, we "weighted" the income for each county by dividing income by the county population.

### Histograms and shape

Rather than showing the exact value of each observation for a single variable, think of the value as belonging to a *bin*. For example, in the `email50` data set, we create a table of counts for the number of cases with character counts between 0 and 5,000, then the number of cases between 5,000 and 10,000, and so on. Observations that fall on the boundary of a bin (e.g. 5,000) are allocated to the lower bin. This tabulation is shown below.

```{r echo=FALSE}
table(cut(email50$num_char, breaks = seq(0, 65, 5)))
```

These binned counts are plotted as bars in @fig-hist5 in what is called a **histogram**[^numerical-data-8].

[^numerical-data-8]: A histogram displays the distribution of a quantitative variable. It shows binned counts, the number of observations in a bin, or range of values.

```{r}
#| label: fig-hist5
#| echo: false
#| fig-cap: "A histogram of `num_char`. This distribution is very strongly skewed to the right."
email50 %>%
   gf_histogram(~num_char, binwidth = 5, boundary = 0, 
                xlab = "The Number of Characters (in thousands)",
                color = "black", fill = "cyan") %>%
   gf_theme(theme_classic())
```

Histograms provide a view of the **data density**. Higher bars represent where the data are relatively more dense. For instance, there are many more emails between 0 and 10,000 characters than emails between 10,000 and 20,000 characters in the data set. The bars make it easy to see how the density of the data changes relative to the number of characters.

Histograms are especially convenient for describing the shape of the data distribution. @fig-hist5 shows that most emails have a relatively small number of characters, while fewer emails have a very large number of characters. When data trail off to the right in this way and have a longer right **tail**, the shape is said to be **right skewed**.[^numerical-data-9]

[^numerical-data-9]: Other ways to describe data that are skewed to the right: **skewed to the right**, **skewed to the high end**, or **skewed to the positive end**.

Data sets with the reverse characteristic -- a long, thin tail to the left -- are said to be **left skewed**. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called **symmetric**.

> **Long tails to identify skew**\
> When data trail off in one direction, the distribution has a **long tail**. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed.

#### Making our own histogram

Let's take some time to make a simple histogram. We will use the **ggformula** package, which is a wrapper for the **ggplot2** package.

Here are two questions:\
*What do we want `R` to do?* and\
*What must we give `R` for it to do this?*

We want `R` to make a histogram. In `ggformula`, the plots have the form `gf_plottype` so we will use the `gf_histogram()`. To find options and more information about the function, type:

```         
?gf_histogram
```

To start, we just have to give the formulas and data to `R`.

```{r}
gf_histogram(~num_char, data = email50, color = "black", fill = "cyan")
```

> **Exercise**:\
> Look at the help menu for `gf_histogram` and change the x-axis label, change the bin width to 5, and have the left bin start at 0.

Here is the code for the exercise:

```         
email50 %>%
   gf_histogram(~num_char, binwidth = 5,boundary = 0,
   xlab = "The Number of Characters (in thousands)", 
   color = "black", fill = "cyan") %>%
   gf_theme(theme_classic())
```

In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A **mode** is represented by a prominent peak in the distribution.[^numerical-data-10] There is only one prominent peak in the histogram of `num_char`.

[^numerical-data-10]: Another definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have *no* observations with the same value in a data set, which makes this other definition useless for many real data sets.

@fig-histmulti shows histograms that have one, two, or three prominent peaks. Such distributions are called **unimodal**, **bimodal**, and **multimodal**, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since the separation between the two peaks is relatively small, and it only differs from its neighboring bins by a few observations.

```{r}
#| echo: false
#| results: hide
data(COL)

set.seed(51)
x1 <- rchisq(65, 6)
x2 <- c(rchisq(22, 5.8), rnorm(40, 16.5, 2))
x3 <- c(rchisq(20, 3), rnorm(35, 12), rnorm(42, 18, 1.5))
```

```{r}
#| label: fig-histmulti
#| echo: false
#| fig-cap: "Histograms that demonstrate unimodal, bimodal, and multimodal data."

# Combine data into a single dataframe
data <- data.frame(group = rep(c("Unimodal", "Bimodal", "Multimodal"), 
                                times = c(length(x1), length(x2), length(x3))),
                   value = c(x1, x2, x3))

# Create the ggplot with histogram and facet wrap
ggplot(data, aes(x = value)) +
  geom_histogram(aes(fill = group), bins = 10) +  # Adjust bins as needed
  labs(title = "Distribution of Data", x = "", y = "Count") +
  facet_wrap(~ factor(group, levels = c("Unimodal", "Bimodal", "Multimodal")), nrow = 1) +    # Arrange side-by-side
  theme_minimal() +  # Optional: adjust theme for aesthetics
  theme(legend.position = "none")
```

> **Exercise**:\
> Height measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you anticipate in this height data set?[^numerical-data-11]

[^numerical-data-11]: There might be two height groups visible in the data set: one for the students and one for the adults. That is, the data are probably bimodal. But it could be multimodal because within each group we may be able to see a difference in males and females.

> **Looking for modes**\
> Looking for modes isn't about finding a clear and correct answer about the number of modes in a distribution, which is why **prominent** is not rigorously defined in these notes. The important part of this examination is to better understand your data and how it might be structured.

### Density plots

Another useful plotting method uses **density plots** to visualize numerical data. A histogram bins data but is highly dependent on the number and boundary of the bins. A density plot also estimates the distribution of a numerical variable but does this by estimating the density of data points in a small window around each data point. The overall curve is the sum of this small density estimate. A density plot can be thought of as a smooth version of the histogram. Several options go into a density estimate, such as the width of the window and type of smoothing function. These ideas are beyond the scope here and we will just use the default options. @fig-dens5 shows the same distribution of the number of characters but in a smoother way than a histogram.

```{r}
#| label: fig-dens5
#| echo: false
#| fig-cap: "A histogram of `num_char`. This distribution is very strongly skewed to the right."
email50 %>%
   gf_dens(~num_char,
                xlab = "The Number of Characters (in thousands)") %>%
   gf_theme(theme_classic())
```

> **Exercise**:\
> Compare and contrast the histogram in @fig-hist5 and the density plot in @fig-dens5. What can you see in the histogram you can't see in the density plot?

### Variance and standard deviation

The mean is used to describe the center of a data set, but the *variability* in the data is also important. Here, we introduce two measures of variability: the **variance** and the **standard deviation**. Both of these are very useful in data analysis, even though the formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to conceptually understand; it roughly describes how far away the typical observation is from the mean. Equation 2 is the equation for sample variance. We will demonstrate it with data so that the notation is easier to understand.

```{=tex}
\begin{align}
s_{}^2 &= \sum_{i = 1}^{n} \frac{(x_i - \bar{x})^2}{n - 1} \\
    &= \frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + (x_3 - \bar{x})^2 + \cdots + (x_n - \bar{x})^2}{n - 1} 
  \tag{2}
\end{align}
```
where $x_1, x_2, \dots, x_n$ represent the $n$ observed values.

We call the distance of an observation from its mean the **deviation**. Below are the deviations for the $1^{st}$, $2^{nd}$, $3^{rd}$, and $50^{th}$ observations of the `num_char` variable. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal.

$$
\begin{aligned}
x_1^{}-\bar{x} &= 21.7 - 11.6 = 10.1 \hspace{5mm}\text{ } \\
x_2^{}-\bar{x} &= 7.0 - 11.6 = -4.6 \\
x_3^{}-\bar{x} &= 0.6 - 11.6 = -11.0 \\
            &\ \vdots \\
x_{50}^{}-\bar{x} &= 15.8 - 11.6 = 4.2
\end{aligned}
$$

```{r echo=FALSE,results='hide',warning=FALSE,message=FALSE}
d <- email50$num_char
round(mean(d), 1)
d[c(1, 2, 3, 50)]
d[c(1, 2, 3, 50)] - round(mean(d), 1) 
(d[c(1, 2, 3, 50)] - round(mean(d)))^2
sum((d - round(mean(d)))^2) / 49 
sqrt(sum((d - round(mean(d)))^2) / 49) 
var(d)
sd(d)
```

If we square these deviations and then take an average, the result is equal to the **sample variance**, denoted by $s_{}^2$:

$$
\begin{aligned}
s_{}^2 &= \frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \cdots + 4.2_{}^2}{50-1} \\
    &= \frac{102.01 + 21.16 + 121.00 + \cdots + 17.64}{49} \\
    &= 172.44
\end{aligned}
$$

We divide by $n - 1$, rather than dividing by $n$, when computing the variance; you need not worry about this mathematical nuance yet. Notice that squaring the deviations does two things. First, it makes large values much larger, seen by comparing $10.1^2$, $(-4.6)^2$, $(-11.0)^2$, and $4.2^2$. Second, it gets rid of any negative signs.

The sample **standard deviation**, $s$, is the square root of the variance:

$$s = \sqrt{172.44} = 13.13$$

The sample standard deviation of the number of characters in an email is 13.13 thousand. A subscript of $_x$ may be added to the variance and standard deviation, i.e. $s_x^2$ and $s_x^{}$, as a reminder that these are the variance and standard deviation of the observations represented by $x_1^{}$, $x_2^{}$, ..., $x_n^{}$. The $_{x}$ subscript is usually omitted when it is clear which data the variance or standard deviation is referencing.

> **Variance and standard deviation**\
> The variance is roughly the average squared distance from the mean. The standard deviation is the square root of the variance and describes how close the data are to the mean.

Formulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.[^numerical-data-12] However, like the mean, the population values have special symbols: $\sigma_{}^2$ for the variance and $\sigma$ for the standard deviation. The symbol $\sigma$ is the Greek letter *sigma*.

[^numerical-data-12]: The only difference is that the population variance has a division by $n$ instead of $n - 1$.

> **Tip: standard deviation describes variability**\
> Focus on the conceptual meaning of the standard deviation as a descriptor of variability rather than the formulas. Usually 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, as we have seen, these percentages are not strict rules.

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide
x1 <- rep(0:1, c(10, 10))
x1 <- (x1 - mean(x1)) / sd(x1)
x2 <- qnorm(seq(0.0025, 0.9975, 0.00049))
x2 <- (x2 - mean(x2)) / sd(x2)
x3 <- qchisq(seq(0.01,0.98, 0.0005), 4)
x3 <- (x3 - mean(x3)) / sd(x3)

drawSDs <- function(m = 0, s = 1){
	abline(v = m, col = '#00000033')
	rect(m - s, -5, m + s, 500, col = '#00000025', border = '#00000000')
	rect(m + s, -5, m + 2*s, 500, col = '#00000015', border = '#00000000')
	rect(m - s, -5, m - 2*s, 500, col = '#00000015', border = '#00000000')
	rect(m + 2*s, -5, m + 3*s, 500, col = '#0000000B', border = '#00000000')
	rect(m - 2*s, -5, m - 3*s, 500, col = '#0000000B', border = '#00000000')
	rect(m + 4*s, -5, m + 3*s, 500, col = '#00000008', border = '#00000000')
	rect(m - 4*s, -5, m - 3*s, 500, col = '#00000008', border = '#00000000')
}

xR <- c(-1, 1)*max(c(x1, x2, x3))
```

```{r}
#| label: fig-hist53
#| echo: false
#| fig-cap: "The first of three very different population distributions with the same mean, 0, and standard deviation, 1."
gf_histogram(~x1, breaks = c(-1.05, -0.95, 0.95, 1.05), 
             add = TRUE, probability = TRUE, fill = COL[1], 
             ylim = c(0, 0.75), xlab = "") 
```

```{r}
#| label: fig-hist54
#| echo: false
#| fig-cap: "The second plot with mean 0 and standard deviation 1."
gf_histogram(~x2, bins = 25, add = TRUE, probability = TRUE, fill = COL[1], 
             ylim = c(0, 0.75), xlab = "") 
```

```{r}
#| label: fig-hist55
#| echo: false
#| fig-cap: "The final plot with mean 0 and standard deviation 1."
gf_histogram(~x3, bins = 25, add = TRUE, probability = TRUE, fill = COL[1], 
             ylim = c(0, 0.75), xlab = "") 
```

> **Exercise**:\
> Earlier, the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using the three figures, [Figures @fig-hist53; -@fig-hist54; -@fig-hist55] as examples, explain why such a description is important.[^numerical-data-13]

[^numerical-data-13]: Starting with Figure \@ref(fig:hist53-fig), the three figures show three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution.

> *Example*:\
> Describe the distribution of the `num_char` variable using the histogram in @fig-hist5. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context: the number of characters in emails. Also note any especially unusual cases/observations.[^numerical-data-14]

[^numerical-data-14]: The distribution of email character counts is unimodal and very strongly skewed to the high end (right skewed). Many of the counts fall near the mean at 11,600, and most fall within one standard deviation (13,130) of the mean. There is one exceptionally long email with about 65,000 characters.

In practice, the variance and standard deviation are sometimes used as a means to an end, where the *end* is being able to accurately estimate the uncertainty associated with a sample statistic. For example, later in the book we will use the variance and standard deviation to assess how close the sample mean is to the population mean.

### Box plots, quartiles, and the median

A **box plot** summarizes a data set using five statistics, while also plotting unusual observations. @fig-box provides an annotated vertical dot plot alongside a box plot of the `num_char` variable from the `email50` data set.

```{r}
#| label: fig-box
#| echo: false
#| fig-cap: "A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6,890), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively."
data(email50)
data(COL)
d <- email50$num_char

boxPlot(d, ylab = 'Number of Characters (in thousands)', 
        xlim = c(0.3, 3), axes = FALSE, ylim = range(d) + sd(d)*c(-1, 1)*0.2,
        cex.lab = 0.5)
axis(2)

arrows(2, 0, 1.35, min(d) - 0.5, length = 0.08)
text(2, 0,'lower whisker', pos = 4, cex = 0.5)

arrows(2, quantile(d, 0.25) + sd(d)/7, 1.35, quantile(d, 0.25), length = 0.08)
text(2, quantile(d, 0.25) + sd(d) / 6.5, expression(Q[1]~~'(first quartile)'), 
     pos = 4, cex = 0.5)

m <- median(d)
arrows(2, m + sd(d) / 5, 1.35, m, length = 0.08)
text(2, m + sd(d) / 4.7, 'median', pos = 4, cex = 0.5)

q <- quantile(d, 0.75)
arrows(2, q + sd(d) / 4, 1.35, q, length = 0.08)
text(2, q + sd(d) / 3.8, expression(Q[3]~~'(third quartile)'), 
     pos = 4, cex = 0.5)

arrows(2, rev(sort(d))[4] - sd(d) / 7, 1.35, rev(sort(d))[4], length = 0.08)
text(2, rev(sort(d))[4] - sd(d) / 6.8, 'upper whisker', pos = 4, cex = 0.5)

y <- quantile(d, 0.75) + 1.5*IQR(d)
arrows(2, y - 0.1*sd(d), 1.35, y, length = 0.08)
lines(c(0.72, 1.28), rep(y, 2), lty = 3, col = '#00000066')
text(2, y - 0.1*sd(d), 'max whisker reach', pos = 4, cex = 0.5)

m <- rev(tail(sort(d), 5))
s <- m[1] - 0.3*sd(m)
arrows(2, s, 1.1, m[1] - 0.2, length = 0.08)
arrows(2, s, 1.1, m[2] + 0.3, length = 0.08)
arrows(2, s, 1.1, m[3] + 0.35, length = 0.08)
text(2, s, 'suspected outliers', pos = 4, cex = 0.5)

points(rep(0.4, 25), rev(sort(d))[1:25], cex = rep(1.3, 25), 
       col = rep(COL[1, 3], 25), pch = rep(1, 25))
points(rep(0.4, 25), sort(d)[1:25], cex = rep(1, 25), 
       col = rep(COL[4, 3], 25), pch = rep("-", 25))
```

The first step in building a box plot is drawing a dark line denoting the **median**, which splits the data in half. @fig-box shows 50% of the data falling below the median (red dashes) and the other 50% falling above the median (blue open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the $50^{th}$ percentile: $(\text{6,768} + \text{7,012}) / 2 = \text{6,890}$. When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed).

> **Median: the number in the middle**\
> If the data are ordered from smallest to largest, the **median** is the observation in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average.

The second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The total length of the box, shown vertically in @fig-box, is called the **interquartile range** (IQR, for short). It, like the standard deviation, is a measure of variability in the data. The more variable the data, the larger the standard deviation and IQR. The two boundaries of the box are called the **first quartile** (the $25^{th}$ percentile, i.e. 25% of the data fall below this value) and the **third quartile** (the $75^{th}$ percentile), and these are often labeled $Q_1$ and $Q_3$, respectively.

> **Interquartile range (IQR)**\
> The IQR is the length of the box in a box plot. It is computed as $$ IQR = Q_3 - Q_1 $$ where $Q_1$ and $Q_3$ are the $25^{th}$ and $75^{th}$ percentiles, respectively.

> **Exercise**:\
> What percent of the data fall between $Q_1$ and the median? What percent is between the median and $Q_3$?[^numerical-data-15]

[^numerical-data-15]: Since $Q_1$ and $Q_3$ capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between $Q_1$ and the median, and another 25% fall between the median and $Q_3$.

Extending out from the box, the **whiskers** attempt to capture the data outside of the box, however, their reach is never allowed to be more than $1.5\times IQR$.[^numerical-data-16] They capture everything within this reach. In @fig-box, the upper whisker does not extend to the last three points, which are beyond $Q_3 + 1.5\times IQR$, and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whisker's limit is not shown in the figure because the plot does not extend down to $Q_1 - 1.5\times IQR$. In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data.

[^numerical-data-16]: While the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.

Any observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points -- instead of just extending the whiskers to the minimum and maximum observed values -- is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called **outliers**. In this case, it would be reasonable to classify the emails with character counts of 41,623, 42,793, and 64,401 as outliers since they are numerically distant from most of the data.

> **Outliers are extreme**\
> An **outlier** is an observation that is extreme, relative to the rest of the data.

> **Why it is important to look for outliers**\
> Examination of data for possible outliers serves many useful purposes, including:\
> 1. Identifying **strong skew** in the distribution.\
> 2. Identifying data collection or entry errors. For instance, we re-examined the email purported to have 64,401 characters to ensure this value was accurate.\
> 3. Providing insight into interesting properties of the data.

> **Exercise**:\
> The observation with value 64,401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?[^numerical-data-17]

[^numerical-data-17]: That occasionally there may be very long emails.

> **Exercise**:\
> Using @fig-box, estimate the following values for `num_char` in the `email50` data set:\
> (a) $Q_1$,\
> (b) $Q_3$, and\
> (c) IQR.[^numerical-data-18]

[^numerical-data-18]: These visual estimates will vary a little from one person to the next: $Q_1$ \~ 3,000, $Q_3$ \~ 15,000, IQR = $Q_3 - Q_1$ \~ 12,000. (The true values: \$Q_1 = \$ 2,536, \$Q_3 = \$ 15,411, IQR = 12,875.)

Of course, `R` can calculate these summary statistics for us. First, we will do these calculations individually and then in one function call. Remember to ask yourself what you want `R` to do and what it needs to do this.

```{r}
mean(~num_char, data = email50)
sd(~num_char, data = email50)
quantile(~num_char, data = email50)
iqr(~num_char, data = email50)
```

```{r}
favstats(~num_char, data = email50)
```

### Robust statistics

How are the *sample statistics* of the `num_char` data set affected by the observation with value 64,401? What would we see if this email wasn't present in the data set? What would happen to these *summary statistics* if the observation at 64,401 had been even larger, say 150,000? These scenarios are plotted alongside the original data in @fig-box2, and sample statistics are computed in `R`.

First, we create a new data frame containing the three scenarios: 1) the original data, 2) the data with the extreme observation dropped, and 3) the data with the extreme observation increased.

```{r}
# code to create the `robust` data frame
p1 <- email50$num_char
p2 <- p1[-which.max(p1)]
p3 <- p1
p3[which.max(p1)] <- 150

robust <- data.frame(value = c(p1, p2, p3),
                     group = c(rep("Original", 50),
                             rep("Dropped", 49), rep("Increased", 50)))
head(robust)
```

Now, we create a side-by-side boxplots for each scenario.

```{r}
#| label: fig-box2
#| fig-cap: "Box plots of the original character count data and two modified data sets, one where the outlier at 64,401 is dropped and one where its value is increased."
gf_boxplot(value ~ group, data = robust, xlab = "Data Group",
           ylab = "Number of Characters (in thousands)") %>%
   gf_theme(theme_classic())
```

We can also use `favstats()` to calculate summary statistics of `value` by `group`, using the `robust` data frame created above.

```{r}
favstats(value ~ group, data = robust)
```

Notice by using the formula notation, we were able to calculate the summary statistics within each group.

> **Exercise**:\
> (a) Which is affected more by extreme observations, the mean or median? The data summary may be helpful.[^numerical-data-19]\
> (b) Which is affected more by extreme observations, the standard deviation or IQR?[^numerical-data-20]

[^numerical-data-19]: The mean is affected more.

[^numerical-data-20]: The standard deviation is affected more.

The median and IQR are called **robust statistics** because extreme observations have little effect on their values. The mean and standard deviation are affected much more by changes in extreme observations.

> *Example*:\
> The median and IQR do not change much under the three scenarios above. Why might this be the case?[^numerical-data-21]

[^numerical-data-21]: The median and IQR are only sensitive to numbers near $Q_1$, the median, and $Q_3$. Since values in these regions are relatively stable -- there aren't large jumps between observations -- the median and IQR estimates are also quite stable.

> **Exercise**:\
> The distribution of vehicle prices tends to be right skewed, with a few luxury and sports cars lingering out into the right tail. If you were searching for a new car and cared about price, should you be more interested in the mean or median price of vehicles sold, assuming you are in the market for a regular car?[^numerical-data-22]

[^numerical-data-22]: Buyers of a *regular car* should be more concerned about the median price. High-end car sales can drastically inflate the mean price while the median will be more robust to the influence of those sales.

### Transforming data

When data are very strongly skewed, we sometimes transform them so they are easier to model. Consider the histogram of Major League Baseball players' salaries from 2010, which is shown in @fig-hist510.

```{r}
#| label: fig-hist510
#| echo: false
#| fig-cap: "Histogram of MLB player salaries for 2010, in millions of dollars."
gf_histogram(~salary / 1000, data = mlb, main = "", bins = 15, 
             xlab = "Salary (millions of dollars)", ylab = "", 
             color = "black", fill = "cyan") %>%
   gf_theme(theme_classic())
```

> *Example*:\
> The histogram of MLB player salaries is somewhat useful because we can see that the data are extremely skewed and centered (as gauged by the median) at about \$1 million. What about this plot is not useful?[^numerical-data-23]

[^numerical-data-23]: Most of the data are collected into one bin in the histogram and the data are so strongly skewed that many details in the data are obscured.

There are some standard transformations that are often applied when much of the data cluster near zero (relative to the larger values in the data set) and all observations are positive. A **transformation** is a rescaling of the data using a function. For instance, a plot of the natural logarithm[^numerical-data-24] of player salaries results in a new histogram in @fig-hist512. Transformed data are sometimes easier to work with when applying statistical models because the transformed data are much less skewed and outliers are usually less extreme.

[^numerical-data-24]: Statisticians often write the natural logarithm as $\log$. You might be more familiar with it being written as $\ln$.

```{r}
#| label: fig-hist512
#| echo: false
#| fig-cap: "Histogram of the log-transformed MLB player salaries for 2010."
gf_histogram(~log(salary / 1000), data = mlb, bins = 15, 
             xlab = expression(log[e]*"(Salary), where Salary is in millions USD"), 
             ylab = "", color = "black", fill = "cyan") %>%
   gf_theme(theme_classic())
```

Transformations can also be applied to one or both variables in a scatterplot. A scatterplot of the original `line_breaks` and `num_char` variables is shown in @fig-scat52 above. We can see a positive association between the variables and that many observations are clustered near zero. Later in this text, we might want to use a straight line to model the data. However, we'll find that the data in their current state cannot be modeled very well. @fig-scat513 shows a scatterplot where both `line_breaks` and `num_char` have been transformed using a natural log (log base $e$) transformation. While there is a positive association in each plot, the transformed data show a steadier trend, which is easier to model than the original (un-transformed) data.

```{r}
#| label: fig-scat513
#| echo: false
#| fig-cap: "A scatterplot of `line_breaks` versus `num_char` for the `email50` data, where both variables have been log-transformed."
data(COL)
plot(log(email50$num_char), log(email50$line_breaks), 
     pch = 19, cex = 1.3, col = COL[1, 4], xlab = "", 
     ylab = "Number of Lines", axes = FALSE)
axis(2)
axis(1)
box()
points(log(email50$num_char), log(email50$line_breaks), 
       cex = 1.3, col = COL[1, 1])
mtext("Number of Characters (in thousands)", 1, 1.9)
```

Transformations other than the logarithm can be useful, too. For instance, the square root ($\sqrt{\text{original observation}}$) and inverse $\left(\frac{1}{\text{original observation}}\right)$ are used commonly by statisticians. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot.
